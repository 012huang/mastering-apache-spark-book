== SQL Parser Framework

*SQL Parser Framework* in Spark SQL uses ANTLR to parse SQL text to a `DataType`, `Expression`, `TableIdentifier`, or link:spark-sql-logical-plan.adoc[LogicalPlan].

CAUTION: FIXME Make the intro more introductory without much technical details.

=== [[ParserInterface]] ParserInterface -- SQL Parser Contract

`ParserInterface` is the parser contract for extracting link:spark-sql-logical-plan.adoc[LogicalPlan], Catalyst `Expressions`, and `TableIdentifiers` from a given SQL string.

[source, scala]
----
package org.apache.spark.sql.catalyst.parser

trait ParserInterface {
  def parsePlan(sqlText: String): LogicalPlan

  def parseExpression(sqlText: String): Expression

  def parseTableIdentifier(sqlText: String): TableIdentifier
}
----

It has the only single abstract subclass <<AbstractSqlParser, AbstractSqlParser>>.

=== [[AbstractSqlParser]] AbstractSqlParser

`AbstractSqlParser` abstract class is a <<ParserInterface, ParserInterface>> that provides the foundation for the SQL parsing infrastructure in Spark SQL with two concrete implementations available at the moment:

1. <<SparkSqlParser, SparkSqlParser>>
2. <<CatalystSqlParser, CatalystSqlParser>>

`AbstractSqlParser` creates an layer of indirection and expects that subclasses provide custom <<AstBuilder, AstBuilder>> that in turn converts a ANTLR `ParseTree` into a `DataType`, `Expression`, `TableIdentifier`, or link:spark-sql-logical-plan.adoc[LogicalPlan].

[source, scala]
----
protected def astBuilder: AstBuilder
----

`AbstractSqlParser` simply routes all the final parsing calls to translate sql string into a respective Spark SQL object to that `AstBuilder`.

=== [[AstBuilder]] AstBuilder

`AstBuilder` is a ANTLR custom `SqlBaseBaseVisitor` to convert a ANTLR `ParseTree` (that represents a SQL string) into Spark SQL's corresponding entity using the following methods:

1. `visitSingleDataType` to produce a `DataType`
2. `visitSingleExpression` to produce a `Expression`
3. `visitSingleTableIdentifier` to produce a `TableIdentifier`
4. `visitSingleStatement` for a link:spark-sql-logical-plan.adoc[LogicalPlan]

When parsing a SQL string, it first uses its own <<AstBuilder-parse, parse>> protected method that sets up a proper ANTLR parsing infrastructure.

`AstBuilder` belongs to `org.apache.spark.sql.catalyst.parser` package.

==== [[AstBuilder-parse]] AstBuilder.parse method

[source, scala]
----
parse[T](command: String)(toResult: SqlBaseParser => T): T
----

`parse` is a protected method that sets up a proper ANTLR parsing infrastructure and uses the ANTLR-specific classes for Spark SQL that are auto-generated at build time.

When called, you should see the following INFO message in the logs:

```
INFO SparkSqlParser: Parsing command: [command]
```

TIP: Enable `INFO` logging level for <<SparkSqlParser, SparkSqlParser>> or <<CatalystSqlParser, CatalystSqlParser>> to see the INFO message.

=== [[SparkSqlParser]] SparkSqlParser

`SparkSqlParser` is the default parser of the SQL statements supported in Spark SQL. It is available as a <<ParserInterface, ParserInterface>> object in link:spark-sql-sessionstate.adoc[SessionState] (as `sqlParser`).

`SparkSqlParser` is <<AbstractSqlParser, AbstractSqlParser>> with the `astBuilder` being `SparkSqlAstBuilder`. It also supports <<SparkSqlParser-VariableSubstitution, variable substitution>>.

`SparkSqlParser` is used for the link:spark-sql-functions.adoc#expr[expr function].

[source, scala]
----
scala> expr("token = 'hello'")
16/07/07 18:32:53 INFO SparkSqlParser: Parsing command: token = 'hello'
res0: org.apache.spark.sql.Column = (token = hello)
----

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.sql.execution.SparkSqlParser` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.SparkSqlParser=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

CAUTION: FIXME Review `parse` method.

==== [[SparkSqlParser-VariableSubstitution]] Variable Substitution

CAUTION: FIXME See `SparkSqlParser` and `substitutor`.

=== [[CatalystSqlParser]] CatalystSqlParser

`CatalystSqlParser` is an <<AbstractSqlParser, AbstractSqlParser>> object with the `astBuilder` being <<AstBuilder, AstBuilder>>.

`CatalystSqlParser` is used to parse data types (using their canonical string representation), e.g. when link:spark-sql-schema-structtype.adoc#add[adding fields to a schema] or link:spark-sql-columns.adoc#cast[casting column to different data types].

[source, scala]
----
import org.apache.spark.sql.types.StructType
scala> val struct = new StructType().add("a", "int")
struct: org.apache.spark.sql.types.StructType = StructType(StructField(a,IntegerType,true))

scala> val asInt = expr("token = 'hello'").cast("int")
asInt: org.apache.spark.sql.Column = CAST((token = hello) AS INT)
----

When parsing, you should see INFO messages in the logs:

```
INFO CatalystSqlParser: Parsing command: int
```

It is also used in `HiveClientImpl` (when converting columns from Hive to Spark) and in `OrcFileOperator` (when inferring the schema for ORC files).

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.sql.catalyst.parser.CatalystSqlParser` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.catalyst.parser.CatalystSqlParser=INFO
```

Refer to link:spark-logging.adoc[Logging].
====
