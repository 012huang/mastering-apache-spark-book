== DataFrameReader

`DataFrameReader` is an interface to return `DataFrame` from <<format, many storage formats>> in external storage systems (e.g. <<creating-dataframes-from-tables, databases>> or <<creating-dataframes-from-files, files>>) and <<stream, streams>>.

Use link:spark-sql-sqlcontext.adoc#read[SQLContext.read] to access it.

[source, scala]
----
import org.apache.spark.sql.DataFrameReader
val reader: DataFrameReader = spark.read
----

It has a direct support for many <<creating-dataframes-from-files, file formats>> and <<format, interface for new ones>>. It assumes <<parquet, parquet>> as the default data source format that you can change using link:spark-sql-settings.adoc[spark.sql.sources.default] setting.

=== [[format]] Specifying Data Format (format method)

[source, scala]
----
format(source: String): DataFrameReader
----

You use `format` to configure `DataFrameReader` to use appropriate `source` format.

Supported data formats:

* `json`
* `csv` (since **2.0.0**)
* `parquet` (see link:spark-parquet.adoc[Parquet])
* `orc`
* `text`
* `jdbc`
* `libsvm` (using `spark.read.format("libsvm")`)

NOTE: You can improve your understanding of `format("jdbc")` with the exercise link:exercises/spark-exercise-dataframe-jdbc-postgresql.adoc[Creating DataFrames from Tables using JDBC and PostgreSQL].

=== [[schema]] Schema Support (schema method)

[source, scala]
----
schema(schema: StructType): DataFrameReader
----

You can apply `schema` to the data source.

TIP: Refer to link:spark-sql-dataframe-schema.adoc[Schema].

=== [[option]] Option Support (option and options methods)

[source, scala]
----
option(key: String, value: String): DataFrameReader
option(key: String, value: Boolean): DataFrameReader  // <1>
option(key: String, value: Long): DataFrameReader     // <1>
option(key: String, value: Double): DataFrameReader   // <1>
----
<1> Available since Spark **2.0.0**

You can also use `options` method to describe different options in a single `Map`.

[source, scala]
----
options(options: scala.collection.Map[String, String]): DataFrameReader
----

=== [[load]] load methods

[source, scala]
----
load(): DataFrame
load(path: String): DataFrame
----

`load` loads input data as a `DataFrame`.

[source, scala]
----
val csv = spark.read
  .format("csv")
  .load("text-id.csv")
----

=== [[stream]] stream methods

[source, scala]
----
stream(): DataFrame
stream(path: String): DataFrame
----

CAUTION: FIXME Review 915a75398ecbccdbf9a1e07333104c857ae1ce5e

`stream` loads input data stream in as a `DataFrame`.

[source, scala]
----
Seq("hello", "world").zipWithIndex.toDF("text", "id").write.format("csv").save("text-id.csv")

val csvStream = spark.read.format("csv").stream("text-id.csv")
----

=== [[creating-dataframes-from-files]] Creating DataFrames from Files

`DataFrameReader` comes with a direct support for multiple file formats:

* <<json, JSON>>
* <<csv, CSV>>
* <<parquet, parquet>>
* <<orc, ORC>>
* <<text, text>>

==== [[json]] json method

[source, scala]
----
json(path: String): DataFrame
json(paths: String*): DataFrame
json(jsonRDD: RDD[String]): DataFrame
----

New in **2.0.0**: `prefersDecimal`

==== [[csv]] csv method

[source, scala]
----
csv(paths: String*): DataFrame
----

==== [[parquet]] parquet method

[source, scala]
----
parquet(paths: String*): DataFrame
----

The supported options:

* <<compression, compression>> (default: `snappy`)

New in *2.0.0*: `snappy` is the default Parquet codec. See https://github.com/apache/spark/commit/2f0b882e5c8787b09bedcc8208e6dcc5662dbbab[[SPARK-14482\][SQL\] Change default Parquet codec from gzip to snappy].

[[compression]] The compressions supported:

* `none` or `uncompressed`
* `snappy` - the default codec in Spark *2.0.0*.
* `gzip` - the default codec in Spark before *2.0.0*
* `lzo`

[source, scala]
----
val tokens = Seq("hello", "henry", "and", "harry")
  .zipWithIndex
  .map(_.swap)
  .toDF("id", "token")

val parquetWriter = tokens.write
parquetWriter.option("compression", "none").save("hello-none")

// The exception is mostly for my learning purposes
// so I know where and how to find the trace to the compressions
// Sorry...
scala> parquetWriter.option("compression", "unsupported").save("hello-unsupported")
java.lang.IllegalArgumentException: Codec [unsupported] is not available. Available codecs are uncompressed, gzip, lzo, snappy, none.
  at org.apache.spark.sql.execution.datasources.parquet.ParquetOptions.<init>(ParquetOptions.scala:43)
  at org.apache.spark.sql.execution.datasources.parquet.DefaultSource.prepareWrite(ParquetRelation.scala:77)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$4.apply(InsertIntoHadoopFsRelation.scala:122)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$4.apply(InsertIntoHadoopFsRelation.scala:122)
  at org.apache.spark.sql.execution.datasources.BaseWriterContainer.driverSideSetup(WriterContainer.scala:103)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:141)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.command.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:61)
  at org.apache.spark.sql.execution.command.ExecutedCommand.sideEffectResult(commands.scala:59)
  at org.apache.spark.sql.execution.command.ExecutedCommand.doExecute(commands.scala:73)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:137)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:134)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:117)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:65)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:65)
  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:390)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:230)
  ... 48 elided
----

==== [[orc]] orc method

[source, scala]
----
orc(path: String): DataFrame
----

*Optimized Row Columnar (ORC)* file format is a highly efficient columnar format to store Hive data with more than 1,000 columns and improve performance. ORC format was introduced in Hive version 0.11 to use and retain the type information from the table definition.

TIP: Read https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC[ORC Files] document to learn about the ORC file format.

==== [[text]] text method

`text` method loads a text file.

[source, scala]
----
text(paths: String*): Dataset[String]
----

===== [[text-example]] Example

[source, scala]
----
val lines: Dataset[String] = spark.read.text("README.md")

scala> lines.show
+--------------------+
|               value|
+--------------------+
|      # Apache Spark|
|                    |
|Spark is a fast a...|
|high-level APIs i...|
|supports general ...|
|rich set of highe...|
|MLlib for machine...|
|and Spark Streami...|
|                    |
|<http://spark.apa...|
|                    |
|                    |
|## Online Documen...|
|                    |
|You can find the ...|
|guide, on the [pr...|
|and [project wiki...|
|This README file ...|
|                    |
|   ## Building Spark|
+--------------------+
only showing top 20 rows
----

=== [[creating-dataframes-from-tables]] Creating DataFrames from Tables

==== [[table]] table method

[source, scala]
----
table(tableName: String): DataFrame
----

`table` method returns the `tableName` table as a `DataFrame`.

[source, scala]
----
scala> spark.sql("SHOW TABLES").show(false)
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
|dafa     |false      |
+---------+-----------+

scala> spark.read.table("dafa").show(false)
+---+-------+
|id |text   |
+---+-------+
|1  |swiecie|
|0  |hello  |
+---+-------+
----

CAUTION: FIXME The method uses `spark.sessionState.sqlParser.parseTableIdentifier(tableName)` and `spark.sessionState.catalog.lookupRelation`. Would be nice to learn a bit more on their internals, huh?

==== [[jdbc]] jdbc method

NOTE: `jdbc` method uses `java.util.Properties` (and appears so Java-centric). Use <<format, format("jdbc")>> instead.

[source, scala]
----
jdbc(url: String, table: String, properties: Properties): DataFrame
jdbc(url: String, table: String,
  parts: Array[Partition],
  connectionProperties: Properties): DataFrame
jdbc(url: String, table: String,
  predicates: Array[String],
  connectionProperties: Properties): DataFrame
jdbc(url: String, table: String,
  columnName: String,
  lowerBound: Long,
  upperBound: Long,
  numPartitions: Int,
  connectionProperties: Properties): DataFrame
----

`jdbc` allows you to create `DataFrame` that represents `table` in the database available as `url`.
