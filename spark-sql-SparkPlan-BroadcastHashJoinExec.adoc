== [[BroadcastHashJoinExec]] BroadcastHashJoinExec Binary Physical Operator

`BroadcastHashJoinExec` is a link:spark-sql-SparkPlan.adoc#BinaryExecNode[physical operator] (with two child <<left, left>> and <<right, right>> physical operators) that supports link:spark-sql-whole-stage-codegen.adoc#CodegenSupport[code generation] (aka _codegen_).

`BroadcastHashJoinExec` is <<creating-instance, created>> after applying link:spark-sql-SparkStrategy-JoinSelection.adoc[JoinSelection] execution planning strategy to link:spark-sql-SparkStrategy-JoinSelection.adoc#ExtractEquiJoinKeys[ExtractEquiJoinKeys]-destructurable logical query plans (i.e. link:spark-sql-SparkStrategy-JoinSelection.adoc#canBuildRight[INNER, CROSS, LEFT OUTER, LEFT SEMI, LEFT ANTI]) of which the `right` physical operator link:spark-sql-SparkStrategy-JoinSelection.adoc#canBroadcast[can be broadcast].

`BroadcastHashJoinExec` <<requiredChildDistribution, requires that partition requirements>> for the two children physical operators match `BroadcastDistribution` (with `HashedRelationBroadcastMode`) and `UnspecifiedDistribution` (for <<left, left>> and <<right, right>> sides of a join or vice versa).

[source, scala]
----
val tokens = Seq(
  (0,"playing"),
  (1, "with"),
  (2, "BroadcastHashJoinExec")
).toDF("id", "token")

scala> spark.conf.get("spark.sql.autoBroadcastJoinThreshold")
res0: String = 10485760

val q = tokens.join(tokens, Seq("id"), "inner")
scala> q.explain(extended = true)
== Parsed Logical Plan ==
'Join UsingJoin(Inner,List(id))
:- Project [_1#12 AS id#15, _2#13 AS token#16]
:  +- LocalRelation [_1#12, _2#13]
+- Project [_1#12 AS id#20, _2#13 AS token#21]
   +- LocalRelation [_1#12, _2#13]

== Analyzed Logical Plan ==
id: int, token: string, token: string
Project [id#15, token#16, token#21]
+- Join Inner, (id#15 = id#20)
   :- Project [_1#12 AS id#15, _2#13 AS token#16]
   :  +- LocalRelation [_1#12, _2#13]
   +- Project [_1#12 AS id#20, _2#13 AS token#21]
      +- LocalRelation [_1#12, _2#13]

== Optimized Logical Plan ==
Project [id#15, token#16, token#21]
+- Join Inner, (id#15 = id#20)
   :- LocalRelation [id#15, token#16]
   +- LocalRelation [id#20, token#21]

== Physical Plan ==
*Project [id#15, token#16, token#21]
+- *BroadcastHashJoin [id#15], [id#20], Inner, BuildRight
   :- LocalTableScan [id#15, token#16]
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
      +- LocalTableScan [id#20, token#21]
----

[[metrics]]
.BroadcastHashJoinExec SQLMetrics (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[numOutputRows]] `numOutputRows`
| Number of output rows
|===

.BroadcastHashJoinExec in Details for Query in web UI
image::images/spark-sql-BroadcastHashJoinExec-webui-query-details.png[align="center"]

NOTE: The prefix for variable names for `BroadcastHashJoinExec` operators in link:spark-sql-whole-stage-codegen.adoc#CodegenSupport[CodegenSupport]-generated code is *bhj*.

TIP: Use link:spark-sql-debugging-execution.adoc#debugCodegen[debugCodegen] or link:spark-sql-QueryExecution.adoc#debug[QueryExecution.debug.codegen] methods to review the ``CodegenSupport``-generated code.

[source, scala]
----
scala> q.queryExecution.debug.codegen
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 ==
*Project [id#15, token#16, token#21]
+- *BroadcastHashJoin [id#15], [id#20], Inner, BuildRight
   :- LocalTableScan [id#15, token#16]
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
      +- LocalTableScan [id#20, token#21]

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private scala.collection.Iterator[] inputs;
/* 008 */   private scala.collection.Iterator inputadapter_input;
/* 009 */   private org.apache.spark.broadcast.TorrentBroadcast bhj_broadcast;
/* 010 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation;
/* 011 */   private org.apache.spark.sql.execution.metric.SQLMetric bhj_numOutputRows;
/* 012 */   private UnsafeRow bhj_result;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder bhj_holder;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter bhj_rowWriter;
/* 015 */   private UnsafeRow project_result;
/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder project_holder;
/* 017 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter project_rowWriter;
/* 018 */
...
----

=== [[creating-instance]] Creating BroadcastHashJoinExec Instance

`BroadcastHashJoinExec` takes the following when created:

* [[leftKeys]] Left key link:spark-sql-catalyst-Expression.adoc[expressions]
* [[rightKeys]] Right key link:spark-sql-catalyst-Expression.adoc[expressions]
* [[joinType]] link:spark-sql-joins.adoc#join-types[Join type]
* [[buildSide]] `BuildSide`
* [[condition]] Optional join condition link:spark-sql-catalyst-Expression.adoc[expressions]
* [[left]] Left link:spark-sql-SparkPlan.adoc[physical operator]
* [[right]] Right link:spark-sql-SparkPlan.adoc[physical operator]

=== [[requiredChildDistribution]] `requiredChildDistribution` Method

[source, scala]
----
requiredChildDistribution: Seq[Distribution]
----

`requiredChildDistribution` gives a two-element collection for two-child partition requirements with the following:

* `BroadcastDistribution` and `UnspecifiedDistribution` for <<buildSide, BuildLeft build side>>

* `UnspecifiedDistribution` and `BroadcastDistribution` for <<buildSide, BuildRight build side>>

NOTE: `BroadcastDistribution` uses `HashedRelationBroadcastMode` broadcast mode.

NOTE: `requiredChildDistribution` is a part of link:spark-sql-SparkPlan.adoc#requiredChildDistribution[SparkPlan Contract] to specify partition requirements on the input data.
