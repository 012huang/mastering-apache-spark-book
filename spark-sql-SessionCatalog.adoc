== [[SessionCatalog]] SessionCatalog

`SessionCatalog` is a proxy between link:spark-sql-sparksession.adoc[SparkSession] and the underlying metastore, e.g. `HiveSessionCatalog`.

`SessionCatalog` <<creating-instance, is created>> when `SessionState` link:spark-sql-SessionState.adoc#catalog[sets `catalog`] (lazily).

You can access the `SessionCatalog` for a link:spark-sql-sparksession.adoc[SparkSession] through link:spark-sql-SessionState.adoc[SessionState].

[source, scala]
----
sparkSession.sessionState.catalog
----

[[internal-registries]]
.SessionCatalog's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[tempTables]] `tempTables`
| FIXME

Used when...FIXME

| [[currentDb]] `currentDb`
| FIXME

Used when...FIXME

| [[functionResourceLoader]] `functionResourceLoader`
| FIXME

Used when...FIXME

| [[tableRelationCache]] `tableRelationCache`
| A cache of fully-qualified table names to link:spark-sql-LogicalPlan.adoc[table relation plans] (i.e. `LogicalPlan`).

Used when `SessionCatalog` <<refreshTable, refreshes a table>>
|===

=== [[refreshTable]] `refreshTable` Method

CAUTION: FIXME

=== [[createTempFunction]] `createTempFunction` Method

CAUTION: FIXME

=== [[loadFunctionResources]] `loadFunctionResources` Method

CAUTION: FIXME

=== [[functionExists]] `functionExists` Method

CAUTION: FIXME

=== [[alterTempViewDefinition]] `alterTempViewDefinition` Method

[source, scala]
----
alterTempViewDefinition(name: TableIdentifier, viewDefinition: LogicalPlan): Boolean
----

`alterTempViewDefinition` alters the temporary view by <<createTempView, updating an in-memory temporary table>> (when a database is not specified and the table has already been registered) or a global temporary table (when a database is specified and it is for global temporary tables).

NOTE: "Temporary table" and "temporary view" are synonyms.

`alterTempViewDefinition` returns `true` when an update could be executed and finished successfully.

=== [[createTempView]] `createTempView` Method

CAUTION: FIXME

=== [[createGlobalTempView]] `createGlobalTempView` Method

CAUTION: FIXME

=== [[createTable]] `createTable` Method

CAUTION: FIXME

=== [[alterTable]] `alterTable` Method

CAUTION: FIXME

=== [[creating-instance]] Creating SessionCatalog Instance

`SessionCatalog` takes the following when created:

* [[externalCatalog]] link:spark-sql-ExternalCatalog.adoc[ExternalCatalog]
* [[globalTempViewManager]] `GlobalTempViewManager`
* [[functionResourceLoader]] `FunctionResourceLoader`
* [[functionRegistry]] link:spark-sql-FunctionRegistry.adoc[FunctionRegistry]
* [[conf]] link:spark-sql-CatalystConf.adoc[CatalystConf]
* [[hadoopConf]] Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[Configuration]
* [[parser]] link:spark-sql-ParserInterface.adoc[ParserInterface]

`SessionCatalog` initializes the <<internal-registries, internal registries and counters>>.

=== [[lookupFunction]] Finding Function by Name (Using FunctionRegistry) -- `lookupFunction` Method

[source, scala]
----
lookupFunction(
  name: FunctionIdentifier,
  children: Seq[Expression]): Expression
----

`lookupFunction` finds a function by `name`.

For a function with no database defined that exists in <<functionRegistry, FunctionRegistry>>, `lookupFunction` requests `FunctionRegistry` to link:spark-sql-FunctionRegistry.adoc#lookupFunction[find the function] (by its unqualified name, i.e. with no database).

If the `name` function has the database defined or does not exist in `FunctionRegistry`, `lookupFunction` uses the fully-qualified function `name` to check if the function exists in <<functionRegistry, FunctionRegistry>> (by its fully-qualified name, i.e. with a database).

For other cases, `lookupFunction` requests <<externalCatalog, ExternalCatalog>> to find the function and <<loadFunctionResources, loads its resources>>. It then <<createTempFunction, creates a corresponding temporary function>> and link:spark-sql-FunctionRegistry.adoc#lookupFunction[looks up the function] again.

NOTE: `lookupFunction` is used exclusively when `Analyzer` link:spark-sql-Analyzer.adoc#ResolveFunctions[resolves functions].
