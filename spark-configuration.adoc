== SparkConf - Configuration for Spark Applications

TIP: Refer to http://spark.apache.org/docs/latest/configuration.html[Spark Configuration] for extensive coverage of how to configure Spark and user programs.

[CAUTION]
====
TODO

* Describe `SparkConf` object for the application configuration.
* the default configs
* system properties
====

There are three ways to configure Spark and user programs:

* Spark Properties - use link:spark-webui.adoc[Web UI] to learn the current properties.
* ...

=== Spark Properties

Every user program starts with creating an instance of `SparkConf` that holds the link:spark-deployment-modes.adoc#master-urls[master URL] to connect to (`spark.master`), the name for your Spark application (that is later displayed in link:spark-webui.adoc[web UI] and becomes `spark.app.name`) and other Spark properties required for proper runs. An instance of SparkConf is then used to create link:spark-sparkcontext.adoc[SparkContext].

[TIP]
====
Start link:spark-shell.adoc[Spark shell] with `--conf spark.logConf=true` to log the effective Spark configuration as INFO when SparkContext is started.

```
$ ./bin/spark-shell --conf spark.logConf=true
...
15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
15/10/19 17:13:49 INFO SparkContext: Spark configuration:
spark.app.name=Spark shell
spark.home=/Users/jacek/dev/oss/spark
spark.jars=
spark.logConf=true
spark.master=local[*]
spark.repl.class.uri=http://10.5.10.20:64055
spark.submit.deployMode=client
...
```

FIXME: Why is this different from `sc.getConf.toDebugString`?
====

You can query for the values of Spark properties in link:spark-shell.adoc[Spark shell] as follows:

```
scala> sc.getConf.getOption("spark.local.dir")
res0: Option[String] = None

scala> sc.getConf.getOption("spark.app.name")
res1: Option[String] = Some(Spark shell)

scala> sc.getConf.get("spark.master")
res2: String = local[*]
```

=== Setting up Properties

There are the following ways to set up properties for Spark and user programs (in the order of importance from the least important to the most important):

* `conf/spark-defaults.conf` - the default
* `--conf` - the command line option used by `spark-shell` and `spark-submit`
* `SparkConf`

=== [[default-configuration]] Default Configuration

The default Spark configuration is created when you execute the following in the code:

[source, scala]
----
import org.apache.spark.SparkConf

val conf = new SparkConf
----

It merely loads any `spark.*` system properties.

You can use `conf.toDebugString` or `conf.getAll` to have the `spark.*` system properties loaded printed out.

[source, scala]
----
scala> conf.getAll
res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,""), (spark.master,local[*]), (spark.submit.deployMode,client))

scala> conf.toDebugString
res1: String =
spark.app.name=Spark shell
spark.jars=
spark.master=local[*]
spark.submit.deployMode=client

scala> println(conf.toDebugString)
spark.app.name=Spark shell
spark.jars=
spark.master=local[*]
spark.submit.deployMode=client
----
