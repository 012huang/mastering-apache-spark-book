== Configuring Spark and Applications

TIP: Refer to http://spark.apache.org/docs/latest/configuration.html[Spark Configuration] for extensive coverage of how to configure Spark and user programs.

[CAUTION]
====
TODO

* Describe `SparkConfig` object for the application configuration.
* the default configs
* system properties
====

There are three ways to configure Spark and user programs:

* Spark Properties - use link:spark-webui.adoc[Web UI] to learn the current properties.
* ...

=== Spark Properties

Every user program starts with instantiating `SparkConf` that holds master, appName and other Spark properties required for proper runs. An instance of SparkConf is then used to create link:spark-sparkcontext.adoc[SparkContext].

[TIP]
====
Start link:spark-shell.adoc[Spark shell] with `--conf spark.logConf=true` to log the effective Spark configuration as INFO when SparkContext is started.

```
$ ./bin/spark-shell --conf spark.logConf=true
...
15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
15/10/19 17:13:49 INFO SparkContext: Spark configuration:
spark.app.name=Spark shell
spark.home=/Users/jacek/dev/oss/spark
spark.jars=
spark.logConf=true
spark.master=local[*]
spark.repl.class.uri=http://10.5.10.20:64055
spark.submit.deployMode=client
...
```
====

You can query for the values of Spark properties in link:spark-shell.adoc[Spark shell] as follows:

```
scala> sc.getConf.getOption("spark.local.dir")
res0: Option[String] = None

scala> sc.getConf.getOption("spark.app.name")
res1: Option[String] = Some(Spark shell)

scala> sc.getConf.get("spark.master")
res2: String = local[*]
```

=== Setting up Properties

There are the following ways to set up properties for Spark and user programs (in the order of importance from the least important to the most important):

* `conf/spark-defaults.conf` - the default
* `--conf` - the command line option used by `spark-shell` and `spark-submit`
* `SparkConf`
