== Your first Spark application (using Scala and sbt)

This page gives you the exact steps to develop and run a Spark application using http://www.scala-lang.org/[Scala] programming language.

Refer to the official http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/quick-start.html#self-contained-applications[Self-Contained Applications].

=== Project's build - build.sbt

```
name         := "SparkMe Project"
version      := "1.0"
organization := "io.deepsense"

scalaVersion := "2.11.7"

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.0-SNAPSHOT"
resolvers += Resolver.mavenLocal
```

=== SparkMeApp

```
package io.deepsense

import org.apache.spark.{SparkContext, SparkConf}

object SparkMeApp {
  def main(args: Array[String]) {
    println(s"Creating context for [${args(0)}]")
    val conf = new SparkConf()
      .setAppName("SparkMe Application")
      .setMaster(args(0))
    val sc = new SparkContext(conf)

    val fileName = "README.md"
    val lines = sc.textFile(fileName).cache()

    val c = lines.count()
    println(s"There are $c lines in $fileName")
  }
}
```

=== sbt version - project/build.properties

```
sbt.version=0.13.9
```

=== Packaging Application

Execute `sbt package` to package the application.

```
➜  spark-me-app  sbt package
[info] Set current project to SparkMe Project (in build file:/Users/jacek/dev/sandbox/spark-me-app/)
[info] Updating {file:/Users/jacek/dev/sandbox/spark-me-app/}spark-me-app...
[info] Resolving jline#jline;2.12.1 ...
[info] Done updating.
[info] Compiling 1 Scala source to /Users/jacek/dev/sandbox/spark-me-app/target/scala-2.11/classes...
[info] Packaging /Users/jacek/dev/sandbox/spark-me-app/target/scala-2.11/sparkme-project_2.11-1.0.jar ...
[info] Done packaging.
[success] Total time: 9 s, completed Sep 22, 2015 11:42:50 PM
```

In `target/scala-2.11/sparkme-project_2.11-1.0.jar` is the final application ready for deployment.

=== Submitting Application to Spark

`spark-submit` the application.

```
➜  spark-me-app  ~/dev/oss/spark/bin/spark-submit --class SparkMeApp target/scala-2.11/sparkme-project_2.11-1.0.jar "local[*]"
```

=== textFile reads compressed files

```
scala> val f = sc.textFile("f.txt.gz")
f: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at textFile at <console>:24

scala> f.foreach(println)
...
15/09/13 19:06:52 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/f.txt.gz:0+38
15/09/13 19:06:52 INFO CodecPool: Got brand-new decompressor [.gz]
Ala ma kota
```

=== Changing log levels

Create `conf/log4j.properties` out of the Spark template:

```
cp conf/log4j.properties.template conf/log4j.properties
```

Edit `conf/log4j.properties` so the line `log4j.rootCategory` uses appropriate log level, e.g.

```
log4j.rootCategory=ERROR, console
```

If you want to do it from the code instead, do as follows:

```
import org.apache.log4j.Logger
import org.apache.log4j.Level

Logger.getLogger("org").setLevel(Level.OFF)
Logger.getLogger("akka").setLevel(Level.OFF)
```

=== FIXME

Describe the other computing models using Spark SQL, MLlib, Spark Streaming, and GraphX.

```
$ ./bin/spark-shell
...
Spark context available as sc.
...
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.5.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc.addFile("/Users/jacek/dev/sandbox/hello.json")

scala> import org.apache.spark.SparkFiles
import org.apache.spark.SparkFiles

scala> SparkFiles.get("/Users/jacek/dev/sandbox/hello.json")
```

See https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkFiles.html[org.apache.spark.SparkFiles].
