== Tasks

*Task*  (aka _command_) is an individual unit of work that an executor runs on a single machine. It is represented by <<Task, Task>> abstract class.

.Tasks correspond to partitions in RDD
image::images/rdd-partitions-job-tasks.png[align="center"]

In other words, a task is a computation on a data partition in a stage in a job attempt.

A task can only belong to one stage and operates on a single partition (that is a part of an RDD). All tasks in a stage must be completed before the stages the follow can start.

Tasks are spawned one by one for each stage and partition.

CAUTION: FIXME What are `stageAttemptId` and `taskAttemptId`?

There are two kinds of tasks:

* *ShuffleMapTask* that executes a task and divides the task's output to multiple buckets (based on the task's partitioner). See <<shufflemaptask, ShuffleMapTask>>.
* *ResultTask* that executes a task and sends the task's output back to the driver application. See <<ResultTask, ResultTask>>.

The very last stage in a job consists of multiple `ResultTasks`, while earlier stages consist of <<shufflemaptask, ShuffleMapTasks>>.

=== [[attributes]] Task Attributes

A `Task` instance is uniquely identified by the following task attributes:

* `stageId` - there can be many stages in a job. Every stage has its own unique `stageId` that the task belongs to.

* `stageAttemptId` - a stage can be re-attempted for execution in case of failure. `stageAttemptId` represents the attempt id of a stage that the task belongs to.

* `partitionId` - a task is a unit of work on a partitioned distributed dataset. Every partition has its own unique `partitionId` that a task processes.

* `metrics` - an instance of link:spark-taskscheduler-taskmetrics.adoc[TaskMetrics] for the task.

* `localProperties` - local private properties of the task.

=== [[execution]] Task Execution

CAUTION: FIXME What does `Task.run` do?

Consult link:spark-executor.adoc#TaskRunner[TaskRunner].

=== [[states]] Task States

A task can be in one of the following states:

* `LAUNCHING`
* `RUNNING`
* `FINISHED`
* `FAILED` - when `FetchFailedException` (see link:spark-executor.adoc#FetchFailedException[FetchFailedException]), `CommitDeniedException` or any `Throwable` occur
* `KILLED` - when an executor kills a task
* `LOST`

States are the values of `org.apache.spark.TaskState`.

Task is finished when it is in one of `FINISHED`, `FAILED`, `KILLED`, `LOST`

`LOST` and `FAILED` states are considered failures.

NOTE: Task states correspond to https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto[org.apache.mesos.Protos.TaskState].

=== [[collectAccumulatorUpdates]] Collect Latest Values of Accumulators (collectAccumulatorUpdates)

[source, scala]
----
collectAccumulatorUpdates(taskFailed: Boolean = false): Seq[AccumulableInfo]
----

`collectAccumulatorUpdates` collects the latest values of accumulators used in a task.

NOTE: It is used in link:spark-executor.adoc#TaskRunner[TaskRunner] to send a task's final results with the latest values of accumulators used.

When `taskFailed` is `true` it filters out link:spark-accumulators.adoc[accumulators] with `countFailedValues` disabled.

CAUTION: FIXME Why is the check `context != null`?

NOTE: It uses `context.taskMetrics.accumulatorUpdates()`.

CAUTION: FIXME What is `context.taskMetrics.accumulatorUpdates()` doing?

=== [[ShuffleMapTask]][[shufflemaptask]] ShuffleMapTask

A *ShuffleMapTask* divides the elements of an RDD into multiple buckets (based on a partitioner specified in link:spark-rdd-dependencies.adoc#ShuffleDependency[ShuffleDependency]).

=== [[ResultTask]] ResultTask
