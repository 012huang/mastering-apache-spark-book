== Tasks

*Task*  (aka _command_) is an individual unit of work for executors to run.

It is an individual unit of physical execution (computation) that runs on a single machine for parts of your Spark application on a data. All tasks in a stage should be completed before moving on to another stage.

.Tasks correspond to partitions in RDD
image::images/rdd-partitions-job-tasks.png[align="center"]

A task can also be considered a computation in a stage on a partition in a given job attempt.

A Task belongs to a single stage and operates on a single partition (a part of an RDD).

Tasks are spawned one by one for each stage and data partition.

CAUTION: FIXME What are `stageAttemptId` and `taskAttemptId`?

There are two kinds of tasks:

* *ShuffleMapTask* that executes a task and divides the task's output to multiple buckets (based on the task's partitioner). See <<shufflemaptask, ShuffleMapTask>>.
* *ResultTask* that executes a task and sends the task's output back to the driver application.

The very last stage in a job consists of multiple `ResultTasks`, while earlier stages consist of <<shufflemaptask, ShuffleMapTask>>'s.

=== [[states]] States

A task can be in one of the following states:

* `LAUNCHING`
* `RUNNING`
* `FINISHED`
* `FAILED` - when `FetchFailedException` (see link:spark-executor.adoc#FetchFailedException[FetchFailedException]), `CommitDeniedException` or any `Throwable` occur
* `KILLED` - when an executor kills a task
* `LOST`

States are the values of `org.apache.spark.TaskState`.

Task is finished when it is in one of `FINISHED`, `FAILED`, `KILLED`, `LOST`

`LOST` and `FAILED` states are considered failures.

NOTE: Task states correspond to https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto[org.apache.mesos.Protos.TaskState].

=== [[shufflemaptask]] ShuffleMapTask

A *ShuffleMapTask* divides the elements of an RDD into multiple buckets (based on a partitioner specified in link:spark-rdd-dependencies.adoc#ShuffleDependency[ShuffleDependency]).
