== Spark local

You can run Spark in *local mode*. In this non-distributed single-JVM mode, Spark spawns all the execution components - the driver et al. - in the same JVM.

The local mode is very convenient for testing, debugging or demonstration purposes as it requires no ealier setup to launch Spark applications.

This mode of operation is also called  http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark[Spark in-process] or (less commonly) *a local version of Spark*.

You can run a Spark application in local mode using `local` or `local[n]` (with `n` being the number of cores to use) or the most general `local[*]` for the master URL.

`SparkContext.isLocal` returns `true` when Spark runs in local mode.

The Spark shell defaults to local mode (unless `--master` is used with non-`local` URL).

```
scala> sc.isLocal
res0: Boolean = true
```
