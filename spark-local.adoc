== Spark local

You can run Spark in *local mode*. In this non-distributed single-JVM deployment mode, Spark spawns all the execution components - the link:spark-execution-model.adoc#executors[executor], backend, and master - in the same JVM.

The local mode is very convenient for testing, debugging or demonstration purposes as it requires no earlier setup to launch Spark applications.

This mode of operation is also called  http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark[Spark in-process] or (less commonly) *a local version of Spark*.

You can run Spark in local mode using `local` or `local[n]` (with `n` being the number of cores to use) or the most general `local[*]` for the master URL.

You can also use *local-with-failures*, i.e. `local[N, M]` where `N` is `*` or the number of cores to use (as explained above) and `M` being the value of link:spark-taskscheduler.adoc#settings[spark.task.maxFailures].

`SparkContext.isLocal` returns `true` when Spark runs in local mode.

link:spark-shell.adoc[Spark shell] defaults to local mode (unless `--master` is used with non-`local` master URL).

```
scala> sc.isLocal
res0: Boolean = true
```

Tasks are not re-executed on failure in local mode (unless the local-with-failures master URL is used).

The link:spark-taskscheduler.adoc[task scheduler] in local mode works with <<local-backend, LocalBackend>> task scheduler backend.

=== [[local-endpoint]] LocalEndpoint

*LocalEndpoint* is a (thread-safe) link:spark-rpc.adoc#rpcendpoint[RPC Endpoint] that hosts an link:spark-execution-model.adoc#executor[executor] with id `driver` and hostname `localhost`.

When Spark local starts up, it starts up LocalEndpoint that prints out the following messages:

```
INFO Executor: Starting executor ID driver on host localhost
INFO Executor: Using REPL class URI: http://192.168.1.4:56131
```

==== [[messages]] Messages

LocalEndpoint accepts the following message types:

* `ReviveOffers` (receive-only, non-blocking) - it offers link:spark-taskscheduler.adoc[TaskScheduler] a `WorkerOffer` and launches tasks on the executor with <<localbackend, LocalBackend>>.
* `StatusUpdate` (receive-only, non-blocking) that passes the message to TaskScheduler (using `statusUpdate`) and if link:spark-taskscheduler.adoc#tasks[the task's status is finished], it revives offers (see `ReviveOffers`).
* `KillTask` (receive-only, non-blocking) that kills the task that is currently running on the executor.
* `StopExecutor` (receive-reply, blocking) that stops the executor.

=== [[local-backend]] LocalBackend

A *LocalBackend* is a SchedulerBackend and a ExecutorBackend for Spark local mode.

CAUTION: FIXME: What does the above sentence mean?

It communicates with <<local-endpoint, LocalEndpoint>> using <<messages, messages>>. This communication effectively sets up LocalBackend in full control of LocalEndpoint's lifecycle.
