== Spark local

You can run Spark in *local mode*. In this non-distributed single-JVM deployment mode, Spark spawns all the execution components - link:spark-driver.adoc[driver], link:spark-executor.adoc[executor], <<LocalBackend, backend>>, and link:spark-master.adoc[master] - in the same JVM.

.Architecture of Spark local
image::diagrams/spark-local-architecture.png[align="center"]

The local mode is very convenient for testing, debugging or demonstration purposes as it requires no earlier setup to launch Spark applications.

This mode of operation is also called  http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark[Spark in-process] or (less commonly) *a local version of Spark*.

You can run Spark in local mode using `local` or `local[n]` (with `n` being the number of cores to use) or the most general `local[*]` for link:spark-deployment-modes.adoc#master-urls[the master URL].

CAUTION: FIXME What happens when there's less cores than `n` in the master URL? It is a question from twitter.

You can also use *local-with-retries*, i.e. `local[N, M]` where `N` is `*` or the number of cores to use (as explained above) and `M` being the value of link:spark-taskscheduler.adoc#settings[spark.task.maxFailures].

`SparkContext.isLocal` returns `true` when Spark runs in local mode.

```
scala> sc.isLocal
res0: Boolean = true
```

link:spark-shell.adoc[Spark shell] defaults to local mode with `local[*]` as the link:spark-deployment-modes.adoc#master-urls[the master URL].

```
scala> sc.master
res0: String = local[*]
```

Tasks are not re-executed on failure in local mode (unless the local-with-retries master URL is used).

The link:spark-taskscheduler.adoc[task scheduler] in local mode works with <<LocalBackend, LocalBackend>> task scheduler backend.

=== [[task-submission]] Task Submission

.TaskSchedulerImpl.submitTasks in local mode
image::images/images/taskscheduler-submitTasks-local-mode.png[align="center"]

When `ReviveOffers` or `StatusUpdate` messages are received, `LocalEndpoint` launches a task (using `reviveOffers()` method). The number of tasks to be launched is controlled by the master URL that says how many threads can be used in total:

* For `local` master URL there is only 1 core available.

* For `local[n]` master URL there are `n` cores available.

* For `local[*]` master URL there are https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--[Runtime.getRuntime.availableProcessors()] cores available, i.e. the number of processors available to the Java virtual machine.

=== [[LocalBackend]] LocalBackend

`LocalBackend` is a link:spark-schedulerbackends.adoc[scheduler backend] and a link:spark-runtime-environment.adoc#executor-backends[ExecutorBackend] for Spark local mode.

It acts as a "cluster manager" for local mode to offer resources on the single link:spark-workers.adoc[worker] it manages. It calls `TaskSchedulerImpl.resourceOffers(offers)` with `offers` being a single-element collection with `WorkerOffer("driver", "localhost", freeCores)`.

CAUTION: FIXME Review `freeCores`. It appears you could have many jobs running simultaneously.

When LocalBackend starts up, it registers a new link:spark-rpc.adoc#rpcendpoint[RPC Endpoint] called *LocalBackendEndpoint* that is backed by <<local-endpoint, LocalEndpoint>>. This is announced over `listenerBus` as `driver` (`SparkListenerExecutorAdded` as a `SparkListenerEvent`).

The application ids are in the format of `local-[current time millis]`.

It communicates with <<local-endpoint, LocalEndpoint>> using <<messages, RPC messages>>.

=== [[local-endpoint]] LocalEndpoint

*LocalEndpoint* is the communication channel between link:spark-taskscheduler.adoc[Task Scheduler] and <<LocalBackend, LocalBackend>>. It is a (thread-safe) link:spark-rpc.adoc#rpcendpoint[RPC Endpoint] that hosts an link:spark-executor.adoc[executor] (with id `driver` and hostname `localhost`) for Spark local mode.

When a LocalEndpoint starts up (as part of Spark local's initialization) it prints out the following INFO messages to the logs:

```
INFO Executor: Starting executor ID driver on host localhost
INFO Executor: Using REPL class URI: http://192.168.1.4:56131
```

==== [[messages]] Messages

LocalEndpoint accepts the following RPC message types:

* `ReviveOffers` (receive-only, non-blocking) - it offers link:spark-taskscheduler.adoc[TaskScheduler] a `WorkerOffer` and launches tasks on the executor with <<localbackend, LocalBackend>>.

* `StatusUpdate` (receive-only, non-blocking) that passes the message to TaskScheduler (using `statusUpdate`) and if link:spark-taskscheduler.adoc#tasks[the task's status is finished], it revives offers (see `ReviveOffers`).

* `KillTask` (receive-only, non-blocking) that kills the task that is currently running on the executor.

* `StopExecutor` (receive-reply, blocking) that stops the executor.

=== [[settings]] Settings

* `spark.default.parallelism` (default: total cores as parameter) - the default parallelism for <<LocalBackend, LocalBackend>>.
