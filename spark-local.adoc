== Spark local

You can run Spark in *local mode*. In this non-distributed single-JVM deployment mode, Spark spawns all the execution components - link:spark-runtime-environment.adoc#executor[driver executor], <<LocalBackend, backend>>, and master - in the same JVM.

.Architecture of Spark local
image::diagrams/spark-local-architecture.png[align="center"]

The local mode is very convenient for testing, debugging or demonstration purposes as it requires no earlier setup to launch Spark applications.

This mode of operation is also called  http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark[Spark in-process] or (less commonly) *a local version of Spark*.

You can run Spark in local mode using `local` or `local[n]` (with `n` being the number of cores to use) or the most general `local[*]` for link:spark-deployment-modes.adoc#master-urls[the master URL].

You can also use *local-with-retries*, i.e. `local[N, M]` where `N` is `*` or the number of cores to use (as explained above) and `M` being the value of link:spark-taskscheduler.adoc#settings[spark.task.maxFailures].

`SparkContext.isLocal` returns `true` when Spark runs in local mode.

link:spark-shell.adoc[Spark shell] defaults to local mode (unless `--master` is used with non-`local` master URL).

```
scala> sc.isLocal
res0: Boolean = true
```

Tasks are not re-executed on failure in local mode (unless the local-with-retries master URL is used).

The link:spark-taskscheduler.adoc[task scheduler] in local mode works with <<LocalBackend, LocalBackend>> task scheduler backend.

=== [[LocalBackend]] LocalBackend

`LocalBackend` is a link:spark-schedulerbackends.adoc[scheduler backend] and a link:spark-runtime-environment.adoc#executor-backends[ExecutorBackend] for Spark local mode.

The application ids are in the format of `local-[current time millis]`.

When LocalBackend starts up, it registers a new link:spark-rpc.adoc#rpcendpoint[RPC Endpoint] called *LocalBackendEndpoint* that is backed by <<local-endpoint, LocalEndpoint>>. This is announced over `listenerBus` as `driver` (`SparkListenerExecutorAdded` as a `SparkListenerEvent`).

It communicates with <<local-endpoint, LocalEndpoint>> using <<messages, RPC messages>>.

=== [[local-endpoint]] LocalEndpoint

*LocalEndpoint* is the communication channel between link:spark-taskscheduler.adoc[Task Scheduler] and <<LocalBackend, LocalBackend>>. It is a (thread-safe) link:spark-rpc.adoc#rpcendpoint[RPC Endpoint] that hosts an link:spark-runtime-environment.adoc#executor[executor] (with id `driver` and hostname `localhost`) for Spark local mode.

When a LocalEndpoint starts up (as part of Spark local's initialization) it prints out the following INFO messages to the logs:

```
INFO Executor: Starting executor ID driver on host localhost
INFO Executor: Using REPL class URI: http://192.168.1.4:56131
```

==== [[messages]] Messages

LocalEndpoint accepts the following message types:

* `ReviveOffers` (receive-only, non-blocking) - it offers link:spark-taskscheduler.adoc[TaskScheduler] a `WorkerOffer` and launches tasks on the executor with <<localbackend, LocalBackend>>.
* `StatusUpdate` (receive-only, non-blocking) that passes the message to TaskScheduler (using `statusUpdate`) and if link:spark-taskscheduler.adoc#tasks[the task's status is finished], it revives offers (see `ReviveOffers`).
* `KillTask` (receive-only, non-blocking) that kills the task that is currently running on the executor.
* `StopExecutor` (receive-reply, blocking) that stops the executor.

=== [[settings]] Settings

* `spark.default.parallelism` (default: total cores as parameter) - the default parallelism for <<LocalBackend, LocalBackend>>.
