=== Spark's Execution model

Jobs, Tasks, Stages in Spark

Explain task execution in Spark and understand Sparkâ€™s underlying execution model.

New vocabulary often faced in Spark UI

When you create `SparkContext`, each worker starts an executor. This is a separate process (JVM), and it loads your jar, too. The executors connect back to your driver program. Now the driver can send them commands, like `flatMap`, `map` and `reduceByKey` in your example. When the driver quits, the executors shut down.

A new process is not started for each step. A new process is started on each worker when the SparkContext is constructed.

A *task* is a command sent from the driver to an executor by serializing your `Function` object. The executor deserializes the command (this is possible because it has loaded your jar), and executes it on a partition. The communication uses Akka's actors.

Shortly speaking, an application in Spark is executed in three steps:

* Create RDD graph
* Create execution plan according to the RDD graph. Stages are created in this step
* Generate tasks based on the plan and get them scheduled across workers

In the WordCount example, the RDD graph is rather simple, and it's something as follows:

file -> lines -> words -> per-word count -> global word count -> output

Based on this graph, two stages are created. The stage creation rule is based on the idea to pipeline as many link:spark-rdd.adoc[narrow transformations] as possible. In the example, the narrow transformation finishes at per-word count. Therefore, you get two stages:

* file -> lines -> words -> per-word count
* global word count -> output

Once stages are defined, Spark will generate tasks from stages. The first stage will create ShuffleMapTasks and the last stage will create ResultTasks because in the last stage, one action operation is included to produce results.

The number of tasks to be generated depends on how your files are distributed. Suppose that you have 3 three different files in three different nodes, the first stage will generate 3 tasks: one task per partition.

Therefore, you should not map your steps to tasks directly. A task belongs to a stage, and is related to a partition.

The number of tasks being generated in each stage will be equal to the number of partitions.

spark compute nodes

* *actions*
* *jobs* - triggered as a result of executing an action in a Spark application. Spark examines the graph of RDDs and creates an execution plan.
* *execution plan* - starts with the earliest RDDs (those with no dependencies on other RDDs or reference cached data) and ends with the RDD that produces the result of the action that has been called to execute.
* *stages* - transformations; a collection of tasks that execute the same code, each on separate subset of data. Each stage contains a sequence of transformations that can be completed without _shuffling_ the entire data set.
* *tasks* - units of physical execution that run parts of your Spark application

Jobs are at the top of the execution hierarchy.
