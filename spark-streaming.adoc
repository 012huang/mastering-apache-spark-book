== Spark Streaming

Spark Streaming runs a collection of SparkStreaming jobs in batches.

Essential concepts in Spark Streaming:

* link:spark-streaming-streamingcontext.adoc[StreamingContext]
* Time
* Job
* link:spark-streaming-jobscheduler.adoc#JobSet[JobSet]

=== [[ReceiverTracker]] ReceiverTracker

CAUTION: FIXME

`ReceiverTracker` manages the execution of the receivers of ReceiverInputDStreams.

If there is at least one `ReceiverInputDStreams` the RPC endpoint `ReceiverTracker` using <<ReceiverTrackerEndpoint, ReceiverTrackerEndpoint>> is set up.

When `ReceiverTracker` starts, you should see the following INFO message in the logs:

```
INFO ReceiverTracker: Starting [receivers.length] receivers
```

It posts `StartAllReceivers` to `ReceiverTracker` RPC endpoint.

You should then see the following INFO message in the logs:

```
INFO ReceiverTracker: ReceiverTracker started
```

==== [[ReceiverTrackerEndpoint]] ReceiverTrackerEndpoint

CAUTION: FIXME

=== [[JobGenerator]] JobGenerator

CAUTION: FIXME

When `JobGenerator` starts, it starts <<JobGenerator-eventLoop, eventLoop - JobGeneratorEvent Handler>>.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobGenerator` logger to see what happens in JobGenerator.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobGenerator=DEBUG
```
====

For every `JobGeneratorEvent` event, you should see the following DEBUG message in the logs:

```
DEBUG Got event [event]
```

You should see the following INFO message in the logs when it starts:

```
INFO JobGenerator: Started JobGenerator at [startTime] ms
```

==== [[JobGenerator-eventLoop]] JobGenerator eventLoop and JobGeneratorEvent Handler

JobGenerator uses `EventLoop` for JobGeneratorEvent events.

See below for extensive coverage of event types it handles.

===== [[GenerateJobs]] GenerateJobs and JobGenerator.generateJobs

CAUTION: FIXME

When `GenerateJobs` is received (FIXME when?) `JobGenerator.generateJobs(time: Time)` is executed to process it.

It calls `JobScheduler.receiverTracker.allocateBlocksToBatch`, and then calls <<DStreamGraph, DStreamGraph.generateJobs(time: Time)>>.

When it finishes successfully, a collection of jobs is given that are passed to link:spark-streaming-jobscheduler.adoc#submitJobSet[JobScheduler.submitJobSet] as a `JobSet`.

Ultimately, it posts <<DoCheckpoint, DoCheckpoint>> event.

===== [[ClearMetadata]] ClearMetadata and clearMetadata

CAUTION: FIXME

===== [[DoCheckpoint]] DoCheckpoint and doCheckpoint

CAUTION: FIXME

===== [[ClearCheckpointData]] ClearCheckpointData and clearCheckpointData

CAUTION: FIXME

=== [[RateController]] RateController

CAUTION: FIXME

InputDStreams can define a `RateController` that is registered to link:spark-streaming-jobscheduler.adoc[JobScheduler]'s `listenerBus`  (using `ssc.addStreamingListener`) when link:spark-streaming-jobscheduler.adoc[JobScheduler] starts.

=== [[StreamingTab]] StreamingTab

CAUTION: FIXME

=== [[DStreamGraph]] DStreamGraph

CAUTION: FIXME

It knows about `inputStreams` and `outputStreams`.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.DStreamGraph` logger to see what happens in DStreamGraph.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.DStreamGraph=DEBUG
```
====

When `DStreamGraph.generateJobs(time: Time)` is called, you should see the following DEBUG message in the logs:

```
DEBUG Generating jobs for time [time]
```

Each registered output stream (using `outputStreams`) gets called using `generateJob`.

=== [[internal-registries]] Internal Registries

* `nextInputStreamId` - the current InputStream id

=== [[StreamingSource]] StreamingSource

CAUTION: FIXME

=== [[settings]] Settings

* `spark.streaming.clock` (default: `org.apache.spark.util.SystemClock`) specifies a fully-qualified class name that extends `org.apache.spark.util.Clock` to mock time.

* `spark.streaming.ui.retainedBatches` (default: `1000`)

* `spark.streaming.checkpoint.directory`
