== Spark Streaming

Spark Streaming runs a collection of SparkStreaming jobs in batches.

Essential concepts in Spark Streaming:

* link:spark-streaming-streamingcontext.adoc[StreamingContext]
* <<batch, Batch>> (time) and link:spark-streaming-jobscheduler.adoc#JobSet[JobSet]
* <<Job, Job>>
* <<DStream, DStream>>

=== [[batch]] Batch

A *batch* is represented as a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

=== [[Job]] Job

A Streaming `Job` represents a Spark computation with one or many Spark jobs.

It is identified (in the logs) as as `streaming job [time].[outputOpId]` with `outputOpId` being the position in the sequence of jobs in a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

When it runs, it calls the computation (the input `func` function).

A collection of Streaming jobs is generated for a batch using <<DStreamGraph-generateJobs, DStreamGraph.generateJobs(time: Time)>>.

=== [[ReceiverTracker]] ReceiverTracker

CAUTION: FIXME

`ReceiverTracker` manages the execution of the receivers of ReceiverInputDStreams.

If there is at least one `ReceiverInputDStreams` the RPC endpoint `ReceiverTracker` using <<ReceiverTrackerEndpoint, ReceiverTrackerEndpoint>> is set up.

When `ReceiverTracker` starts, you should see the following INFO message in the logs:

```
INFO ReceiverTracker: Starting [receivers.length] receivers
```

It posts `StartAllReceivers` to `ReceiverTracker` RPC endpoint.

You should then see the following INFO message in the logs:

```
INFO ReceiverTracker: ReceiverTracker started
```

==== [[ReceiverTrackerEndpoint]] ReceiverTrackerEndpoint

CAUTION: FIXME

=== [[RateController]] RateController

CAUTION: FIXME

InputDStreams can define a `RateController` that is registered to link:spark-streaming-jobscheduler.adoc[JobScheduler]'s `listenerBus`  (using `ssc.addStreamingListener`) when link:spark-streaming-jobscheduler.adoc[JobScheduler] starts.

=== [[StreamingTab]] StreamingTab

CAUTION: FIXME

=== [[DStream]] DStream

CAUTION: FIXME

=== [[DStreamGraph]] DStreamGraph

CAUTION: FIXME

It knows about `inputStreams` and `outputStreams`.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.DStreamGraph` logger to see what happens in DStreamGraph.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.DStreamGraph=DEBUG
```
====

==== [[DStreamGraph-generateJobs]] DStreamGraph.generateJobs(time: Time)

`DStreamGraph.generateJobs(time: Time)` method is called to generate a Spark job for each registered output stream (using `DStream.generateJob`).

When it runs, you should see the following DEBUG message in the logs before the jobs are generated for a time:

```
DEBUG DStreamGraph: Generating jobs for time [time] ms
```

You should also see the following DEBUG message when the jobs have been generated:

```
DEBUG DStreamGraph: Generated [jobs.length] jobs for time [time] ms
```

=== [[internal-registries]] Internal Registries

* `nextInputStreamId` - the current InputStream id

=== [[StreamingSource]] StreamingSource

CAUTION: FIXME

=== [[settings]] Settings

* `spark.streaming.clock` (default: `org.apache.spark.util.SystemClock`) specifies a fully-qualified class name that extends `org.apache.spark.util.Clock` to mock time.

* `spark.streaming.ui.retainedBatches` (default: `1000`)

* `spark.streaming.checkpoint.directory`
