== Spark Streaming

Spark Streaming runs <<Job, streaming jobs>> per <<batch, batch>> (time) to pull and process data (often called _records_) from one or many link:spark-streaming-inputdstreams.adoc[input streams] periodically at fixed <<batch-interval, batch interval>>. Each batch link:spark-streaming-dstreams.adoc#contract[computes] (_generates_) RDDs based on data received from input streams and link:spark-streaming-jobgenerator.adoc#GenerateJobs[submits a Spark job to compute the final result for a batch]. It does this over and over again until link:spark-streaming-streamingcontext.adoc#stopping[the streaming context is stopped] (and the owning streaming application terminated).

To avoid losing records in case of failure, Spark Streaming supports link:spark-streaming-checkpointing.adoc[checkpointing that writes received records to a highly-available HDFS-compatible storage] and allows to recover from temporary downtimes.

Checkpointing is also the foundation of link:spark-streaming-operators-stateful.adoc[stateful] and link:spark-streaming-windowedoperators.adoc[windowed] operations.

http://spark.apache.org/docs/latest/streaming-programming-guide.html#overview[About Spark Streaming from the official documentation] (that pretty much nails what it offers):

> Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Sparkâ€™s machine learning and graph processing algorithms on data streams.

Essential concepts in Spark Streaming:

* link:spark-streaming-streamingcontext.adoc[StreamingContext]
* link:spark-streaming-operators.adoc[Stream Operators]
* <<batch, Batch>>, Batch time, and link:spark-streaming-jobscheduler.adoc#JobSet[JobSet]
* <<Job, Streaming Job>>
* link:spark-streaming-dstreams.adoc[Discretized Streams (DStreams)]
* link:spark-streaming-receivers.adoc[Receivers]

Other concepts often used in Spark Streaming:

* *ingestion* = the act of processing streaming data.

=== [[batch]][[micro-batch]] Micro Batch

*Micro Batch* is a collection of input records as collected by Spark Streaming that is later represented as an RDD.

A *batch* is internally represented as a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

=== [[batchDuration]][[batch-interval]] Batch Interval (aka batchDuration)

*Batch Interval* is a property of a Streaming application that describes how often an RDD of input records is generated. It is the time to collect input records before they become a <<micro-batch, micro-batch>>.

=== [[Job]] Streaming Job

A streaming `Job` represents a Spark computation with one or many Spark jobs.

It is identified (in the logs) as `streaming job [time].[outputOpId]` with `outputOpId` being the position in the sequence of jobs in a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

When executed, it runs the computation (the input `func` function).

NOTE: A collection of streaming jobs is generated for a batch using link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph.generateJobs(time: Time)].

=== [[internal-registries]] Internal Registries

* `nextInputStreamId` - the current InputStream id

=== [[StreamingSource]] StreamingSource

CAUTION: FIXME
