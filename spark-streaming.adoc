== Spark Streaming

Spark Streaming runs a collection of SparkStreaming jobs in batches.

http://spark.apache.org/docs/latest/streaming-programming-guide.html#overview[About Spark Streaming from the official documentation] (that pretty much nails what it offers):

> Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Sparkâ€™s machine learning and graph processing algorithms on data streams.

Essential concepts in Spark Streaming:

* link:spark-streaming-streamingcontext.adoc[StreamingContext]
* <<batch, Batch>> (time) and link:spark-streaming-jobscheduler.adoc#JobSet[JobSet]
* <<Job, Job>>
* link:spark-streaming-dstreams.adoc[DStream]
* link:spark-streaming-receivers.adoc[Receivers]

Other concepts often used in Spark Streaming:

* *ingestion* = the act of processing streaming data.

=== [[batch]] Batch

A *batch* is represented as a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

=== [[Job]] Streaming Job

A Streaming `Job` represents a Spark computation with one or many Spark jobs.

It is identified (in the logs) as `streaming job [time].[outputOpId]` with `outputOpId` being the position in the sequence of jobs in a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

When it runs, it calls the computation (the input `func` function).

A collection of Streaming jobs is generated for a batch using link:spark-streaming-dstreams.adoc#DStreamGraph-generateJobs[DStreamGraph.generateJobs(time: Time)].

=== [[RateController]] RateController

CAUTION: FIXME

link:spark-streaming-inputdstreams.adoc[InputDStreams] can define a `RateController` that is registered to link:spark-streaming-jobscheduler.adoc[JobScheduler]'s `listenerBus`  (using `ssc.addStreamingListener`) when link:spark-streaming-jobscheduler.adoc[JobScheduler] starts.

=== [[StreamingTab]] StreamingTab

CAUTION: FIXME

=== [[internal-registries]] Internal Registries

* `nextInputStreamId` - the current InputStream id

=== [[StreamingSource]] StreamingSource

CAUTION: FIXME
