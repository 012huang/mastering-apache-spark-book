== Spark Streaming

Spark Streaming runs a collection of SparkStreaming jobs in batches.

Essential concepts in Spark Streaming:

* link:spark-streaming-streamingcontext.adoc[StreamingContext]
* <<batch, Batch>> (time) and link:spark-streaming-jobscheduler.adoc#JobSet[JobSet]
* <<Job, Job>>
* link:spark-streaming-dstreams.adoc[DStream]

=== [[batch]] Batch

A *batch* is represented as a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

=== [[Job]] Job

A Streaming `Job` represents a Spark computation with one or many Spark jobs.

It is identified (in the logs) as as `streaming job [time].[outputOpId]` with `outputOpId` being the position in the sequence of jobs in a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

When it runs, it calls the computation (the input `func` function).

A collection of Streaming jobs is generated for a batch using <<DStreamGraph-generateJobs, DStreamGraph.generateJobs(time: Time)>>.

=== [[RateController]] RateController

CAUTION: FIXME

InputDStreams can define a `RateController` that is registered to link:spark-streaming-jobscheduler.adoc[JobScheduler]'s `listenerBus`  (using `ssc.addStreamingListener`) when link:spark-streaming-jobscheduler.adoc[JobScheduler] starts.

=== [[StreamingTab]] StreamingTab

CAUTION: FIXME

=== [[DStreamGraph]] DStreamGraph

`DStreamGraph` (is a helper class that) manages link:spark-streaming-dstreams.adoc[input and output streams].

It maintains the collections of `inputStreams`, i.e. `InputDStream` instances and `outputStreams`, i.e. `DStream` instances.

When it starts, it goes over the output streams and initializes them (one by one). It then starts the input streams (in parallel).

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.DStreamGraph` logger to see what happens in DStreamGraph.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.DStreamGraph=DEBUG
```
====

==== [[DStreamGraph-generateJobs]] DStreamGraph.generateJobs(time: Time)

`DStreamGraph.generateJobs(time: Time)` method is called to generate a Spark job for each registered output stream (using `DStream.generateJob`).

When it runs, you should see the following DEBUG message in the logs before the jobs are generated for a time:

```
DEBUG DStreamGraph: Generating jobs for time [time] ms
```

You should also see the following DEBUG message when the jobs have been generated:

```
DEBUG DStreamGraph: Generated [jobs.length] jobs for time [time] ms
```

=== [[internal-registries]] Internal Registries

* `nextInputStreamId` - the current InputStream id

=== [[StreamingSource]] StreamingSource

CAUTION: FIXME

=== [[settings]] Settings

* `spark.streaming.clock` (default: `org.apache.spark.util.SystemClock`) specifies a fully-qualified class name that extends `org.apache.spark.util.Clock` to mock time.

* `spark.streaming.ui.retainedBatches` (default: `1000`)

* `spark.streaming.checkpoint.directory`
