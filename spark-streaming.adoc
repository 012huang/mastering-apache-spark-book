== Spark Streaming

Spark Streaming runs a collection of SparkStreaming jobs in batches.

Essential concepts in Spark Streaming:

* link:spark-streaming-streamingcontext.adoc[StreamingContext]
* <<batch, Batch>> (time) and link:spark-streaming-jobscheduler.adoc#JobSet[JobSet]
* <<Job, Job>>
* <<DStream, DStream>>

=== [[batch]] Batch

A *batch* is represented as a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

=== [[Job]] Job

A Streaming `Job` represents a Spark computation with one or many Spark jobs.

It is identified (in the logs) as as `streaming job [time].[outputOpId]` with `outputOpId` being the position in the sequence of jobs in a link:spark-streaming-jobscheduler.adoc#JobSet[JobSet].

When it runs, it calls the computation (the input `func` function).

A collection of Streaming jobs is generated for a batch using <<DStreamGraph-generateJobs, DStreamGraph.generateJobs(time: Time)>>.

=== [[ReceiverTracker]] ReceiverTracker

CAUTION: FIXME

`ReceiverTracker` manages the execution of the receivers of ReceiverInputDStreams.

If there is at least one `ReceiverInputDStreams` the RPC endpoint `ReceiverTracker` using <<ReceiverTrackerEndpoint, ReceiverTrackerEndpoint>> is set up.

When `ReceiverTracker` starts, you should see the following INFO message in the logs:

```
INFO ReceiverTracker: Starting [receivers.length] receivers
```

It posts `StartAllReceivers` to `ReceiverTracker` RPC endpoint.

You should then see the following INFO message in the logs:

```
INFO ReceiverTracker: ReceiverTracker started
```

==== [[ReceiverTrackerEndpoint]] ReceiverTrackerEndpoint

CAUTION: FIXME

=== [[JobGenerator]] JobGenerator

CAUTION: FIXME

When `JobGenerator` is created, it creates <<RecurringTimer, RecurringTimer>> (as `timer` with name being `JobGenerator`) to post link:spark-streaming.adoc#GenerateJobs[GenerateJobs] events to link:spark-streaming.adoc#JobGenerator-eventLoop[JobGenerator eventLoop].

When `JobGenerator` starts, it starts <<JobGenerator-eventLoop, eventLoop - JobGeneratorEvent Handler>>.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobGenerator` logger to see what happens in JobGenerator.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobGenerator=DEBUG
```
====

For every `JobGeneratorEvent` event, you should see the following DEBUG message in the logs:

```
DEBUG Got event [event]
```

You should see the following INFO message in the logs when it starts:

```
INFO JobGenerator: Started JobGenerator at [startTime] ms
```

==== [[JobGenerator-eventLoop]] JobGenerator eventLoop and JobGeneratorEvent Handler

JobGenerator uses `EventLoop` for JobGeneratorEvent events.

See below for extensive coverage of event types it handles.

===== [[GenerateJobs]] GenerateJobs and JobGenerator.generateJobs

CAUTION: FIXME

When `GenerateJobs` is received (FIXME when?) `JobGenerator.generateJobs(time: Time)` is executed to process it.

It calls `JobScheduler.receiverTracker.allocateBlocksToBatch`, and then calls <<DStreamGraph-generateJobs, DStreamGraph.generateJobs(time: Time)>>.

When it finishes successfully, a collection of jobs is given that are passed to link:spark-streaming-jobscheduler.adoc#submitJobSet[JobScheduler.submitJobSet] as a `JobSet`.

Ultimately, it posts <<DoCheckpoint, DoCheckpoint>> event.

===== [[ClearMetadata]] ClearMetadata and clearMetadata

CAUTION: FIXME

===== [[DoCheckpoint]] DoCheckpoint and doCheckpoint

CAUTION: FIXME

===== [[ClearCheckpointData]] ClearCheckpointData and clearCheckpointData

CAUTION: FIXME

=== [[RateController]] RateController

CAUTION: FIXME

InputDStreams can define a `RateController` that is registered to link:spark-streaming-jobscheduler.adoc[JobScheduler]'s `listenerBus`  (using `ssc.addStreamingListener`) when link:spark-streaming-jobscheduler.adoc[JobScheduler] starts.

=== [[StreamingTab]] StreamingTab

CAUTION: FIXME

=== [[DStream]] DStream

CAUTION: FIXME

=== [[DStreamGraph]] DStreamGraph

CAUTION: FIXME

It knows about `inputStreams` and `outputStreams`.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.DStreamGraph` logger to see what happens in DStreamGraph.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.DStreamGraph=DEBUG
```
====

==== [[DStreamGraph-generateJobs]] DStreamGraph.generateJobs(time: Time)

`DStreamGraph.generateJobs(time: Time)` method is called to generate a Spark job for each registered output stream (using `DStream.generateJob`).

When it runs, you should see the following DEBUG message in the logs before the jobs are generated for a time:

```
DEBUG Generating jobs for time [time]
```

You should also see the following DEBUG message when the jobs have been generated:

```
DEBUG Generated [jobs.length] jobs for time [time]
```

=== [[internal-registries]] Internal Registries

* `nextInputStreamId` - the current InputStream id

=== [[StreamingSource]] StreamingSource

CAUTION: FIXME

=== [[settings]] Settings

* `spark.streaming.clock` (default: `org.apache.spark.util.SystemClock`) specifies a fully-qualified class name that extends `org.apache.spark.util.Clock` to mock time.

* `spark.streaming.ui.retainedBatches` (default: `1000`)

* `spark.streaming.checkpoint.directory`
