== [[KafkaUtils]] KafkaUtils -- Creating Kafka Streams and RDDs

`KafkaUtils` is the object with the factory methods to create <<createDirectStream, dstreams>> and <<createRDD, RDDs>> from Apache Kafka.

[source, scala]
----
import org.apache.spark.streaming.kafka010.KafkaUtils
----

TIP: Use link:spark-streaming-kafka.adoc#spark-streaming-kafka-0-10[`spark-streaming-kafka-0-10` Library Dependency].

=== [[createDirectStream]] Creating Kafka Stream -- `createDirectStream` Method

[source, scala]
----
createDirectStream[K, V](
  ssc: StreamingContext,
  locationStrategy: LocationStrategy,
  consumerStrategy: ConsumerStrategy[K, V]): InputDStream[ConsumerRecord[K, V]]
----

`createDirectStream` creates a link:spark-streaming-kafka-DirectKafkaInputDStream.adoc[DirectKafkaInputDStream].

[TIP]
====
Enable `DEBUG` logging level for `org.apache.kafka.clients.consumer.KafkaConsumer` logger to see what happens inside the Kafka consumer that is used under the covers to talk to a Kafka topic(s).

```
DEBUG KafkaConsumer: Starting the Kafka consumer
DEBUG KafkaConsumer: Kafka consumer created
DEBUG KafkaConsumer: Subscribed to topic(s): basic1, basic2, basic3
```

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.kafka.clients.consumer.KafkaConsumer=DEBUG
```

Refer to link:../spark-logging.adoc[Logging].
====

[source, scala]
----
// Include org.apache.spark:spark-streaming-kafka-0-10_2.11:2.1.0-SNAPSHOT dependency in the CLASSPATH, e.g.
// $ ./bin/spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.1.0-SNAPSHOT

import org.apache.spark.streaming._
import org.apache.spark.SparkContext
val sc = SparkContext.getOrCreate
val ssc = new StreamingContext(sc, Seconds(5))

import org.apache.spark.streaming.kafka010._

val preferredHosts = LocationStrategies.PreferConsistent
val topics = List("basic1", "basic2", "basic3")
import org.apache.kafka.common.serialization.StringDeserializer
val kafkaParams = Map(
  "bootstrap.servers" -> "localhost:9092",
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "example",
  "auto.offset.reset" -> "earliest"
)
import org.apache.kafka.common.TopicPartition
val offsets = Map(new TopicPartition("basic3", 0) -> 2L)

val dstream = KafkaUtils.createDirectStream[String, String](
  ssc,
  preferredHosts,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams, offsets))

val tf = dstream.transform { rdd =>
  rdd.map(r => (r.key, r.value))
}

tf.foreachRDD { rdd =>
  // Get the offset ranges in the RDD
  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
  for (o <- offsetRanges) {
    println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
  }
}

ssc.start

// ...

ssc.stop(stopSparkContext = false)
----

=== [[createRDD]] Creating Kafka RDD -- `createRDD` Method

[source, scala]
----
def createRDD[K, V](
  sc: SparkContext,
  kafkaParams: java.util.Map[String, Object],
  offsetRanges: Array[OffsetRange],
  locationStrategy: LocationStrategy): RDD[ConsumerRecord[K, V]]
----

`createRDD` creates a link:spark-streaming-kafka-kafkardd.adoc[KafkaRDD].

CAUTION: FIXME
