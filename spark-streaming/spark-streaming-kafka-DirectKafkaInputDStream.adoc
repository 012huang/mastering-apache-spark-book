== [[DirectKafkaInputDStream]] DirectKafkaInputDStream

`DirectKafkaInputDStream` is an link:spark-streaming-inputdstreams.adoc[input stream] of link:spark-streaming-kafka-kafkardd.adoc[KafkaRDD] batches.

As an input stream, it implements the _five_ mandatory abstract methods - three from `DStream` and two from `InputDStream`:

* `dependencies: List[DStream[_]]` returns an empty collection, i.e. it has no dependencies on other streams (other than Kafka brokers to read data from).
* `slideDuration: Duration` passes all calls on to link:spark-streaming-dstreamgraph.adoc[DStreamGraph.batchDuration].
* `compute(validTime: Time): Option[RDD[T]]` - consult <<compute, Computing RDDs (using compute Method)>> section.
* <<start, start>>
* `stop()` does nothing.

The `name` of the input stream is *Kafka direct stream [id]*. You can find the name in the link:spark-streaming-webui.adoc[Streaming tab] in web UI (in the details of a batch in *Input Metadata* section).

It uses link:spark-streaming-settings.adoc[spark.streaming.kafka.maxRetries] setting while computing `latestLeaderOffsets` (i.e. a mapping of `kafka.common.TopicAndPartition` and <<LeaderOffset, LeaderOffset>>).

=== [[creating-instance]] Creating DirectKafkaInputDStream Instance

CAUTION: FIXME

=== [[executorKafkaParams]] `executorKafkaParams` Method

CAUTION: FIXME

=== [[start]] Starting DirectKafkaInputDStream -- `start` Method

[source, scala]
----
start(): Unit
----

`start` grabs the current <<consumer, Kafka consumer>> and polls (using Kafka's `Consumer.poll` and `0` as input).

NOTE: `start` is part of the link:spark-streaming-inputdstreams.adoc[InputDStream Contract].

After the polling, `start` checks if the internal `currentOffsets` is empty, and if it is, it requests Kafka for topic (using Kafka's `Consumer.assignment`) and builds a map with topics and their offsets (using Kafka's `Consumer.position`).

Ultimately, `start` pauses all partitions (using Kafka's `Consumer.pause` and the internal collection of topics and their current offsets).

=== [[consumer]] Creating Kafka Consumer -- `consumer` Method

[source, scala]
----
consumer(): Consumer[K, V]
----

`consumer` creates a Kafka `Consumer` with keys of type `K` and values of type `V` (specified when the <<creating-instance, `DirectKafkaInputDStream` is created>>).

`consumer` starts the link:spark-streaming-kafka-ConsumerStrategy.adoc#onStart[ConsumerStrategy] (that was used when the `DirectKafkaInputDStream` was created). It passes the internal collection of ``TopicPartition``s and their offsets.

CAUTION: FIXME A note with What `ConsumerStrategy` is for?

=== [[compute]] Computing RDDs -- `compute` Method

`DirectKafkaInputDStream.compute` _always_ computes a link:spark-streaming-kafka-kafkardd.adoc[KafkaRDD] instance (despite the link:spark-streaming-dstreams.adoc#contract[DStream contract] that says it may or may not generate one).

NOTE: It is link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph to request generating streaming jobs for batches].

Every time the method is called, `latestLeaderOffsets` calculates the latest offsets (as `Map[TopicAndPartition, LeaderOffset]`).

NOTE: Every call to `compute` does call Kafka brokers for the offsets.

The _moving_ parts of generated `KafkaRDD` instances are offsets. Others are taken directly from `DirectKafkaInputDStream` (given at the time of instantiation).

It then filters out empty offset ranges to build `StreamInputInfo` for link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker.reportInfo].

It sets the just-calculated offsets as current (using `currentOffsets`) and returns a new link:spark-streaming-kafka-kafkardd.adoc[KafkaRDD] instance.

=== [[back-pressure]] Back Pressure

CAUTION: FIXME

link:spark-streaming-backpressure.adoc[Back pressure] for Direct Kafka input dstream can be configured using link:spark-streaming-settings.adoc#back-pressure[spark.streaming.backpressure.enabled] setting.

NOTE: Back pressure is disabled by default.
