== [[DirectKafkaInputDStream]] DirectKafkaInputDStream -- Direct Kafka DStream

`DirectKafkaInputDStream` is an link:spark-streaming-inputdstreams.adoc[input dstream] of link:spark-streaming-kafka-KafkaRDD.adoc[KafkaRDD] batches.

As an input dstream, `DirectKafkaInputDStream` implements the mandatory abstract methods (from link:spark-streaming-dstreams.adoc#contract[DStream Contract] and link:spark-streaming-inputdstreams.adoc#contract[InputDStream Contract]):

1. `dependencies: List[DStream[_]]` returns an empty collection, i.e. it has no dependencies on other streams (other than Kafka brokers to read data from).
2. `slideDuration: Duration` passes all calls on to link:spark-streaming-dstreamgraph.adoc[DStreamGraph.batchDuration].
3. `compute(validTime: Time): Option[RDD[T]]` - consult <<compute, Computing RDDs (using compute Method)>> section.
4. <<start, start>> to start polling for messages from Kafka.
5. <<stop, stop>> to close the Kafka consumer (and therefore polling for messages from Kafka).

The `name` of a `DirectKafkaInputDStream` is *Kafka 0.10 direct stream [id]* (that you can use to differentiate between the different implementations for Kafka 0.10+ and older releases).

TIP: You can find the name of a input dstream in the link:spark-streaming-webui.adoc[Streaming tab] in web UI (in the details of a batch in *Input Metadata* section).

It uses link:spark-streaming-settings.adoc[spark.streaming.kafka.maxRetries] setting while computing `latestLeaderOffsets` (i.e. a mapping of `kafka.common.TopicAndPartition` and <<LeaderOffset, LeaderOffset>>).

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.streaming.kafka010.DirectKafkaInputDStream` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.kafka010.DirectKafkaInputDStream=INFO
```

Refer to link:../spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating DirectKafkaInputDStream Instance

CAUTION: FIXME

=== [[executorKafkaParams]] `executorKafkaParams` Method

CAUTION: FIXME

=== [[start]] Starting DirectKafkaInputDStream -- `start` Method

[source, scala]
----
start(): Unit
----

`start` grabs the current <<consumer, Kafka consumer>> and polls (using Kafka's `Consumer.poll` and `0` as input).

NOTE: `start` is part of the link:spark-streaming-inputdstreams.adoc[InputDStream Contract].

After the polling, `start` checks if the internal `currentOffsets` is empty, and if it is, it requests Kafka for topic (using Kafka's `Consumer.assignment`) and builds a map with topics and their offsets (using Kafka's `Consumer.position`).

Ultimately, `start` pauses all partitions (using Kafka's `Consumer.pause` and the internal collection of topics and their current offsets).

=== [[compute]] Computing KafkaRDD for Batch Interval -- `compute` Method

[source, scala]
----
compute(validTime: Time): Option[KafkaRDD[K, V]]
----

NOTE: `compute` is a part of the link:spark-streaming-dstreams.adoc[DStream Contract].

Despite the return type of `compute` (that allows for no RDDs), `DirectKafkaInputDStream` _always_ computes a link:spark-streaming-kafka-KafkaRDD.adoc[KafkaRDD].

NOTE: It is link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph to request generating streaming jobs for batches].

When a batch interval passes and `compute` is called, `latestLeaderOffsets` calculates the latest offsets (as `Map[TopicAndPartition, LeaderOffset]`).

NOTE: Every call to `compute` does call Kafka brokers for the offsets.

The _moving_ parts of generated `KafkaRDD` instances are offsets. Others are taken directly from `DirectKafkaInputDStream` (given at the time of instantiation).

It then filters out empty offset ranges to build `StreamInputInfo` for link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker.reportInfo].

It sets the just-calculated offsets as current (using `currentOffsets`) and returns a new link:spark-streaming-kafka-KafkaRDD.adoc[KafkaRDD] instance.

=== [[consumer]] Creating Kafka Consumer -- `consumer` Method

[source, scala]
----
consumer(): Consumer[K, V]
----

`consumer` creates a Kafka `Consumer` with keys of type `K` and values of type `V` (specified when the <<creating-instance, `DirectKafkaInputDStream` is created>>).

`consumer` starts the link:spark-streaming-kafka-ConsumerStrategy.adoc#onStart[ConsumerStrategy] (that was used when the `DirectKafkaInputDStream` was created). It passes the internal collection of ``TopicPartition``s and their offsets.

CAUTION: FIXME A note with What `ConsumerStrategy` is for?

=== [[getPreferredHosts]] Calculating Preferred Hosts Using `LocationStrategy` -- `getPreferredHosts` Method

[source, scala]
----
getPreferredHosts: java.util.Map[TopicPartition, String]
----

`getPreferredHosts` calculates preferred hosts per topic partition (that are later used to map link:spark-streaming-kafka-KafkaRDD.adoc[KafkaRDD] partitions to host leaders of topic partitions that Spark executors read records from).

`getPreferredHosts` relies exclusively on the link:spark-streaming-kafka-LocationStrategy.adoc[LocationStrategy] that was passed in when <<creating-instance, creating a `DirectKafkaInputDStream` instance>>.

.DirectKafkaInputDStream.getPreferredHosts and Location Strategies
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Location Strategy | DirectKafkaInputDStream.getPreferredHosts
| `PreferBrokers`
| <<getBrokers, Calls Kafka broker(s) for topic partition assignments>>.

| `PreferConsistent`
| No host preference. Returns an empty collection of preferred hosts per topic partition.

It does not call Kafka broker(s) for topic assignments.

| `PreferFixed`
| Returns the preferred hosts that were passed in when `PreferFixed` was created.

It does not call Kafka broker(s) for topic assignments.
|======================

NOTE: `getPreferredHosts` is used when <<compute, creating a KafkaRDD for a batch interval>>.

==== [[getBrokers]] Requesting Partition Assignments from Kafka -- `getBrokers` Method

[source, scala]
----
getBrokers: ju.Map[TopicPartition, String]
----

`getBrokers` uses the internal <<consumer, Kafka Consumer>> instance to request Kafka broker(s) for partition assignments, i.e. the leader host per topic partition.

NOTE: `getBrokers` uses Kafka's  link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assignment()++[Consumer.assignment()].

=== [[stop]] Stopping DirectKafkaInputDStream -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` closes the current <<consumer, Kafka consumer>>.

NOTE: `stop` is a part of the link:spark-streaming-inputdstreams.adoc[InputDStream Contract].

=== [[back-pressure]] Back Pressure

CAUTION: FIXME

link:spark-streaming-backpressure.adoc[Back pressure] for Direct Kafka input dstream can be configured using link:spark-streaming-settings.adoc#back-pressure[spark.streaming.backpressure.enabled] setting.

NOTE: Back pressure is disabled by default.
