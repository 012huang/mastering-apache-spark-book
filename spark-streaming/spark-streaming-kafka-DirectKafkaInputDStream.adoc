== [[DirectKafkaInputDStream]] DirectKafkaInputDStream

`DirectKafkaInputDStream` is an link:spark-streaming-inputdstreams.adoc[input dstream] of link:spark-streaming-kafka-kafkardd.adoc[KafkaRDD] batches.

As an input dstream, `DirectKafkaInputDStream` implements the mandatory abstract methods (from link:spark-streaming-dstreams.adoc#contract[DStream Contract] and link:spark-streaming-inputdstreams.adoc#contract[InputDStream Contract]):

1. `dependencies: List[DStream[_]]` returns an empty collection, i.e. it has no dependencies on other streams (other than Kafka brokers to read data from).
2. `slideDuration: Duration` passes all calls on to link:spark-streaming-dstreamgraph.adoc[DStreamGraph.batchDuration].
3. `compute(validTime: Time): Option[RDD[T]]` - consult <<compute, Computing RDDs (using compute Method)>> section.
4. <<start, start>> to start polling for messages from Kafka.
5. <<stop, stop>> to close the Kafka consumer (and therefore polling for messages from Kafka).

The `name` of a `DirectKafkaInputDStream` is *Kafka 0.10 direct stream [id]* (that you can use to differentiate between the different implementations for Kafka 0.10+ and older releases).

TIP: You can find the name of a input dstream in the link:spark-streaming-webui.adoc[Streaming tab] in web UI (in the details of a batch in *Input Metadata* section).

It uses link:spark-streaming-settings.adoc[spark.streaming.kafka.maxRetries] setting while computing `latestLeaderOffsets` (i.e. a mapping of `kafka.common.TopicAndPartition` and <<LeaderOffset, LeaderOffset>>).

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.streaming.kafka010.DirectKafkaInputDStream` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.kafka010.DirectKafkaInputDStream=INFO
```

Refer to link:../spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating DirectKafkaInputDStream Instance

CAUTION: FIXME

=== [[executorKafkaParams]] `executorKafkaParams` Method

CAUTION: FIXME

=== [[start]] Starting DirectKafkaInputDStream -- `start` Method

[source, scala]
----
start(): Unit
----

`start` grabs the current <<consumer, Kafka consumer>> and polls (using Kafka's `Consumer.poll` and `0` as input).

NOTE: `start` is part of the link:spark-streaming-inputdstreams.adoc[InputDStream Contract].

After the polling, `start` checks if the internal `currentOffsets` is empty, and if it is, it requests Kafka for topic (using Kafka's `Consumer.assignment`) and builds a map with topics and their offsets (using Kafka's `Consumer.position`).

Ultimately, `start` pauses all partitions (using Kafka's `Consumer.pause` and the internal collection of topics and their current offsets).

=== [[consumer]] Creating Kafka Consumer -- `consumer` Method

[source, scala]
----
consumer(): Consumer[K, V]
----

`consumer` creates a Kafka `Consumer` with keys of type `K` and values of type `V` (specified when the <<creating-instance, `DirectKafkaInputDStream` is created>>).

`consumer` starts the link:spark-streaming-kafka-ConsumerStrategy.adoc#onStart[ConsumerStrategy] (that was used when the `DirectKafkaInputDStream` was created). It passes the internal collection of ``TopicPartition``s and their offsets.

CAUTION: FIXME A note with What `ConsumerStrategy` is for?

=== [[compute]] Computing RDDs -- `compute` Method

`DirectKafkaInputDStream.compute` _always_ computes a link:spark-streaming-kafka-kafkardd.adoc[KafkaRDD] instance (despite the link:spark-streaming-dstreams.adoc#contract[DStream contract] that says it may or may not generate one).

NOTE: It is link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph to request generating streaming jobs for batches].

Every time the method is called, `latestLeaderOffsets` calculates the latest offsets (as `Map[TopicAndPartition, LeaderOffset]`).

NOTE: Every call to `compute` does call Kafka brokers for the offsets.

The _moving_ parts of generated `KafkaRDD` instances are offsets. Others are taken directly from `DirectKafkaInputDStream` (given at the time of instantiation).

It then filters out empty offset ranges to build `StreamInputInfo` for link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker.reportInfo].

It sets the just-calculated offsets as current (using `currentOffsets`) and returns a new link:spark-streaming-kafka-kafkardd.adoc[KafkaRDD] instance.

=== [[stop]] Stopping DirectKafkaInputDStream -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` closes the current <<consumer, Kafka consumer>>.

NOTE: `stop` is a part of the link:spark-streaming-inputdstreams.adoc[InputDStream Contract].

=== [[back-pressure]] Back Pressure

CAUTION: FIXME

link:spark-streaming-backpressure.adoc[Back pressure] for Direct Kafka input dstream can be configured using link:spark-streaming-settings.adoc#back-pressure[spark.streaming.backpressure.enabled] setting.

NOTE: Back pressure is disabled by default.
