== [[DirectKafkaInputDStream]] DirectKafkaInputDStream -- Direct Kafka DStream

`DirectKafkaInputDStream` is an link:spark-streaming-inputdstreams.adoc[input dstream] of Kafka's https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html[ConsumerRecord]s that produces link:spark-streaming-kafka-KafkaRDD.adoc[KafkaRDD] batches.

NOTE: Kafka's https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html[ConsumerRecord] holds a topic name, a partition number, the offset of the record in the Kafka partition and the record itself (as a key-value pair).

`DirectKafkaInputDStream` is also a `CanCommitOffsets` object.

As an input dstream, `DirectKafkaInputDStream` implements the mandatory abstract methods (from link:spark-streaming-dstreams.adoc#contract[DStream Contract] and link:spark-streaming-inputdstreams.adoc#contract[InputDStream Contract]):

1. `dependencies: List[DStream[_]]` returns an empty collection, i.e. it has no dependencies on other streams (other than Kafka brokers to read data from).
2. `slideDuration: Duration` passes all calls on to link:spark-streaming-dstreamgraph.adoc[DStreamGraph.batchDuration].
3. `compute(validTime: Time): Option[RDD[T]]` - consult <<compute, Computing RDDs (using compute Method)>> section.
4. <<start, start>> to start polling for messages from Kafka.
5. <<stop, stop>> to close the Kafka consumer (and therefore polling for messages from Kafka).

The `name` of a `DirectKafkaInputDStream` is *Kafka 0.10 direct stream [id]* (that you can use to differentiate between the different implementations for Kafka 0.10+ and older releases).

TIP: You can find the name of a input dstream in the link:spark-streaming-webui.adoc[Streaming tab] in web UI (in the details of a batch in *Input Metadata* section).

It uses link:spark-streaming-settings.adoc[spark.streaming.kafka.maxRetries] setting while computing `latestLeaderOffsets` (i.e. a mapping of `kafka.common.TopicAndPartition` and <<LeaderOffset, LeaderOffset>>).

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.streaming.kafka010.DirectKafkaInputDStream` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.kafka010.DirectKafkaInputDStream=INFO
```

Refer to link:../spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating DirectKafkaInputDStream Instance

You can create a `DirectKafkaInputDStream` instance using link:spark-streaming-kafka-KafkaUtils.adoc#createDirectStream[KafkaUtils.createDirectStream] factory method.

[source, scala]
----
import org.apache.spark.streaming.kafka010.KafkaUtils

// WARN: Incomplete to show only relevant parts
val dstream = KafkaUtils.createDirectStream[String, String](
  ssc = streamingContext,
  locationStrategy = hosts,
  consumerStrategy = ConsumerStrategies.Subscribe[String, String](topics, kafkaParams, offsets))
----

Internally, when a `DirectKafkaInputDStream` instance is created, it initializes the internal <<executorKafkaParams, executorKafkaParams>> using the input ``consumerStrategy``'s link:spark-streaming-kafka-ConsumerStrategy.adoc#executorKafkaParams[executorKafkaParams].

TIP: Use link:spark-streaming-kafka-ConsumerStrategy.adoc[ConsumerStrategy] for a Kafka Consumer configuration.

With link:spark-streaming-kafka-KafkaUtils.adoc#logging[WARN logging level enabled for the KafkaUtils logger], you may see the following WARN messages and one ERROR in the logs (the number of messages depends on how correct the Kafka Consumer configuration is):

```
WARN KafkaUtils: overriding enable.auto.commit to false for executor
WARN KafkaUtils: overriding auto.offset.reset to none for executor
ERROR KafkaUtils: group.id is null, you should probably set it
WARN KafkaUtils: overriding executor group.id to spark-executor-null
WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
```

[TIP]
====
You should always set `group.id` in Kafka parameters for `DirectKafkaInputDStream`.

Refer to link:spark-streaming-kafka-ConsumerStrategy.adoc[ConsumerStrategy -- Kafka Consumers' Post-Configuration API].
====

It initializes the internal <<currentOffsets, currentOffsets>> property.

It creates an instance of `DirectKafkaInputDStreamCheckpointData` as `checkpointData`.

It sets up `rateController` as `DirectKafkaRateController` when backpressure is enabled.

It sets up `maxRateLimitPerPartition` as link:spark-streaming-settings.adoc#spark_streaming_kafka_maxRatePerPartition[spark.streaming.kafka.maxRatePerPartition].

It initializes <<commitQueue, commitQueue>> and <<commitCallback, commitCallback>> properties.

=== [[currentOffsets]] `currentOffsets` Property

[source, scala]
----
currentOffsets: Map[TopicPartition, Long]
----

`currentOffsets` holds the latest (highest) available offsets (as set by `latestOffsets` and <<compute, compute>>).

CAUTION: FIXME When are `latestOffsets` and `compute` called?

`currentOffsets` is initialized when <<creating-instance, `DirectKafkaInputDStream` is created afresh>> (it could also be re-created from a checkpoint).

The link:spark-streaming-kafka-ConsumerStrategy.adoc#onStart[ConsumerStrategy] (that was used to initialize `DirectKafkaInputDStream`) uses it to <<consumer, create a Kafka Consumer>>.

It is then set to the available offsets when  <<start, `DirectKafkaInputDStream` is started>>.

=== [[commitCallback]] `commitCallback` Property

[source, scala]
----
commitCallback: AtomicReference[OffsetCommitCallback]
----

`commitCallback` is initialized when <<creating-instance, `DirectKafkaInputDStream` is created>>. It is set to a `OffsetCommitCallback` that is the input parameter of `commitAsync` when it is called (as part of the `CanCommitOffsets` contract that `DirectKafkaInputDStream` implements).

=== [[commitQueue]] `commitQueue` Property

[source, scala]
----
commitQueue: ConcurrentLinkedQueue[OffsetRange]
----

`commitQueue` is initialized when <<creating-instance, `DirectKafkaInputDStream` is created>>. It is used in `commitAsync` (that is part of the `CanCommitOffsets` contract that `DirectKafkaInputDStream` implements) to queue up offsets for commit to Kafka at a future time (i.e. when the internal `commitAll` is called).

TIP: Read https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentLinkedQueue.html[java.util.concurrent.ConcurrentLinkedQueue] javadoc.

=== [[executorKafkaParams]] `executorKafkaParams` Method

CAUTION: FIXME

=== [[start]] Starting DirectKafkaInputDStream -- `start` Method

[source, scala]
----
start(): Unit
----

`start` grabs the current <<consumer, Kafka consumer>> and polls (using Kafka's `Consumer.poll` and `0` as input).

NOTE: `start` is part of the link:spark-streaming-inputdstreams.adoc[InputDStream Contract].

After the polling, `start` checks if the internal `currentOffsets` is empty, and if it is, it requests Kafka for topic (using Kafka's `Consumer.assignment`) and builds a map with topics and their offsets (using Kafka's `Consumer.position`).

Ultimately, `start` pauses all partitions (using Kafka's `Consumer.pause` and the internal collection of topics and their current offsets).

=== [[compute]] Computing KafkaRDD for Batch Interval -- `compute` Method

[source, scala]
----
compute(validTime: Time): Option[KafkaRDD[K, V]]
----

NOTE: `compute` is a part of the link:spark-streaming-dstreams.adoc[DStream Contract].

Despite the return type of `compute` (that allows for no RDDs), `DirectKafkaInputDStream` _always_ computes a link:spark-streaming-kafka-KafkaRDD.adoc[KafkaRDD].

NOTE: It is link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph to request generating streaming jobs for batches].

When a batch interval passes and `compute` is called, `latestLeaderOffsets` calculates the latest offsets (as `Map[TopicAndPartition, LeaderOffset]`).

NOTE: Every call to `compute` does call Kafka brokers for the offsets.

The _moving_ parts of generated `KafkaRDD` instances are offsets. Others are taken directly from `DirectKafkaInputDStream` (given at the time of instantiation).

It then filters out empty offset ranges to build `StreamInputInfo` for link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker.reportInfo].

It sets the just-calculated offsets as current (using `currentOffsets`) and returns a new link:spark-streaming-kafka-KafkaRDD.adoc[KafkaRDD] instance.

=== [[consumer]] Creating Kafka Consumer -- `consumer` Method

[source, scala]
----
consumer(): Consumer[K, V]
----

`consumer` creates a Kafka `Consumer` with keys of type `K` and values of type `V` (specified when the <<creating-instance, `DirectKafkaInputDStream` is created>>).

`consumer` starts the link:spark-streaming-kafka-ConsumerStrategy.adoc#onStart[ConsumerStrategy] (that was used when the `DirectKafkaInputDStream` was created). It passes the internal collection of ``TopicPartition``s and their offsets.

CAUTION: FIXME A note with What `ConsumerStrategy` is for?

=== [[getPreferredHosts]] Calculating Preferred Hosts Using `LocationStrategy` -- `getPreferredHosts` Method

[source, scala]
----
getPreferredHosts: java.util.Map[TopicPartition, String]
----

`getPreferredHosts` calculates preferred hosts per topic partition (that are later used to map link:spark-streaming-kafka-KafkaRDD.adoc[KafkaRDD] partitions to host leaders of topic partitions that Spark executors read records from).

`getPreferredHosts` relies exclusively on the link:spark-streaming-kafka-LocationStrategy.adoc[LocationStrategy] that was passed in when <<creating-instance, creating a `DirectKafkaInputDStream` instance>>.

.DirectKafkaInputDStream.getPreferredHosts and Location Strategies
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Location Strategy | DirectKafkaInputDStream.getPreferredHosts
| `PreferBrokers`
| <<getBrokers, Calls Kafka broker(s) for topic partition assignments>>.

| `PreferConsistent`
| No host preference. Returns an empty collection of preferred hosts per topic partition.

It does not call Kafka broker(s) for topic assignments.

| `PreferFixed`
| Returns the preferred hosts that were passed in when `PreferFixed` was created.

It does not call Kafka broker(s) for topic assignments.
|======================

NOTE: `getPreferredHosts` is used when <<compute, creating a KafkaRDD for a batch interval>>.

==== [[getBrokers]] Requesting Partition Assignments from Kafka -- `getBrokers` Method

[source, scala]
----
getBrokers: ju.Map[TopicPartition, String]
----

`getBrokers` uses the internal <<consumer, Kafka Consumer>> instance to request Kafka broker(s) for partition assignments, i.e. the leader host per topic partition.

NOTE: `getBrokers` uses Kafka's  link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assignment()++[Consumer.assignment()].

=== [[stop]] Stopping DirectKafkaInputDStream -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` closes the current <<consumer, Kafka consumer>>.

NOTE: `stop` is a part of the link:spark-streaming-inputdstreams.adoc[InputDStream Contract].

=== [[back-pressure]] Back Pressure

CAUTION: FIXME

link:spark-streaming-backpressure.adoc[Back pressure] for Direct Kafka input dstream can be configured using link:spark-streaming-settings.adoc#back-pressure[spark.streaming.backpressure.enabled] setting.

NOTE: Back pressure is disabled by default.
