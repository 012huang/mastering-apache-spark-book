== Ingesting Data from Apache Kafka

Spark Streaming comes with two ways of ingesting data from http://kafka.apache.org/[Apache Kafka]:

* Using receivers
* <<no-receivers, With no receivers>>

There is yet another "middle-ground" approach (so-called unofficial since it is not available by default in Spark Streaming):

* ...

=== [[no-receivers]] Data Ingestion with no Receivers

In this approach, *with no receivers*, you find two modes of ingesting data from Kafka:

* *Streaming mode* using `KafkaUtils.createDirectStream` that creates an link:spark-streaming-inputdstreams.adoc[input stream] that directly pulls messages from Kafka brokers (with no receivers). See <<streaming-mode, Streaming mode>> section.
* *Non-streaming mode* using `KafkaUtils.createRDD` that just creates a link:spark-streaming-kafka-kafkardd.adoc[KafkaRDD] of key-value pairs, i.e. `RDD[(K, V)]`.

==== [[streaming-mode]] Streaming mode

You create link:spark-streaming-kafka-DirectKafkaInputDStream.adoc[DirectKafkaInputDStream] using link:spark-streaming-kafka-KafkaUtils.adoc#createDirectStream[KafkaUtils.createDirectStream].

NOTE: Define the types of keys and values in `KafkaUtils.createDirectStream`, e.g. `KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder]`, so proper decoders are used to decode messages from Kafka.

You have to specify `metadata.broker.list` or `bootstrap.servers` (in that order of precedence) for your Kafka environment. `metadata.broker.list` is a comma-separated list of Kafka's (seed) brokers in the format of `<host>:<port>`.

NOTE: Kafka brokers have to be up and running _before_ you can create a direct stream.

[source, scala]
----
val conf = new SparkConf().setMaster("local[*]").setAppName("Ingesting Data from Kafka")
conf.set("spark.streaming.ui.retainedBatches", "5")

// Enable Back Pressure
conf.set("spark.streaming.backpressure.enabled", "true")

val ssc = new StreamingContext(conf, batchDuration = Seconds(5))

// Enable checkpointing
ssc.checkpoint("_checkpoint")

// You may or may not want to enable some additional DEBUG logging
import org.apache.log4j._
Logger.getLogger("org.apache.spark.streaming.dstream.DStream").setLevel(Level.DEBUG)
Logger.getLogger("org.apache.spark.streaming.dstream.WindowedDStream").setLevel(Level.DEBUG)
Logger.getLogger("org.apache.spark.streaming.DStreamGraph").setLevel(Level.DEBUG)
Logger.getLogger("org.apache.spark.streaming.scheduler.JobGenerator").setLevel(Level.DEBUG)

// Connect to Kafka
import org.apache.spark.streaming.kafka.KafkaUtils
import _root_.kafka.serializer.StringDecoder
val kafkaParams = Map("metadata.broker.list" -> "localhost:9092")
val kafkaTopics = Set("spark-topic")
val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, kafkaTopics)

// print 10 last messages
messages.print()

// start streaming computation
ssc.start
----

If `zookeeper.connect` or `group.id` parameters are not set, they are added with their values being empty strings.

In this mode, you will only see jobs submitted (in the *Jobs* tab in link:spark-webui.adoc[web UI]) when a message comes in.

.Complete Jobs in web UI for batch time 22:17:15
image::../images/spark-streaming-kafka-webui-jobs.png[align="center"]

It corresponds to *Input size* larger than `0` in the *Streaming* tab in the web UI.

.Completed Batch in web UI for batch time 22:17:15
image::../images/spark-streaming-kafka-webui-streaming.png[align="center"]

Click the link in Completed Jobs for a batch and you see the details.

.Details of batch in web UI for batch time 22:17:15
image::../images/spark-streaming-kafka-webui-details-batch.png[align="center"]

=== `spark-streaming-kafka-0-10` Library Dependency

The new API for both Kafka RDD and DStream is in the `spark-streaming-kafka` artifact. Add the following dependency to sbt project to use the streaming integration:

```
libraryDependencies += "org.apache.spark" %% "spark-streaming-kafka-0-10" % "2.0.0"
```

NOTE: Replace `"2.0.0"` with available version as found at http://search.maven.org/#search%7Cga%7C1%7Ca%3A%22spark-streaming-kafka-0-10_2.11%22[The Central Repository's search].

=== Kafka Concepts

* `broker`
* `leader`
* `topic`
* `partition`
* `offset`
* `exactly-once semantics`
* `Kafka high-level consumer`

=== [[LeaderOffset]] LeaderOffset

`LeaderOffset` is an internal class to represent an offset on the topic partition on the broker that works on a host and a port.

=== Recommended Reading

* http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/[Exactly-once Spark Streaming from Apache Kafka]
