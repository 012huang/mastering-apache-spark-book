== [[InputInfoTracker]] InputInfoTracker

`InputInfoTracker` tracks batch times and batch statistics for link:spark-streaming-inputdstreams.adoc[input streams] (per input stream id with `StreamInputInfo`). It is later used when link:spark-streaming-jobgenerator.adoc#GenerateJobs[JobGenerator submits streaming jobs for a batch interval] (and propagated to interested listeners as link:spark-streaming-streaminglisteners.adoc#StreamingListenerEvent[StreamingListenerBatchSubmitted] event).

NOTE: `InputInfoTracker` is managed by `JobScheduler`, i.e. it is created when link:spark-streaming-jobscheduler.adoc#starting[JobScheduler starts] and link:spark-streaming-jobscheduler.adoc#stopping[is stopped alongside].

`InputInfoTracker` uses internal registry `batchTimeToInputInfos` to maintain the mapping of batch times and link:spark-streaming-inputdstreams.adoc[input streams] (i.e. another mapping between input stream ids and `StreamInputInfo`).

It accumulates batch statistics at every batch time when link:spark-streaming-dstreams.adoc#contract[input streams are computing RDDs] (and call <<reportInfo, reportInfo>>).

[NOTE]
====
It is up to input streams to have these batch statistics collected (and requires calling <<reportInfo, reportInfo>> method explicitly).

The following input streams report information:

* link:spark-streaming-kafka-DirectKafkaInputDStream.adoc[DirectKafkaInputDStream]
* link:spark-streaming-receiverinputdstreams.adoc[ReceiverInputDStreams - Input Streams with Receivers]
* FileInputDStream
====

=== [[reportInfo]] `reportInfo` Method

CAUTION: FIXME

=== [[cleanup]] Cleaning up -- `cleanup` Method

[source, scala]
----
cleanup(batchThreshTime: Time): Unit
----

You should see the following INFO message when `cleanup` of old batch times is requested (akin to _garbage collection_):

```
INFO InputInfoTracker: remove old batch metadata: [timesToCleanup]
```

CAUTION: FIXME When is this called?
