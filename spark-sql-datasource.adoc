== [[DataSource]] DataSource -- Pluggable Data Sources

`DataSource` belongs to the Data Source API (along with link:spark-sql-DataFrameReader.adoc[DataFrameReader] for loading datasets, link:spark-sql-dataframewriter.adoc[DataFrameWriter] for saving datasets and `StreamSourceProvider` for creating streaming sources).

`DataSource` is an internal class that represents a **pluggable data source** in Spark SQL with few extension points to further enrich the capabilities of Spark SQL.

.DataSource's Extension Points
[cols="1,3",options="header",width="100%"]
|===
| Extension Point
| Description

| [[CreatableRelationProvider]] `CreatableRelationProvider`
a| Used in:

* <<write, write>> for writing a `DataFrame` to a `DataSource` (as part of creating a table as select)

* <<writeAndRead, writeAndRead>>

| `FileFormat`
a| Used in:

* <<sourceSchema, sourceSchema>> for streamed reading

* <<write, write>> for writing a `DataFrame` to a `DataSource` (as part of creating a table as select)

| [[RelationProvider]] `RelationProvider`
|

| `StreamSourceProvider`
a| Used in:

* <<sourceSchema, sourceSchema>> and <<createSource, createSource>> for streamed reading

* <<createSink, createSink>> for streamed writing

* <<resolveRelation, resolveRelation>> for resolved link:spark-sql-BaseRelation.adoc[BaseRelation].
|===

As a user, you interact with `DataSource` by link:spark-sql-DataFrameReader.adoc[DataFrameReader] (when you execute link:spark-sql-SparkSession.adoc#read[spark.read] or link:spark-sql-SparkSession.adoc#readStream[spark.readStream]) or `CREATE TABLE USING DDL`.

[source, scala]
----
// Batch reading
val people: DataFrame = spark.read
  .format("csv")
  .load("people.csv")

// Streamed reading
val messages: DataFrame = spark.readStream
  .format("kafka")
  .option("subscribe", "topic")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load
----

`DataSource` uses a link:spark-sql-SparkSession.adoc[SparkSession], a class name, a collection of `paths`, optional user-specified link:spark-sql-schema.adoc[schema], a collection of partition columns, a bucket specification, and configuration options.

=== [[writeAndRead]] `writeAndRead` Method

[source, scala]
----
writeAndRead(mode: SaveMode, data: DataFrame): BaseRelation
----

CAUTION: FIXME

NOTE: `writeAndRead` is used exclusively when link:spark-sql-LogicalPlan-RunnableCommand.adoc#CreateDataSourceTableAsSelectCommand[CreateDataSourceTableAsSelectCommand] is executed.

=== [[providingClass]] `providingClass` Property

CAUTION: FIXME

=== [[write]] Writing DataFrame to Data Source Per Save Mode -- `write` Method

[source, scala]
----
write(mode: SaveMode, data: DataFrame): BaseRelation
----

`write` writes the result of executing a structured query (as link:spark-sql-dataframe.adoc[DataFrame]) to a data source per save `mode`.

Internally, `write` <<lookupDataSource, looks up the data source>> and branches off per <<providingClass, providingClass>>.

[[write-providingClass-branches]]
.write's Branches per Supported providingClass (in execution order)
[width="100%",cols="1,2",options="header"]
|===
| providingClass
| Description

| <<CreatableRelationProvider, CreatableRelationProvider>>
| Executes `CreatableRelationProvider.createRelation`

| `FileFormat`
| <<writeInFileFormat, writeInFileFormat>>

| _others_
| Reports a `RuntimeException`
|===

NOTE: `write` does not support the internal `CalendarIntervalType` in the link:spark-sql-schema.adoc[schema of `data` `DataFrame`] and throws a `AnalysisException` when there is one.

NOTE: `write` is used exclusively when link:spark-sql-LogicalPlan-RunnableCommand.adoc#SaveIntoDataSourceCommand[SaveIntoDataSourceCommand] is executed.

=== [[writeInFileFormat]] `writeInFileFormat` Internal Method

CAUTION: FIXME

For `FileFormat` data sources, `write` takes all `paths` and `path` option and makes sure that there is only one.

NOTE: `write` uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/Path.html[Path] to access the https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem] and calculate the qualified output path.

`write` does `PartitioningUtils.validatePartitionColumn`.

CAUTION: FIXME What is `PartitioningUtils.validatePartitionColumn` for?

When appending to a table, ...FIXME

In the end, `write` (for a `FileFormat` data source) link:spark-sql-SessionState.adoc#executePlan[prepares a `InsertIntoHadoopFsRelationCommand` logical plan] with link:spark-sql-QueryExecution.adoc#toRdd[executes] it.

CAUTION: FIXME Is `toRdd` a job execution?

=== [[createSource]] `createSource` Method

[source, scala]
----
createSource(metadataPath: String): Source
----

CAUTION: FIXME

=== [[createSink]] `createSink` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `DataSource` Instance

[source, scala]
----
class DataSource(
  sparkSession: SparkSession,
  className: String,
  paths: Seq[String] = Nil,
  userSpecifiedSchema: Option[StructType] = None,
  partitionColumns: Seq[String] = Seq.empty,
  bucketSpec: Option[BucketSpec] = None,
  options: Map[String, String] = Map.empty,
  catalogTable: Option[CatalogTable] = None)
----

When being created, `DataSource` first <<lookupDataSource, looks up the providing class>> given `className` (considering it an alias or a fully-qualified class name) and computes the <<sourceSchema, name and schema>> of the data source.

NOTE: `DataSource` does the initialization lazily on demand and only once.

==== [[sourceSchema]] `sourceSchema` Internal Method

[source, scala]
----
sourceSchema(): SourceInfo
----

`sourceSchema` returns the name and link:spark-sql-schema.adoc[schema] of the data source for streamed reading.

CAUTION: FIXME Why is the method called? Why does this bother with streamed reading and data sources?!

It supports two class hierarchies, i.e. `FileFormat` and Structured Streaming's `StreamSourceProvider` data sources.

Internally, `sourceSchema` first creates an instance of the data source and...

CAUTION: FIXME Finish...

For Structured Streaming's `StreamSourceProvider` data sources, `sourceSchema` relays calls to `StreamSourceProvider.sourceSchema`.

For `FileFormat` data sources, `sourceSchema` makes sure that `path` option was specified.

TIP: `path` is looked up in a case-insensitive way so `paTh` and `PATH` and `pAtH` are all acceptable. Use the lower-case version of `path`, though.

NOTE: `path` can use https://en.wikipedia.org/wiki/Glob_%28programming%29[glob pattern] (not regex syntax), i.e. contain any of `{}[]*?\` characters.

It checks whether the path exists if a glob pattern is not used. In case it did not exist you will see the following `AnalysisException` exception in the logs:

```
scala> spark.read.load("the.file.does.not.exist.parquet")
org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/jacek/dev/oss/spark/the.file.does.not.exist.parquet;
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:375)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:364)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
  at scala.collection.immutable.List.flatMap(List.scala:344)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:364)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:132)
  ... 48 elided
```

If link:spark-sql-SQLConf.adoc#spark.sql.streaming.schemaInference[spark.sql.streaming.schemaInference] is disabled and the data source is different than `TextFileFormat`, and the input `userSpecifiedSchema` is not specified, the following `IllegalArgumentException` exception is thrown:

[options="wrap"]
----
Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.
----

CAUTION: FIXME I don't think the exception will ever happen for non-streaming sources since the schema is going to be defined earlier. When?

Eventually, it returns a `SourceInfo` with `FileSource[path]` and the schema (as calculated using the <<inferFileFormatSchema, inferFileFormatSchema>> internal method).

For any other data source, it throws `UnsupportedOperationException` exception:

```
Data source [className] does not support streamed reading
```

==== [[inferFileFormatSchema]] `inferFileFormatSchema` Internal Method

[source, scala]
----
inferFileFormatSchema(format: FileFormat): StructType
----

`inferFileFormatSchema` private method computes (aka _infers_) schema (as link:spark-sql-StructType.adoc[StructType]). It returns `userSpecifiedSchema` if specified or uses `FileFormat.inferSchema`. It throws a `AnalysisException` when is unable to infer schema.

It uses `path` option for the list of directory paths.

NOTE: It is used by <<sourceSchema, DataSource.sourceSchema>> and <<createSource, DataSource.createSource>> when `FileFormat` is processed.

==== [[lookupDataSource]] `lookupDataSource` Internal Method

[source, scala]
----
lookupDataSource(provider0: String): Class[_]
----

Internally, `lookupDataSource` first searches the classpath for available link:spark-sql-DataSourceRegister.adoc[DataSourceRegister] providers (using Java's link:++https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load] method) to find the requested data source by short name (alias), e.g. `parquet` or `kafka`.

If a `DataSource` could not be found by short name, `lookupDataSource` tries to load the class given the input `provider0` or its variant `provider0.DefaultSource` (with `.DefaultSource` suffix).

NOTE: You can reference your own custom `DataSource` in your code by link:spark-sql-dataframewriter.adoc#format[DataFrameWriter.format] method which is the alias or fully-qualified class name.

There has to be one data source registered only or you will see the following `RuntimeException`:

[options="wrap"]
----
Multiple sources found for [provider] ([comma-separated class names]), please specify the fully qualified class name.
----

=== [[resolveRelation]] Creating `BaseRelation` for Reading or Writing -- `resolveRelation` Method

[source, scala]
----
resolveRelation(checkFilesExist: Boolean = true): BaseRelation
----

`resolveRelation` resolves (i.e. creates) a link:spark-sql-BaseRelation.adoc[BaseRelation] to read from or write to a `DataSource`.

Internally, `resolveRelation` creates an instance of `providingClass` (for a `DataSource`) and acts according to its type, i.e. `SchemaRelationProvider`, `RelationProvider` or `FileFormat`.

.`resolveRelation` and Resolving `BaseRelation` per (Schema) Providers
[cols="1,3",options="header",width="100%"]
|===
| Provider | Behaviour
| `SchemaRelationProvider` | Executes `SchemaRelationProvider.createRelation` with the provided schema.

| `RelationProvider` | Executes `RelationProvider.createRelation`.

| `FileFormat` | Creates a link:spark-sql-BaseRelation.adoc#HadoopFsRelation[HadoopFsRelation].
|===
