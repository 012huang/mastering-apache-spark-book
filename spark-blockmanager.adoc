== Block Manager

*Block Manager* is a key-value store for blocks of data. It acts as a local cache that runs on every node in Spark cluster, i.e. a link:spark-driver.adoc[driver] and link:spark-executor.adoc[executors]. It provides interface for putting and retrieving blocks both locally and remotely into various stores, i.e. memory, disk, and off-heap. See <<stores, Stores>> in this document.

A `BlockManager` manages the storage for blocks that represents a cached RDD partition, an intermediate shuffle data, a broadcast data, etc. See <<BlockId, BlockId>> in this document.

*Cached blocks* are blocks with non-zero sum of memory and disk sizes.

It is created when a Spark application starts (as part of link:spark-sparkenv.adoc#create[SparkEnv.create]).

A BlockManager must be <<initialize,initialized>> before it is fully operable.

A BlockManager relies on the following services:

* RpcEnv
* BlockManagerMaster
* Serializer
* MemoryManager
* link:spark-service-mapoutputtracker.adoc[MapOutputTracker]
* link:spark-shuffle-manager.adoc[ShuffleManager]
* <<BlockTransferService, BlockTransferService>>
* SecurityManager

BlockManager is a BlockDataManager.

CAUTION: FIXME Review `BlockDataManager`

[TIP]
====
Enable `TRACE` logging level for `org.apache.spark.storage.BlockManager` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManager=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

[TIP]
====
You may want to shut off WARN messages being printed out about the current state of blocks using the following line to cut the noise:

```
log4j.logger.org.apache.spark.storage.BlockManager=OFF
```
====

=== [[stores]] Stores

A *Store* is the place where blocks are held.

There are the following possible stores:

* `MemoryStore` for memory storage level.
* `DiskStore` for disk storage level.
* `ExternalBlockStore` for OFF_HEAP storage level.

==== [[MemoryStore]] MemoryStore

CAUTION: FIXME

==== [[DiskStore]] DiskStore

CAUTION: FIXME

=== [[doPutBytes]] doPutBytes

CAUTION: FIXME

=== [[doPutIterator]] doPutIterator

CAUTION: FIXME

=== [[removeBlock]] Removing Blocks From Memory and Disk (removeBlock method)

CAUTION: FIXME

=== [[dropFromMemory]] Removing Blocks From Memory Only (dropFromMemory method)

CAUTION: FIXME

=== [[initialize]] Initializing BlockManager (initialize method)

[source, scala]
----
initialize(appId: String): Unit
----

`initialize` method is called to initialize a `BlockManager` instance.

NOTE: The method must be called before a BlockManager can be fully operable.

It does the following:

* It initializes <<BlockTransferService, BlockTransferService>>
* It initializes a shuffle client, be it <<ExternalShuffleClient, ExternalShuffleClient>> or <<BlockTransferService, BlockTransferService>>.
* It creates an instance of <<BlockManagerId, BlockManagerId>> given an executor id, host name and port for BlockTransferService.
* It creates the address of the server that serves this executor's shuffle files (using `shuffleServerId`)

If an external shuffle service is used, the following INFO appears in the logs:

```
INFO external shuffle service port = [externalShuffleServicePort]
```

* It registers itself to <<BlockManagerMaster, BlockManagerMaster>> (using <<registerBlockManager, BlockManagerMaster.registerBlockManager>>) passing the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the <<BlockManagerSlaveEndpoint, BlockManagerSlaveEndpoint>>.

* At the end, if an external shuffle service is used, and it is not a driver, it registers to the external shuffle service.

While registering to the external shuffle service, you should see the following INFO message in the logs:

```
INFO Registering executor with local external shuffle service.
```

Using `shuffleClient` (that is <<ExternalShuffleClient, ExternalShuffleClient>>) it calls `registerWithShuffleServer` synchronously using `shuffleServerId` and a ExecutorShuffleInfo (based on <<DiskBlockManager, DiskBlockManager>> for the executor and the short name of link:spark-shuffle-manager.adoc[ShuffleManager]).

Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs:

```
ERROR Failed to connect to external shuffle server, will retry [attempts] more times after waiting [SLEEP_TIME_SECS] seconds...
```

=== [[reregister]] Re-registering Blocks to Master (reregister method)

[source, scala]
----
reregister(): Unit
----

When is called, you should see the following INFO in the logs:

```
INFO BlockManager: BlockManager re-registering with master
```

It registers itself to the driver's <<BlockManagerMaster, BlockManagerMaster>> (using <<registerBlockManager, BlockManagerMaster.registerBlockManager>> just like it has happened while <<initialize, BlockManager was initializing>>). It passes the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the <<BlockManagerSlaveEndpoint, BlockManagerSlaveEndpoint>>.

CAUTION: FIXME Where is `maxMemory` used once passed to the driver?

`reregister` will then report all the local blocks to the <<BlockManagerMaster, BlockManagerMaster>>.

You should see the following INFO message in the logs:

```
INFO BlockManager: Reporting [blockInfoManager.size] blocks to the master.
```

For each block metadata (in `BlockInfoManager`) it <<getCurrentBlockStatus, gets block current status>> and <<tryToReportBlockStatus, sends it to the BlockManagerMaster>>.

If there is an issue communicating to the `BlockManagerMaster`, you should see the following ERROR message in the logs:

```
ERROR BlockManager: Failed to report [blockId] to master; giving up.
```

After the ERROR message, `reregister` stops reporting.

NOTE: `reregister` is called by link:spark-executor.adoc#heartbeats-and-active-task-metrics[Executor when it was told to re-register while sending heartbeats].

=== [[getCurrentBlockStatus]] Calculate Current Block Status (getCurrentBlockStatus method)

[source, scala]
----
getCurrentBlockStatus(blockId: BlockId, info: BlockInfo): BlockStatus
----

`getCurrentBlockStatus` returns the current `BlockStatus` of the `BlockId` block (with the block's current link:spark-rdd-caching.adoc#StorageLevel[StorageLevel], memory and disk sizes). It uses <<MemoryStore, MemoryStore>> and <<DiskStore, DiskStore>> for size and other information.

NOTE: Most of the information to build `BlockStatus` is already in `BlockInfo` except that it may not necessarily reflect the current state per <<MemoryStore, MemoryStore>> and <<DiskStore, DiskStore>>.

Internally, it uses the input `BlockInfo` to know about the block's storage level. If the storage level is not set (i.e. `null`), the returned `BlockStatus` assumes the link:spark-rdd-caching.adoc#StorageLevel[default NONE storage level] and the memory and disk sizes being `0`.

If however the storage level is set, `getCurrentBlockStatus` uses <<MemoryStore, MemoryStore>> or <<DiskStore, DiskStore>> to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their `getSize` or assume `0`).

NOTE: It is acceptable that the `BlockInfo` says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status.

NOTE: `getCurrentBlockStatus` is used when <<reregister, executor's BlockManager is requested to report the current status of the local blocks to the master>>, <<doPutBytes, saving a block to a storage>> or <<dropFromMemory, removing a block from memory only>> or <<removeBlock, both, i.e. from memory and disk>>.

=== [[BlockManagerSlaveEndpoint]] BlockManagerSlaveEndpoint

`BlockManagerSlaveEndpoint` is a RPC endpoint for remote communication between workers and the driver.

When a BlockManager is created, it sets up the RPC endpoint with the name *BlockManagerEndpoint[randomId]* and `BlockManagerSlaveEndpoint` as the class to handle <<BlockManagerSlaveEndpoint-messages, RPC messages>>.

==== [[BlockManagerSlaveEndpoint-messages]] RPC Messages

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.storage.BlockManagerSlaveEndpoint` logger to see what happens in BlockManagerSlaveEndpoint.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=DEBUG
```
====

`BlockManagerSlaveEndpoint` accepts the following RPC messages. The processing is slow and hence is deliberately done asynchronously (on a separate thread).

* `RemoveBlock(blockId)` to remove a block `blockId`. It calls `BlockManager.removeBlock`.

* `RemoveRdd(rddId)` to remove a RDD `rddId`. It calls `BlockManager.removeRdd`.

* `RemoveShuffle(shuffleId)` to remove a shuffle `shuffleId`. It unregisters it from MapOutputTracker if available (using  `MapOutputTracker.unregisterShuffle`). It calls ShuffleManager to unregister the shuffle (using `ShuffleManager.unregisterShuffle`).

* `RemoveBroadcast(broadcastId, _)` to remove a broadcast `broadcastId`. It calls `BlockManager.removeBroadcast`.

* `GetBlockStatus(blockId, _)` to return the status of a block `blockId` (using `BlockManager.getStatus`).

* `GetMatchingBlockIds(filter, _)` to return the matching block ids for `filter` (using `BlockManager.getMatchingBlockIds`).

* `TriggerThreadDump` to get a thread dump of all threads (using `Utils.getThreadDump()`).

=== [[BlockTransferService]] BlockTransferService

CAUTION: FIXME

=== [[ExternalShuffleClient]] ExternalShuffleClient

CAUTION: FIXME

=== [[BlockId]] BlockId

*BlockId* identifies a block of data. It has a globally unique identifier (`name`)

There are the following types of `BlockId`:

* *RDDBlockId* - described by `rddId` and `splitIndex`
* *ShuffleBlockId* - described by `shuffleId`, `mapId` and `reduceId`
* *ShuffleDataBlockId* - described by `shuffleId`, `mapId` and `reduceId`
* *ShuffleIndexBlockId* - described by `shuffleId`, `mapId` and `reduceId`
* *BroadcastBlockId* - described by `broadcastId` and optional `field` - a piece of broadcast value
* *TaskResultBlockId* - described by `taskId`
* *StreamBlockId* - described by `streamId` and `uniqueId`

=== [[broadcast]] Broadcast Values

When a new broadcast value is created, `TorrentBroadcast` - the default implementation of `Broadcast` - blocks are put in the block manager. See link:spark-service-broadcastmanager.adoc#TorrentBroadcast[TorrentBroadcast].

You should see the following `TRACE` message:

```
TRACE Put for block [blockId] took [startTimeMs] to get into synchronized block
```

It puts the data in the memory first and drop to disk if the memory store can't hold it.

```
DEBUG Put block [blockId] locally took [startTimeMs]
```

=== [[BlockManagerMaster]] BlockManagerMaster

CAUTION: FIXME

*BlockManagerMaster* is the Block Manager that runs on the driver only. It registers itself as `BlockManagerMaster` endpoint in link:spark-rpc.adoc[RPC Environment].

==== [[registerBlockManager]] BlockManagerMaster.registerBlockManager

CAUTION: FIXME

==== [[BlockManagerMasterEndpoint]] BlockManagerMasterEndpoint

CAUTION: FIXME

*BlockManagerMasterEndpoint* is the RPC endpoint for <<BlockManagerMaster, BlockManagerMaster>> on the master node to track statuses of all slaves' block managers.

The following two-way events are handled:

* RegisterBlockManager
* UpdateBlockInfo
* GetLocations
* GetLocationsMultipleBlockIds
* GetPeers
* GetRpcHostPortForExecutor
* GetMemoryStatus
* GetStorageStatus
* GetBlockStatus
* GetMatchingBlockIds
* RemoveRdd
* RemoveShuffle
* RemoveBroadcast
* RemoveBlock
* RemoveExecutor
* StopBlockManagerMaster
* BlockManagerHeartbeat
* HasCachedBlocks

=== [[BlockManagerId]] BlockManagerId

FIXME

=== [[DiskBlockManager]] DiskBlockManager

DiskBlockManager creates and maintains the logical mapping between logical blocks and physical on-disk locations.

By default, one block is mapped to one file with a name given by its BlockId. It is however possible to have a block map to only a segment of a file.

Block files are hashed among the directories listed in `spark.local.dir` (or in `SPARK_LOCAL_DIRS` if set).

CAUTION: FIXME Review me.

=== [[execution-context]] Execution Context

*block-manager-future* is the execution context for...FIXME

=== [[metrics]] Metrics

Block Manager uses link:spark-metrics.adoc[Spark Metrics System] (via `BlockManagerSource`) to report metrics about internal status.

The name of the source is *BlockManager*.

It emits the following numbers:

* memory / maxMem_MB - the maximum memory configured
* memory / remainingMem_MB - the remaining memory
* memory / memUsed_MB - the memory used
* memory / diskSpaceUsed_MB - the disk used

=== Misc

The underlying abstraction for blocks in Spark is a `ByteBuffer` that limits the size of a block to 2GB (`Integer.MAX_VALUE` - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for `long`), ser-deser via byte array-backed output streams.

When a non-local executor starts, it initializes a `BlockManager` object for the `spark.app.id` id.

If a task result is bigger than the message frame size - `spark.akka.frameSize` - executors use the block manager to send the result back. Task results are configured using `spark.driver.maxResultSize` (default: `1g`).

=== [[settings]] Settings

* `spark.shuffle.service.enabled` (default: `false`) whether an external shuffle service is enabled or not. See link:spark-shuffle-manager.adoc#external-shuffle-service[External Shuffle Service].

* `spark.broadcast.compress` (default: `true`) whether to compress stored broadcast variables.

* `spark.shuffle.compress` (default: `true`) whether to compress stored shuffle output.

* `spark.rdd.compress` (default: `false`) whether to compress RDD partitions that are stored serialized.

* `spark.shuffle.spill.compress` (default: `true`) whether to compress shuffle output temporarily spilled to disk.
