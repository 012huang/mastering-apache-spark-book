== Block Manager

*Block Manager* is a key-value store for blocks that acts as a cache. It runs on every node, i.e. a driver and executors, in a Spark runtime environment. It provides interfaces for putting and retrieving blocks both locally and remotely into various stores, i.e. memory, disk, and off-heap.

A BlockManager manages the storage for most of the data in Spark, i.e. block that represent a cached RDD partition, intermediate shuffle data, broadcast data, etc. See <<BlockId, BlockId>>.

It is created when a Spark application starts, i.e. as part of link:spark-sparkenv.adoc#create[SparkEnv.create()].

A BlockManager must be <<initialize,initialized>> before it is fully operable.

A BlockManager relies on the following services:

* RpcEnv
* BlockManagerMaster
* Serializer
* MemoryManager
* link:spark-service-mapoutputtracker.adoc[MapOutputTracker]
* ShuffleManager
* <<BlockTransferService, BlockTransferService>>
* SecurityManager

BlockManager is a BlockDataManager.

CAUTION: FIXME Review `BlockDataManager`

[TIP]
====
Enable `TRACE` logging level for `org.apache.spark.storage.BlockManager` logger to see what happens in BlockManager.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManager=TRACE
```
====

[TIP]
====
You may want to shut off WARN messages being printed out about the current state of blocks using the following line:

```
log4j.logger.org.apache.spark.storage.BlockManager=OFF
```
====

=== [[initialize]] BlockManager.initialize

`initialize(appId: String)` method is called to initialize a BlockManager instance.

NOTE: The method must be called before a BlockManager can be fully operable.

It does the following:

* It initializes <<BlockTransferService, BlockTransferService>>
* It initializes a shuffle client, be it <<ExternalShuffleClient, ExternalShuffleClient>> or <<BlockTransferService, BlockTransferService>>.
* It creates an instance of <<BlockManagerId, BlockManagerId>> given an executor id, host name and port for BlockTransferService.
* It creates the address of the server that serves this executor's shuffle files (using `shuffleServerId`)

If an external shuffle service is used, the following INFO appears in the logs:

```
INFO external shuffle service port = [externalShuffleServicePort]
```

* It registers itself to <<BlockManagerMaster, BlockManagerMaster>> (using <<registerBlockManager, BlockManagerMaster.registerBlockManager>>).

* At the end, if an external shuffle service is used, and it is not a driver, it registers to the external shuffle service.

While registering to the external shuffle service, you should see the following INFO message in the logs:

```
INFO Registering executor with local external shuffle service.
```

Using `shuffleClient` (that is <<ExternalShuffleClient, ExternalShuffleClient>>) it calls `registerWithShuffleServer` synchronously using `shuffleServerId` and a ExecutorShuffleInfo (based on <<DiskBlockManager, DiskBlockManager>> for the executor and the short name of link:spark-shuffle-manager.adoc[ShuffleManager]).

Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs:

```
ERROR Failed to connect to external shuffle server, will retry [attempts] more times after waiting [SLEEP_TIME_SECS] seconds...
```

=== [[BlockManagerSlaveEndpoint]] BlockManagerSlaveEndpoint

`BlockManagerSlaveEndpoint` is a RPC endpoint for remote communication between workers and the driver.

When a BlockManager is created, it sets up the RPC endpoint with the name *BlockManagerEndpoint[randomId]* and `BlockManagerSlaveEndpoint` as the class to handle <<BlockManagerSlaveEndpoint-messages, RPC messages>>.

==== [[BlockManagerSlaveEndpoint-messages]] RPC Messages

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.storage.BlockManagerSlaveEndpoint` logger to see what happens in BlockManagerSlaveEndpoint.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=DEBUG
```
====

`BlockManagerSlaveEndpoint` accepts the following RPC messages. The processing is slow and hence is deliberately done asynchronously (on a separate thread).

* `RemoveBlock(blockId)` to remove a block `blockId`. It calls `BlockManager.removeBlock`.

* `RemoveRdd(rddId)` to remove a RDD `rddId`. It calls `BlockManager.removeRdd`.

* `RemoveShuffle(shuffleId)` to remove a shuffle `shuffleId`. It unregisters it from MapOutputTracker if available (using  `MapOutputTracker.unregisterShuffle`). It calls ShuffleManager to unregister the shuffle (using `ShuffleManager.unregisterShuffle`).

* `RemoveBroadcast(broadcastId, _)` to remove a broadcast `broadcastId`. It calls `BlockManager.removeBroadcast`.

* `GetBlockStatus(blockId, _)` to return the status of a block `blockId` (using `BlockManager.getStatus`).

* `GetMatchingBlockIds(filter, _)` to return the matching block ids for `filter` (using `BlockManager.getMatchingBlockIds`).

* `TriggerThreadDump` to get a thread dump of all threads (using `Utils.getThreadDump()`).

=== [[BlockTransferService]] BlockTransferService

CAUTION: FIXME

=== [[ExternalShuffleClient]] ExternalShuffleClient

CAUTION: FIXME

=== [[BlockId]] BlockId

*BlockId* identifies a block of data. It has a globally unique identifier (`name`)

There are the following types of `BlockId`:

* *RDDBlockId* - described by `rddId` and `splitIndex`
* *ShuffleBlockId* - described by `shuffleId`, `mapId` and `reduceId`
* *ShuffleDataBlockId* - described by `shuffleId`, `mapId` and `reduceId`
* *ShuffleIndexBlockId* - described by `shuffleId`, `mapId` and `reduceId`
* *BroadcastBlockId* - described by `broadcastId` and optional `field` - a piece of broadcast value
* *TaskResultBlockId* - described by `taskId`
* *StreamBlockId* - described by `streamId` and `uniqueId`

=== [[broadcast]] Broadcast Values

When a new broadcast value is created, `TorrentBroadcast` - the default implementation of `Broadcast` - blocks are put in the block manager. See link:spark-service-broadcastmanager.adoc#TorrentBroadcast[TorrentBroadcast].

You should see the following `TRACE` message:

```
TRACE Put for block [blockId] took [startTimeMs] to get into synchronized block
```

It puts the data in the memory first and drop to disk if the memory store can't hold it.

```
DEBUG Put block [blockId] locally took [startTimeMs]
```

=== [[stores]] Stores

A *Store* is the place where blocks are held.

There are the following possible stores:

* `MemoryStore` for memory storage level.
* `DiskStore` for disk storage level.
* `ExternalBlockStore` for OFF_HEAP storage level.

=== [[BlockManagerMaster]] BlockManagerMaster

CAUTION: FIXME

*BlockManagerMaster* is the Block Manager that runs on the driver only. It registers itself as `BlockManagerMaster` endpoint in link:spark-rpc.adoc[RPC Environment].

==== [[registerBlockManager]] BlockManagerMaster.registerBlockManager

CAUTION: FIXME

==== [[BlockManagerMasterEndpoint]] BlockManagerMasterEndpoint

CAUTION: FIXME

*BlockManagerMasterEndpoint* is the RPC endpoint for <<BlockManagerMaster, BlockManagerMaster>> on the master node to track statuses of all slaves' block managers.

The following two-way events are handled:

* RegisterBlockManager
* UpdateBlockInfo
* GetLocations
* GetLocationsMultipleBlockIds
* GetPeers
* GetRpcHostPortForExecutor
* GetMemoryStatus
* GetStorageStatus
* GetBlockStatus
* GetMatchingBlockIds
* RemoveRdd
* RemoveShuffle
* RemoveBroadcast
* RemoveBlock
* RemoveExecutor
* StopBlockManagerMaster
* BlockManagerHeartbeat
* HasCachedBlocks

=== [[BlockManagerId]] BlockManagerId

FIXME

=== [[DiskBlockManager]] DiskBlockManager

DiskBlockManager creates and maintains the logical mapping between logical blocks and physical on-disk locations.

By default, one block is mapped to one file with a name given by its BlockId. It is however possible to have a block map to only a segment of a file.

Block files are hashed among the directories listed in `spark.local.dir` (or in `SPARK_LOCAL_DIRS` if set).

CAUTION: FIXME Review me.

=== [[execution-context]] Execution Context

*block-manager-future* is the execution context for...FIXME

=== [[metrics]] Metrics

Block Manager uses link:spark-metrics.adoc[Spark Metrics System] (via `BlockManagerSource`) to report metrics about internal status.

The name of the source is *BlockManager*.

It emits the following numbers:

* memory / maxMem_MB - the maximum memory configured
* memory / remainingMem_MB - the remaining memory
* memory / memUsed_MB - the memory used
* memory / diskSpaceUsed_MB - the disk used

=== Misc

The underlying abstraction for blocks in Spark is a `ByteBuffer` that limits the size of a block to 2GB (`Integer.MAX_VALUE` - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for `long`), ser-deser via byte array-backed output streams.

When a non-local executor starts, it initializes a Block Manager object for `spark.app.id` id.

If a task result is bigger than Akka's message frame size - `spark.akka.frameSize` - executors use the block manager to send the result back. Task results are configured using `spark.driver.maxResultSize` (default: `1g`).

=== [[settings]] Settings

* `spark.shuffle.service.enabled` (default: `false`) whether an external shuffle service is enabled or not. See link:spark-shuffle-manager.adoc#external-shuffle-service[External Shuffle Service].

* `spark.broadcast.compress` (default: `true`) whether to compress stored broadcast variables.

* `spark.shuffle.compress` (default: `true`) whether to compress stored shuffle output.

* `spark.rdd.compress` (default: `false`) whether to compress RDD partitions that are stored serialized.

* `spark.shuffle.spill.compress` (default: `true`) whether to compress shuffle output temporarily spilled to disk.
