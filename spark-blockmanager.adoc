== Block Manager

The *Block Manager* is a key-value store for blocks that acts as a cache. It provides interfaces for putting and retrieving blocks both locally and remotely into various stores, i.e. memory, disk, and off-heap.

A Block Manager manages the storage for most of the data in Spark, i.e. block that represent a cached RDD partition, intermediate shuffle data, broadcast data etc.

A Block Manager instance runs on every node - driver and executors - in a Spark network. It is created when `SparkEnv.create` is called.

FIXME: Review `SparkEnv.create`

A Block Manager relies on the following services:

* RpcEnv
* BlockManagerMaster
* Serializer
* MemoryManager
* MapOutputTracker
* ShuffleManager
* BlockTransferService
* SecurityManager

Block Manager is a BlockDataManager.

[CAUTION]
====
FIXME

* When is Block Manager instantiates? What are the initialization params?
* BlockDataManager?
====

=== [[BlockManagerMaster]] BlockManagerMaster

CAUTION: FIXME

*BlockManagerMaster* is the Block Manager that runs on the driver only. It registers itself as `BlockManagerMaster` endpoint in link:spark-rpc.adoc[RPC Environment].

==== [[BlockManagerMasterEndpoint]] BlockManagerMasterEndpoint

CAUTION: FIXME

*BlockManagerMasterEndpoint* is the RPC endpoint for <<BlockManagerMaster, BlockManagerMaster>> on the master node to track statuses of all slaves' block managers.

The following two-way events are handled:

* RegisterBlockManager
* UpdateBlockInfo
* GetLocations
* GetLocationsMultipleBlockIds
* GetPeers
* GetRpcHostPortForExecutor
* GetMemoryStatus
* GetStorageStatus
* GetBlockStatus
* GetMatchingBlockIds
* RemoveRdd
* RemoveShuffle
* RemoveBroadcast
* RemoveBlock
* RemoveExecutor
* StopBlockManagerMaster
* BlockManagerHeartbeat
* HasCachedBlocks

=== [[BlockManagerId]] BlockManagerId

FIXME

=== [[DiskBlockManager]] DiskBlockManager

DiskBlockManager creates and maintains the logical mapping between logical blocks and physical on-disk locations.

By default, one block is mapped to one file with a name given by its BlockId. It is however possible to have a block map to only a segment of a file.

Block files are hashed among the directories listed in `spark.local.dir` (or in `SPARK_LOCAL_DIRS` if set).

CAUTION: FIXME Review me.

=== [[execution-context]] Execution Context

*block-manager-future* is the execution context for...FIXME

=== [[metrics]] Metrics

Block Manager uses link:spark-metrics.adoc[Spark Metrics System] (via `BlockManagerSource`) to report metrics about internal status.

The name of the source is *BlockManager*.

It emits the following numbers:

* memory / maxMem_MB - the maximum memory configured
* memory / remainingMem_MB - the remaining memory
* memory / memUsed_MB - the memory used
* memory / diskSpaceUsed_MB - the disk used

=== Misc

The underlying abstraction for blocks in Spark is a `ByteBuffer` that limits the size of a block to 2GB (`Integer.MAX_VALUE` - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for `long`), ser-deser via byte array-backed output streams.

When a non-local executor starts, it initializes a Block Manager object for `spark.app.id` id.

If a task result is bigger than Akka's message frame size - `spark.akka.frameSize` - executors use the block manager to send the result back. Task results are configured using `spark.driver.maxResultSize` (default: `1g`).
