== [[ShuffleDependency]] `ShuffleDependency` -- Shuffle Dependencies

`ShuffleDependency` is a link:spark-rdd-dependencies.adoc[RDD Dependency] on the output of a link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage], i.e. *shuffle map stage*.

A `ShuffleDependency` is <<creating-instance, created>> for a single key-value pair RDD (i.e. `RDD[Product2[K, V]]`).

```
scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2)
myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[11] at groupBy at <console>:24

scala> myRdd.dependencies
res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@1193caff)
```

TIP: Use link:spark-rdd-lineage.adoc#toDebugString[`toDebugString` method] with an RDD to know the link:spark-rdd-lineage.adoc[RDD lineage graph].

NOTE: A `ShuffleDependency` dependency is the dependency of link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD]. It is also a dependency of link:spark-rdd-cogroupedrdd.adoc[CoGroupedRDD] and link:spark-rdd-SubtractedRDD.adoc[SubtractedRDD] (but only when partitioners of the RDDs are different).

[[shuffleId]]
Every `ShuffleDependency` has a unique application-wide *shuffleId* number.

NOTE: Shuffle ids are link:spark-sparkcontext.adoc#nextShuffleId[tracked by `SparkContext`].

=== [[rdd]] `rdd` Property

CAUTION: FIXME

=== [[partitioner]] `partitioner` Property

CAUTION: FIXME

=== [[creating-instance]] Creating `ShuffleDependency` Instance

`ShuffleDependency` takes the following when created:

1. A single key-value pair RDD (i.e. `RDD[Product2[K, V]]`),
2. link:spark-rdd-Partitioner.adoc[Partitioner],
3. link:spark-sparkenv.adoc#serializer[Serializer],
4. Optional key ordering (as Scala's link:http://www.scala-lang.org/api/current/scala/math/Ordering.html[scala.math.Ordering] type),
5. Optional <<aggregator, Aggregator>>,
6. <<mapSideCombine, mapSideCombine>> flag which is disabled (i.e. `false`) by default.

When created, `ShuffleDependency` gets link:spark-sparkcontext.adoc#nextShuffleId[shuffleId].

NOTE: `ShuffleDependency` uses the link:spark-rdd.adoc#context[input RDD to access `SparkContext`] and hence the `shuffleId`.

`ShuffleDependency` link:spark-shuffle-manager.adoc#registerShuffle[registers itself with `ShuffleManager`] and gets a `ShuffleHandle` (available as <<shuffleHandle, shuffleHandle>> property).

NOTE: `ShuffleDependency` accesses link:spark-sparkenv.adoc#shuffleManager[`ShuffleManager` using `SparkEnv`].

In the end, `ShuffleDependency` link:spark-service-contextcleaner.adoc#registerShuffleForCleanup[registers itself for cleanup with `ContextCleaner`].

NOTE: `ShuffleDependency` accesses the link:spark-sparkcontext.adoc#cleaner[optional `ContextCleaner` through `SparkContext`].

NOTE: `ShuffleDependency` is created when link:spark-rdd-ShuffledRDD.adoc#getDependencies[ShuffledRDD], link:spark-rdd-cogroupedrdd.adoc#getDependencies[CoGroupedRDD], and link:spark-rdd-SubtractedRDD.adoc#getDependencies[SubtractedRDD] return their RDD dependencies.

=== [[shuffleHandle]] `shuffleHandle` Property

[source, scala]
----
shuffleHandle: ShuffleHandle
----

`shuffleHandle` is the `ShuffleHandle` of a `ShuffleDependency` as assigned eagerly when <<creating-instance, `ShuffleDependency` was created>>.

NOTE: `shuffleHandle` is used to compute link:spark-rdd-cogroupedrdd.adoc#compute[CoGroupedRDDs], link:spark-rdd-ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a `ShuffleReader` for a `ShuffleDependency`) and when a link:spark-taskscheduler-ShuffleMapTask.adoc#runTask[`ShuffleMapTask` runs] (to get a `ShuffleWriter` for a `ShuffleDependency`).

=== [[mapSideCombine]] `mapSideCombine` Flag

`mapSideCombine` is a flag to control whether to use *partial aggregation* (aka *map-side combine*).

`mapSideCombine` is by default disabled (i.e. `false`) when <<creating-instance, creating a `ShuffleDependency`>>.

When enabled, link:spark-SortShuffleWriter.adoc[SortShuffleWriter] and link:spark-BlockStoreShuffleReader.adoc[BlockStoreShuffleReader] assume that an link:spark-Aggregator.adoc[Aggregator] is also defined.

NOTE: `mapSideCombine` is exclusively set (and hence can be enabled) when link:spark-rdd-ShuffledRDD.adoc#getDependencies[`ShuffledRDD` returns the dependencies] (which is a single `ShuffleDependency`).

=== [[aggregator]] `aggregator` Property

[source, scala]
----
aggregator: Option[Aggregator[K, V, C]] = None
----

`aggregator` is a link:spark-Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle).

`aggregator` is by default undefined (i.e. `None`) when <<creating-instance, `ShuffleDependency` is created>>.

NOTE: `aggregator` is used when link:spark-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-values for a reduce task].

=== Usage

The places where `ShuffleDependency` is used:

* link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD] and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] that are RDDs from a shuffle

The RDD operations that may or may not use the above RDDs and hence shuffling:

* link:spark-rdd-partitions.adoc#coalesce[coalesce]
** link:spark-rdd-partitions.adoc#repartition[repartition]

* `cogroup`
** `intersection`
* `subtractByKey`
** `subtract`
* `sortByKey`
** `sortBy`
* `repartitionAndSortWithinPartitions`
* link:spark-rdd-PairRDDFunctions.adoc#combineByKeyWithClassTag[combineByKeyWithClassTag]
** `combineByKey`
** `aggregateByKey`
** `foldByKey`
** `reduceByKey`
** `countApproxDistinctByKey`
** `groupByKey`
* `partitionBy`

NOTE: There may be other dependent methods that use the above.
