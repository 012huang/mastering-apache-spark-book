== Standard Functions for Date and Time

[[functions]]
.(Subset of) Standard Functions for Date and Time
[align="center",cols="1,2",width="100%",options="header"]
|===
| Name
| Description

| <<current_timestamp, current_timestamp>>
|

| <<to_date, to_date>>
|

| <<to_timestamp, to_timestamp>>
|

| <<unix_timestamp, unix_timestamp>>
| Converts current or specified time to Unix timestamp (in seconds)

| <<window, window>>
| Generates tumbling time windows
|===

=== [[current_date]] `current_date` Function

[source, scala]
----
to_date(e: Column, fmt: String): Column
----

CAUTION: FIXME

=== [[current_timestamp]] `current_timestamp` Function

[source, scala]
----
current_timestamp(): Column
----

CAUTION: FIXME

NOTE: `current_timestamp` is also `now` function in SQL.

=== [[to_date]] `to_date` Function

[source, scala]
----
to_date(e: Column, fmt: String): Column
----

CAUTION: FIXME

=== [[to_timestamp]] `to_timestamp` Function

[source, scala]
----
to_timestamp(s: Column): Column
to_timestamp(s: Column, fmt: String): Column
----

CAUTION: FIXME

=== [[unix_timestamp]] Converting Current or Specified Time to Unix Timestamp -- `unix_timestamp` Function

[source, scala]
----
unix_timestamp(): Column  // <1>
unix_timestamp(time: Column): Column // <2>
unix_timestamp(time: Column, format: String): Column
----
<1> Gives current timestamp (in seconds)
<2> Converts `time` string in format `yyyy-MM-dd HH:mm:ss` to Unix timestamp (in seconds)

`unix_timestamp` converts the current or specified `time` in the specified `format` to a Unix timestamp (in seconds).

`unix_timestamp` supports a column of type `Date`, `Timestamp` or `String`.

```
// no time and format => current time
scala> spark.range(1).select(unix_timestamp as "current_timestamp").show
+-----------------+
|current_timestamp|
+-----------------+
|       1493362850|
+-----------------+

// no format so yyyy-MM-dd HH:mm:ss assumed
scala> Seq("2017-01-01 00:00:00").toDF("time").withColumn("unix_timestamp", unix_timestamp($"time")).show
+-------------------+--------------+
|               time|unix_timestamp|
+-------------------+--------------+
|2017-01-01 00:00:00|    1483225200|
+-------------------+--------------+

scala> Seq("2017/01/01 00:00:00").toDF("time").withColumn("unix_timestamp", unix_timestamp($"time", "yyyy/MM/dd")).show
+-------------------+--------------+
|               time|unix_timestamp|
+-------------------+--------------+
|2017/01/01 00:00:00|    1483225200|
+-------------------+--------------+
```

`unix_timestamp` returns `null` if conversion fails.

```
// note slashes as date separators
scala> Seq("2017/01/01 00:00:00").toDF("time").withColumn("unix_timestamp", unix_timestamp($"time")).show
+-------------------+--------------+
|               time|unix_timestamp|
+-------------------+--------------+
|2017/01/01 00:00:00|          null|
+-------------------+--------------+
```

[NOTE]
====
`unix_timestamp` is also supported in link:spark-sql-SparkSession.adoc#sql[SQL mode].

```
scala> spark.sql("SELECT unix_timestamp() as unix_timestamp").show
+--------------+
|unix_timestamp|
+--------------+
|    1493369225|
+--------------+
```
====

Internally, `unix_timestamp` creates a link:spark-sql-Column.adoc[Column] with link:spark-sql-Expression-UnixTimestamp.adoc[UnixTimestamp] binary expression (possibly with `CurrentTimestamp`).

=== [[window]] Generating Tumbling Time Windows -- `window` Function

[source, scala]
----
window(
  timeColumn: Column,
  windowDuration: String): Column  // <1>
window(
  timeColumn: Column,
  windowDuration: String,
  slideDuration: String): Column   // <2>
window(
  timeColumn: Column,
  windowDuration: String,
  slideDuration: String,
  startTime: String): Column
----
<1> With `slideDuration` as `windowDuration` and `0 second` for `startTime`
<2> With `0 second` for `startTime`

`window` generates tumbling time windows of `windowDuration` duration given a `timeColumn` timestamp specifying column.

[source, scala]
----
scala> val timeColumn = window('time, "5 seconds")
timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS `window`
----

`timeColumn` has to be of link:spark-sql-DataType.adoc#TimestampType[TimestampType], i.e. with https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html[java.sql.Timestamp] values.

TIP: Use link:++https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#from-java.time.Instant-++[java.sql.Timestamp.from] or link:++https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#valueOf-java.time.LocalDateTime-++[java.sql.Timestamp.valueOf] factory methods to create `Timestamp` instances.

`windowDuration` and `slideDuration` are strings specifying the width of the window for duration and sliding identifiers, respectively.

TIP: Use `CalendarInterval` for valid window identifiers.

NOTE: `window` is available as of Spark *2.0.0*.

Internally, `window` creates a link:spark-sql-Column.adoc[Column] (with link:spark-sql-Expression-TimeWindow.adoc[TimeWindow] expression) as `window`.

```
scala> timeColumn.expr.numberedTreeString
res9: String =
00 timewindow('time, 5000000, 5000000, 0) AS window#1
01 +- timewindow('time, 5000000, 5000000, 0)
02    +- 'time

import org.apache.spark.sql.catalyst.expressions.TimeWindow
scala> val timeWindow = timeColumn.expr.children(0).asInstanceOf[TimeWindow]
timeWindow: org.apache.spark.sql.catalyst.expressions.TimeWindow = timewindow('time, 5000000, 5000000, 0)
```
