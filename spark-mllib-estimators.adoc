== Estimators

An *estimator* takes a `DataFrame` and parameters (as `ParamMap`) and *fits a model*, i.e. it produces a link:spark-mllib-models.adoc[model] for a `DataFrame` and parameters (as `ParamMap`).

It is basically a function that maps a `DataFrame` onto a `Model` that takes a `DataFrame`, trains on it and produces a `Model`.

```
estimator: DataFrame => Model
```

Estimators are instances of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Estimator[org.apache.spark.ml.Estimator] abstract class that comes with `fit` method (with the return type `M` being a link:spark-mllib-models.adoc[Model]):

[source, scala]
----
fit(dataset: DataFrame): M
----

A `Estimator` is a link:spark-mllib-pipelines.adoc#PipelineStage[PipelineStage] (so it can be a part of a link:spark-mllib-pipelines.adoc#Pipeline[Pipeline]).

The direct specialized extensions of the `Estimator` abstract class are:

* <<Predictor, Predictor>>

=== [[Predictor]] Predictors

A `Predictor` is an link:spark-mllib-pipelines.adoc#Estimator[Estimator] for a `PredictionModel`.

A `Predictor` is basically a functions that maps a `DataFrame` onto a `PredictionModel`.

```
estimator: DataFrame => PredictionModel
```

It implements the abstract `fit(dataset: DataFrame)` of the `Estimator` abstract class that validates and transforms the schema of a dataset (using a custom `transformSchema` of link:spark-mllib-pipelines.adoc#PipelineStage[PipelineStage]), and then calls the abstract `train` method.

[source, scala]
----
train(dataset: DataFrame): M
----

The `train` method is supposed to ease dealing with schema validation and copying parameters into a trained model. It also sets the parent of the model to itself.

==== [[LinearRegression]] LinearRegression

`LinearRegression` is an example of <<Predictor, Predictor>> (indirectly through the specialized `Regressor` private abstract class), and hence Estimator, that represents the https://en.wikipedia.org/wiki/Simple_linear_regression[linear regression] in machine learning.

`LinearRegression` belongs to `org.apache.spark.ml.regression` package.

TIP: Read the scaladoc of https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression[LinearRegression].

It expects `org.apache.spark.mllib.linalg.Vector` as the input type of the column in a dataset and produces link:spark-mllib-models.adoc#LinearRegressionModel[LinearRegressionModel].

[source, scala]
----
import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression
----

The acceptable parameters:

[source, scala]
----
scala> println(lr.explainParams)
elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)
featuresCol: features column name (default: features)
fitIntercept: whether to fit an intercept term (default: true)
labelCol: label column name (default: label)
maxIter: maximum number of iterations (>= 0) (default: 100)
predictionCol: prediction column name (default: prediction)
regParam: regularization parameter (>= 0) (default: 0.0)
solver: the solver algorithm for optimization. If this is not set or empty, default value is 'auto' (default: auto)
standardization: whether to standardize the training features before fitting the model (default: true)
tol: the convergence tolerance for iterative algorithms (default: 1.0E-6)
weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (default: )
----

===== [[LinearRegression-train]] LinearRegression.train

[source, scala]
----
train(dataset: DataFrame): LinearRegressionModel
----

`train` (protected) method of `LinearRegression` expects a `dataset` DataFrame with two columns:

1. `label` of type `DoubleType`.
2. `features` of type link:spark-mllib-vector.adoc[Vector].

It returns `LinearRegressionModel`.

It first counts the number of elements in features column (usually `features`). The column has to be of link:spark-mllib-vector.adoc[mllib.linalg.Vector] type.

[source, scala]
----
// Create a Vector
import org.apache.spark.mllib.linalg.Vectors
val indices = 0 to 4
val elements = indices.zip(Stream.continually(1.0))
val sv = Vectors.sparse(elements.size, elements)

// Create a proper DataFrame
val ds = sc.parallelize(Seq((0.5, sv))).toDF("label", "features")

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression

// the following calls train by the Predictor contract (see above)
val model = lr.fit(ds)
----

=== [[example]] Example

The following example uses <<LinearRegression, LinearRegression>> estimator.

[source, scala]
----
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
val data = (0.0 to 9.0 by 1)                      // create a collection of Doubles
  .map(n => (n, n))                               // make it pairs
  .map { case (label, feature) =>
    LabeledPoint(label, Vectors.dense(feature)) } // create labeled points of dense vectors
  .toDF                                           // make it a DataFrame

scala> data.show
+-----+--------+
|label|features|
+-----+--------+
|  0.0|   [0.0]|
|  1.0|   [1.0]|
|  2.0|   [2.0]|
|  3.0|   [3.0]|
|  4.0|   [4.0]|
|  5.0|   [5.0]|
|  6.0|   [6.0]|
|  7.0|   [7.0]|
|  8.0|   [8.0]|
|  9.0|   [9.0]|
+-----+--------+

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression

val model = lr.fit(data)

scala> model.intercept
res1: Double = 0.0

scala> model.coefficients
res2: org.apache.spark.mllib.linalg.Vector = [1.0]

// make predictions
scala> val predictions = model.transform(data)
predictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]

scala> predictions.show
+-----+--------+----------+
|label|features|prediction|
+-----+--------+----------+
|  0.0|   [0.0]|       0.0|
|  1.0|   [1.0]|       1.0|
|  2.0|   [2.0]|       2.0|
|  3.0|   [3.0]|       3.0|
|  4.0|   [4.0]|       4.0|
|  5.0|   [5.0]|       5.0|
|  6.0|   [6.0]|       6.0|
|  7.0|   [7.0]|       7.0|
|  8.0|   [8.0]|       8.0|
|  9.0|   [9.0]|       9.0|
+-----+--------+----------+

import org.apache.spark.ml.evaluation.RegressionEvaluator

// rmse is the default metric
// We're explicit here for learning purposes
val evaluator = new RegressionEvaluator().setMetricName("rmse")
val rmse = evaluator.evaluate(predictions)

scala> println(s"Root Mean Squared Error: $rmse")
Root Mean Squared Error: 0.0

import org.apache.spark.mllib.linalg.DenseVector
// NOTE Follow along to learn spark.ml-way (not RDD-way)
predictions.rdd.map { r =>
  (r(0).asInstanceOf[Double], r(1).asInstanceOf[DenseVector](0).toDouble, r(2).asInstanceOf[Double]))
  .toDF("label", "feature0", "prediction").show
+-----+--------+----------+
|label|feature0|prediction|
+-----+--------+----------+
|  0.0|     0.0|       0.0|
|  1.0|     1.0|       1.0|
|  2.0|     2.0|       2.0|
|  3.0|     3.0|       3.0|
|  4.0|     4.0|       4.0|
|  5.0|     5.0|       5.0|
|  6.0|     6.0|       6.0|
|  7.0|     7.0|       7.0|
|  8.0|     8.0|       8.0|
|  9.0|     9.0|       9.0|
+-----+--------+----------+

// Let's make it nicer to the eyes using a Scala case class
scala> :pa
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.Row
import org.apache.spark.mllib.linalg.DenseVector
case class Prediction(label: Double, feature0: Double, prediction: Double)
object Prediction {
  def apply(r: Row) = new Prediction(
    label = r(0).asInstanceOf[Double],
    feature0 = r(1).asInstanceOf[DenseVector](0).toDouble,
    prediction = r(2).asInstanceOf[Double])
}

// Exiting paste mode, now interpreting.

import org.apache.spark.sql.Row
import org.apache.spark.mllib.linalg.DenseVector
defined class Prediction
defined object Prediction

scala> predictions.rdd.map(Prediction.apply).toDF.show
+-----+--------+----------+
|label|feature0|prediction|
+-----+--------+----------+
|  0.0|     0.0|       0.0|
|  1.0|     1.0|       1.0|
|  2.0|     2.0|       2.0|
|  3.0|     3.0|       3.0|
|  4.0|     4.0|       4.0|
|  5.0|     5.0|       5.0|
|  6.0|     6.0|       6.0|
|  7.0|     7.0|       7.0|
|  8.0|     8.0|       8.0|
|  9.0|     9.0|       9.0|
+-----+--------+----------+
----
