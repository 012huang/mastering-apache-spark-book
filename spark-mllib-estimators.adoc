== Estimators

An *estimator* is an abstraction of a *learning algorithm* that *fits a model* on dataset.

NOTE: That was so machine learning to explain an estimator this way, _wasn't it?_  It is that the more I spend time with Pipeline API the often I use the terms and phrases from this space. Sorry.

Technically, an `Estimator` produces a link:spark-mllib-models.adoc[Model] (i.e. a link:spark-mllib-transformers.adoc[Transformer]) for a given `DataFrame` and parameters (as `ParamMap`). It fits a model to the input `DataFrame` and `ParamMap` to produce a `Transformer` (a `Model`) that can calculate predictions for any `DataFrame`-based input datasets.

It is basically a function that maps a `DataFrame` onto a `Model` through `fit` method, i.e. it takes a `DataFrame` and produces a `Transformer` as a `Model`.

```
estimator: DataFrame =[fit]=> Model
```

Estimators are instances of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Estimator[org.apache.spark.ml.Estimator] abstract class that comes with `fit` method (with the return type `M` being a `Model`):

[source, scala]
----
fit(dataset: DataFrame): M
----

An `Estimator` is a link:spark-mllib-pipelines.adoc#PipelineStage[PipelineStage] (so it can be a part of a link:spark-mllib-pipelines.adoc#Pipeline[Pipeline]).

NOTE: `Pipeline` considers `Estimator` special and executes `fit` method before `transform` (as for other `Transformer` objects in a pipeline). Consult link:spark-mllib-pipelines.adoc#Pipeline[Pipeline] document.

As an example you could use <<LinearRegression, LinearRegression>> learning algorithm estimator to train a link:spark-mllib-models.adoc#LinearRegressionModel[LinearRegressionModel].

Some of the direct specialized implementations of the `Estimator` abstract class are as follows:

* <<KMeans, KMeans>>
* <<Predictor, Predictor>>

=== [[KMeans]] KMeans

`KMeans` class is the implementation of the Machine Learning clustering algorithm with support for *k-means||* (aka *k-means parallel*) in Spark MLlib.

It is a specialization of `Estimator` to produce a link:spark-mllib-models.adoc#KMeansModel[KMeansModel].

TIP: Do `import org.apache.spark.ml.clustering._` to work with `KMeans` algorithm.

`KMeans` defaults to use the following values:

* Number of clusters (`k`): `2`
* Maximum number of iterations (`maxIter`): `20`
* Initialization algorithm (`initMode`): `k-means||`
* Number of steps for the k-means|| (`initSteps`): `5`
* Convergence tolerance (`tol`): `1e-4`

CAUTION: FIXME Explain the parameters

`fit` method simply passes calls on to the MLlib variant of KMeans in `org.apache.spark.mllib.clustering.KMeans`.

[source,scala]
----
// Prepare a training dataset
import org.apache.spark.mllib.linalg.Vectors
val trainingDF = Seq(
  (0.1, Vectors.dense(0.1, 0.1)),
  (0.2, Vectors.dense(0.1, 0.2)),
  (0.3, Vectors.dense(0.1, 0.3)),
  (0.4, Vectors.dense(0.1, 0.4))).toDF("label", "features")

import org.apache.spark.ml.clustering.KMeans
val kmeans = new KMeans

val kmModel = kmeans.fit(trainingDF)

scala> kmModel.clusterCenters
res0: Array[org.apache.spark.mllib.linalg.Vector] = Array([0.1,0.3], [0.1,0.1])
----

=== [[Predictor]] Predictors

A `Predictor` is a specialization of `Estimator` for a link:spark-mllib-models.adoc#PredictionModel[PredictionModel] with its own abstract `train` method.

[source, scala]
----
train(dataset: DataFrame): M
----

The `train` method is supposed to ease dealing with schema validation and copying parameters to a trained `PredictionModel` model. It also sets the parent of the model to itself.

A `Predictor` is basically a function that maps a `DataFrame` onto a `PredictionModel`.

```
predictor: DataFrame =[train]=> PredictionModel
```

It implements the abstract `fit(dataset: DataFrame)` of the `Estimator` abstract class that validates and transforms the schema of a dataset (using a custom `transformSchema` of link:spark-mllib-pipelines.adoc#PipelineStage[PipelineStage]), and then calls the abstract `train` method.

Validation and transformation of a schema (using `transformSchema`) makes sure that:

1. `features` column exists and is of correct type (defaults to link:spark-mllib-vector.adoc[Vector]).
1. `label` column exists and is of `Double` type.

As the last step, it adds the `prediction` column of `Double` type.

The following is a list of `Predictor` examples for different learning algorithms:

* <<LinearRegression, LinearRegression>>
* <<RandomForestRegressor, RandomForestRegressor>>

==== [[LinearRegression]] LinearRegression

`LinearRegression` is an example of <<Predictor, Predictor>> (indirectly through the specialized `Regressor` private abstract class), and hence Estimator, that represents the https://en.wikipedia.org/wiki/Simple_linear_regression[linear regression] algorithm in Machine Learning.

`LinearRegression` belongs to `org.apache.spark.ml.regression` package.

TIP: Read the scaladoc of https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression[LinearRegression].

It expects `org.apache.spark.mllib.linalg.Vector` as the input type of the column in a dataset and produces link:spark-mllib-models.adoc#LinearRegressionModel[LinearRegressionModel].

[source, scala]
----
import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression
----

The acceptable parameters:

[source, scala]
----
scala> println(lr.explainParams)
elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)
featuresCol: features column name (default: features)
fitIntercept: whether to fit an intercept term (default: true)
labelCol: label column name (default: label)
maxIter: maximum number of iterations (>= 0) (default: 100)
predictionCol: prediction column name (default: prediction)
regParam: regularization parameter (>= 0) (default: 0.0)
solver: the solver algorithm for optimization. If this is not set or empty, default value is 'auto' (default: auto)
standardization: whether to standardize the training features before fitting the model (default: true)
tol: the convergence tolerance for iterative algorithms (default: 1.0E-6)
weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (default: )
----

===== [[LinearRegression-train]] LinearRegression.train

[source, scala]
----
train(dataset: DataFrame): LinearRegressionModel
----

`train` (protected) method of `LinearRegression` expects a `dataset` DataFrame with two columns:

1. `label` of type `DoubleType`.
2. `features` of type link:spark-mllib-vector.adoc[Vector].

It returns `LinearRegressionModel`.

It first counts the number of elements in features column (usually `features`). The column has to be of link:spark-mllib-vector.adoc[mllib.linalg.Vector] type.

[source, scala]
----
import org.apache.spark.mllib.linalg.Vectors
val features = Vectors.sparse(10, Seq((2, 0.2), (4, 0.4)))

// Create a training dataset
val trainData = Seq((1.0, features)).toDF("label", "features")

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression

// the following calls train by the Predictor contract (see above)
val lrModel = lr.fit(trainData)
----

==== [[RandomForestRegressor]] RandomForestRegressor

`RandomForestRegressor` is a concrete <<Predictor, Predictor>> for http://en.wikipedia.org/wiki/Random_forest[Random Forest] learning algorithm. It trains link:spark-mllib-models.adoc#RandomForestRegressionModel[RandomForestRegressionModel] (a subtype of link:spark-mllib-models.adoc#PredictionModel[PredictionModel]) using `DataFrame` with `features` column of `Vector` type.

CAUTION: FIXME

[source, scala]
----
import org.apache.spark.mllib.linalg.Vectors
val features = Vectors.sparse(10, Seq((2, 0.2), (4, 0.4)))

val data = (0.0 to 4.0 by 1).map(d => (d, features)).toDF("label", "features")
// data.as[LabeledPoint]

scala> data.show(false)
+-----+--------------------------+
|label|features                  |
+-----+--------------------------+
|0.0  |(10,[2,4,6],[0.2,0.4,0.6])|
|1.0  |(10,[2,4,6],[0.2,0.4,0.6])|
|2.0  |(10,[2,4,6],[0.2,0.4,0.6])|
|3.0  |(10,[2,4,6],[0.2,0.4,0.6])|
|4.0  |(10,[2,4,6],[0.2,0.4,0.6])|
+-----+--------------------------+

import org.apache.spark.ml.regression.{ RandomForestRegressor, RandomForestRegressionModel }
val rfr = new RandomForestRegressor
val model: RandomForestRegressionModel = rfr.fit(data)

scala> model.trees.foreach(println)
DecisionTreeRegressionModel (uid=dtr_247e77e2f8e0) of depth 1 with 3 nodes
DecisionTreeRegressionModel (uid=dtr_61f8eacb2b61) of depth 2 with 7 nodes
DecisionTreeRegressionModel (uid=dtr_63fc5bde051c) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_64d4e42de85f) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_693626422894) of depth 3 with 9 nodes
DecisionTreeRegressionModel (uid=dtr_927f8a0bc35e) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_82da39f6e4e1) of depth 3 with 7 nodes
DecisionTreeRegressionModel (uid=dtr_cb94c2e75bd1) of depth 0 with 1 nodes
DecisionTreeRegressionModel (uid=dtr_29e3362adfb2) of depth 1 with 3 nodes
DecisionTreeRegressionModel (uid=dtr_d6d896abcc75) of depth 3 with 7 nodes
DecisionTreeRegressionModel (uid=dtr_aacb22a9143d) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_18d07dadb5b9) of depth 2 with 7 nodes
DecisionTreeRegressionModel (uid=dtr_f0615c28637c) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_4619362d02fc) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_d39502f828f4) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_896f3a4272ad) of depth 3 with 9 nodes
DecisionTreeRegressionModel (uid=dtr_891323c29838) of depth 3 with 7 nodes
DecisionTreeRegressionModel (uid=dtr_d658fe871e99) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_d91227b13d41) of depth 2 with 5 nodes
DecisionTreeRegressionModel (uid=dtr_4a7976921f4b) of depth 2 with 5 nodes

scala> model.treeWeights
res12: Array[Double] = Array(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)

scala> model.featureImportances
res13: org.apache.spark.mllib.linalg.Vector = (1,[0],[1.0])
----

=== [[example]] Example

The following example uses <<LinearRegression, LinearRegression>> estimator.

[source, scala]
----
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
val data = (0.0 to 9.0 by 1)                      // create a collection of Doubles
  .map(n => (n, n))                               // make it pairs
  .map { case (label, feature) =>
    LabeledPoint(label, Vectors.dense(feature)) } // create labeled points of dense vectors
  .toDF                                           // make it a DataFrame

scala> data.show
+-----+--------+
|label|features|
+-----+--------+
|  0.0|   [0.0]|
|  1.0|   [1.0]|
|  2.0|   [2.0]|
|  3.0|   [3.0]|
|  4.0|   [4.0]|
|  5.0|   [5.0]|
|  6.0|   [6.0]|
|  7.0|   [7.0]|
|  8.0|   [8.0]|
|  9.0|   [9.0]|
+-----+--------+

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression

val model = lr.fit(data)

scala> model.intercept
res1: Double = 0.0

scala> model.coefficients
res2: org.apache.spark.mllib.linalg.Vector = [1.0]

// make predictions
scala> val predictions = model.transform(data)
predictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]

scala> predictions.show
+-----+--------+----------+
|label|features|prediction|
+-----+--------+----------+
|  0.0|   [0.0]|       0.0|
|  1.0|   [1.0]|       1.0|
|  2.0|   [2.0]|       2.0|
|  3.0|   [3.0]|       3.0|
|  4.0|   [4.0]|       4.0|
|  5.0|   [5.0]|       5.0|
|  6.0|   [6.0]|       6.0|
|  7.0|   [7.0]|       7.0|
|  8.0|   [8.0]|       8.0|
|  9.0|   [9.0]|       9.0|
+-----+--------+----------+

import org.apache.spark.ml.evaluation.RegressionEvaluator

// rmse is the default metric
// We're explicit here for learning purposes
val regEval = new RegressionEvaluator().setMetricName("rmse")
val rmse = regEval.evaluate(predictions)

scala> println(s"Root Mean Squared Error: $rmse")
Root Mean Squared Error: 0.0

import org.apache.spark.mllib.linalg.DenseVector
// NOTE Follow along to learn spark.ml-way (not RDD-way)
predictions.rdd.map { r =>
  (r(0).asInstanceOf[Double], r(1).asInstanceOf[DenseVector](0).toDouble, r(2).asInstanceOf[Double]))
  .toDF("label", "feature0", "prediction").show
+-----+--------+----------+
|label|feature0|prediction|
+-----+--------+----------+
|  0.0|     0.0|       0.0|
|  1.0|     1.0|       1.0|
|  2.0|     2.0|       2.0|
|  3.0|     3.0|       3.0|
|  4.0|     4.0|       4.0|
|  5.0|     5.0|       5.0|
|  6.0|     6.0|       6.0|
|  7.0|     7.0|       7.0|
|  8.0|     8.0|       8.0|
|  9.0|     9.0|       9.0|
+-----+--------+----------+

// Let's make it nicer to the eyes using a Scala case class
scala> :pa
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.Row
import org.apache.spark.mllib.linalg.DenseVector
case class Prediction(label: Double, feature0: Double, prediction: Double)
object Prediction {
  def apply(r: Row) = new Prediction(
    label = r(0).asInstanceOf[Double],
    feature0 = r(1).asInstanceOf[DenseVector](0).toDouble,
    prediction = r(2).asInstanceOf[Double])
}

// Exiting paste mode, now interpreting.

import org.apache.spark.sql.Row
import org.apache.spark.mllib.linalg.DenseVector
defined class Prediction
defined object Prediction

scala> predictions.rdd.map(Prediction.apply).toDF.show
+-----+--------+----------+
|label|feature0|prediction|
+-----+--------+----------+
|  0.0|     0.0|       0.0|
|  1.0|     1.0|       1.0|
|  2.0|     2.0|       2.0|
|  3.0|     3.0|       3.0|
|  4.0|     4.0|       4.0|
|  5.0|     5.0|       5.0|
|  6.0|     6.0|       6.0|
|  7.0|     7.0|       7.0|
|  8.0|     8.0|       8.0|
|  9.0|     9.0|       9.0|
+-----+--------+----------+
----
