== [[NettyBlockTransferService]] `NettyBlockTransferService` -- Netty-Based BlockTransferService

`NettyBlockTransferService` is a link:spark-blocktransferservice.adoc[BlockTransferService] that uses Netty for block transport (when <<uploadBlock, uploading>> or <<fetchBlocks, fetching blocks>>).

NOTE: `NettyBlockTransferService` is created when link:spark-sparkenv.adoc#NettyBlockTransferService[`SparkEnv` is created] (and later passed on to create a link:spark-blockmanager.adoc#creating-instance[BlockManager] per driver and executors).

[TIP]
====
Enable `INFO` or `TRACE` logging level for `org.apache.spark.network.netty.NettyBlockTransferService` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.network.netty.NettyBlockTransferService=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[init]] Initializing `NettyBlockTransferService` -- `init` Method

[source, scala]
----
init(blockDataManager: BlockDataManager): Unit
----

`init` starts a server for...FIXME

Internally, `init` link:spark-NettyBlockRpcServer.adoc#creating-instance[creates a `NettyBlockRpcServer`] (using the application id, a `JavaSerializer` and the input `blockDataManager`).

CAUTION: FIXME Describe security when `authEnabled` is enabled.

`init` creates a `TransportContext` with the `NettyBlockRpcServer` created earlier.

CAUTION: FIXME Describe `transportConf` and `TransportContext`.

`init` creates the internal `clientFactory` and a server.

CAUTION: FIXME What's the "a server"?

In the end, you should see the INFO message in the logs:

```
INFO NettyBlockTransferService: Server created on [hostName]:[port]
```

NOTE: `hostname` is given when link:spark-sparkenv.adoc#NettyBlockTransferService[`NettyBlockTransferService` is created] and is controlled by link:spark-driver.adoc#spark_driver_host[`spark.driver.host` Spark property] for the driver and differs per deployment environment for executors (as controlled by link:spark-executor-backends-CoarseGrainedExecutorBackend.adoc#main[`--hostname` for `CoarseGrainedExecutorBackend`]).

NOTE: `init` is a part of link:spark-blocktransferservice.adoc#contract[`BlockTransferService` contract].

=== [[creating-instance]] Creating `NettyBlockTransferService` Instance

=== [[uploadBlock]] `uploadBlock` Method

[source, scala]
----
uploadBlock(
  hostname: String,
  port: Int,
  execId: String,
  blockId: BlockId,
  blockData: ManagedBuffer,
  level: StorageLevel,
  classTag: ClassTag[_]): Future[Unit]
----

`uploadBlock` creates a `TransportClient` client to send a `UploadBlock` message. The message contains the <<appId, application id>>, `execId`, and `blockId` with `blockData`, `level` and `classTag` serialized.

CAUTION: FIXME Describe `TransportClient`.

When `blockId` block was successfully uploaded, you should see the following TRACE message in the logs:

```
TRACE NettyBlockTransferService: Successfully uploaded block [blockId]
```

When an upload failed, you should see the following ERROR message in the logs:

```
ERROR Error while uploading block [blockId]
```

NOTE: `uploadBlock` is executed when link:spark-blocktransferservice.adoc#uploadBlockSync[`BlockTransferService` does block upload in blocking way] (which simply allows for custom block upload in a link:spark-blocktransferservice.adoc[BlockTransferService]).

=== [[fetchBlocks]] `fetchBlocks` Method

=== [[appId]] Application Id -- `appId` Property
