== [[Analyzer]] Analyzer -- Logical Query Plan Analyzer

`Analyzer` is a logical query plan analyzer in Spark SQL that `QueryExecution` uses to link:spark-sql-QueryExecution.adoc#analyzed[resolve the managed `LogicalPlan`] (and, as a sort of follow-up, link:spark-sql-QueryExecution.adoc#assertAnalyzed[assert that a structured query has already been properly analyzed], i.e. no failed or unresolved or somehow broken logical plan operators and expressions exist).

[[execute]]
`Analyzer` is a link:spark-sql-catalyst-RuleExecutor.adoc[RuleExecutor] that defines the <<batches, collection of logical plan evaluation rules>> (i.e. resolving, removing, and in general modifying it).

NOTE: The result of applying the <<batches, batches>> of `Analyzer` to a link:spark-sql-LogicalPlan.adoc[LogicalPlan] is called *analyzed logical plan*.

`Analyzer` <<creating-instance, is created>> when link:spark-sql-SessionState.adoc#apply[SessionState] and link:spark-sql-HiveSessionState.adoc#apply[HiveSessionState] are created.

`Analyzer` is available at runtime through link:spark-sql-SessionState.adoc#analyzer[`analyzer` attribute of a `SessionState`] (which is available through link:spark-sql-sparksession.adoc[SparkSession] that is available directly or indirectly, using link:spark-sql-sparksession-builder.adoc#getOrCreate[SparkSession.builder.getOrCreate] or link:spark-sql-Dataset.adoc#sparkSession[Dataset.sparkSession]).

[source, scala]
----
// Different ways to access the current session's Analyzer

// Using a directly accessible SparkSession
spark.sessionState.analyzer

// Using an indirectly accessible current SparkSession
SparkSession.builder.getOrCreate.sessionState.analyzer

// Using Dataset
myDataset.sparkSession.sessionState.analyzer
----

[[batches]]
.Analyzer's Evaluation Rules (in the order of execution)
[cols="2,1,3,3",options="header",width="100%"]
|===
^.^| Batch Name
^.^| Strategy
| Rules
| Description

.2+^.^| [[Hints]] Hints
.2+^.^| <<fixedPoint, FixedPoint>>
| ResolveBroadcastHints
| Resolves `BROADCAST`, `BROADCASTJOIN` and `MAPJOIN` broadcast hints (by adding link:spark-sql-LogicalPlan-BroadcastHint.adoc[BroadcastHint] operator to a logical plan)

| RemoveAllHints
| Removes all the hints (valid or not).

^.^| Simple Sanity Check
^.^| `Once`
| LookupFunctions
| Checks whether a function identifier (referenced by an link:spark-sql-LogicalPlan-UnresolvedFunction.adoc[UnresolvedFunction]) link:spark-sql-SessionCatalog.adoc#functionExists[exists in the function registry]. Throws a `NoSuchFunctionException` if not.

.4+^.^| [[Substitution]] Substitution
.4+^.^| <<fixedPoint, FixedPoint>>
| CTESubstitution
| Resolves `With` operators (and substitutes named common table expressions -- CTEs)

| WindowsSubstitution
| Substitutes `UnresolvedWindowExpression` with `WindowExpression` for `WithWindowDefinition` operators

| EliminateUnions
| Eliminates `Union` of one child into that child

| SubstituteUnresolvedOrdinals
| Replaces ordinals in `Sort` and `Aggregate` operators with `UnresolvedOrdinal`

.26+^.^| [[Resolution]] Resolution
.26+^.^| <<fixedPoint, FixedPoint>>
| ResolveTableValuedFunctions
| Replaces `UnresolvedTableValuedFunction` with table-valued function.

| ResolveRelations
| Resolves `InsertIntoTable` and `UnresolvedRelation` operators

| ResolveReferences
|
| ResolveCreateNamedStruct
|
| ResolveDeserializer
|
| ResolveNewInstance
|
| ResolveUpCast
|
| ResolveGroupingAnalytics
|
| ResolvePivot
|
| ResolveOrdinalInOrderByAndGroupBy
|
| ResolveMissingReferences
|
| ExtractGenerator
|
| ResolveGenerate
|
| [[ResolveFunctions]] ResolveFunctions
a| Resolves link:spark-sql-Generator.adoc#UnresolvedGenerator[UnresolvedGenerator] (to a concrete link:spark-sql-Generator.adoc[Generator]) and `UnresolvedFunction` using link:spark-sql-SessionCatalog.adoc#lookupFunction[SessionCatalog].

If `Generator` is not found, `ResolveFunctions` reports the error:

[options="wrap"]
----
[name] is expected to be a generator. However, its class is [className], which is not a generator.
----

| ResolveAliases
|
| ResolveSubquery
|
| ResolveWindowOrder
|
| ResolveWindowFrame
| Resolves `WindowExpression`

| ResolveNaturalAndUsingJoin
|
| ExtractWindowExpressions
|
| GlobalAggregates
|
| ResolveAggregateFunctions
| Resolves aggregate functions in `Filter` and `Sort` operators

| TimeWindowing
| Resolves single `TimeWindow` expressions

| ResolveInlineTables
| Resolves `UnresolvedInlineTable` with `LocalRelation`

| TypeCoercion.typeCoercionRules
|
| <<extendedResolutionRules, extendedResolutionRules>>
|

^.^| [[Post-Hoc-Resolution]] Post-Hoc Resolution
^.^| `Once`
| <<postHocResolutionRules, postHocResolutionRules>>
|

^.^| View
^.^| `Once`
| AliasViewChild
|

^.^| Nondeterministic
^.^| `Once`
| PullOutNondeterministic
|

^.^| UDF
^.^| `Once`
| HandleNullInputsForUDF
|

^.^| FixNullability
^.^| `Once`
| FixNullability
|

^.^| ResolveTimeZone
^.^| `Once`
| ResolveTimeZone
| Replaces `TimeZoneAwareExpression` with no timezone with one with link:spark-sql-CatalystConf.adoc#sessionLocalTimeZone[session-local time zone].

^.^| [[Cleanup]] Cleanup
^.^| <<fixedPoint, FixedPoint>>
| CleanupAliases
|
|===

TIP: Consult the https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L116-L167[sources of `Analyzer`] for the up-to-date list of the evaluation rules.

`Analyzer` resolves unresolved attributes and relations to typed objects using information from a link:spark-sql-SessionCatalog.adoc[SessionCatalog] and link:spark-sql-FunctionRegistry.adoc[FunctionRegistry].

NOTE: link:spark-sql-SessionState.adoc[SessionState] and the Hive-specific link:spark-sql-HiveSessionState.adoc[HiveSessionState] use their own `Analyzer` with custom <<extendedResolutionRules, extendedResolutionRules>>, <<postHocResolutionRules, postHocResolutionRules>>, and <<extendedCheckRules, extendedCheckRules>> extension methods.

`Analyzer` defines <<extendedResolutionRules, extendedResolutionRules>> extension that is a collection of rules (that process a `LogicalPlan`) as an extension point that a custom `Analyzer` can use to extend the `Resolution` batch. The collection of rules is added at the end of the `Resolution` batch.

You can access the result of applying the <<batches, batches>> of `Analyzer` to the link:spark-sql-LogicalPlan.adoc[logical plan] of a link:spark-sql-Dataset.adoc[Dataset], i.e. *analyzed logical plan*, using link:spark-sql-dataset-operators.adoc#explain[explain] (with `extended` flag enabled) or SQL's `EXPLAIN EXTENDED` operators.

[source, scala]
----
// sample Dataset
val dataset = spark.range(5).withColumn("new_column", 'id + 5 as "plus5")

// Using explain operator (with extended flag enabled)
scala> dataset.explain(extended = true)
== Parsed Logical Plan ==
'Project [*, ('id + 5) AS plus5#148 AS new_column#149]
+- Range (0, 5, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint, new_column: bigint
Project [id#145L, (id#145L + cast(5 as bigint)) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))

== Optimized Logical Plan ==
Project [id#145L, (id#145L + 5) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))

== Physical Plan ==
*Project [id#145L, (id#145L + 5) AS new_column#149L]
+- *Range (0, 5, step=1, splits=Some(8))
----

Alternatively, you can also access the analyzed logical plan through ``QueryExecution``'s link:spark-sql-QueryExecution.adoc#analyzed[analyzed] attribute.

[source, scala]
----
scala> dataset.queryExecution.analyzed
res14: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Project [id#145L, (id#145L + cast(5 as bigint)) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))
----

[[internal-registries]]
.Analyzer's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[extendedResolutionRules]] `extendedResolutionRules`
| Additional link:spark-sql-catalyst-RuleExecutor.adoc#Rule[rules] for <<Resolution, Resolution>> batch. Empty by default

| [[fixedPoint]] `fixedPoint`
| `FixedPoint` with <<maxIterations, maxIterations>> for <<Hints, Hints>>, <<Substitution, Substitution>>, <<Resolution, Resolution>> and <<Cleanup, Cleanup>> batches.

Set when `Analyzer` <<creating-instance, is created>> (and can be defined explicitly or through link:spark-sql-CatalystConf.adoc#optimizerMaxIterations[optimizerMaxIterations] configuration setting.

| [[postHocResolutionRules]] `postHocResolutionRules`
| The only link:spark-sql-catalyst-RuleExecutor.adoc#Rule[rules] in <<Post-Hoc-Resolution, Post-Hoc Resolution>> batch if defined (that are executed in one pass, i.e. `Once` strategy). Empty by default
|===

[TIP]
====
Enable `TRACE` or `DEBUG` logging level for `pass:[org.apache.spark.sql.hive.HiveSessionState$$anon$1]` (when link:spark-sql-sparksession.adoc#enableHiveSupport[Hive support is enabled]) or `pass:[org.apache.spark.sql.internal.SessionState$$anon$1]` logger to see what happens inside `Analyzer`.

Add the following line to `conf/log4j.properties`:

```
# when Hive support is enabled
log4j.logger.org.apache.spark.sql.hive.HiveSessionState$$anon$1=TRACE

# with no Hive support
log4j.logger.org.apache.spark.sql.internal.SessionState$$anon$1=TRACE
```

Refer to link:spark-logging.adoc[Logging].

---

The reason for such weird-looking logger names is that `analyzer` attribute is created as an anonymous subclass of `Analyzer` class in the respective `SessionStates`.
====

=== [[creating-instance]] Creating Analyzer Instance

`Analyzer` takes the following when created:

* [[catalog]] link:spark-sql-SessionCatalog.adoc[SessionCatalog]
* [[conf]] link:spark-sql-CatalystConf.adoc[CatalystConf]
* [[maxIterations]] Number of iterations before <<fixedPoint, FixedPoint>> rule batches have converged (i.e. <<Hints, Hints>>, <<Substitution, Substitution>>, <<Resolution, Resolution>> and <<Cleanup, Cleanup>>)

`Analyzer` initializes the <<internal-registries, internal registries and counters>>.

NOTE: `Analyzer` can also be created without specifying the <<maxIterations, maxIterations>> which is then configured using link:spark-sql-CatalystConf.adoc#optimizerMaxIterations[optimizerMaxIterations] configuration setting.

=== [[resolver]] `resolver` Method

[source, scala]
----
resolver: Resolver
----

`resolver` requests <<conf, CatalystConf>> for link:spark-sql-CatalystConf.adoc#resolver[Resolver].

NOTE: `Resolver` is a mere function of two `String` parameters that returns `true` if both refer to the same entity (i.e. for case insensitive equality).
