== [[Analyzer]] Logical Query Plan Analyzer

`Analyzer` is a link:spark-sql-catalyst-RuleExecutor.adoc[RuleExecutor] (with link:spark-sql-catalyst-analyzer-CheckAnalysis.adoc[CheckAnalysis]) that defines `Substitution`, `Resolution`, `Nondeterministic`, `UDF`, `FixNullability`, and `Cleanup` batches of rules to transform a link:spark-sql-LogicalPlan.adoc[LogicalPlan].

`Analyzer` resolves unresolved attributes and relations to typed objects using information from a link:spark-sql-SessionCatalog.adoc[SessionCatalog] and link:spark-sql-FunctionRegistry.adoc[FunctionRegistry].

`Analyzer` is available at runtime through link:spark-sql-sessionstate.adoc#analyzer[`analyzer` attribute of a `SessionState`] (which is available through link:spark-sql-sparksession.adoc[SparkSession]).

[source, scala]
----
sparkSession.sessionState.analyzer
----

NOTE: link:spark-sql-sessionstate.adoc[SessionState] and the Hive-specific link:spark-sql-HiveSessionState.adoc[HiveSessionState] use their own `Analyzer` with custom <<extendedResolutionRules, extendedResolutionRules>>, <<postHocResolutionRules, postHocResolutionRules>>, and <<extendedCheckRules, extendedCheckRules>> extension methods.

`Analyzer` defines <<extendedResolutionRules, extendedResolutionRules>> extension that is a collection of rules (that process a `LogicalPlan`) as an extension point that a custom `Analyzer` can use to extend the `Resolution` batch. The collection of rules is added at the end of the `Resolution` batch.

You can access the result of executing `Analyzer` against the link:spark-sql-LogicalPlan.adoc[logical plan] of a link:spark-sql-dataset.adoc[Dataset] using link:spark-sql-dataset.adoc#explain[explain] method or link:spark-sql-query-execution.adoc[QueryExecution]:

[source, scala]
----
val dataset = spark.range(5).withColumn("new_column", 'id + 5 as "plus5")

scala> dataset.explain(extended = true)
== Parsed Logical Plan ==
'Project [*, ('id + 5) AS plus5#148 AS new_column#149]
+- Range (0, 5, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint, new_column: bigint
Project [id#145L, (id#145L + cast(5 as bigint)) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))

== Optimized Logical Plan ==
Project [id#145L, (id#145L + 5) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))

== Physical Plan ==
*Project [id#145L, (id#145L + 5) AS new_column#149L]
+- *Range (0, 5, step=1, splits=Some(8))

scala> dataset.queryExecution.analyzed
res14: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Project [id#145L, (id#145L + cast(5 as bigint)) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))
----

[[internal-registries]]
.Analyzer's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[extendedResolutionRules]] `extendedResolutionRules`
| FIXME

Used when...FIXME

| [[fixedPoint]] `fixedPoint`
| FIXME

Used when...FIXME

| [[postHocResolutionRules]] `postHocResolutionRules`
| FIXME

Used when...FIXME
|===

[TIP]
====
Enable `TRACE` or `DEBUG` logging level for `pass:[org.apache.spark.sql.hive.HiveSessionState$$anon$1]` (when link:spark-sql-sparksession.adoc#enableHiveSupport[Hive support is enabled]) or `pass:[org.apache.spark.sql.internal.SessionState$$anon$1]` logger to see what happens inside `Analyzer`.

Add the following line to `conf/log4j.properties`:

```
# when Hive support is enabled
log4j.logger.org.apache.spark.sql.hive.HiveSessionState$$anon$1=TRACE

# with no Hive support
log4j.logger.org.apache.spark.sql.internal.SessionState$$anon$1=TRACE
```

Refer to link:spark-logging.adoc[Logging].
---

The reason for such a weird-looking logger name is that `analyzer` attribute is created as an anonymous subclass of `Analyzer` class.
====

=== [[creating-instance]] Creating Analyzer Instance

`Analyzer` takes the following when created:

* [[catalog]] link:spark-sql-SessionCatalog.adoc[SessionCatalog]
* [[conf]] CatalystConf
* [[maxIterations]] Number of iterations that...FIXME

`Analyzer` initializes the <<internal-registries, internal registries and counters>>.

NOTE: `Analyzer` can also be created without specifying the <<maxIterations, maxIterations>> which is then configured using `optimizerMaxIterations`.

=== [[batches]] `batches` Extension Point

CAUTION: FIXME

=== [[resolver]] `resolver` Method

CAUTION: FIXME
