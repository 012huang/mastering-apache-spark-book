== [[Analyzer]] Logical Query Plan Analyzer

`Analyzer` is a link:spark-sql-catalyst-RuleExecutor.adoc[RuleExecutor] (with link:spark-sql-catalyst-analyzer-CheckAnalysis.adoc[CheckAnalysis]) that defines the <<batches, batches of rules>> for analyzing a link:spark-sql-LogicalPlan.adoc[LogicalPlan] (i.e. resolving, removing, ).

[[batches]]
.Analyzer's Batches of LogicalPlan Rules (in the order of execution)
[cols="2,1,3,3",options="header",width="100%"]
|===
^.^| Batch Name
^.^| Strategy
| Rules
| Description

.2+^.^| [[Hints]] Hints
.2+^.^| <<fixedPoint, FixedPoint>>
| ResolveBroadcastHints
|

| RemoveAllHints
|

^.^| Simple Sanity Check
^.^| `Once`
| LookupFunctions
|

.4+^.^| [[Substitution]] Substitution
.4+^.^| <<fixedPoint, FixedPoint>>
| CTESubstitution
|
| WindowsSubstitution
|
| EliminateUnions
|
| SubstituteUnresolvedOrdinals
|

.26+^.^| [[Resolution]] Resolution
.26+^.^| <<fixedPoint, FixedPoint>>
| ResolveTableValuedFunctions
|
| ResolveRelations
|
| ResolveReferences
|
| ResolveCreateNamedStruct
|
| ResolveDeserializer
|
| ResolveNewInstance
|
| ResolveUpCast
|
| ResolveGroupingAnalytics
|
| ResolvePivot
|
| ResolveOrdinalInOrderByAndGroupBy
|
| ResolveMissingReferences
|
| ExtractGenerator
|
| ResolveGenerate
|
| ResolveFunctions
|
| ResolveAliases
|
| ResolveSubquery
|
| ResolveWindowOrder
|
| ResolveWindowFrame
|
| ResolveNaturalAndUsingJoin
|
| ExtractWindowExpressions
|
| GlobalAggregates
|
| ResolveAggregateFunctions
|
| TimeWindowing
|
| ResolveInlineTables
|
| TypeCoercion.typeCoercionRules
|
| <<extendedResolutionRules, extendedResolutionRules>>
|

^.^| [[Post-Hoc-Resolution]] Post-Hoc Resolution
^.^| `Once`
| <<postHocResolutionRules, postHocResolutionRules>>
|

^.^| View
^.^| `Once`
| AliasViewChild
|

^.^| Nondeterministic
^.^| `Once`
| PullOutNondeterministic
|

^.^| UDF
^.^| `Once`
| HandleNullInputsForUDF
|

^.^| FixNullability
^.^| `Once`
| FixNullability
|

^.^| ResolveTimeZone
^.^| `Once`
| ResolveTimeZone
|

^.^| [[Cleanup]] Cleanup
^.^| <<fixedPoint, FixedPoint>>
| CleanupAliases
|
|===

TIP: Consult the https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L116-L167[sources of `Analyzer`] for the up-to-date list of the batches of rules.

`Analyzer` resolves unresolved attributes and relations to typed objects using information from a link:spark-sql-SessionCatalog.adoc[SessionCatalog] and link:spark-sql-FunctionRegistry.adoc[FunctionRegistry].

`Analyzer` is available at runtime through link:spark-sql-SessionState.adoc#analyzer[`analyzer` attribute of a `SessionState`] (which is available through link:spark-sql-sparksession.adoc[SparkSession]).

[source, scala]
----
sparkSession.sessionState.analyzer
----

NOTE: link:spark-sql-SessionState.adoc[SessionState] and the Hive-specific link:spark-sql-HiveSessionState.adoc[HiveSessionState] use their own `Analyzer` with custom <<extendedResolutionRules, extendedResolutionRules>>, <<postHocResolutionRules, postHocResolutionRules>>, and <<extendedCheckRules, extendedCheckRules>> extension methods.

`Analyzer` defines <<extendedResolutionRules, extendedResolutionRules>> extension that is a collection of rules (that process a `LogicalPlan`) as an extension point that a custom `Analyzer` can use to extend the `Resolution` batch. The collection of rules is added at the end of the `Resolution` batch.

You can access the result of executing `Analyzer` against the link:spark-sql-LogicalPlan.adoc[logical plan] of a link:spark-sql-dataset.adoc[Dataset] using link:spark-sql-dataset.adoc#explain[explain] method or link:spark-sql-query-execution.adoc[QueryExecution]:

[source, scala]
----
val dataset = spark.range(5).withColumn("new_column", 'id + 5 as "plus5")

scala> dataset.explain(extended = true)
== Parsed Logical Plan ==
'Project [*, ('id + 5) AS plus5#148 AS new_column#149]
+- Range (0, 5, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint, new_column: bigint
Project [id#145L, (id#145L + cast(5 as bigint)) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))

== Optimized Logical Plan ==
Project [id#145L, (id#145L + 5) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))

== Physical Plan ==
*Project [id#145L, (id#145L + 5) AS new_column#149L]
+- *Range (0, 5, step=1, splits=Some(8))

scala> dataset.queryExecution.analyzed
res14: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Project [id#145L, (id#145L + cast(5 as bigint)) AS new_column#149L]
+- Range (0, 5, step=1, splits=Some(8))
----

[[internal-registries]]
.Analyzer's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[extendedResolutionRules]] `extendedResolutionRules`
| Additional link:spark-sql-catalyst-RuleExecutor.adoc#Rule[rules] for <<Resolution, Resolution>> batch.

Empty by default

| [[fixedPoint]] `fixedPoint`
| `FixedPoint` with <<maxIterations, maxIterations>> for <<Hints, Hints>>, <<Substitution, Substitution>>, <<Resolution, Resolution>> and <<Cleanup, Cleanup>> batches.

Set when `Analyzer` <<creating-instance, is created>> (and can be defined explicitly or through link:spark-sql-catalyst-CatalystConf.adoc#optimizerMaxIterations[optimizerMaxIterations] configuration setting.

| [[postHocResolutionRules]] `postHocResolutionRules`
| link:spark-sql-catalyst-RuleExecutor.adoc#Rule[Rules] for <<Post-Hoc-Resolution, Post-Hoc Resolution>> batch (that are executed in one pass, i.e. `Once` strategy).

Empty by default
|===

[TIP]
====
Enable `TRACE` or `DEBUG` logging level for `pass:[org.apache.spark.sql.hive.HiveSessionState$$anon$1]` (when link:spark-sql-sparksession.adoc#enableHiveSupport[Hive support is enabled]) or `pass:[org.apache.spark.sql.internal.SessionState$$anon$1]` logger to see what happens inside `Analyzer`.

Add the following line to `conf/log4j.properties`:

```
# when Hive support is enabled
log4j.logger.org.apache.spark.sql.hive.HiveSessionState$$anon$1=TRACE

# with no Hive support
log4j.logger.org.apache.spark.sql.internal.SessionState$$anon$1=TRACE
```

Refer to link:spark-logging.adoc[Logging].

---

The reason for such weird-looking logger names is that `analyzer` attribute is created as an anonymous subclass of `Analyzer` class in the respective `SessionStates`.
====

=== [[resolver]] `resolver` Method

[source, scala]
----
resolver: Resolver
----

`resolver` requests <<conf, CatalystConf>> for link:spark-sql-catalyst-CatalystConf.adoc#resolver[Resolver].

NOTE: `Resolver` is a mere function of two `String` parameters that returns `true` if both refer to the same entity (i.e. for case insensitive equality).

=== [[creating-instance]] Creating Analyzer Instance

`Analyzer` takes the following when created:

* [[catalog]] link:spark-sql-SessionCatalog.adoc[SessionCatalog]
* [[conf]] link:spark-sql-catalyst-CatalystConf.adoc[CatalystConf]
* [[maxIterations]] Number of iterations before <<fixedPoint, FixedPoint>> rule batches have converged (i.e. <<Hints, Hints>>, <<Substitution, Substitution>>, <<Resolution, Resolution>> and <<Cleanup, Cleanup>>)

`Analyzer` initializes the <<internal-registries, internal registries and counters>>.

NOTE: `Analyzer` can also be created without specifying the <<maxIterations, maxIterations>> which is then configured using link:spark-sql-catalyst-CatalystConf.adoc#optimizerMaxIterations[optimizerMaxIterations] configuration setting.
