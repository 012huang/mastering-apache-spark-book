== [[SparkPlan]] SparkPlan -- Physical Query Plan / Physical Operator

`SparkPlan` is the Catalyst link:spark-sql-catalyst-QueryPlan.adoc[query plan] for *physical operators* that used (_composed_) together build a *physical query plan* (aka _query execution plan_).

NOTE: A physical operator is a link:spark-sql-catalyst-TreeNode.adoc[Catalyst node] that may have zero or more link:spark-sql-catalyst-TreeNode.adoc#children[children]. Spark SQL uses link:spark-sql-catalyst.adoc[Catalyst] (tree manipulation framework) to compose nodes to build a tree that, in this context, translates to composing physical plans to build a physical plan.

When <<execute, executed>>, a physical operator produces a RDD of records (in the link:spark-sql-InternalRow.adoc[internal binary row format]).

NOTE: <<execute, execute>> is called when `QueryExecution` is requested for a link:spark-sql-QueryExecution.adoc#toRdd[RDD] which is exactly when your query is executed.

TIP: Use link:spark-sql-dataset-operators.adoc#explain[explain] operator to see the execution plan of a structured query.

The <<contract, SparkPlan contract>> assumes that concrete physical operators define <<doExecute, doExecute>> method (with optional <<hooks, hooks>> like <<doPrepare, doPrepare>>) which are executed when the physical operator is <<execute, executed>>.

CAUTION: FIXME A picture with methods/hooks called.

CAUTION: FIXME `SparkPlan` is `Serializable`. Why?

[[attributes]]
.SparkPlan's Atributes
[cols="1,2",options="header",width="100%"]
|===
| Name | Description

| `metadata`
|

| <<metrics, metrics>>
|

| `outputOrdering`
|

|===

`SparkPlan` has the following `final` methods that prepare execution environment and pass calls on to corresponding methods (that constitute <<contract, SparkPlan Contract>>).

[[final-methods]]
.SparkPlan's Final Methods
[cols="1,3",options="header",width="100%"]
|===
| Name
| Description

| [[execute]] `execute`
a| Executes a query and generates a `RDD` of link:spark-sql-InternalRow.adoc[InternalRows].

[source, scala]
----
final def execute(): RDD[InternalRow]
----

Used _most importantly_ when `QueryExecution` is requested for a <<toRdd, RDD>> (that in turn triggers execution of any children the physical operator may have).

Internally, `execute` first <<prepare, prepares the query for execution>>, <<waitForSubqueries, waits for any subqueries>> (the query may have) and eventually calls <<doExecute, doExecute>>.

| [[prepare]] `prepare`
| Prepares a query for execution.

Calls <<doPrepare, doPrepare>>

| <<executeBroadcast, executeBroadcast>>
| Calls <<doExecuteBroadcast, doExecuteBroadcast>>
|===

[[specialized-spark-plans]]
.Physical Query Operators / Specialized SparkPlans
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[UnaryExecNode]] `UnaryExecNode`
| Physical operator with one `child` physical operator.

| [[LeafExecNode]] `LeafExecNode`
|

| [[BinaryExecNode]] `BinaryExecNode`
| Physical operator with two child `left` and `right` physical operators.
|===

NOTE: The naming convention for physical operators in Spark's source code is to have their names end with the *Exec* prefix, e.g. `DebugExec` or link:spark-sql-SparkPlan-LocalTableScanExec.adoc[LocalTableScanExec] that is however removed when the operator is displayed, e.g. in link:spark-webui-sql.adoc[web UI].

=== [[getByteArrayRdd]] `getByteArrayRdd` Internal Method

[source, scala]
----
getByteArrayRdd(n: Int = -1): RDD[Array[Byte]]
----

CAUTION: FIXME

=== [[waitForSubqueries]] `waitForSubqueries` Method

CAUTION: FIXME

=== [[executeCollect]] `executeCollect` Method

CAUTION: FIXME

NOTE: `executeCollect` does not convert data to Scala types.

=== [[contract]] SparkPlan Contract

`SparkPlan` contract requires that concrete physical operators (aka physical plans) define their own custom `doExecute`.

[[doExecute]]
[source, scala]
----
doExecute(): RDD[InternalRow]
----

`doExecute` produces the result of a structured query as an `RDD` of link:spark-sql-InternalRow.adoc[InternalRow] objects.

[[hooks]]
.SparkPlan's Extension Hooks (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[doExecuteBroadcast]] `doExecuteBroadcast`
| Reports a `UnsupportedOperationException`. Executed exclusively as part of <<executeBroadcast, executeBroadcast>> to return the result of a structured query as a broadcast variable.

| [[doPrepare]] `doPrepare`
| Prepares a physical operator for execution. Executed exclusively as part of <<prepare, prepare>> and is meant for setting up some state before executing a query (e.g. link:spark-sql-SparkPlan-BroadcastExchangeExec.adoc#doPrepare[BroadcastExchangeExec] to broadcast asynchronously).

| [[outputPartitioning]] `outputPartitioning`
| Specifies how data is partitioned across different nodes in the cluster

| [[requiredChildDistribution]] `requiredChildDistribution`
| Specifies any partition requirements on the input data

Used exclusively when `EnsureRequirements` link:spark-sql-EnsureRequirements.adoc#ensureDistributionAndOrdering[enforces partition requirements of a physical operator].
|===

=== [[executeQuery]] Executing Query in Scope (after Preparations) -- `executeQuery` Final Method

[source, scala]
----
executeQuery[T](query: => T): T
----

`executeQuery` executes `query` in a scope (i.e. so that all RDDs created will have the same scope).

Internally, `executeQuery` calls <<prepare, prepare>> and <<waitForSubqueries, waitForSubqueries>> before executing `query`.

NOTE: `executeQuery` is executed as part of <<execute, execute>>, <<executeBroadcast, executeBroadcast>> and when link:spark-sql-whole-stage-codegen.adoc#CodegenSupport[CodegenSupport] produces a Java source code.

=== [[executeBroadcast]] Broadcasting Result of Structured Query -- `executeBroadcast` Final Method

[source, scala]
----
executeBroadcast[T](): broadcast.Broadcast[T]
----

`executeBroadcast` returns the result of a structured query as a broadcast variable.

Internally, `executeBroadcast` calls <<doExecuteBroadcast, doExecuteBroadcast>> inside <<executeQuery, executeQuery>>.

NOTE: `executeBroadcast` is called in link:spark-sql-SparkPlan-BroadcastHashJoinExec.adoc[BroadcastHashJoinExec], link:spark-sql-SparkPlan-BroadcastNestedLoopJoinExec.adoc[BroadcastNestedLoopJoinExec] and `ReusedExchangeExec` physical operators.

=== [[metrics]] metrics Internal Registry

[source, scala]
----
metrics: Map[String, SQLMetric] = Map.empty
----

`metrics` is a registry of supported link:spark-sql-SQLMetric.adoc[SQLMetrics] by their names.
