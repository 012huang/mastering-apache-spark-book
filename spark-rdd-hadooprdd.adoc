== HadoopRDD

https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.HadoopRDD[HadoopRDD] is an RDD that provides core functionality for reading data stored in HDFS using the older MapReduce API (https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/package-summary.html[org.apache.hadoop.mapred]).

HadoopRDD is created as a result of calling the following methods in link:spark-sparkcontext.adoc[SparkContext]:

* `hadoopFile`
* `textFile` (the most often used in examples!)
* `sequenceFile`

Partitions are of type `HadoopPartition`.

When an HadoopRDD is computed, i.e. an action is called, you should see the INFO message `Input split:` in the logs.

```
scala> sc.textFile("README.md").count
...
15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784
15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:1784+1784
...
```

The following properties are set upon partition exection:

* *mapred.tip.id* - task id of this task's attempt
* *mapred.task.id* - task attempt's id
* *mapred.task.is.map* as `true`
* *mapred.task.partition* - split id
* *mapred.job.id*

Spark settings for HadoopRDD:

* *spark.hadoop.cloneConf* (default: `false`) - shouldCloneJobConf - should a Hadoop job configuration `JobConf` object be cloned before spawning a Hadoop job. Refer to https://issues.apache.org/jira/browse/SPARK-2546[[SPARK-2546\] Configuration object thread safety issue]. When `true`, you should see a DEBUG message `Cloning Hadoop Configuration`.

You can register callbacks on TaskContext.

HadoopRDDs are not checkpointed. They do nothing when `checkpoint()` is called.

[CAUTION]
====
FIXME

* What are `InputMetrics`?
* What is `JobConf`?
* What are the InputSplits: `FileSplit` and `CombineFileSplit`? (review `SparkHadoopUtil.get.getFSBytesReadOnThreadCallback()`)
* What are `InputFormat` and `Configurable` subtypes?
* What's InputFormat's RecordReader? It creates a key and a value. What are they?
* What does TaskContext do?
* What's Hadoop Split? input splits for Hadoop reads? See `InputFormat.getSplits`
====
