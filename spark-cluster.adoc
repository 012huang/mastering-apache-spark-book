== Running Spark on cluster

Spark can run on the following (open source) *cluster managers* (also called *schedulers*):

* link:spark-standalone.adoc[Spark's own Standalone cluster manager]
* link:spark-yarn.adoc[Hadoop YARN]
* link:spark-mesos.adoc[Apache Mesos]

It can be on-premise or in cloud.

Running Spark in cluster requires workload and resource management on distributed systems.

Spark driver communicates with a cluster manager for resources, e.g. CPU, memory, disk. The cluster manager spawns Spark executors in the cluster.

[CAUTION]
====
FIXME

* Spark execution in cluster - Diagram of the communication between driver, cluster manager, workers with executors and tasks. See http://spark.apache.org/docs/latest/cluster-overview.html[Cluster Mode Overview].
** Show Spark's driver with the main code in Scala in the box
** Nodes with executors with tasks
====

The workers are in charge of communicating the cluster manager the availability of their resources.

Communication with a driver is through a RPC interface (at the moment Akka), except link:spark-mesos.adoc[Mesos in fine-grained mode].

Executors remain alive after jobs are finished for future ones. This allows for better data utilization as intermediate data is cached in memory.

Spark reuses resources in a cluster for:

* efficient data sharing
* fine-grained partitioning
* low-latency scheduling

Reusing also means the the resources can be hold onto for a long time.

Spark reuses long-running executors for speed (contrary to Hadoop MapReduce using short-lived containers for each task).

=== Spark Application Submission to Cluster

When you submit a Spark application to the cluster this is what happens (see the answers to http://stackoverflow.com/q/32621990/1305344[the answer to What are workers, executors, cores in Spark Standalone cluster?] on StackOverflow):

* The Spark driver is launched to invoke the `main` method of the Spark application.
* The driver asks the cluster manager for resources to run the application, i.e. to launch executors that run tasks.
* The cluster manager launches executors.
* The driver runs the Spark application and sends tasks to the executors.
* Executors run the tasks and save the results.
* Right after `SparkContext.stop()` is executed from the driver or the `main` method has exited all the executors are terminated and the cluster resources are released by the cluster manager.

NOTE: _"There's not a good reason to run more than one worker per machine."_ by *Sean Owen* in http://stackoverflow.com/q/24696777/1305344[What is the relationship between workers, worker instances, and executors?]

CAUTION: One executor per node may not always be ideal, esp. when your nodes have lots of RAM. On the other hand, using fewer executors has benefits like more efficient broadcasts.

==== [[coarse-grained]] CoarseGrainedExecutorBackend

CAUTION: FIXME `org.apache.spark.executor.CoarseGrainedExecutorBackend` object comes with `main`. Where is this used?

[CAUTION]
====
FIXME Review the use of:

** Used in `SparkContext.createTaskScheduler`
* `SparkClassCommandBuilder`
====

=== Two modes of launching executors

WARNING: Review core/src/main/scala/org/apache/spark/deploy/master/Master.scala

=== Others

*Spark application* can be split into the part written in Scala, Java, and Python with the cluster itself in which the application is going to run.

Spark application runs on a cluster with the help of *cluster manager*.

A Spark application consists of a single driver process and a set of executor processes scattered across nodes on the cluster.

Both the driver and the executors usually run as long as the application. The concept of *dynamic resource allocation* has changed it.

CAUTION: FIXME Figure

A node is a machine, and there's not a good reason to run more than one worker per machine. So two worker nodes typically means two machines, each a Spark worker.

Workers hold many executors for many applications. One application has executors on many workers.

=== Spark Master

* Hosts drivers
* Manages a cluster

=== Spark Driver

* A separate Java process running on its own JVM
* Executes `main` of your application
* High-level control flow of work
* Your Spark application runs as long as the Spark driver.
** Once the driver terminates, so does your Spark application.
* Creates `SparkContext`, `RDD`'s, and executes transformations and actions
* Spark shell is the driver, too.
** Creates SparkContext that's available as `sc`.
* Launches link:spark-execution-model.adoc[tasks]

=== [[submit-modes]] Submit modes

There are two submit modes, i.e. where a driver runs:

* *client-mode* - a driver runs on the machine that submits the job
* *cluster-mode* - a driver runs on a (random) machine in the cluster
