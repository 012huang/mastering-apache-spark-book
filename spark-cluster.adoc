== Running Spark in cluster

Spark can run on the following (open source) cluster managers:

* Spark's Standalone cluster manager
* Hadoop YARN
* Apache Mesos

It can be on-premise or in cloud.

Master/slave architecture of Spark in cluster:

* a single *driver* = coordinates workers and execution; that is the process that launches the `main` method of your application.
* *executors* = one or more distributed workers

All run as separate Java processes.

*Spark application* can be split into the part written in Scala, Java, and Python with the cluster itself in which the application is going to run.

Spark application runs on a cluster with the help of *cluster manager*.

A Spark application consists of a single driver process and a set of executor processes scattered across nodes on the cluster.

Both the driver and the executors usually run as long as the application. The concept of *dynamic resource allocation* has changed it.

*TODO* Figure

=== Spark Driver

* A separate Java process running on its own JVM
* Executes `main` of your application
* High-level control flow of work
* Your Spark application runs as long as the Spark driver.
** Once the driver terminates, so does your Spark application.
* Creates `SparkContext`, `RDD`'s, and executes transformations and actions
* Spark shell is the driver, too.
** Creates SparkContext that's available as `sc`.
* Launches link:spark-execution-model.adoc[tasks]

=== Executors

* Distributed workers
* Responsible for executing link:spark-execution-model.adoc[tasks]
* Responsible for storing any data that the user chooses to cache
* Can run many tasks in parallel

=== Cluster Managers

==== Spark's Standalone cluster manager

It's *a Spark built-in cluster manager* that comes with the Apache Spark distribution.

==== Spark on Hadoop YARN

...

==== Spark on Mesos

...
