== Running Spark in cluster

Spark can run on the following (open source) cluster managers:

* Spark's Standalone cluster manager
* Hadoop YARN
* Apache Mesos

It can be on-premise or in cloud.

Master/slave architecture of Spark in cluster:

* a single *driver* = coordinates workers and execution; that is the process that launches the `main` method of your application.
* *executors* = one or more distributed workers

All run as separate Java processes.

*Spark application* can be split into the part written in Scala, Java, and Python with the cluster itself in which the application is going to run.

Spark application runs on a cluster with the help of *cluster manager*.

=== Spark Driver

* A separate Java process running on its own JVM
* Executes `main` of your application
* Your Spark application runs as long as the Spark driver.
** Once the driver terminates, so does your Spark application.
* Creates `SparkContext`, `RDD`'s, and executes transformations and actions
* Spark shell is the driver, too.
** Creates SparkContext that's available as `sc`.
* Launches *tasks* - units of physical execution that run parts of your Spark application

=== Cluster Managers

==== Spark's Standalone cluster manager

It's *a Spark built-in cluster manager* that comes with the Apache Spark distribution.

==== Spark on Hadoop YARN

...

==== Spark on Mesos

...
