== TaskSetManager

*TaskSetManager* manages execution of the tasks in a single <<taskset, TaskSet>> (after having been handed over by link:spark-taskscheduler.adoc[TaskScheduler]).

.TaskSetManager and its Dependencies
image::images/TaskSetManager-TaskSchedulerImpl-TaskSet.png[align="center"]

The responsibilities of a TaskSetManager include (follow along the links to learn more in the corresponding sections):

* <<scheduling-tasks, Scheduling the tasks in a taskset>>
* <<task-retries, Retrying tasks on failure>>
* <<locality-aware-scheduling, Locality-aware scheduling via delay scheduling>>

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.scheduler.TaskSetManager` logger to see what happens under the covers in TaskSetManager.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG
```
====

[CAUTION]
====
FIXME

1. When is this class called? By whom?
* `SchedulableBuilder`
** FIFO
** Fair

====

It is an `Schedulable` that works with `TaskSchedulerImpl`. As an `Schedulable` it has a *priority* property (among others).

=== [[parent-pool]] Parent Pool

CAUTION: FIXME When is `parent` Pool set and how is this used?

=== [[resourceOffer]] resourceOffer

CAUTION: FIXME Review `TaskSetManager.resourceOffer` + Does this have anything related to the following section about scheduling tasks?

For every TaskSet submitted for execution, TaskSchedulerImpl creates a new instance of TaskSetManager. It then calls `SchedulerBackend.reviveOffers()` (refer to link:spark-taskschedulerimpl.adoc#submitTasks[submitTasks]).

CAUTION: FIXME picture of the calls between components

`resourceOffer` method responds to an offer of a single executor from the scheduler by finding a task (as a `TaskDescription`). It works in <<zombie-state, non-zombie state>> only. It dequeues a pending task from the taskset by checking pending tasks per executor (using `pendingTasksForExecutor`), host (using `pendingTasksForHost`), with no localization preferences (using `pendingTasksWithNoPrefs`), rack (uses `TaskSchedulerImpl.getRackForHost` that seems to return "non-zero" value for link:spark-yarn.adoc#YarnScheduler[YarnScheduler] only)

From `TaskSetManager.resourceOffer`:

```
INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.4, partition 0,PROCESS_LOCAL, 1997 bytes)
```

If a serialized task is bigger than `100` kB (it is not a configurable value), a WARN message is printed out to the logs (only once per taskset):

```
WARN TaskSetManager: Stage [task.stageId] contains a task of very large size ([serializedTask.limit / 1024] KB). The maximum recommended task size is 100 KB.
```

A task id is added to `runningTasksSet` set and <<parent-pool, parent pool>> notified (using `increaseRunningTasks(1)` up the chain of pools).

The following INFO message appears in the logs:

```
INFO TaskSetManager: Starting task [id] in stage [taskSet.id] (TID [taskId], [host], partition [task.partitionId],[taskLocality], [serializedTask.limit] bytes)
```

For example:

```
INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2054 bytes)
```

=== [[scheduling-tasks]] Scheduling Tasks in TaskSet

CAUTION: FIXME

For each submitted <<taskset, TaskSet>>, a new TaskSetManager is created. The TaskSetManager completely and exclusively owns a TaskSet submitted for execution.

CAUTION: FIXME A picture with TaskSetManager owning TaskSet

CAUTION: FIXME What component knows about TaskSet and TaskSetManager. Isn't it that TaskSets are *created* by  DAGScheduler while TaskSetManager is used by TaskSchedulerImpl only?

TaskSetManager requests the current epoch from link:spark-service-mapoutputtracker.adoc[MapOutputTracker] and sets it on all tasks in the taskset.

You should see the following DEBUG in the logs:

```
DEBUG Epoch for [taskSet]: [epoch]
```

CAUTION: FIXME What's epoch. Why is this important?

TaskSetManager keeps track of the tasks pending execution per executor, host, rack or with no locality preferences.

=== [[locality-aware-scheduling]] Locality-Aware Scheduling aka Delay Scheduling

TaskSetManager computes locality levels for the TaskSet for delay scheduling. While computing you should see the following DEBUG in the logs:

```
DEBUG Valid locality levels for [taskSet]:  [levels]
```

CAUTION: FIXME What's delay scheduling?

=== [[events]] Events

When a task has finished, TaskSetManager sends link:spark-dagscheduler.adoc#CompletionEvent[a CompletionEvent message] to DAGScheduler.

CAUTION: FIXME Make it less code-oriented

The following "events" trigger communication between TaskSetManager and DAGScheduler:

* `handleSuccessfulTask` - `Success` (`TaskEndReason`)
** `TaskSchedulerImpl` calls `taskSetManager.handleSuccessfulTask`
** link:spark-taskscheduler.adoc#TaskResultGetter[TaskResultGetter] calls `scheduler.handleSuccessfulTask`
** ...FIXME Finish me...
* `executorLost` - `Resubmitted` (`TaskFailedReason`)

=== [[task-retries]] Retrying Tasks on Failure

FIXME

Up to <<settings, spark.task.maxFailures>> attempts

=== Task retries and spark.task.maxFailures

When you start Spark program you set up <<settings, spark.task.maxFailures>> for the number of failures that are acceptable until TaskSetManager gives up and marks a job failed.

In Spark shell with local master, `spark.task.maxFailures` is fixed to `1` and you need to use link:spark-local.adoc[local-with-retries master] to change it to some other value.

In the following example, you are going to execute a job with two partitions and keep one failing at all times (by throwing an exception). The aim is to learn the behavior of retrying task execution in a stage in TaskSet. You will only look at a single task execution, namely `0.0`.

```
$ ./bin/spark-shell --master "local[*, 5]"
...
scala> sc.textFile("README.md", 2).mapPartitionsWithIndex((idx, it) => if (idx == 0) throw new Exception("Partition 2 marked failed") else it).count
...
15/10/27 17:24:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitionsWithIndex at <console>:25)
15/10/27 17:24:56 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
15/10/27 17:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
...
15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2062 bytes)
...
15/10/27 17:24:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
...
15/10/27 17:24:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2062 bytes)
15/10/27 17:24:56 INFO Executor: Running task 0.1 in stage 1.0 (TID 4)
15/10/27 17:24:56 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784
15/10/27 17:24:56 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 4)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 ERROR Executor: Exception in task 0.4 in stage 1.0 (TID 7)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 INFO TaskSetManager: Lost task 0.4 in stage 1.0 (TID 7) on executor localhost: java.lang.Exception (Partition 2 marked failed) [duplicate 4]
15/10/27 17:24:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 5 times; aborting job
15/10/27 17:24:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
15/10/27 17:24:56 INFO TaskSchedulerImpl: Cancelling stage 1
15/10/27 17:24:56 INFO DAGScheduler: ResultStage 1 (count at <console>:25) failed in 0.058 s
15/10/27 17:24:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
15/10/27 17:24:56 INFO DAGScheduler: Job 1 failed: count at <console>:25, took 0.085810 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 5 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 7, localhost): java.lang.Exception: Partition 2 marked failed
```

=== [[zombie-state]] Zombie state

TaskSetManager enters *zombie* state when all tasks in a taskset have completed successfully (regardless of the number of task attempts), or if the task set has been aborted (see <<aborting-taskset, Aborting TaskSet>>).

While in zombie state, TaskSetManager can launch no new tasks and <<resourceOffer, responds with no `TaskDescription` to resourceOffers>>.

TaskSetManager remains in the zombie state until all tasks have finished running, i.e. to continue to track and account for the running tasks.

=== [[aborting-taskset]] Aborting TaskSet

When a task in a TaskSet exceeds the largest acceptable frame size, the TaskSetManager for the task is called using `TaskSetManager.abort` method (see link:spark-scheduler-backends-coarse-grained.adoc#launching-tasks[Launching Tasks] in CoarseGrainedSchedulerBackend).

DAGScheduler is also told about it using `taskSetFailed` method.

The TaskSetManager enters <<zombie-state, zombie state>>.

`maybeFinishTaskSet` method is called.

CAUTION: FIXME Why is `maybeFinishTaskSet` method called? When is `runningTasks` `0`?

=== [[tasksetmanager-settings]] Settings

* `spark.scheduler.executorTaskBlacklistTime` (default: `0L`) - time interval to pass after which a task can be re-launched on the executor where it has once failed. It is to prevent repeated task failures due to executor failures.
* `spark.speculation` (default: `false`)
* `spark.speculation.quantile` (default: `0.75`) - the percentage of tasks that has not finished yet at which to start speculation.
* `spark.speculation.multiplier` (default: `1.5`)
* `spark.driver.maxResultSize` (default: `1g`) is the limit of bytes for total size of results. If the value is smaller than `1m` or `1048576` (1024 * 1024), it becomes 0.
* `spark.logging.exceptionPrintInterval` (default: `10000`) - how frequently to reprint duplicate exceptions in full, in milliseconds
* `spark.locality.wait` (default: `3s`) - for locality-aware delay scheduling for PROCESS_LOCAL, NODE_LOCAL, and RACK_LOCAL when locality-specific setting is not set.
* `spark.locality.wait.process` (default: the value of `spark.locality.wait`) - delay for PROCESS_LOCAL
* `spark.locality.wait.node` (default: the value of `spark.locality.wait`) - delay for NODE_LOCAL
* `spark.locality.wait.rack` (default: the value of `spark.locality.wait`) - delay for RACK_LOCAL
