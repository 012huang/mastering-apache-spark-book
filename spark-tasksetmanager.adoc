== TaskSetManager

*TaskSetManager* manages execution of the tasks in a single <<taskset, TaskSet>> (after having been handed over by link:spark-taskscheduler.adoc[TaskScheduler]).

.TaskSetManager and its Dependencies
image::images/TaskSetManager-TaskSchedulerImpl-TaskSet.png[align="center"]

The responsibilities of a TaskSetManager include (follow along the links to learn more in the corresponding sections):

* <<scheduling-tasks, Scheduling the tasks from a taskset>>
* <<task-retries, Retrying tasks on failure>>
* <<locality-aware-scheduling, Locality-aware scheduling via delay scheduling>>

[TIP]
====
Add the following line to `conf/log4j.properties` to see what happens under the covers of `TaskSetManager`:

```
log4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG
```
====

[CAUTION]
====
FIXME

1. When is this class called? By whom?
* `SchedulableBuilder`
** FIFO
** Fair

====

It is an `Schedulable` that works with `TaskSchedulerImpl`. As an `Schedulable` it has a *priority* property (among others).

=== [[resourceOffer]] resourceOffer

CAUTION: FIXME Review `TaskSetManager.resourceOffer` + Does this have anything related to the following section about scheduling tasks?

For every TaskSet submitted for execution, TaskSchedulerImpl creates a new instance of TaskSetManager. It then calls `SchedulerBackend.reviveOffers()` (refer to link:spark-taskscheduler.adoc#submitTasks[submitTasks]).

From `TaskSetManager.resourceOffer`:

```
INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.4, partition 0,PROCESS_LOCAL, 1997 bytes)
```

=== [[scheduling-tasks]] Scheduling Tasks from TaskSet

CAUTION: FIXME

For each submitted <<taskset, TaskSet>>, a new *TaskSetManager* is created. A TaskSetManager completely and exclusively owns a TaskSet submitted for execution.

TaskSetManager requests the current epoch from link:spark-service-mapoutputtracker.adoc[MapOutputTracker] and sets it on all tasks in the taskset.

You should see the following DEBUG in the logs:

```
DEBUG Epoch for [taskSet]: [epoch]
```

CAUTION: FIXME What's epoch. Why is this important?

TaskSetManager keeps track of the tasks pending execution per executor, host, rack or with no locality preferences.

=== [[task-retries]] Retrying Tasks on Failure

FIXME

Up to <<settings, spark.task.maxFailures>> attempts

=== [[locality-aware-scheduling]] Locality-Aware Scheduling aka Delay Scheduling

TaskSetManager computes locality levels for the TaskSet for delay scheduling. While computing you should see the following DEBUG in the logs:

```
DEBUG Valid locality levels for [taskSet]:  [levels]
```

CAUTION: FIXME What's delay scheduling?

=== [[events]] Events

When a task has finished, TaskSetManager sends link:spark-dagscheduler.adoc#CompletionEvent[a CompletionEvent message] to DAGScheduler.

CAUTION: FIXME Make it less code-oriented

The following "events" trigger communication between TaskSetManager and DAGScheduler:

* `handleSuccessfulTask` - `Success` (`TaskEndReason`)
** `TaskSchedulerImpl` calls `taskSetManager.handleSuccessfulTask`
** `TaskResultGetter` calls `scheduler.handleSuccessfulTask`
** `TaskSchedulerImpl.statusUpdate` calls `TaskResultGetter.enqueueSuccessfulTask`
** ...FIXME Finish me...
* `handleFailedTask` with the reason for the failure
* `executorLost` - `Resubmitted` (`TaskFailedReason`)

=== Task retries and spark.task.maxFailures

CAUTION: FIXME Review `handleFailedTask`

When you start Spark program you set up <<settings, spark.task.maxFailures>> for the number of failures that are acceptable until TaskSetManager gives up and marks a job failed.

In Spark shell with local master, `spark.task.maxFailures` is fixed to `1` and you need to use link:spark-local.adoc[local-with-retries master] to change it to some other value.

In the following example, you are going to execute a job with two partitions and keep one failing at all times (by throwing an exception). The aim is to learn the behavior of retrying task execution in a stage in TaskSet. You will only look at a single task execution, namely `0.0`.

```
$ ./bin/spark-shell --master "local[*, 5]"
...
scala> sc.textFile("README.md", 2).mapPartitionsWithIndex((idx, it) => if (idx == 0) throw new Exception("Partition 2 marked failed") else it).count
...
15/10/27 17:24:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitionsWithIndex at <console>:25)
15/10/27 17:24:56 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
15/10/27 17:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
...
15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2062 bytes)
...
15/10/27 17:24:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
...
15/10/27 17:24:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2062 bytes)
15/10/27 17:24:56 INFO Executor: Running task 0.1 in stage 1.0 (TID 4)
15/10/27 17:24:56 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784
15/10/27 17:24:56 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 4)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 ERROR Executor: Exception in task 0.4 in stage 1.0 (TID 7)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 INFO TaskSetManager: Lost task 0.4 in stage 1.0 (TID 7) on executor localhost: java.lang.Exception (Partition 2 marked failed) [duplicate 4]
15/10/27 17:24:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 5 times; aborting job
15/10/27 17:24:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
15/10/27 17:24:56 INFO TaskSchedulerImpl: Cancelling stage 1
15/10/27 17:24:56 INFO DAGScheduler: ResultStage 1 (count at <console>:25) failed in 0.058 s
15/10/27 17:24:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
15/10/27 17:24:56 INFO DAGScheduler: Job 1 failed: count at <console>:25, took 0.085810 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 5 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 7, localhost): java.lang.Exception: Partition 2 marked failed
```

=== [[aborting-taskset]] Aborting TaskSet

CAUTION: When can `TaskSetManager.abort` be called?

=== [[zombie-state]] Zombie state

TaskSetManager enters *zombie* state when all tasks in a taskset have completed successfully (regardless of the number of task attempts), or if the task set has been aborted (see <<aborting-taskset, Aborting TaskSet>>).

While in zombie state, TaskSetManager can launch no more tasks.

TaskSetManager remains in the zombie state until all tasks have finished running, i.e. to continue to track and account for the running tasks.

=== [[tasksetmanager-settings]] Settings

* `spark.scheduler.executorTaskBlacklistTime` (default: `0L`) - time interval to pass after which a task can be re-launched on the executor where it has once failed. It is to prevent repeated task failures due to executor failures.
* `spark.speculation` (default: `false`)
* `spark.speculation.quantile` (default: `0.75`) - the percentage of tasks that has not finished yet at which to start speculation.
* `spark.speculation.multiplier` (default: `1.5`)
* `spark.driver.maxResultSize` (default: `1g`) is the limit of bytes for total size of results. If the value is smaller than `1m` or `1048576` (1024 * 1024), it becomes 0.
* `spark.logging.exceptionPrintInterval` (default: `10000`) - how frequently to reprint duplicate exceptions in full, in milliseconds
* `spark.locality.wait` (default: `3s`) - for locality-aware delay scheduling for PROCESS_LOCAL, NODE_LOCAL, and RACK_LOCAL when locality-specific setting is not set.
* `spark.locality.wait.process` (default: the value of `spark.locality.wait`) - delay for PROCESS_LOCAL
* `spark.locality.wait.node` (default: the value of `spark.locality.wait`) - delay for NODE_LOCAL
* `spark.locality.wait.rack` (default: the value of `spark.locality.wait`) - delay for RACK_LOCAL
