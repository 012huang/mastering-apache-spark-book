== TaskSetManager

*TaskSetManager* manages execution of the tasks in a single <<taskset, TaskSet>> (after having been handed over by link:spark-taskscheduler.adoc[TaskScheduler]).

.TaskSetManager and its Dependencies
image::images/TaskSetManager-TaskSchedulerImpl-TaskSet.png[align="center"]

The responsibilities of a TaskSetManager include (follow along the links to learn more in the corresponding sections):

* <<scheduling-tasks, Scheduling the tasks in a taskset>>
* <<task-retries, Retrying tasks on failure>>
* <<locality-aware-scheduling, Locality-aware scheduling via delay scheduling>>

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.scheduler.TaskSetManager` logger to see what happens under the covers in TaskSetManager.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG
```
====

[CAUTION]
====
FIXME

1. When is this class called? By whom?
* `SchedulableBuilder`
** FIFO
** Fair

====

It is an `Schedulable` that works with `TaskSchedulerImpl`. As an `Schedulable` it has a *priority* property (among others).

=== [[parent-pool]] Parent Pool

CAUTION: FIXME When is `parent` Pool set and how is this used?

=== [[resourceOffer]] resourceOffer

CAUTION: FIXME Review `TaskSetManager.resourceOffer` + Does this have anything related to the following section about scheduling tasks?

For every TaskSet submitted for execution, TaskSchedulerImpl creates a new instance of TaskSetManager. It then calls `SchedulerBackend.reviveOffers()` (refer to link:spark-taskschedulerimpl.adoc#submitTasks[submitTasks]).

CAUTION: FIXME picture of the calls between components

`resourceOffer` method responds to an offer of a single executor from the scheduler by finding a task (as a `TaskDescription`). It works in <<zombie-state, non-zombie state>> only. It dequeues a pending task from the taskset by checking pending tasks per executor (using `pendingTasksForExecutor`), host (using `pendingTasksForHost`), with no localization preferences (using `pendingTasksWithNoPrefs`), rack (uses `TaskSchedulerImpl.getRackForHost` that seems to return "non-zero" value for link:spark-yarn.adoc#YarnScheduler[YarnScheduler] only)

From `TaskSetManager.resourceOffer`:

```
INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.4, partition 0,PROCESS_LOCAL, 1997 bytes)
```

If a serialized task is bigger than `100` kB (it is not a configurable value), a WARN message is printed out to the logs (only once per taskset):

```
WARN TaskSetManager: Stage [task.stageId] contains a task of very large size ([serializedTask.limit / 1024] KB). The maximum recommended task size is 100 KB.
```

A task id is added to `runningTasksSet` set and <<parent-pool, parent pool>> notified (using `increaseRunningTasks(1)` up the chain of pools).

The following INFO message appears in the logs:

```
INFO TaskSetManager: Starting task [id] in stage [taskSet.id] (TID [taskId], [host], partition [task.partitionId],[taskLocality], [serializedTask.limit] bytes)
```

For example:

```
INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2054 bytes)
```

=== [[scheduling-tasks]] Scheduling Tasks in TaskSet

CAUTION: FIXME

For each submitted <<taskset, TaskSet>>, a new TaskSetManager is created. The TaskSetManager completely and exclusively owns a TaskSet submitted for execution.

CAUTION: FIXME A picture with TaskSetManager owning TaskSet

CAUTION: FIXME What component knows about TaskSet and TaskSetManager. Isn't it that TaskSets are *created* by  DAGScheduler while TaskSetManager is used by TaskSchedulerImpl only?

TaskSetManager requests the current epoch from link:spark-service-mapoutputtracker.adoc[MapOutputTracker] and sets it on all tasks in the taskset.

You should see the following DEBUG in the logs:

```
DEBUG Epoch for [taskSet]: [epoch]
```

CAUTION: FIXME What's epoch. Why is this important?

TaskSetManager keeps track of the tasks pending execution per executor, host, rack or with no locality preferences.

=== [[locality-aware-scheduling]] Locality-Aware Scheduling aka Delay Scheduling

TaskSetManager computes locality levels for the TaskSet for delay scheduling. While computing you should see the following DEBUG in the logs:

```
DEBUG Valid locality levels for [taskSet]:  [levels]
```

CAUTION: FIXME What's delay scheduling?

=== [[events]] Events

When a task has finished, TaskSetManager sends link:spark-dagscheduler.adoc#CompletionEvent[a CompletionEvent message] to DAGScheduler.

CAUTION: FIXME Make it less code-oriented

The following "events" trigger communication between TaskSetManager and DAGScheduler:

* `handleSuccessfulTask` - `Success` (`TaskEndReason`)
** `TaskSchedulerImpl` calls `taskSetManager.handleSuccessfulTask`
** link:spark-taskscheduler.adoc#TaskResultGetter[TaskResultGetter] calls `scheduler.handleSuccessfulTask`
** ...FIXME Finish me...
* `executorLost` - `Resubmitted` (`TaskFailedReason`)

=== [[handleFailedTask]] TaskSetManager.handleFailedTask

`handleFailedTask(tid: Long, state: TaskState, reason: TaskEndReason)` method is called by link:spark-taskschedulerimpl.adoc#handleFailedTask[TaskSchedulerImpl] or <<executorLost, executorLost>>.

CAUTION: FIXME image with `handleFailedTask` (and perhaps the other parties involved)

The method first checks whether the task has already been marked as failed (using <<internal-data-structures, taskInfos>>) and if it has, it quits.

It removes the task from <<internal-data-structures, runningTasksSet>> and informs <<internal-data-structures, the parent pool>> to decrease its running tasks.

It marks the TaskInfo as failed and grabs its index so the number of copies running of the task is decremented (see <<internal-data-structures, copiesRunning>>).

CAUTION: FIXME Describe `TaskInfo`

The method calculates the failure exception to report per `TaskEndReason`. See below for the possible cases of TaskEndReason.

CAUTION: FIXME Describe `TaskEndReason`.

The executor for the failed task is added to <<internal-data-structures, failedExecutors>>.

It informs DAGScheduler that the task ended (using  `DAGScheduler.taskEnded`).

The task is then added to the list of pending tasks.

CAUTION: FIXME Review `addPendingTask`

If the TaskSetManager is not a <<zombie-state, zombie>>, and the task was not `KILLED`, and the task failure should be counted towards the maximum number of times the task is allowed to fail before the stage is aborted (`TaskFailedReason.countTowardsTaskFailures` is `true`), <<internal-data-structures, numFailures>> is incremented and if the number of failures of the task equals or is greater than assigned to the TaskSetManager (`maxTaskFailures`), the ERROR appears in the logs:

```
ERROR Task [id] in stage [id] failed [maxTaskFailures] times; aborting job
```

And <<aborting-taskset, abort>> is called, and the method quits.

Otherwise, `TaskSchedulerImpl.taskSetFinished` is called when the TaskSetManager is <<zombie-state, zombie>> and there are no running tasks.

==== FetchFailed

For `FetchFailed`, it logs WARNING:

```
WARNING Lost task [id] in stage [id] (TID [id], [host]): [reason.toErrorString]
```

Unless it has already been marked as successful (in <<internal-data-structures, successful>>), the task becomes so and <<internal-data-structures, tasksSuccessful>> is incremented.

The TaskSetManager becomes <<zombie-state, zombie>>.

No exception is returned.

==== ExceptionFailure

For `ExceptionFailure`, it grabs `TaskMetrics` if available.

If it is a `NotSerializableException`, it logs ERROR:

```
ERROR Task [id] in stage [id] (TID [tid]) had a not serializable result: [exception.description]; not retrying"
```

It calls <<aborting-taskset, abort>> and returns no failure exception.

It continues if not being a `NotSerializableException`.

It grabs the description and the time of the ExceptionFailure.

If the description, i.e. the ExceptionFailure, has already been reported (and is therefore a duplication), <<settings, spark.logging.exceptionPrintInterval>> is checked before reprinting the duplicate exception in full.

For full printout of the ExceptionFailure, the following WARNING appears in the logs:

```
WARNING Lost task [id] in stage [id] (TID [id], [host]): [reason.toErrorString]
```

Otherwise, the following INFO appears in the logs:

```
INFO Lost task [id] in stage [id] (TID [id]) on executor [host]: [ef.className] ([ef.description]) [duplicate [count]]
```

The ExceptionFailure becomes failure exception.

==== ExecutorLostFailure

For `ExecutorLostFailure` if not `exitCausedByApp`, the following INFO appears in the logs:

```
INFO Task [tid] failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
```

No failure exception is returned.

==== Other TaskFailedReasons

For the other TaskFailedReasons, the WARNING appears in the logs:

```
WARNING Lost task [id] in stage [id] (TID [id], [host]): [reason.toErrorString]
```

No failure exception is returned.

==== Other TaskEndReason

For the other TaskEndReasons, the ERROR appears in the logs:

```
ERROR Unknown TaskEndReason: [e]
```

No failure exception is returned.

=== [[executorLost]] executorLost

CAUTION: FIXME

=== [[task-retries]] Retrying Tasks on Failure

CAUTION: FIXME

Up to <<settings, spark.task.maxFailures>> attempts

=== Task retries and spark.task.maxFailures

When you start Spark program you set up <<settings, spark.task.maxFailures>> for the number of failures that are acceptable until TaskSetManager gives up and marks a job failed.

In Spark shell with local master, `spark.task.maxFailures` is fixed to `1` and you need to use link:spark-local.adoc[local-with-retries master] to change it to some other value.

In the following example, you are going to execute a job with two partitions and keep one failing at all times (by throwing an exception). The aim is to learn the behavior of retrying task execution in a stage in TaskSet. You will only look at a single task execution, namely `0.0`.

```
$ ./bin/spark-shell --master "local[*, 5]"
...
scala> sc.textFile("README.md", 2).mapPartitionsWithIndex((idx, it) => if (idx == 0) throw new Exception("Partition 2 marked failed") else it).count
...
15/10/27 17:24:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitionsWithIndex at <console>:25)
15/10/27 17:24:56 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
15/10/27 17:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
...
15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2062 bytes)
...
15/10/27 17:24:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
...
15/10/27 17:24:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2062 bytes)
15/10/27 17:24:56 INFO Executor: Running task 0.1 in stage 1.0 (TID 4)
15/10/27 17:24:56 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784
15/10/27 17:24:56 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 4)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 ERROR Executor: Exception in task 0.4 in stage 1.0 (TID 7)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 INFO TaskSetManager: Lost task 0.4 in stage 1.0 (TID 7) on executor localhost: java.lang.Exception (Partition 2 marked failed) [duplicate 4]
15/10/27 17:24:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 5 times; aborting job
15/10/27 17:24:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
15/10/27 17:24:56 INFO TaskSchedulerImpl: Cancelling stage 1
15/10/27 17:24:56 INFO DAGScheduler: ResultStage 1 (count at <console>:25) failed in 0.058 s
15/10/27 17:24:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
15/10/27 17:24:56 INFO DAGScheduler: Job 1 failed: count at <console>:25, took 0.085810 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 5 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 7, localhost): java.lang.Exception: Partition 2 marked failed
```

=== [[zombie-state]] Zombie state

TaskSetManager enters *zombie* state when all tasks in a taskset have completed successfully (regardless of the number of task attempts), or if the task set has been aborted (see <<aborting-taskset, Aborting TaskSet>>).

While in zombie state, TaskSetManager can launch no new tasks and <<resourceOffer, responds with no `TaskDescription` to resourceOffers>>.

TaskSetManager remains in the zombie state until all tasks have finished running, i.e. to continue to track and account for the running tasks.

=== [[aborting-taskset]] Aborting TaskSet using abort Method

`abort(message: String, exception: Option[Throwable] = None)` method informs link:spark-dagscheduler.adoc[DAGScheduler] that a TaskSet was aborted (using `DAGScheduler.taskSetFailed` method).

CAUTION: FIXME image with DAGScheduler call

The TaskSetManager enters <<zombie-state, zombie state>>.

Finally, `maybeFinishTaskSet` method is called.

CAUTION: FIXME Why is `maybeFinishTaskSet` method called? When is `runningTasks` `0`?

=== [[internal-data-structures]] Internal Data Structures

* `copiesRunning`
* `successful`
* `numFailures`
* `failedExecutors` contains a mapping of TaskInfo's indices that failed to executor ids and the time of the failure. It is used in <<handleFailedTask, handleFailedTask>>.
* `taskAttempts`
* `tasksSuccessful`
* `weight` (default: `1`)
* `minShare` (default: `0`)
* `priority` (default: `taskSet.priority`)
* `stageId` (default: `taskSet.stageId`)
* `name` (default: `TaskSet_[taskSet.stageId]`)
* `parent`
* `totalResultSize`
* `calculatedTasks`
* `runningTasksSet`
* `isZombie` (default: `false`)
* `pendingTasksForExecutor`
* `pendingTasksForHost`
* `pendingTasksForRack`
* `pendingTasksWithNoPrefs`
* `allPendingTasks`
* `speculatableTasks`
* `taskInfos` is the mapping between task ids and their `TaskInfo`
* `recentExceptions`

=== [[tasksetmanager-settings]] Settings

* `spark.scheduler.executorTaskBlacklistTime` (default: `0L`) - time interval to pass after which a task can be re-launched on the executor where it has once failed. It is to prevent repeated task failures due to executor failures.
* `spark.speculation` (default: `false`)
* `spark.speculation.quantile` (default: `0.75`) - the percentage of tasks that has not finished yet at which to start speculation.
* `spark.speculation.multiplier` (default: `1.5`)
* `spark.driver.maxResultSize` (default: `1g`) is the limit of bytes for total size of results. If the value is smaller than `1m` or `1048576` (1024 * 1024), it becomes 0.
* `spark.logging.exceptionPrintInterval` (default: `10000`) - how frequently to reprint duplicate exceptions in full, in milliseconds
* `spark.locality.wait` (default: `3s`) - for locality-aware delay scheduling for PROCESS_LOCAL, NODE_LOCAL, and RACK_LOCAL when locality-specific setting is not set.
* `spark.locality.wait.process` (default: the value of `spark.locality.wait`) - delay for PROCESS_LOCAL
* `spark.locality.wait.node` (default: the value of `spark.locality.wait`) - delay for NODE_LOCAL
* `spark.locality.wait.rack` (default: the value of `spark.locality.wait`) - delay for RACK_LOCAL
