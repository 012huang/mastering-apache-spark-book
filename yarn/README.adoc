== Spark on YARN

You can submit Spark applications to a Hadoop YARN cluster using `yarn` <<masterURL, master URL>>. There are two deploy modes for YARN -- link:spark-yarn-client-yarnclientschedulerbackend.adoc[client] (default) or link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc[cluster] deploy modes. They are about where the Spark driver runs. In client mode it runs on a node outside a YARN cluster whereas in cluster mode it runs inside a container of link:spark-yarn-applicationmaster.adoc[ApplicationMaster] in a YARN cluster.

[source, scala]
----
spark-submit --master yarn --deploy-mode cluster mySparkApp.jar
----

NOTE: Since Spark *2.0.0*, `yarn` master URL is the only proper master URL and you can use `--deploy-mode` to choose between `client` (default) or `cluster` modes.

In that sense, a Spark application is a YARN-compatible execution framework that can be deployed to a YARN cluster (alongside other Hadoop workloads). On YARN, a Spark executor maps to a single YARN container.

NOTE: In order to deploy applications to YARN clusters, you need to <<yarn-support, use Spark with YARN support>>.

Spark on YARN supports <<multiple-application-attempts, multiple application attempts>> and supports link:spark-data-locality.adoc[data locality for data in HDFS]. You can also take advantage of Hadoop's security and run Spark in a link:spark-yarn-kerberos.adoc[secure Hadoop environment using Kerberos authentication] (aka _Kerberized clusters_).

There are few settings that are specific to YARN (see <<settings, Settings>>). Among them, you can particularly like the link:spark-submit.adoc#queue[support for YARN resource queues] (to divide cluster resources and allocate shares to different teams and users based on advanced policies).

TIP: You can start link:spark-submit.adoc[spark-submit] with `--verbose` command-line option to have some settings displayed, including YARN-specific. See <<spark-submit, spark-submit and YARN options>>.

The memory in the YARN resource requests is `--executor-memory` + what's set for `spark.yarn.executor.memoryOverhead`, which defaults to 10% of `--executor-memory`.

If YARN has enough resources it will deploy the executors distributed across the cluster, then each of them will try to process the data locally (`NODE_LOCAL` in Spark Web UI), with as many splits in parallel as you defined in link:spark-executor.adoc#spark.executor.cores[spark.executor.cores].

=== [[spark-submit]] spark-submit and YARN options

When you submit your Spark applications using link:spark-submit.adoc[spark-submit] you can use the following YARN-specific command-line options:

* `--archives`
* `--executor-cores`
* `--keytab`
* `--num-executors`
* `--principal`
* link:spark-submit.adoc#queue[--queue]

TIP: Read about the corresponding settings in <<settings, Settings>> in this document.

=== [[yarn-support]] Spark with YARN support

You need to have Spark that link:spark-building-from-sources.adoc[has been compiled with YARN support], i.e. the class link:spark-yarn-client.adoc[org.apache.spark.deploy.yarn.Client] must be on the CLASSPATH.

Otherwise, you will see the following error in the logs and Spark will exit.

```
Error: Could not load YARN classes. This copy of Spark may not have been compiled with YARN support.
```

=== [[masterURL]] Master URL

Since Spark *2.0.0*, the only proper master URL is `yarn`.

```
./bin/spark-submit --master yarn ...
```

Before Spark 2.0.0, you could have used `yarn-client` or `yarn-cluster`, but it is now deprecated. When you use the deprecated master URLs, you should see the following warning in the logs:

```
Warning: Master yarn-client is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
```

=== [[keytab]] Keytab

CAUTION: FIXME

When a principal is specified a keytab must be specified, too.

The settings link:spark-yarn-settings.adoc#spark.yarn.principal[spark.yarn.principal] and `spark.yarn.principal` will be set to respective values and `UserGroupInformation.loginUserFromKeytab` will be called with their values as input arguments.

=== [[environment-variables]] Environment Variables

==== [[SPARK_DIST_CLASSPATH]] SPARK_DIST_CLASSPATH

`SPARK_DIST_CLASSPATH` is a distribution-defined CLASSPATH to add to processes.

It is used to link:spark-yarn-client.adoc#populateClasspath[populate CLASSPATH for ApplicationMaster and executors].

=== [[settings]] Settings

CAUTION: FIXME Where and how are they used?

=== [[multiple-application-attempts]] Multiple Application Attempts

Spark on YARN supports *multiple application attempts* in link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc[cluster mode].

CAUTION: FIXME

=== [[i-want-more]] Further reading or watching

* (video) https://youtu.be/N6pJhxCPe-Y[Spark on YARN: a Deep Dive -- Sandy Ryza (Cloudera)]
* (video) https://youtu.be/sritCTJWQes[Spark on YARN: The Road Ahead -- Marcelo Vanzin (Cloudera)] from Spark Summit 2015
