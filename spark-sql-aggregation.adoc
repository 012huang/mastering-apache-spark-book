== Aggregation -- Typed and Untyped Grouping

You can group data to compute aggregates over a collection of (grouped) records in a link:spark-sql-dataset.adoc[Dataset].

The following aggregate operators are available for `Datasets`:

1. <<groupBy, groupBy>> for untyped aggregations with link:spark-sql-columns.adoc[Column]- or String-based column names.
2. <<groupByKey, groupByKey>> for strongly-typed aggregations where the data is grouped by a given key function.
3. `rollup`
4. `cube`

The untyped aggregations, e.g. `groupBy`, `rollup`, and `cube`, return <<RelationalGroupedDataset, RelationalGroupedDatasets>> while `groupByKey` returns a <<KeyValueGroupedDataset, KeyValueGroupedDataset>>.

=== [[groupBy]] groupBy Untyped Operator

[source, scala]
----
groupBy(cols: Column*): RelationalGroupedDataset
groupBy(col1: String, cols: String*): RelationalGroupedDataset
----

`groupBy` family of methods groups the `Dataset` using the specified columns to run aggregations on.

NOTE: The following session uses the data setup as described in <<test-setup, Test Setup>> section below.

```
scala> dataset.show
+----+---------+-----+
|name|productId|score|
+----+---------+-----+
| aaa|      100| 0.12|
| aaa|      200| 0.29|
| bbb|      200| 0.53|
| bbb|      300| 0.42|
+----+---------+-----+

scala> dataset.groupBy('name).avg().show
+----+--------------+----------+
|name|avg(productId)|avg(score)|
+----+--------------+----------+
| aaa|         150.0|     0.205|
| bbb|         250.0|     0.475|
+----+--------------+----------+

scala> dataset.groupBy('name, 'productId).agg(Map("score" -> "avg")).show
+----+---------+----------+
|name|productId|avg(score)|
+----+---------+----------+
| aaa|      200|      0.29|
| bbb|      200|      0.53|
| bbb|      300|      0.42|
| aaa|      100|      0.12|
+----+---------+----------+

scala> dataset.groupBy('name).count.show
+----+-----+
|name|count|
+----+-----+
| aaa|    2|
| bbb|    2|
+----+-----+

scala> dataset.groupBy('name).max("score").show
+----+----------+
|name|max(score)|
+----+----------+
| aaa|      0.29|
| bbb|      0.53|
+----+----------+

scala> dataset.groupBy('name).sum("score").show
+----+----------+
|name|sum(score)|
+----+----------+
| aaa|      0.41|
| bbb|      0.95|
+----+----------+

scala> dataset.groupBy('productId).sum("score").show
+---------+------------------+
|productId|        sum(score)|
+---------+------------------+
|      300|              0.42|
|      100|              0.12|
|      200|0.8200000000000001|
+---------+------------------+
```

=== [[groupByKey]] groupByKey Typed Operator

[source, scala]
----
scala> dataset.groupByKey(_.productId).count.show
+-----+--------+
|value|count(1)|
+-----+--------+
|  300|       1|
|  100|       1|
|  200|       2|
+-----+--------+

import org.apache.spark.sql.expressions.scalalang._
scala> dataset.groupByKey(_.productId).agg(typed.sum[Token](_.score)).toDF("productId", "sum").orderBy('productId).show
+---------+------------------+
|productId|               sum|
+---------+------------------+
|      100|              0.12|
|      200|0.8200000000000001|
|      300|              0.42|
+---------+------------------+
----

=== [[RelationalGroupedDataset]] RelationalGroupedDataset

`RelationalGroupedDataset` is a result of executing the untyped operators <<groupBy, groupBy>>, <<rollup, rollup>> and <<cube, cube>>.

`RelationalGroupedDataset` is also a result of executing <<pivot, pivot>> operator on a grouped records as `RelationalGroupedDataset`.

It offers the following operators to work on a grouped collection of records:

* `agg`
* `count`
* `mean`
* `max`
* `avg`
* `min`
* `sum`
* `pivot`

=== [[KeyValueGroupedDataset]] KeyValueGroupedDataset

`KeyValueGroupedDataset` is a result of executing the strongly-typed operator <<groupByKey, groupByKey>>.

CAUTION: FIXME Elaborate

=== [[test-setup]] Test Setup

This is a setup for learning `GroupedData`. Paste it into Spark Shell using `:paste`.

[source, scala]
----
import spark.implicits._

case class Token(name: String, productId: Int, score: Double)
val data = Token("aaa", 100, 0.12) ::
  Token("aaa", 200, 0.29) ::
  Token("bbb", 200, 0.53) ::
  Token("bbb", 300, 0.42) :: Nil
val dataset = data.toDS.cache  // <1>
----
<1> Cache the dataset so the following queries won't load/recompute data over and over again.
