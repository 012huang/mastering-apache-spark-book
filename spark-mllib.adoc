== Spark MLlib

CAUTION: I'm new to Machine Learning as a discipline and Spark MLlib in particular so mistakes in this document are considered a norm (not an exception).

*Spark MLlib* is a module (a library / an extension) of Apache Spark to provide distributed machine learning algorithms on top of Spark. Its goal is to simplify the development and usage of large scale machine learning.

You can find the following types of machine learning algorithms in MLlib:

* Classification
* Regression
* Frequent itemsets (via <<fp-growth-algorithm, FP-growth Algorithm>>)
* Recommendation
* Feature extraction and selection
* Clustering
* Statistics
* Linear Algebra

You can also do the following using MLlib:

* Model import and export
* link:spark-mllib-pipelines.adoc[Pipelines]

*Machine Learning* uses large datasets to identify (infer) patterns and make decisions (aka _predictions_). Automated decision making is what makes Machine Learning so appealing. You can teach a system from a dataset and let the system act by itself to predict future.

The amount of data (measured in TB or PB) is what makes Spark MLlib especially important since a human could not possibly extract much value from the dataset in a short time.

Spark handles data distribution and makes the huge data available by means of link:spark-rdd.adoc[RDDs], link:spark-sql-dataframe.adoc[DataFrames], and recently link:spark-sql-dataset.adoc[Datasets].

Use cases for Machine Learning (and hence Spark MLlib that comes with appropriate algorithms):

* Security monitoring and fraud detection
* Operational optimizations
* Product recommendations or (more broadly) Marketing optimization
* Ad serving and optimization

=== [[concepts]] Concepts

This section introduces the concepts of Machine Learning and how they are modelled in Spark MLlib.

==== [[feature]] Feature

A *feature* (aka _dimension_ or _variable_) is an attribute of an observation. It is an *independent variable*.

Spark models observations as rows in a `DataFrame` with features as columns (one per feature or a set of features).

NOTE: Ultimately, it is up to an algorithm to expect one or many features per column.

There are two classes of features:

* *Categorical* with _discrete_ values, i.e. the set of possible values is limited, and can range from one to many thousands. There is no ordering implied, and so the values are incomparable.
* *Numerical* with _quantitative_ values, i.e. any numerical values that you can compare to each other. You can further classify them into *discrete* and *continuous* features.

==== [[label]] Label

A *label* is a variable that a machine learning system learns to predict.

There are *categorical* and *numerical* labels.

A label is a *dependent variable* that depends on other dependent or independent variables like features.

=== [[fp-growth-algorithm]] FP-growth Algorithm

Spark 1.5 have significantly improved on frequent pattern mining capabilities with new algorithms for association rule generation and sequential pattern mining.

* *Frequent Itemset Mining* using the *Parallel FP-growth* algorithm (since Spark 1.3)
** https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html[Frequent Pattern Mining in MLlib User Guide]
** *frequent pattern mining*
*** reveals the most frequently visited site in a particular period
*** finds popular routing paths that generate most traffic in a particular region
** models its input as a set of *transactions*, e.g. a path of nodes.
** A transaction is a set of *items*, e.g. network nodes.
** the algorithm looks for common *subsets of items* that appear across transactions, e.g. sub-paths of the network that are frequently traversed.
** A naive solution: generate all possible itemsets and count their occurrence
** A subset is considered *a pattern* when it appears in some minimum proportion of all transactions - *the support*.
** the items in a transaction are unordered
** analyzing traffic patterns from network logs
** the algorithm finds all frequent itemsets without generating and testing all candidates
* suffix trees (FP-trees) constructed and grown from filtered transactions
* Also available in Mahout, but slower.
* Distributed generation of https://en.wikipedia.org/wiki/Association_rule_learning[association rules] (since Spark 1.5).
** in a retailer’s transaction database, a rule `{toothbrush, floss} => {toothpaste}` with a confidence value `0.8` would indicate that `80%` of customers who buy a toothbrush and floss also purchase a toothpaste in the same transaction. The retailer could then use this information, put both toothbrush and floss on sale, but raise the price of toothpaste to increase overall profit.
** http://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html#fp-growth[FPGrowth] model
* *parallel sequential pattern mining* (since Spark 1.5)
** *PrefixSpan* algorithm with modifications to parallelize the algorithm for Spark.
** extract frequent sequential patterns like routing updates, activation failures, and broadcasting timeouts that could potentially lead to customer complaints and proactively reach out to customers when it happens.

=== Power Iteration Clustering

* since Spark 1.3
* unsupervised learning including clustering
* identifying similar behaviors among users or network clusters
* *Power Iteration Clustering (PIC)* in MLlib, a simple and scalable graph clustering method
** https://spark.apache.org/docs/latest/mllib-clustering.html#power-iteration-clustering-pic[PIC in MLlib User Guide]
** `org.apache.spark.mllib.clustering.PowerIterationClustering`
** a graph algorithm
** Among the first MLlib algorithms built upon link:spark-graphx.adoc[GraphX].
** takes an undirected graph with similarities defined on edges and outputs clustering assignment on nodes
** uses truncated http://en.wikipedia.org/wiki/Power_iteration[power iteration] to find a very low-dimensional embedding of the nodes, and this embedding leads to effective graph clustering.
** stores the normalized similarity matrix as a graph with normalized similarities defined as edge properties
** The edge properties are cached and remain static during the power iterations.
** The embedding of nodes is defined as node properties on the same graph topology.
** update the embedding through power iterations, where aggregateMessages is used to compute matrix-vector multiplications, the essential operation in a power iteration method
** k-means is used to cluster nodes using the embedding.
** able to distinguish clearly the degree of similarity – as represented by the Euclidean distance among the points – even though their relationship is non-linear

=== [[LabeledPoint]] LabeledPoint

CAUTION: FIXME

`LabeledPoint` is a convenient class for declaring a schema for DataFrames that are used as input data for <<LinearRegression, Linear Regression>> algorithm in Spark MLlib.

=== [[LinearRegression]] LinearRegression

`LinearRegression` class represents the linear regression algorithm in machine learning.

NOTE: The class belongs to `org.apache.spark.ml.regression` package.

==== Example

```
$ bin/spark-shell
...
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_74)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LabeledPoint

scala> val data = (0 to 9).map(_.toDouble).map(n => (n, n)).map { case (label, feature) => LabeledPoint(label, Vectors.dense(feature)) }.toDF
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]

scala> data.show
+-----+--------+
|label|features|
+-----+--------+
|  0.0|   [0.0]|
|  1.0|   [1.0]|
|  2.0|   [2.0]|
|  3.0|   [3.0]|
|  4.0|   [4.0]|
|  5.0|   [5.0]|
|  6.0|   [6.0]|
|  7.0|   [7.0]|
|  8.0|   [8.0]|
|  9.0|   [9.0]|
+-----+--------+


scala> import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.regression.LinearRegression

scala> val lr = new LinearRegression
lr: org.apache.spark.ml.regression.LinearRegression = linReg_c227301cf2c3

scala> val model = lr.fit(data)
16/03/04 10:07:45 WARN WeightedLeastSquares: regParam is zero, which might cause numerical instability and overfitting.
16/03/04 10:07:45 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
16/03/04 10:07:45 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
16/03/04 10:07:45 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
16/03/04 10:07:45 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
model: org.apache.spark.ml.regression.LinearRegressionModel = linReg_c227301cf2c3

scala> model.intercept
res1: Double = 0.0

scala> model.coefficients
res2: org.apache.spark.mllib.linalg.Vector = [1.0]

// make predictions
scala> val predictions = model.transform(data)
predictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]

scala> predictions.show
+-----+--------+----------+
|label|features|prediction|
+-----+--------+----------+
|  0.0|   [0.0]|       0.0|
|  1.0|   [1.0]|       1.0|
|  2.0|   [2.0]|       2.0|
|  3.0|   [3.0]|       3.0|
|  4.0|   [4.0]|       4.0|
|  5.0|   [5.0]|       5.0|
|  6.0|   [6.0]|       6.0|
|  7.0|   [7.0]|       7.0|
|  8.0|   [8.0]|       8.0|
|  9.0|   [9.0]|       9.0|
+-----+--------+----------+

scala> import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.evaluation.RegressionEvaluator

// rmse is the default metric
// We're explicit here for learning purposes
scala> val evaluator = new RegressionEvaluator().setMetricName("rmse")
evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_bed1f4fc840d

scala> val rmse = evaluator.evaluate(predictions)
rmse: Double = 0.0

scala> println(s"Root Mean Squared Error: $rmse")
Root Mean Squared Error: 0.0

scala> import org.apache.spark.mllib.linalg.DenseVector
import org.apache.spark.mllib.linalg.DenseVector

scala> predictions.rdd.map(r => (r(0).asInstanceOf[Double], r(1).asInstanceOf[DenseVector](0).toDouble, r(2).asInstanceOf[Double])).toDF("label", "feature0", "prediction").show
+-----+--------+----------+
|label|feature0|prediction|
+-----+--------+----------+
|  0.0|     0.0|       0.0|
|  1.0|     1.0|       1.0|
|  2.0|     2.0|       2.0|
|  3.0|     3.0|       3.0|
|  4.0|     4.0|       4.0|
|  5.0|     5.0|       5.0|
|  6.0|     6.0|       6.0|
|  7.0|     7.0|       7.0|
|  8.0|     8.0|       8.0|
|  9.0|     9.0|       9.0|
+-----+--------+----------+

// Let's make it nicer to the eyes using a Scala case class
scala> :pa
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.Row
import org.apache.spark.mllib.linalg.DenseVector
case class Prediction(label: Double, feature0: Double, prediction: Double)
object Prediction {
  def apply(r: Row) = new Prediction(
    label = r(0).asInstanceOf[Double],
    feature0 = r(1).asInstanceOf[DenseVector](0).toDouble,
    prediction = r(2).asInstanceOf[Double])
}

// Exiting paste mode, now interpreting.

import org.apache.spark.sql.Row
import org.apache.spark.mllib.linalg.DenseVector
defined class Prediction
defined object Prediction

scala> predictions.rdd.map(Prediction.apply).toDF.show
+-----+--------+----------+
|label|feature0|prediction|
+-----+--------+----------+
|  0.0|     0.0|       0.0|
|  1.0|     1.0|       1.0|
|  2.0|     2.0|       2.0|
|  3.0|     3.0|       3.0|
|  4.0|     4.0|       4.0|
|  5.0|     5.0|       5.0|
|  6.0|     6.0|       6.0|
|  7.0|     7.0|       7.0|
|  8.0|     8.0|       8.0|
|  9.0|     9.0|       9.0|
+-----+--------+----------+

```

=== Further reading

* https://databricks.com/blog/2015/09/28/improved-frequent-pattern-mining-in-spark-1-5-association-rules-and-sequential-patterns.html[Improved Frequent Pattern Mining in Spark 1.5: Association Rules and Sequential Patterns]
* https://databricks.com/blog/2015/04/17/new-mllib-algorithms-in-spark-1-3-fp-growth-and-power-iteration-clustering.html[New MLlib Algorithms in Spark 1.3: FP-Growth and Power Iteration Clustering]
