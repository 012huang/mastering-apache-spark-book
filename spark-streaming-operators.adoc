== Stream Operators

You use *stream operators* to apply *transformations* to the elements received (often called *records*) from input streams and ultimately trigger computations using *output operators*.

Transformations are mostly *stateless*, but Spark Streaming comes with an _experimental_ support for link:spark-streaming-operators-stateful.adoc[stateful operators] (e.g. link:spark-streaming-operators-stateful.adoc#mapWithState[mapWithState] or link:spark-streaming-operators-stateful.adoc#updateStateByKey[updateStateByKey]). It also offers link:spark-streaming-windowedoperators.adoc[windowed operators] that can work across batches.

NOTE: You may use RDDs from other (non-streaming) data sources to build more advanced pipelines.

There are two main types of operators:

* *transformations* that transform elements in input data RDDs
* *output operators* that link:spark-streaming-dstreams.adoc#register[register input streams as output streams] so the execution starts.

Every `DStream` offers the following operators:

* (output operator) `print` to print 10 elements only or the more general version `print(num: Int)` to print up to `num` elements. See <<print, print operation>> in this document.
* link:spark-streaming-windowedoperators.adoc#slice[slice]
* link:spark-streaming-windowedoperators.adoc#slice[window]
* link:spark-streaming-windowedoperators.adoc#reduceByWindow[reduceByWindow]
* <<reduce, reduce>>
* <<map, map>>
* (output operator) <<foreachRDD, foreachRDD>>
* <<glom, glom>>
* (output operator) link:spark-streaming-operators-saveas.adoc[saveAsObjectFiles]
* (output operator) link:spark-streaming-operators-saveas.adoc[saveAsTextFiles]
* <<transform, transform>>
* `flatMap`
* `filter`
* `repartition`
* `mapPartitions`
* `count`
* `countByValue`
* a family of `transformWith`
* `countByWindow`
* `countByValueAndWindow`
* `union`

NOTE: `DStream` companion object offers a Scala implicit to convert `DStream[(K, V)]` to `PairDStreamFunctions` with methods on DStreams of key-value pairs, e.g. link:spark-streaming-operators-stateful.adoc#mapWithState[mapWithState] or link:spark-streaming-operators-stateful.adoc#updateStateByKey[updateStateByKey].

Most streaming operators come with their own custom `DStream` to offer the service. It however very often boils down to overriding the link:spark-streaming-dstreams.adoc#contract[compute] method and applying corresponding link:spark-rdd-operations.adoc[RDD operator] on a generated RDD.

=== [[print]] print Operator

`print(num: Int)` operator prints `num` first elements of each RDD in the input stream.

`print` uses `print(num: Int)` with `num` being `10`.

It is a *output operator* (that returns `Unit`).

For each batch, `print` operator prints the following header to the standard output (regardless of the number of elements to be printed out):

```
-------------------------------------------
Time: [time] ms
-------------------------------------------
```

Internally, it calls `RDD.take(num + 1)` (see link:spark-rdd-operations.adoc#actions[take action]) on each RDD in the stream to print `num` elements. It then prints `...` if there are more elements in the RDD (that would otherwise exceed `num` elements being requested to print).

It creates a link:spark-streaming-foreachdstreams.adoc[ForEachDStream] stream and link:spark-streaming-dstreams.adoc#register[registers it as an output stream].

=== [[foreachRDD]] foreachRDD Operators

```
foreachRDD(foreachFunc: RDD[T] => Unit): Unit
foreachRDD(foreachFunc: (RDD[T], Time) => Unit): Unit
```

`foreachRDD` operator applies `foreachFunc` function to every RDD in the stream.

It creates a link:spark-streaming-foreachdstreams.adoc[ForEachDStream] stream and link:spark-streaming-dstreams.adoc#register[registers it as an output stream].

==== [[foreachRDD-example]] foreachRDD Example

```
val clicks: InputDStream[(String, String)] = messages
// println every single data received in clicks input stream
clicks.foreachRDD(rdd => rdd.foreach(println))
```

=== [[glom]] glom Operator

```
glom(): DStream[Array[T]]
```

`glom` operator creates a new stream in which RDDs in the source stream are link:spark-rdd-operations.adoc[RDD.glom] over, i.e. it link:spark-rdd-partitions.adoc#coalesce[coalesces] all elements in RDDs within each partition into an array.

=== [[reduce]] reduce Operator

```
reduce(reduceFunc: (T, T) => T): DStream[T]
```

`reduce` operator creates a new stream of RDDs of a single element that is a result of applying `reduceFunc` to the data received.

Internally, it uses <<map, map>> and <<reduceByKey, reduceByKey>> operators.

==== [[reduce-example]] reduce Example

[source, scala]
----
val clicks: InputDStream[(String, String)] = messages
type T = (String, String)
val reduceFunc: (T, T) => T = {
  case in @ ((k1, v1), (k2, v2)) =>
    println(s">>> input: $in")
    (k2, s"$v1 + $v2")
}
val reduceClicks: DStream[(String, String)] = clicks.reduce(reduceFunc)
reduceClicks.print
----

=== [[map]] map Operator

```
map[U](mapFunc: T => U): DStream[U]
```

`map` operator creates a new stream with the source elements being mapped over using `mapFunc` function.

It creates `MappedDStream` stream that, when requested to compute a RDD, uses link:spark-rdd-operations.adoc[RDD.map] operator.

==== [[map-example]] map Example

[source, scala]
----
val clicks: DStream[...] = ...
val mappedClicks: ... = clicks.map(...)
----

=== [[reduceByKey]] reduceByKey Operator

[source, scala]
----
reduceByKey(reduceFunc: (V, V) => V): DStream[(K, V)]
reduceByKey(reduceFunc: (V, V) => V, numPartitions: Int): DStream[(K, V)]
reduceByKey(reduceFunc: (V, V) => V, partitioner: Partitioner): DStream[(K, V)]
----

=== [[transform]] transform Operators

```
transform[U](transformFunc: RDD[T] => RDD[U]): DStream[U]
transform[U](transformFunc: (RDD[T], Time) => RDD[U]): DStream[U]
```

`transform` operator applies `transformFunc` function to the generated RDD for a batch.

It creates a link:spark-streaming-transformeddstreams.adoc[TransformedDStream] stream.

==== [[transform-example]] transform Example

```
import org.apache.spark.streaming.{ StreamingContext, Seconds }
val ssc = new StreamingContext(sc, batchDuration = Seconds(5))

val rdd = sc.parallelize(0 to 9)
import org.apache.spark.streaming.dstream.ConstantInputDStream
val clicks = new ConstantInputDStream(ssc, rdd)

import org.apache.spark.rdd.RDD
val transformFunc: RDD[Int] => RDD[Int] = { inputRDD =>
  println(s">>> inputRDD: $inputRDD")

  // Use SparkSQL's DataFrame to manipulate the input records
  import sqlContext.implicits._
  inputRDD.toDF("num").show

  inputRDD
}
clicks.transform(transformFunc).print
```
