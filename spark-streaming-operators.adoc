== Stream Operators

You use *stream operators* to apply transformations to the elements received from input streams.

NOTE: You may use RDDs from other (non-streaming) data sources.

There are two types of operators:

* *transformations* that transform elements in input data RDDs
* *output operators* that link:spark-streaming-dstreams.adoc#register[register input streams as output streams] so the execution starts.

Every `DStream` offers the following operators:

* (output operator) `print` to print 10 elements only or the more general version `print(num: Int)` to print up to `num` elements. See <<print, print operation>> in this document.
* link:spark-streaming-windowedoperators.adoc#slice[slice]
* link:spark-streaming-windowedoperators.adoc#slice[window]
* link:spark-streaming-windowedoperators.adoc#reduceByWindow[reduceByWindow]
* <<reduce, reduce>>
* <<map, map>>
* (output operator) <<foreachRDD, foreachRDD>>
* <<glom, glom>>
* (output operator) link:spark-streaming-operators-saveas.adoc[saveAsObjectFiles]
* (output operator) link:spark-streaming-operators-saveas.adoc[saveAsTextFiles]
* `flatMap`
* `filter`
* `repartition`
* `mapPartitions`
* `count`
* `countByValue`
* a family of `transform`
* a family of `transformWith`
* `countByWindow`
* `countByValueAndWindow`
* `union`

NOTE: `DStream` companion object offers a Scala implicit to convert `DStream[(K, V)]` to `PairDStreamFunctions` with methods on DStreams of key-value pairs.

Most streaming operators come with their own custom `DStream` to offer the service that very often boils down to overriding the link:spark-streaming-dstreams.adoc#contract[compute] method and applying corresponding link:spark-rdd-operations.adoc[RDD operator] on a generated RDD.

=== [[print]] print Operator

`print(num: Int)` operator prints `num` first elements of each RDD in the input stream.

`print` uses `print(num: Int)` with `num` being `10`.

It is a *output operator* (that returns `Unit`).

For each batch, `print` operator prints the following header to the standard output (regardless of the number of elements to be printed out):

```
-------------------------------------------
Time: [time] ms
-------------------------------------------
```

Internally, it calls `RDD.take(num + 1)` (see link:spark-rdd-operations.adoc#actions[take action]) on each RDD in the stream to print `num` elements. It then prints `...` if there are more elements in the RDD (that would otherwise exceed `num` elements being requested to print).

It creates link:spark-streaming-foreachdstreams.adoc[ForEachDStream] stream and register it as an output stream.

=== [[foreachRDD]] foreachRDD Operators

```
foreachRDD(foreachFunc: RDD[T] => Unit): Unit
foreachRDD(foreachFunc: (RDD[T], Time) => Unit): Unit
```

`foreachRDD` operator applies `foreachFunc` function to every RDD in the stream.

It creates link:spark-streaming-foreachdstreams.adoc[ForEachDStream] stream and link:spark-streaming-dstreams.adoc#register[register it as an output stream].

==== [[foreachRDD-example]] foreachRDD Example

```
val clicks: InputDStream[(String, String)] = messages
// println every single data received in clicks input stream
clicks.foreachRDD(rdd => rdd.foreach(println))
```

=== [[glom]] glom Operator

```
glom(): DStream[Array[T]]
```

`glom` operator creates a new stream in which RDDs in the source stream are link:spark-rdd-operations.adoc[RDD.glom] over, i.e. it link:spark-rdd-partitions.adoc#coalesce[coalesces] all elements in RDDs within each partition into an array.

=== [[reduce]] reduce Operator

```
reduce(reduceFunc: (T, T) => T): DStream[T]
```

`reduce` operator creates a new stream of RDDs of a single element that is a result of applying `reduceFunc` to the data received.

Internally, it uses <<map, map>> and <<reduceByKey, reduceByKey>> operators.

==== [[reduce-example]] reduce Example

[source, scala]
----
val clicks: InputDStream[(String, String)] = messages
type T = (String, String)
val reduceFunc: (T, T) => T = {
  case in @ ((k1, v1), (k2, v2)) =>
    println(s">>> input: $in")
    (k2, s"$v1 + $v2")
}
val reduceClicks: DStream[(String, String)] = clicks.reduce(reduceFunc)
reduceClicks.print
----

=== [[map]] map Operator

```
map[U](mapFunc: T => U): DStream[U]
```

`map` operator creates a new stream with the source elements being mapped over using `mapFunc` function.

It creates `MappedDStream` stream that, when requested to compute a RDD, uses link:spark-rdd-operations.adoc[RDD.map] operator.

==== [[map-example]] map Example

[source, scala]
----
val clicks: DStream[...] = ...
val mappedClicks: ... = clicks.map(...)
----

=== [[reduceByKey]] reduceByKey Operator

[source, scala]
----
reduceByKey(reduceFunc: (V, V) => V): DStream[(K, V)]
reduceByKey(reduceFunc: (V, V) => V, numPartitions: Int): DStream[(K, V)]
reduceByKey(reduceFunc: (V, V) => V, partitioner: Partitioner): DStream[(K, V)]
----
