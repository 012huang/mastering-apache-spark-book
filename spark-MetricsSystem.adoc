== [[MetricsSystem]] MetricsSystem

Spark uses http://metrics.dropwizard.io/[Metrics] - a Java library to measure the behaviour of the components.

link:spark-metrics-Source.adoc[org.apache.spark.metrics.source.Source] is the top-level class for the metric registries in Spark. Sources expose their internal status.

Spark uses http://metrics.dropwizard.io/3.1.0/[Metrics 3.1.0].

NOTE: Metrics are only available for cluster modes, i.e. `local` mode turns metrics off.

Metrics System is available at http://localhost:4040/metrics/json/ (for the default setup of a Spark application).

```
$ http http://localhost:4040/metrics/json/
HTTP/1.1 200 OK
Cache-Control: no-cache, no-store, must-revalidate
Content-Length: 2200
Content-Type: text/json;charset=utf-8
Date: Sat, 25 Feb 2017 14:14:16 GMT
Server: Jetty(9.2.z-SNAPSHOT)
X-Frame-Options: SAMEORIGIN

{
    "counters": {
        "app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits": {
            "count": 0
        },
        "app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered": {
            "count": 0
        },
        "app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls": {
            "count": 2
        },
        "app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount": {
            "count": 0
        },
        "app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched": {
            "count": 0
        }
    },
    "gauges": {
    ...
    "timers": {
        "app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime": {
            "count": 0,
            "duration_units": "milliseconds",
            "m15_rate": 0.0,
            "m1_rate": 0.0,
            "m5_rate": 0.0,
            "max": 0.0,
            "mean": 0.0,
            "mean_rate": 0.0,
            "min": 0.0,
            "p50": 0.0,
            "p75": 0.0,
            "p95": 0.0,
            "p98": 0.0,
            "p99": 0.0,
            "p999": 0.0,
            "rate_units": "calls/second",
            "stddev": 0.0
        }
    },
    "version": "3.0.0"
}
```

NOTE: Separate ``MetricsSystem``s are created for `driver`, `executor`, external `shuffleService`, `master`, `applications`, `worker`, `mesos_cluster`. You can access the metrics using corresponding URLs for the services, e.g. `4040` for the `driver`, `8080` for Spark Standalone's `master` and `applications`.

NOTE: You have to use the trailing slash (`/`) to have the output.

Enable `org.apache.spark.metrics.sink.JmxSink` in `conf/metrics.properties` and use jconsole to access Spark metrics through JMX.

```
*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink
```

.jconsole and JmxSink in spark-shell
image::images/spark-metrics-jconsole.png[align="center"]

[TIP]
====
Enable `WARN` logging level for `org.apache.spark.metrics.MetricsSystem` logger to see what happens in `MetricsSystem`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.metrics.MetricsSystem=WARN
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating MetricsSystem Instance

When created, `MetricsSystem` link:spark-metrics-MetricsConfig.adoc#creating-instance[creates a `MetricsConfig`] and link:spark-metrics-MetricsConfig.adoc#initialize[initializes] it immediately.

=== [[createMetricsSystem]] `createMetricsSystem` Method

[source, scala]
----
createMetricsSystem(
  instance: String,
  conf: SparkConf,
  securityMgr: SecurityManager): MetricsSystem
----

`createMetricsSystem` <<creating-instance, creates a `MetricsSystem`>>.

NOTE: `createMetricsSystem` is executed when `SparkEnv` link:spark-sparkenv.adoc#create[is created] for the driver and executors.

=== [[stop]] `stop` Method

CAUTION: FIXME

=== [[start]] `start` Method

CAUTION: FIXME

=== [[registerSource]] `registerSource` Method

CAUTION: FIXME

=== [[removeSource]] `removeSource` Method

CAUTION: FIXME

=== [[report]] `report` Method

CAUTION: FIXME

=== Master

```
$ http http://192.168.1.4:8080/metrics/master/json/path
HTTP/1.1 200 OK
Cache-Control: no-cache, no-store, must-revalidate
Content-Length: 207
Content-Type: text/json;charset=UTF-8
Server: Jetty(8.y.z-SNAPSHOT)
X-Frame-Options: SAMEORIGIN

{
    "counters": {},
    "gauges": {
        "master.aliveWorkers": {
            "value": 0
        },
        "master.apps": {
            "value": 0
        },
        "master.waitingApps": {
            "value": 0
        },
        "master.workers": {
            "value": 0
        }
    },
    "histograms": {},
    "meters": {},
    "timers": {},
    "version": "3.0.0"
}
```

=== [[buildRegistryName]] Building Metrics Source Identifier -- `buildRegistryName` Method

[source, scala]
----
buildRegistryName(source: Source): String
----

NOTE: `buildRegistryName` is used to build the metrics source identifiers for a Spark application's driver and executors, but also for other Spark framework's components (e.g. Spark Standalone's master and workers).

NOTE: `buildRegistryName` uses <<spark.metrics.namespace, spark.metrics.namespace>> and link:spark-Executor.adoc#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components.

(only when <<instance, instance>> is `driver` or `executor`) `buildRegistryName` builds metrics source name that is made up of <<spark.metrics.namespace, spark.metrics.namespace>>, link:spark-Executor.adoc#spark.executor.id[spark.executor.id] and the name of the `source`.

NOTE: `buildRegistryName` uses Metrics' http://metrics.dropwizard.io/3.2.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry] to build metrics source identifiers.

CAUTION: FIXME Finish for the other components.

NOTE: `buildRegistryName` is used when `MetricsSystem` <<registerSource, registers>> or <<removeSource, removes>> a metrics source.

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark.metrics.namespace]] `spark.metrics.namespace`
| link:spark-SparkConf.adoc#spark.app.id[Spark application's ID] (aka `spark.app.id`)
| Root namespace for metrics reporting.

Given a Spark application's ID changes with every invocation of a Spark application, a custom `spark.metrics.namespace` can be specified for metrics reporting.

Used when `MetricsSystem` is requested for a <<buildRegistryName, metrics source identifier>>.
|===
