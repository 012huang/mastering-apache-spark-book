== [[EnsureRequirements]] EnsureRequirements Physical Preparation Rule

[[apply]]
`EnsureRequirements` is a link:spark-sql-QueryExecution-SparkPlan-Preparations.adoc[physical preparation rule] that transforms link:spark-sql-SparkPlan.adoc[physical operators] (up the plan tree):

1. Removes two adjacent link:spark-sql-SparkPlan-ShuffleExchange.adoc[ShuffleExchange] physical operators if the child partitioning scheme guarantees the parent's partitioning

1. For other non-``ShuffleExchange`` physical operators, <<ensureDistributionAndOrdering, ensures partition distribution and ordering>>

`EnsureRequirements` is a part of link:spark-sql-QueryExecution.adoc#preparations[preparations] batch of physical plan rules and is executed in link:spark-sql-QueryExecution.adoc#executedPlan[executedPlan] phase of a link:spark-sql-QueryExecution.adoc[query execution].

[[conf]]
`EnsureRequirements` takes a link:spark-sql-SQLConf.adoc[SQLConf] when created.

=== [[ensureDistributionAndOrdering]] Ensuring Partition Requirements of Physical Operator -- `ensureDistributionAndOrdering` Internal Method

[source, scala]
----
ensureDistributionAndOrdering(operator: SparkPlan): SparkPlan
----

CAUTION: FIXME

NOTE: `ensureDistributionAndOrdering` is used exclusively when `EnsureRequirements` is <<apply, executed>> (i.e. applied to a physical plan).

=== [[createPartitioning]] `createPartitioning` Internal Method

CAUTION: FIXME

=== [[defaultNumPreShufflePartitions]] `defaultNumPreShufflePartitions` Internal Method

CAUTION: FIXME

=== [[withExchangeCoordinator]] Adding ExchangeCoordinator (When Adaptive Query Execution Enabled) -- `withExchangeCoordinator` Internal Method

[source, scala]
----
withExchangeCoordinator(
  children: Seq[SparkPlan],
  requiredChildDistributions: Seq[Distribution]): Seq[SparkPlan]
----

`withExchangeCoordinator` adds link:spark-sql-ExchangeCoordinator.adoc[ExchangeCoordinator] to link:spark-sql-SparkPlan-ShuffleExchange.adoc[ShuffleExchange] operators if link:spark-sql-SQLConf.adoc#adaptiveExecutionEnabled[adaptive query execution is enabled] and partitioning scheme of the `ShuffleExchanges` support `ExchangeCoordinator`.

[[supportsCoordinator]]
Internally, `withExchangeCoordinator` checks if the input `children` operators support `ExchangeCoordinator` which is that either holds:

* If there is at least one link:spark-sql-SparkPlan-ShuffleExchange.adoc[ShuffleExchange] operator, all children are either `ShuffleExchange` with `HashPartitioning` or their link:spark-sql-SparkPlan.adoc#outputPartitioning[output partitioning] is `HashPartitioning` (even inside `PartitioningCollection`)

* There are at least two `children` operators and the input `requiredChildDistributions` are all `ClusteredDistribution`

With link:spark-sql-SQLConf.adoc#adaptiveExecutionEnabled[adaptive query execution enabled] (i.e. when link:spark-sql-SQLConf.adoc#spark.sql.adaptive.enabled[spark.sql.adaptive.enabled] flag is `true`) and the <<supportsCoordinator, operator supports ExchangeCoordinator>>, `withExchangeCoordinator` creates a `ExchangeCoordinator` and:

* For every `ShuffleExchange`, link:spark-sql-SparkPlan-ShuffleExchange.adoc#coordinator[registers the `ExchangeCoordinator`]

* <<createPartitioning, Creates `HashPartitioning` partitioning scheme>> with the link:spark-sql-SQLConf.adoc#numShufflePartitions[default number of partitions to use when shuffling data for joins or aggregations] (as `spark.sql.shuffle.partitions` which is `200` by default) and adds `ShuffleExchange` to the final result (for the current physical operator)

Otherwise (when adaptive query execution is disabled or `children` do not support `ExchangeCoordinator`), `withExchangeCoordinator` returns the input `children` unchanged.

NOTE: `withExchangeCoordinator` is used exclusively for <<ensureDistributionAndOrdering, enforcing partition requirements of a physical operator>>.
