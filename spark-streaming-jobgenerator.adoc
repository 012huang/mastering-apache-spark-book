== JobGenerator

`JobGenerator` asynchronously <<GenerateJobs, generates streaming jobs>> at link:spark-streaming-dstreamgraph.adoc#batchDuration[batch intervals].

NOTE: `JobGenerator` is completely owned and managed by link:spark-streaming-jobscheduler.adoc[JobScheduler], i.e. `JobScheduler` creates an instance and starts it (while link:spark-streaming-jobscheduler.adoc#starting[being started]).

When `JobGenerator` is created, it creates `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer] (with the name being `JobGenerator`) that later, when <<starting, started>>, posts <<GenerateJobs, GenerateJobs>> every link:spark-streaming-dstreamgraph.adoc[DStreamGraph.batchDuration] milliseconds (it is only when `JobGenerator` is started when the JobGenerator eventLoop starts processing events).

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobGenerator` logger to see what happens in JobGenerator.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobGenerator=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[starting]] Starting JobGenerator (start method)

[source, scala]
----
start(): Unit
----

`start` method creates and starts the internal <<eventLoop, eventLoop>>.

NOTE: `start` is called when link:spark-streaming-jobscheduler.adoc#starting[JobScheduler starts].

It first checks whether or not the internal <<eventLoop, eventLoop>> has already been created which is the way to know that the JobScheduler was started. If so, it does nothing and exists.

It creates `checkpointWriter`. It then creates and starts the internal <<eventLoop, eventLoop>>.

Depending on whether checkpoint directory is available or not it <<restarting, restarts itself>> (from the checkpoint directory) or <<startFirstTime, starts>>, respectively.

=== [[startFirstTime]] startFirstTime Method

[source, scala]
----
startFirstTime(): Unit
----

`startFirstTime` starts link:spark-streaming-dstreamgraph.adoc[DStreamGraph] and the `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer].

It first requests `timer` for the *start time* and passes it along to link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph.start] and link:spark-streaming-recurringtimer.adoc[RecurringTimer.start].

NOTE: The start time has the property of being a multiple of link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval] and after the current system time. It is in the hands of link:spark-streaming-recurringtimer.adoc[RecurringTimer] to offer a time with the property.

Because of the property of the start time, link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph.start] is passed the time of one batch interval before the start time.

NOTE: `startFirstTime` is called when <<starting, JobGenerator starts (and checkpoint directory is not available)>>.

You should see the following INFO message in the logs:

```
INFO JobGenerator: Started JobGenerator at [startTime] ms
```

=== [[stopping]] Stopping JobGenerator (stop method)

`JobGenerator.stop(processReceivedData: Boolean)` stops a `JobGenerator`.

NOTE: `JobGenerator` is stopped when...TK

CAUTION: FIXME

=== [[restarting]] Restarting JobGenerator from Checkpoint (restart method)

CAUTION: FIXME

=== [[eventLoop]] JobGenerator eventLoop and JobGeneratorEvent Handler

`JobGenerator` uses `EventLoop` event loop to process `JobGeneratorEvent` events asynchronously (one event at a time) by a dedicated _single_ event thread.

NOTE: `EventLoop` uses unbounded https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingDeque.html[java.util.concurrent.LinkedBlockingDeque] internally.

For every `JobGeneratorEvent` event, you should see the following DEBUG message in the logs:

```
DEBUG JobGenerator: Got event [event]
```

See below for the extensive coverage of supported event types.

==== [[GenerateJobs]] GenerateJobs Event

NOTE: `GenerateJobs` events are posted regularly by the internal `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer], every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval].

When `GenerateJobs(time: Time)` event is received `JobGenerator.generateJobs(time: Time)` is called to link:spark-streaming-jobscheduler.adoc#submitJobSet[submit a collection of streaming jobs for execution].

It first calls link:spark-streaming-receivertracker.adoc#allocateBlocksToBatch[ReceiverTracker.allocateBlocksToBatch] (it does nothing when there are no link:spark-streaming-receiverinputdstreams.adoc[receiver input streams] in use), and then requests link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph for streaming jobs for a given batch time].

If the above two calls have finished successfully, link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker] is requested for data statistics of every registered input stream for the given batch time that together with the collection of streaming jobs (from link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph]) is passed on to link:spark-streaming-jobscheduler.adoc#submitJobSet[JobScheduler.submitJobSet] (as a link:spark-streaming-jobscheduler.adoc[JobSet]).

In case of failure, `JobScheduler.reportError` is called.

Ultimately, <<DoCheckpoint, DoCheckpoint>> event is posted (with `clearCheckpointDataLater` being `false`).

==== [[ClearMetadata]] ClearMetadata Event

It is called to periodically remove old RDDs that have been generated and collected so far by output streams (managed by link:spark-streaming-dstreamgraph.adoc[DStreamGraph]). It is a sort of _garbage collector_.

When `ClearMetadata(time)` arrives, it first asks link:spark-streaming-dstreamgraph.adoc#clearMetadata[DStreamGraph to clear metadata for the given time].

If checkpointing is enabled, it posts <<DoCheckpoint, DoCheckpoint>> (with `clearCheckpointDataLater = true`).

Otherwise, when checkpointing is disabled, it asks link:spark-streaming-dstreamgraph.adoc[DStreamGraph for the maximum remember duration across all the input streams] and requests ReceiverTracker and InputInfoTracker to do their cleanups.

CAUTION: FIXME Finish that part.

It marks the batch fully processed (saving the `time` in the internal `lastProcessedBatch` that tracks the time of the last batch of which cleanup metadata completed successfully).

==== [[DoCheckpoint]] DoCheckpoint Event

CAUTION: FIXME

It calls `doCheckpoint(time: Time, clearCheckpointDataLater: Boolean)` method.

`doCheckpoint` method does nothing when `shouldCheckpoint` is disabled or the current batch `time` is not a multiplier of checkpoint interval (as `ssc.checkpointDuration`).

Otherwise, when it should do checkpointing, you should see the following INFO message in the logs:

```
INFO JobGenerator: Checkpointing graph for time [time] ms
```

It then executes link:spark-streaming-dstreamgraph.adoc[DStreamGraph.updateCheckpointData(time)] and `CheckpointWriter.write(new Checkpoint(ssc, time), clearCheckpointDataLater)`.

==== [[ClearCheckpointData]] ClearCheckpointData Event

CAUTION: FIXME
