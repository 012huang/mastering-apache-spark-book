== JobGenerator

`JobGenerator` asynchronously <<GenerateJobs, generates streaming jobs>> every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval] (using <<timer, recurring timer>>). It also periodically requests <<clearMetadata, clearing up metadata>> and <<clearCheckpointData, checkpoint data>> for each input stream.

NOTE: `JobGenerator` is completely owned and managed by link:spark-streaming-jobscheduler.adoc[JobScheduler], i.e. `JobScheduler` creates an instance of JobGenerator and starts it (while link:spark-streaming-jobscheduler.adoc#starting[being started itself]).

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobGenerator` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobGenerator=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[timer]] timer RecurringTimer

`timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer] (with the name being `JobGenerator`) is used to posts <<GenerateJobs, GenerateJobs>> events to the internal <<eventLoop, event loop>> every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval].

NOTE: `timer` is created (but not started) when `JobGenerator` is.

NOTE: It is started while <<startFirstTime, JobGenerator starts for the first time>>.

=== [[start]][[starting]] Starting JobGenerator (start method)

[source, scala]
----
start(): Unit
----

`start` method creates and starts the internal <<eventLoop, event loop>>.

NOTE: `start` is called when link:spark-streaming-jobscheduler.adoc#starting[JobScheduler starts].

It first checks whether or not the internal event loop has already been created which is the way to know that the JobScheduler was started. If so, it does nothing and exits.

If <<shouldCheckpoint, checkpointing is enabled>>, it creates <<CheckpointWriter, CheckpointWriter>>.

It then creates and starts the internal event loop.

Depending on whether checkpoint directory is available or not it <<restarting, restarts itself>> or <<startFirstTime, starts>>, respectively.

When it is started it starts its own link:spark-streaming-recurringtimer.adoc[recurring timer] and so you should see the following INFO message in the logs:

```
INFO RecurringTimer: Started timer for JobGenerator at time [nextTime]
```

=== [[startFirstTime]] Start Time and startFirstTime Method

[source, scala]
----
startFirstTime(): Unit
----

`startFirstTime` starts link:spark-streaming-dstreamgraph.adoc[DStreamGraph] and the <<timer, timer>>.

NOTE: `startFirstTime` is called when <<starting, JobGenerator starts (and no checkpoint directory is available)>>.

It first requests <<timer, timer>> for the *start time* and passes it along to link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph.start] and link:spark-streaming-recurringtimer.adoc[RecurringTimer.start].

NOTE: The start time has the property of being a multiple of link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval] and after the current system time. It is in the hands of link:spark-streaming-recurringtimer.adoc[RecurringTimer] to calculate a time with the property given a batch interval.

NOTE: Because of the property of the start time, link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph.start] is passed the time of one batch interval before the calculated start time.

You should see the following INFO message in the logs:

```
INFO JobGenerator: Started JobGenerator at [startTime] ms
```

=== [[stop]] Stopping JobGenerator (stop method)

[source, scala]
----
stop(processReceivedData: Boolean): Unit
----

`stop` stops a `JobGenerator`. The `processReceivedData` flag tells whether to stop `JobGenerator` gracefully, i.e. after having processed all received data and pending streaming jobs, or not.

[NOTE]
====
`JobGenerator` is stopped as link:spark-streaming-jobscheduler.adoc#stop[JobScheduler stops].

`processReceivedData` flag in `JobGenerator` corresponds to the value of `processAllReceivedData` in `JobScheduler`.
====

It first checks whether `eventLoop` internal event loop was ever started (through checking `null`).

WARNING: It doesn't set `eventLoop` to `null` (but it is assumed to be the marker).

When `JobGenerator` should stop immediately, i.e. ignoring unprocessed data and pending streaming jobs (`processReceivedData` flag is disabled), you should see the following INFO message in the logs:

```
INFO JobGenerator: Stopping JobGenerator immediately
```

It requests link:spark-streaming-recurringtimer.adoc#stop[the timer to stop forcefully] (`interruptTimer` is enabled) and link:spark-streaming-dstreamgraph.adoc#stop[stops the graph].

Otherwise, when `JobGenerator` should stop gracefully, i.e. `processReceivedData` flag is enabled, you should see the following INFO message in the logs:

```
INFO JobGenerator: Stopping JobGenerator gracefully
```

You should immediately see the following INFO message in the logs:

```
INFO JobGenerator: Waiting for all received blocks to be consumed for job generation
```

`JobGenerator` waits link:spark-streaming-settings.adoc[spark.streaming.gracefulStopTimeout] milliseconds or until link:spark-streaming-receivertracker.adoc#hasUnallocatedBlocks[ReceiverTracker has any blocks left to be processed] (whatever is shorter) before continuing.

NOTE: Poll (sleeping) time is `100` milliseconds and is not configurable.

When a timeout occurs, you should see the WARN message in the logs:

```
WARN JobGenerator: Timed out while stopping the job generator (timeout = [stopTimeoutMs])
```

After the waiting is over, you should see the following INFO message in the logs:

```
INFO JobGenerator: Waited for all received blocks to be consumed for job generation
```

It requests link:spark-streaming-recurringtimer.adoc#stop[timer to stop generating streaming jobs] (`interruptTimer` flag is disabled) and link:spark-streaming-dstreamgraph.adoc#stop[stops the graph].

You should see the following INFO message in the logs:

```
INFO JobGenerator: Stopped generation timer
```

You should immediately see the following INFO message in the logs:

```
INFO JobGenerator: Waiting for jobs to be processed and checkpoints to be written
```

`JobGenerator` waits link:spark-streaming-settings.adoc[spark.streaming.gracefulStopTimeout] milliseconds or until all the batches have been processed (whatever is shorter) before continuing. It waits for batches to complete using <<lastProcessedBatch, last processed batch>> internal property that should eventually be exactly the time when the link:spark-streaming-recurringtimer.adoc#stop[timer was stopped] (it returns the last time for which the streaming job was generated).

NOTE: link:spark-streaming-settings.adoc[spark.streaming.gracefulStopTimeout] is ten times the link:spark-streaming-dstreamgraph.adoc#batch-interval[batch interval] by default.

After the waiting is over, you should see the following INFO message in the logs:

```
INFO JobGenerator: Waited for jobs to be processed and checkpoints to be written
```

Regardless of `processReceivedData` flag, if <<checkpointing, checkpointing was enabled>>, it stops <<CheckpointWriter, CheckpointWriter>>.

It then stops the <<eventLoop, event loop>>.

As the last step, when `JobGenerator` is assumed to be stopped completely, you should see the following INFO message in the logs:

```
INFO JobGenerator: Stopped JobGenerator
```

=== [[restarting]] Restarting from Checkpoint (restart method)

[source, scala]
----
restart(): Unit
----

`restart` restarts JobGenerator from checkpoint.

NOTE: `restart` is called when <<start, JobGenerator starts and checkpoint is present>>.

`restart` first calculates the batches that have been missed while `JobGenerator` was down, i.e. time between the current restart time and the time of link:spark-streaming-streamingcontext.adoc#initialCheckpoint[initial checkpoint].

WARNING: `restart` doesn't check whether the initial checkpoint exists or not that may lead to NPE.

You should see the following INFO message in the logs:

```
INFO JobGenerator: Batches during down time ([size] batches): [downTimes]
```

It then ask the initial checkpoint for pending batches, i.e. the times of streaming job sets.

CAUTION: FIXME What are the pending batches? How does the initial checkpoint track them?

You should see the following INFO message in the logs:

```
INFO JobGenerator: Batches pending processing ([size] batches): [pendingTimes]
```

It then computes the batches to reschedule, i.e. pending and down time batches that are before restart time.

You should see the following INFO message in the logs:

```
INFO JobGenerator: Batches to reschedule ([size] batches): [timesToReschedule]
```

For each batch to reschedule, `restart` requests link:spark-streaming-receivertracker.adoc#allocateBlocksToBatch[ReceiverTracker to allocate blocks to batch] and link:spark-streaming-jobscheduler.adoc#submitJobSet[submits streaming job sets for execution].

NOTE: `restart` mimics <<generateJobs, generateJobs>> method.

It link:spark-streaming-recurringtimer.adoc#start[restarts the timer] (by using `restartTime` as `startTime`).

You should see the following INFO message in the logs:

```
INFO JobGenerator: Restarted JobGenerator at [restartTime]
```

=== [[lastProcessedBatch]] Last Processed Batch (aka lastProcessedBatch)

JobGenerator tracks the last batch time for which the batch was completed and cleanups performed as `lastProcessedBatch` internal property.

The only purpose of the `lastProcessedBatch` property is to allow for <<stop, stopping the streaming context gracefully>>, i.e. to wait until all generated streaming jobs are completed.

NOTE: It is set to the batch time after <<ClearMetadata, ClearMetadata Event>> is processed (when <<checkpointing, checkpointing is disabled>>).

=== [[JobGeneratorEvent]][[eventLoop]] JobGenerator eventLoop and JobGeneratorEvent Handler

`JobGenerator` uses the internal `EventLoop` event loop to process `JobGeneratorEvent` events asynchronously (one event at a time) on a separate dedicated _single_ thread.

NOTE: `EventLoop` uses unbounded https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingDeque.html[java.util.concurrent.LinkedBlockingDeque].

For every `JobGeneratorEvent` event, you should see the following DEBUG message in the logs:

```
DEBUG JobGenerator: Got event [event]
```

There are 4 `JobGeneratorEvent` event types:

* <<GenerateJobs, GenerateJobs>>
* <<ClearMetadata, ClearMetadata>>
* <<DoCheckpoint, DoCheckpoint>>
* <<ClearCheckpointData, ClearCheckpointData>>

See below in the document for the extensive coverage of the supported `JobGeneratorEvent` event types.

==== [[generateJobs]][[GenerateJobs]] GenerateJobs Event and generateJobs method

NOTE: `GenerateJobs` events are posted regularly by the internal `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer] every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval]. The `time` parameter is exactly the current batch time.

When `GenerateJobs(time: Time)` event is received the internal `generateJobs` method is called that link:spark-streaming-jobscheduler.adoc#submitJobSet[submits a collection of streaming jobs for execution].

[source, scala]
----
generateJobs(time: Time)
----

It first calls link:spark-streaming-receivertracker.adoc#allocateBlocksToBatch[ReceiverTracker.allocateBlocksToBatch] (it does nothing when there are no link:spark-streaming-receiverinputdstreams.adoc[receiver input streams] in use), and then requests link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph for streaming jobs for a given batch time].

If the above two calls have finished successfully, link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker] is requested for data statistics of every registered input stream for the given batch time that together with the collection of streaming jobs (from link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph]) is passed on to link:spark-streaming-jobscheduler.adoc#submitJobSet[JobScheduler.submitJobSet] (as a link:spark-streaming-jobscheduler.adoc[JobSet]).

In case of failure, `JobScheduler.reportError` is called.

Ultimately, <<DoCheckpoint, DoCheckpoint>> event is posted (with `clearCheckpointDataLater` being disabled, i.e. `false`).

==== [[DoCheckpoint]][[doCheckpoint]] DoCheckpoint Event and doCheckpoint method

NOTE: `DoCheckpoint` events are posted by JobGenerator itself as part of <<generateJobs, generating streaming jobs>> (with `clearCheckpointDataLater` being disabled, i.e. `false`) and <<clearMetadata, clearing metadata>> (with `clearCheckpointDataLater` being enabled, i.e. `true`).

`DoCheckpoint` events trigger execution of `doCheckpoint` method.

[source, scala]
----
doCheckpoint(time: Time, clearCheckpointDataLater: Boolean)
----

If <<checkpointing, checkpointing is disabled>> or the current batch `time` is not eligible for checkpointing, the method does nothing and exits.

NOTE: A current batch is *eligible for checkpointing* when the time interval between current batch `time` and link:spark-streaming-dstreamgraph.adoc#zero-time[zero time] is a multiple of link:spark-streaming-streamingcontext.adoc#checkpoint-interval[checkpoint interval].

CAUTION: FIXME Can a streaming context have different batch intervals across checkpoints? When can the note above be missed?

Otherwise, when checkpointing should be performed, you should see the following INFO message in the logs:

```
INFO JobGenerator: Checkpointing graph for time [time] ms
```

It requests link:spark-streaming-dstreamgraph.adoc#updateCheckpointData[DStreamGraph for updating checkpoint data] and <<CheckpointWriter-write, CheckpointWriter for writing a new checkpoint>>. Both are given the current batch `time`.

==== [[clearMetadata]][[ClearMetadata]] ClearMetadata Event and clearMetadata method

NOTE: `ClearMetadata` are posted after a micro-batch for a batch time has completed.

It removes old RDDs that have been generated and collected so far by output streams (managed by link:spark-streaming-dstreamgraph.adoc[DStreamGraph]). It is a sort of _garbage collector_.

When `ClearMetadata(time)` arrives, it first asks link:spark-streaming-dstreamgraph.adoc#clearMetadata[DStreamGraph to clear metadata for the given time].

If <<checkpointing, checkpointing is enabled>>, it posts a <<DoCheckpoint, DoCheckpoint>> event (with `clearCheckpointDataLater` being enabled, i.e. `true`) and exits.

Otherwise, when checkpointing is disabled, it asks link:spark-streaming-dstreamgraph.adoc[DStreamGraph for the maximum remember duration across all the input streams] and requests ReceiverTracker and InputInfoTracker to do their cleanups.

CAUTION: FIXME Describe cleanups of ReceiverTracker and InputInfoTracker.

Eventually, it marks the batch as fully processed, i.e. that the batch completed as well as checkpointing or metadata cleanups, using the <<lastProcessedBatch, internal lastProcessedBatch marker>>.

==== [[ClearCheckpointData]][[clearCheckpointData]] ClearCheckpointData Event and clearCheckpointData method

NOTE: `ClearCheckpointData` event is posted after  <<CheckpointWriter-write, checkpoint is saved>> and <<clearMetadata, checkpoint cleanup is requested>>.

`ClearCheckpointData` events trigger execution of `clearCheckpointData` method.

[source, scala]
----
clearCheckpointData(time: Time)
----

In short, `clearCheckpointData` requests link:spark-streaming-dstreamgraph.adoc#clearCheckpointData[DStreamGraph], link:spark-streaming-receivertracker.adoc#cleanupOldBlocksAndBatches[ReceiverTracker], and link:spark-streaming-jobscheduler.adoc#InputInfoTracker-cleanup[InputInfoTracker] to do the cleaning and marks the current batch `time` as <<lastProcessedBatch, fully processed>>.

.JobGenerator and ClearCheckpointData event
image::images/spark-streaming-JobGenerator-ClearCheckpointData-event.png[align="center"]

When executed, `clearCheckpointData` first requests link:spark-streaming-dstreamgraph.adoc#clearCheckpointData[DStreamGraph to clear checkpoint data for the given batch time].

It then asks link:spark-streaming-dstreamgraph.adoc#maximum-remember-interval[DStreamGraph for the maximum remember interval]. Given the maximum remember interval `JobGenerator` requests link:spark-streaming-receivertracker.adoc#cleanupOldBlocksAndBatches[ReceiverTracker to cleanup old blocks and batches] and link:spark-streaming-jobscheduler.adoc#InputInfoTracker-cleanup[InputInfoTracker to do cleanup] for data accumulated before the maximum remember interval (from `time`).

Having done that, the current batch `time` is marked as <<lastProcessedBatch, fully processed>>.

=== [[shouldCheckpoint]][[checkpointing]] Whether or Not to Checkpoint (aka shouldCheckpoint)

`shouldCheckpoint` flag is used to control a <<CheckpointWriter, CheckpointWriter>> as well as whether to <<clearMetadata, post DoCheckpoint in clearMetadata or not>>.

`shouldCheckpoint` flag is enabled (i.e. `true`) when link:spark-streaming-streamingcontext.adoc#checkpointDuration[checkpoint interval] and link:spark-streaming-streamingcontext.adoc#checkpointDir[checkpoint directory] are defined (i.e. not `null`) in StreamingContext.

NOTE: However the flag is completely based on the properties of StreamingContext, these dependent properties are used by JobScheduler only. _Really?_

[CAUTION]
====
FIXME Report an issue

When and what for are they set? Can one of `ssc.checkpointDuration` and `ssc.checkpointDir` be `null`? Do they all have to be set and is this checked somewhere?

Answer: See link:spark-streaming-streamingcontext.adoc#validate[Setup Validation].
====

CAUTION: Potential bug: Can `StreamingContext` have no checkpoint duration set? At least, the batch interval *must* be set. In other words, it's StreamingContext to say whether to checkpoint or not and there should be a method in StreamingContext _not_ JobGenerator.

=== [[CheckpointWriter]] CheckpointWriter

An instance of `CheckpointWriter` is created (lazily) when `JobGenerator` is, but only when <<shouldCheckpoint, JobGenerator should checkpoint>>.

==== [[CheckpointWriter-write]] Writing Checkpoint for Batch Time (write method)

[source, scala]
----
write(checkpoint: Checkpoint, clearCheckpointDataLater: Boolean): Unit
----

`write` method serializes the checkpoint and attempts to write the serialized checkpoint data asynchronously (i.e. on a separate thread).

NOTE: It is called when <<checkpointing, checkpointing is enabled>> and <<DoCheckpoint, JobGenerator processes a DoCheckpoint event>>.

CAUTION: FIXME Describe `Checkpoint.serialize(checkpoint, conf)` and `executor.execute(new CheckpointWriteHandler...`.

You should see the following INFO message in the logs:

```
INFO Submitted checkpoint of time [checkpoint.checkpointTime] writer queue
```

If the asynchronous checkpoint write fails, you should see the following ERROR in the logs:

```
ERROR Could not submit checkpoint task to the thread pool executor
```
