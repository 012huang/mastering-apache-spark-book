== JobGenerator

`JobGenerator` asynchronously <<GenerateJobs, generates streaming jobs>> every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval].

NOTE: `JobGenerator` is completely owned and managed by link:spark-streaming-jobscheduler.adoc[JobScheduler], i.e. `JobScheduler` creates an instance of JobGenerator and starts it (while link:spark-streaming-jobscheduler.adoc#starting[being started itself]).

When `JobGenerator` is created, it creates `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer] (with the name being `JobGenerator`) that later, when <<starting, started>>, posts <<GenerateJobs, GenerateJobs>> events to the internal <<eventLoop, event loop>> every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval]. Using the event loop allows for asynchronous processing of <<JobGeneratorEvent, JobGeneratorEvent events>> on a separate dedicated thread.

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobGenerator` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobGenerator=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[starting]] Starting JobGenerator (start method)

[source, scala]
----
start(): Unit
----

`start` method creates and starts the internal <<eventLoop, event loop>>.

NOTE: `start` is called when link:spark-streaming-jobscheduler.adoc#starting[JobScheduler starts].

It first checks whether or not the internal event loop has already been created which is the way to know that the JobScheduler was started. If so, it does nothing and exits.

If <<shouldCheckpoint, checkpointing is enabled>>, it creates <<CheckpointWriter, CheckpointWriter>>.

It then creates and starts the internal event loop.

Depending on whether checkpoint directory is available or not it <<restarting, restarts itself>> or <<startFirstTime, starts>>, respectively.

When it is started it starts its own link:spark-streaming-recurringtimer.adoc[recurring timer] and so you should see the following INFO message in the logs:

```
INFO RecurringTimer: Started timer for JobGenerator at time [nextTime]
```

=== [[startFirstTime]] Start Time and startFirstTime Method

[source, scala]
----
startFirstTime(): Unit
----

`startFirstTime` starts link:spark-streaming-dstreamgraph.adoc[DStreamGraph] and the `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer].

NOTE: `startFirstTime` is called when <<starting, JobGenerator starts (and checkpoint directory is not available)>>.

It first requests `timer` for the *start time* and passes it along to link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph.start] and link:spark-streaming-recurringtimer.adoc[RecurringTimer.start].

NOTE: The start time has the property of being a multiple of link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval] and after the current system time. It is in the hands of link:spark-streaming-recurringtimer.adoc[RecurringTimer] to calculate a time with the property.

NOTE: Because of the property of the start time, link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph.start] is passed the time of one batch interval before the calculated start time.

You should see the following INFO message in the logs:

```
INFO JobGenerator: Started JobGenerator at [startTime] ms
```

=== [[stop]] Stopping JobGenerator (stop method)

[source, scala]
----
stop(processReceivedData: Boolean): Unit
----

`stop` stops a `JobGenerator`. The `processReceivedData` flag tells whether to stop `JobGenerator` gracefully, i.e. after having processed all received data and pending streaming jobs, or not.

[NOTE]
====
`JobGenerator` is stopped as link:spark-streaming-jobscheduler.adoc#stop[JobScheduler stops].

`processReceivedData` flag in `JobGenerator` corresponds to the value of `processAllReceivedData` in `JobScheduler`.
====

It first checks whether `eventLoop` internal event loop was ever started (through checking `null`).

WARNING: It doesn't set `eventLoop` to `null` (but it is assumed to be the marker).

When `JobGenerator` should stop immediately, i.e. ignoring unprocessed data and pending streaming jobs (`processReceivedData` flag is disabled), you should see the following INFO message in the logs:

```
INFO JobGenerator: Stopping JobGenerator immediately
```

It requests link:spark-streaming-recurringtimer.adoc#stop[the timer to stop forcefully] (`interruptTimer` is enabled) and link:spark-streaming-dstreamgraph.adoc#stop[stops the graph].

Otherwise, when `JobGenerator` should stop gracefully, i.e. `processReceivedData` flag is enabled, you should see the following INFO message in the logs:

```
INFO JobGenerator: Stopping JobGenerator gracefully
```

You should immediately see the following INFO message in the logs:

```
INFO JobGenerator: Waiting for all received blocks to be consumed for job generation
```

`JobGenerator` waits link:spark-streaming-settings.adoc[spark.streaming.gracefulStopTimeout] milliseconds or until link:spark-streaming-receivertracker.adoc#hasUnallocatedBlocks[ReceiverTracker has any blocks left to be processed] (whatever is shorter) before continuing.

NOTE: Poll (sleeping) time is `100` milliseconds and is not configurable.

When a timeout occurs, you should see the WARN message in the logs:

```
WARN JobGenerator: Timed out while stopping the job generator (timeout = [stopTimeoutMs])
```

After the waiting is over, you should see the following INFO message in the logs:

```
INFO JobGenerator: Waited for all received blocks to be consumed for job generation
```

It requests link:spark-streaming-recurringtimer.adoc#stop[timer to stop generating streaming jobs] (`interruptTimer` flag is disabled) and link:spark-streaming-dstreamgraph.adoc#stop[stops the graph].

You should see the following INFO message in the logs:

```
INFO JobGenerator: Stopped generation timer
```

You should immediately see the following INFO message in the logs:

```
INFO JobGenerator: Waiting for jobs to be processed and checkpoints to be written
```

`JobGenerator` waits link:spark-streaming-settings.adoc[spark.streaming.gracefulStopTimeout] milliseconds or until all the batches have been processed (whatever is shorter) before continuing. It waits for batches to complete using <<lastProcessedBatch, last processed batch>> internal property that should eventually be exactly the time when the link:spark-streaming-recurringtimer.adoc#stop[timer was stopped] (it returns the last time for which the streaming job was generated).

NOTE: link:spark-streaming-settings.adoc[spark.streaming.gracefulStopTimeout] is ten times the link:spark-streaming-dstreamgraph.adoc#batch-interval[batch interval] by default.

After the waiting is over, you should see the following INFO message in the logs:

```
INFO JobGenerator: Waited for jobs to be processed and checkpoints to be written
```

Regardless of `processReceivedData` flag, if <<checkpointing, checkpointing was enabled>>, it stops <<CheckpointWriter, CheckpointWriter>>.

It then stops the <<eventLoop, event loop>>.

As the last step, when `JobGenerator` is assumed to be stopped completely, you should see the following INFO message in the logs:

```
INFO JobGenerator: Stopped JobGenerator
```

=== [[restarting]] Restarting JobGenerator from Checkpoint (restart method)

CAUTION: FIXME

=== [[lastProcessedBatch]] Last Processed Batch (aka lastProcessedBatch)

JobGenerator tracks the last batch time for which the batch was completed and cleanups performed as `lastProcessedBatch` internal property.

The only purpose of the `lastProcessedBatch` property is to allow for <<stop, stopping the streaming context gracefully>>, i.e. to wait until all generated streaming jobs are completed.

NOTE: It is set to the batch time after <<ClearMetadata, ClearMetadata Event>> is processed (when <<checkpointing, checkpointing is disabled>>).

=== [[JobGeneratorEvent]][[eventLoop]] JobGenerator eventLoop and JobGeneratorEvent Handler

`JobGenerator` uses the internal `EventLoop` event loop to process `JobGeneratorEvent` events asynchronously (one event at a time) by a dedicated _single_ event thread.

NOTE: `EventLoop` uses unbounded https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingDeque.html[java.util.concurrent.LinkedBlockingDeque].

For every `JobGeneratorEvent` event, you should see the following DEBUG message in the logs:

```
DEBUG JobGenerator: Got event [event]
```

There are 4 `JobGeneratorEvent` event types:

* <<GenerateJobs, GenerateJobs>>
* <<ClearMetadata, ClearMetadata>>
* <<DoCheckpoint, DoCheckpoint>>
* <<ClearCheckpointData, ClearCheckpointData>>

See below in the document for the extensive coverage of the supported `JobGeneratorEvent` event types.

==== [[generateJobs]][[GenerateJobs]] GenerateJobs Event and generateJobs method

NOTE: `GenerateJobs` events are posted regularly by the internal `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer] every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval]. The `time` parameter is exactly the current batch time.

When `GenerateJobs(time: Time)` event is received the internal `generateJobs` method is called that link:spark-streaming-jobscheduler.adoc#submitJobSet[submits a collection of streaming jobs for execution].

[source, scala]
----
generateJobs(time: Time)
----

It first calls link:spark-streaming-receivertracker.adoc#allocateBlocksToBatch[ReceiverTracker.allocateBlocksToBatch] (it does nothing when there are no link:spark-streaming-receiverinputdstreams.adoc[receiver input streams] in use), and then requests link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph for streaming jobs for a given batch time].

If the above two calls have finished successfully, link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker] is requested for data statistics of every registered input stream for the given batch time that together with the collection of streaming jobs (from link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph]) is passed on to link:spark-streaming-jobscheduler.adoc#submitJobSet[JobScheduler.submitJobSet] (as a link:spark-streaming-jobscheduler.adoc[JobSet]).

In case of failure, `JobScheduler.reportError` is called.

Ultimately, <<DoCheckpoint, DoCheckpoint>> event is posted (with `clearCheckpointDataLater` being disabled, i.e. `false`).

==== [[clearMetadata]][[ClearMetadata]] ClearMetadata Event and clearMetadata method

NOTE: `ClearMetadata` are posted after a micro-batch for a batch time has completed.

It removes old RDDs that have been generated and collected so far by output streams (managed by link:spark-streaming-dstreamgraph.adoc[DStreamGraph]). It is a sort of _garbage collector_.

When `ClearMetadata(time)` arrives, it first asks link:spark-streaming-dstreamgraph.adoc#clearMetadata[DStreamGraph to clear metadata for the given time].

If <<checkpointing, checkpointing is enabled>>, it posts a <<DoCheckpoint, DoCheckpoint>> event (with `clearCheckpointDataLater` being enabled, i.e. `true`) and exits.

Otherwise, when checkpointing is disabled, it asks link:spark-streaming-dstreamgraph.adoc[DStreamGraph for the maximum remember duration across all the input streams] and requests ReceiverTracker and InputInfoTracker to do their cleanups.

CAUTION: FIXME Describe cleanups of ReceiverTracker and InputInfoTracker.

Eventually, it marks the batch as fully processed, i.e. that the batch completed as well as checkpointing or metadata cleanups, using the <<lastProcessedBatch, internal lastProcessedBatch marker>>.

==== [[DoCheckpoint]][[doCheckpoint]] DoCheckpoint Event and doCheckpoint method

NOTE: `DoCheckpoint` events are posted by JobGenerator itself as part of <<generateJobs, generating streaming jobs>> (with `clearCheckpointDataLater` being disabled, i.e. `false`) and <<clearMetadata, clearing metadata>> (with `clearCheckpointDataLater` being enabled, i.e. `true`).

`DoCheckpoint` events trigger execution of `doCheckpoint` method.

[source, scala]
----
doCheckpoint(time: Time, clearCheckpointDataLater: Boolean)
----

If <<checkpointing, checkpointing is disabled>> or the current batch `time` is not eligible for checkpointing, the method does nothing and exits.

NOTE: A current batch is *eligible for checkpointing* when the time interval between current batch `time` and link:spark-streaming-dstreamgraph.adoc#zero-time[zero time] is a multiple of link:spark-streaming-streamingcontext.adoc#checkpoint-interval[checkpoint interval].

CAUTION: FIXME Can a streaming context have different batch intervals across checkpoints? When can the note above be missed?

Otherwise, when checkpointing should be performed, you should see the following INFO message in the logs:

```
INFO JobGenerator: Checkpointing graph for time [time] ms
```

It requests link:spark-streaming-dstreamgraph.adoc#updateCheckpointData[DStreamGraph for updating checkpoint data] and <<CheckpointWriter-write, CheckpointWriter for writing a new checkpoint>>. Both are given the current batch `time`.

==== [[ClearCheckpointData]][[clearCheckpointData]] ClearCheckpointData Event and clearCheckpointData method

`ClearCheckpointData` events trigger execution of `clearCheckpointData` method.

[source, scala]
----
clearCheckpointData(time: Time)
----

When executed, `clearCheckpointData` first requests link:spark-streaming-dstreamgraph.adoc#clearCheckpointData[DStreamGraph to clear checkpoint data for the given batch time].

.JobGenerator and ClearCheckpointData event
image::images/spark-streaming-JobGenerator-ClearCheckpointData-event.png[align="center"]

CAUTION: FIXME Finish me.

=== [[shouldCheckpoint]][[checkpointing]] Whether or Not to Checkpoint (aka shouldCheckpoint)

`JobGenerator` requests StreamingContext for link:spark-streaming-streamingcontext.adoc#checkpointDuration[checkpoint interval] and link:spark-streaming-streamingcontext.adoc#checkpointDir[checkpoint directory] to control the internal `shouldCheckpoint` flag.

`shouldCheckpoint` flag is enabled (i.e. `true`) when the checkpoint interval and directory are defined.

CAUTION: FIXME When and what for are they set? Can one of `ssc.checkpointDuration` and `ssc.checkpointDir` be `null`? Do they all have to be set and is this checked somewhere?

NOTE: `shouldCheckpoint` flag is later used to control a <<CheckpointWriter, CheckpointWriter>> as well as whether to <<clearMetadata, post DoCheckpoint in clearMetadata or not>>.

CAUTION: Potential bug: Can `StreamingContext` have no checkpoint duration set? At least, the batch interval *must* be set. In other words, it's StreamingContext to say whether to checkpoint or not and there should be a method in StreamingContext _not_ JobGenerator.

=== [[CheckpointWriter]] CheckpointWriter

An instance of `CheckpointWriter` is created (lazily) when `JobGenerator` is, but only when <<shouldCheckpoint, JobGenerator should checkpoint>>.

==== [[CheckpointWriter-write]] Writing Checkpoint for Batch Time (write method)

[source, scala]
----
write(checkpoint: Checkpoint, clearCheckpointDataLater: Boolean): Unit
----

`write` method serializes the checkpoint and attempts to write the serialized checkpoint data asynchronously (i.e. on a separate thread).

NOTE: It is called when <<checkpointing, checkpointing is enabled>> and <<DoCheckpoint, JobGenerator processes a DoCheckpoint event>>.

CAUTION: FIXME Describe `Checkpoint.serialize(checkpoint, conf)` and `executor.execute(new CheckpointWriteHandler...`.

You should see the following INFO message in the logs:

```
INFO Submitted checkpoint of time [checkpoint.checkpointTime] writer queue
```

If the asynchronous checkpoint write fails, you should see the following ERROR in the logs:

```
ERROR Could not submit checkpoint task to the thread pool executor
```
