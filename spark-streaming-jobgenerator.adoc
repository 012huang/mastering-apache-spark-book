== JobGenerator

`JobGenerator` asynchronously <<GenerateJobs, generates streaming jobs>> at link:spark-streaming-dstreamgraph.adoc#batchDuration[batch duration] intervals.

NOTE: `JobGenerator` is completely owned and managed by link:spark-streaming-jobscheduler.adoc[JobScheduler], i.e. `JobScheduler` creates an instance and starts it (while link:spark-streaming-jobscheduler.adoc#starting[being started]).

When `JobGenerator` is created, it creates `timer` link:spark-streaming-jobscheduler.adoc#RecurringTimer[RecurringTimer] (with the name being `JobGenerator`) that later, when started, posts link:spark-streaming.adoc#GenerateJobs[GenerateJobs] events to <<eventLoop, JobGenerator eventLoop>> (it is only when `JobGenerator` is started when the JobGenerator eventLoop starts processing events).

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobGenerator` logger to see what happens in JobGenerator.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobGenerator=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[starting]] Starting JobGenerator (start method)

`JobGenerator.start()` starts a `JobGenerator`.

NOTE: `JobGenerator` is started when link:spark-streaming-jobscheduler.adoc#starting[JobScheduler starts].

It starts the internal <<eventLoop, eventLoop>> that processes `JobGeneratorEvent` events.

When it started for the first time (not from checkpoint), it starts link:spark-streaming-dstreamgraph.adoc[DStreamGraph] and the `timer` that posts <<GenerateJobs, GenerateJobs>> every link:spark-streaming-dstreamgraph.adoc[DStreamGraph.batchDuration] milliseconds.

You should see the following INFO message in the logs:

```
INFO JobGenerator: Started JobGenerator at [startTime] ms
```

CAUTION: FIXME What happens when checkpoint exists.

=== [[stopping]] Stopping JobGenerator (stop method)

`JobGenerator.stop(processReceivedData: Boolean)` stops a `JobGenerator`.

NOTE: `JobGenerator` is stopped when...TK

CAUTION: FIXME

=== [[restarting]] Restarting JobGenerator from Checkpoint (restart method)

CAUTION: FIXME

=== [[eventLoop]] JobGenerator eventLoop and JobGeneratorEvent Handler

`JobGenerator` uses `EventLoop` event loop to process `JobGeneratorEvent` events asynchronously (one event at a time) by a dedicated _single_ event thread.

NOTE: `EventLoop` uses unbounded https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingDeque.html[java.util.concurrent.LinkedBlockingDeque] internally.

For every `JobGeneratorEvent` event, you should see the following DEBUG message in the logs:

```
DEBUG JobGenerator: Got event [event]
```

See below for the extensive coverage of supported event types.

==== [[GenerateJobs]] GenerateJobs Event

NOTE: `GenerateJobs` events are posted regularly by the internal `timer` RecurringTimer, every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval].

When `GenerateJobs(time: Time)` event is received `JobGenerator.generateJobs(time: Time)` is called.

It calls link:spark-streaming-receivertracker.adoc[ReceiverTracker.allocateBlocksToBatch], and then calls link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph.generateJobs].

If the above two calls have finished successfully, link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker] is requested for information about input streams for the given batch time that together with the collection of jobs (from link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph.generateJobs]) is passed on to link:spark-streaming-jobscheduler.adoc#submitJobSet[JobScheduler.submitJobSet] (as a link:spark-streaming-jobscheduler.adoc[JobSet]).

In case of failure, `JobScheduler.reportError` is called.

Ultimately, <<DoCheckpoint, DoCheckpoint>> event is posted (with `clearCheckpointDataLater` being `false`).

==== [[ClearMetadata]] ClearMetadata Event

It is called to periodically remove old RDDs that have been generated and collected so far by output streams (managed by link:spark-streaming-dstreamgraph.adoc[DStreamGraph]). It is a sort of _garbage collector_.

When `ClearMetadata(time)` arrives, it first asks link:spark-streaming-dstreamgraph.adoc#clearMetadata[DStreamGraph to clear metadata for the given time].

If checkpointing is enabled, it posts <<DoCheckpoint, DoCheckpoint>> (with `clearCheckpointDataLater = true`).

Otherwise, when checkpointing is disabled, it asks link:spark-streaming-dstreamgraph.adoc[DStreamGraph for the maximum remember duration across all the input streams] and requests ReceiverTracker and InputInfoTracker to do their cleanups.

CAUTION: FIXME Finish that part.

It marks the batch fully processed (saving the `time` in the internal `lastProcessedBatch` that tracks the time of the last batch of which cleanup metadata completed successfully).

==== [[DoCheckpoint]] DoCheckpoint Event

CAUTION: FIXME

==== [[ClearCheckpointData]] ClearCheckpointData Event

CAUTION: FIXME
