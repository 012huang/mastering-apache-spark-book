== JobGenerator

`JobGenerator` asynchronously <<GenerateJobs, generates streaming jobs>> every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval].

NOTE: `JobGenerator` is completely owned and managed by link:spark-streaming-jobscheduler.adoc[JobScheduler], i.e. `JobScheduler` creates an instance of JobGenerator and starts it (while link:spark-streaming-jobscheduler.adoc#starting[being started itself]).

When `JobGenerator` is created, it creates `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer] (with the name being `JobGenerator`) that later, when <<starting, started>>, posts <<GenerateJobs, GenerateJobs>> events to the internal <<eventLoop, event loop>> every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval]. Using the event loop allows for asynchronous processing of JobGeneratorEvent events on a separate dedicated thread.

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobGenerator` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobGenerator=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[starting]] Starting JobGenerator (start method)

[source, scala]
----
start(): Unit
----

`start` method creates and starts the internal <<eventLoop, event loop>>.

NOTE: `start` is called when link:spark-streaming-jobscheduler.adoc#starting[JobScheduler starts].

It first checks whether or not the internal event loop has already been created which is the way to know that the JobScheduler was started. If so, it does nothing and exits.

If checkpointing is enabled, it creates <<CheckpointWriter, CheckpointWriter>>.

NOTE: Whether or not checkpointing is enabled is controlled by two settings of StreamingContext - `checkpointDuration` and `checkpointDir`. See <<CheckpointWriter, CheckpointWriter>> later in the document.

It then creates and starts the internal event loop.

Depending on whether checkpoint directory is available or not it <<restarting, restarts itself>> or <<startFirstTime, starts>>, respectively.

When it is started it starts its own link:spark-streaming-recurringtimer.adoc[recurring timer] and so you should see the following INFO message in the logs:

```
INFO RecurringTimer: Started timer for JobGenerator at time [nextTime]
```

=== [[startFirstTime]] Start Time and startFirstTime Method

[source, scala]
----
startFirstTime(): Unit
----

`startFirstTime` starts link:spark-streaming-dstreamgraph.adoc[DStreamGraph] and the `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer].

NOTE: `startFirstTime` is called when <<starting, JobGenerator starts (and checkpoint directory is not available)>>.

It first requests `timer` for the *start time* and passes it along to link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph.start] and link:spark-streaming-recurringtimer.adoc[RecurringTimer.start].

NOTE: The start time has the property of being a multiple of link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval] and after the current system time. It is in the hands of link:spark-streaming-recurringtimer.adoc[RecurringTimer] to calculate a time with the property.

NOTE: Because of the property of the start time, link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph.start] is passed the time of one batch interval before the calculated start time.

You should see the following INFO message in the logs:

```
INFO JobGenerator: Started JobGenerator at [startTime] ms
```

=== [[stopping]] Stopping JobGenerator (stop method)

CAUTION: FIXME

[source, scala]
----
stop(processReceivedData: Boolean): Unit
----

`stop(processReceivedData: Boolean)` stops a `JobGenerator`.

NOTE: `JobGenerator` is stopped when...TK

If checkpointing was enabled, it stops <<CheckpointWriter, CheckpointWriter>>.

=== [[restarting]] Restarting JobGenerator from Checkpoint (restart method)

CAUTION: FIXME

=== [[eventLoop]] JobGenerator eventLoop and JobGeneratorEvent Handler

`JobGenerator` uses the internal `EventLoop` event loop to process `JobGeneratorEvent` events asynchronously (one event at a time) by a dedicated _single_ event thread.

NOTE: `EventLoop` uses unbounded https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingDeque.html[java.util.concurrent.LinkedBlockingDeque].

For every `JobGeneratorEvent` event, you should see the following DEBUG message in the logs:

```
DEBUG JobGenerator: Got event [event]
```

There are 4 `JobGeneratorEvent` event types:

* <<GenerateJobs, GenerateJobs>>
* <<ClearMetadata, ClearMetadata>>
* <<DoCheckpoint, DoCheckpoint>>
* <<ClearCheckpointData, ClearCheckpointData>>

See below in the document for the extensive coverage of the supported `JobGeneratorEvent` event types.

==== [[GenerateJobs]] GenerateJobs Event

NOTE: `GenerateJobs` events are posted regularly by the internal `timer` link:spark-streaming-recurringtimer.adoc[RecurringTimer] every link:spark-streaming-dstreamgraph.adoc#batchDuration[batch interval].

When `GenerateJobs(time: Time)` event is received `JobGenerator.generateJobs(time: Time)` method is called that link:spark-streaming-jobscheduler.adoc#submitJobSet[submits a collection of streaming jobs for execution].

It first calls link:spark-streaming-receivertracker.adoc#allocateBlocksToBatch[ReceiverTracker.allocateBlocksToBatch] (it does nothing when there are no link:spark-streaming-receiverinputdstreams.adoc[receiver input streams] in use), and then requests link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph for streaming jobs for a given batch time].

If the above two calls have finished successfully, link:spark-streaming-jobscheduler.adoc#InputInfoTracker[InputInfoTracker] is requested for data statistics of every registered input stream for the given batch time that together with the collection of streaming jobs (from link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph]) is passed on to link:spark-streaming-jobscheduler.adoc#submitJobSet[JobScheduler.submitJobSet] (as a link:spark-streaming-jobscheduler.adoc[JobSet]).

In case of failure, `JobScheduler.reportError` is called.

Ultimately, <<DoCheckpoint, DoCheckpoint>> event is posted (with `clearCheckpointDataLater` being `false`).

==== [[ClearMetadata]] ClearMetadata Event

It is called to periodically remove old RDDs that have been generated and collected so far by output streams (managed by link:spark-streaming-dstreamgraph.adoc[DStreamGraph]). It is a sort of _garbage collector_.

When `ClearMetadata(time)` arrives, it first asks link:spark-streaming-dstreamgraph.adoc#clearMetadata[DStreamGraph to clear metadata for the given time].

If checkpointing is enabled, it posts <<DoCheckpoint, DoCheckpoint>> (with `clearCheckpointDataLater = true`).

Otherwise, when checkpointing is disabled, it asks link:spark-streaming-dstreamgraph.adoc[DStreamGraph for the maximum remember duration across all the input streams] and requests ReceiverTracker and InputInfoTracker to do their cleanups.

CAUTION: FIXME Finish that part.

It marks the batch fully processed (saving the `time` in the internal `lastProcessedBatch` that tracks the time of the last batch of which cleanup metadata completed successfully).

==== [[DoCheckpoint]] DoCheckpoint Event and doCheckpoint method

CAUTION: When is `DoCheckpoint` posted? When is `clearCheckpointDataLater` set?

A `DoCheckpoint` event triggers `doCheckpoint(time: Time, clearCheckpointDataLater: Boolean)` method.

If checkpointing is disabled or the current batch `time` is not eligible for checkpointing, the method does nothing and exits.

CAUTION: FIXME Explain "eligible for checkpointing", i.e. `(time - graph.zeroTime).isMultipleOf(ssc.checkpointDuration)`

Otherwise, when checkpointing should be executed, you should see the following INFO message in the logs:

```
INFO JobGenerator: Checkpointing graph for time [time] ms
```

It requests link:spark-streaming-dstreamgraph.adoc#updateCheckpointData[DStreamGraph for updating checkpoint data] and <<CheckpointWriter-write, CheckpointWriter for writing a new checkpoint>>. Both are given the current batch `time`.

==== [[ClearCheckpointData]] ClearCheckpointData Event

CAUTION: FIXME

=== [[checkpointing]] Checkpointing

CAUTION: FIXME

When is checkpointing enabled?

=== [[CheckpointWriter]] CheckpointWriter

An instance of `CheckpointWriter` is created (lazily) when `JobGenerator` is and `shouldCheckpoint` is enabled.

`shouldCheckpoint` is an internal flag that is enabled (i.e. `true`) when `ssc.checkpointDuration` and `ssc.checkpointDir` are set (i.e. not `null`)

CAUTION: FIXME When and what for are they set?

CAUTION: FIXME Can one of `ssc.checkpointDuration` and `ssc.checkpointDir` be `null`? Do they all have to be set and is this checked somewhere?

==== [[CheckpointWriter-write]] Writing Checkpoint for Batch Time (write method)

[source, scala]
----
write(checkpoint: Checkpoint, clearCheckpointDataLater: Boolean): Unit
----

`write` method serializes the checkpoint and attempts to write the serialized checkpoint data asynchronously (i.e. on a separate thread).

NOTE: It is called when <<checkpointing, checkpointing is enabled>> and <<DoCheckpoint, JobGenerator processes a DoCheckpoint event>>.

CAUTION: FIXME Describe `Checkpoint.serialize(checkpoint, conf)` and `executor.execute(new CheckpointWriteHandler...`.

You should see the following INFO message in the logs:

```
INFO Submitted checkpoint of time [checkpoint.checkpointTime] writer queue
```

If the asynchronous checkpoint write fails, you should see the following ERROR in the logs:

```
ERROR Could not submit checkpoint task to the thread pool executor
```
