== [[Aggregation]] Aggregation Execution Planning Strategy for Aggregate Physical Operators

`Aggregation` is an link:spark-sql-SparkStrategy.adoc[execution planning strategy] that link:spark-sql-SparkPlanner.adoc[SparkPlanner] uses to <<apply, select aggregate physical operator for Aggregate logical operator>> (in a query's logical plan).

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...
// structured query with count aggregate function
val q = spark.range(5).
  groupBy($"id" % 2 as "group").
  agg(count("id") as "count")
import q.queryExecution.optimizedPlan
scala> println(optimizedPlan.numberedTreeString)
00 Aggregate [(id#0L % 2)], [(id#0L % 2) AS group#3L, count(1) AS count#8L]
01 +- Range (0, 5, step=1, splits=Some(8))

import spark.sessionState.planner.Aggregation
val physicalPlan = Aggregation.apply(optimizedPlan)

// HashAggregateExec selected
scala> println(physicalPlan.head.numberedTreeString)
00 HashAggregate(keys=[(id#0L % 2)#12L], functions=[count(1)], output=[group#3L, count#8L])
01 +- HashAggregate(keys=[(id#0L % 2) AS (id#0L % 2)#12L], functions=[partial_count(1)], output=[(id#0L % 2)#12L, count#14L])
02    +- PlanLater Range (0, 5, step=1, splits=Some(8))
----

[[aggregate-physical-operator-preference]]
`Aggregation` <<aggregate-physical-operator-selection-criteria, can select>> the following aggregate physical operators (in order of preference):

1. link:spark-sql-SparkPlan-HashAggregateExec.adoc[HashAggregateExec]

1. link:spark-sql-SparkPlan-ObjectHashAggregateExec.adoc[ObjectHashAggregateExec]

1. link:spark-sql-SparkPlan-SortAggregateExec.adoc[SortAggregateExec]

CAUTION: FIXME Show the other physical operators chosen.

=== [[apply]] Executing Planning Strategy -- `apply` Method

[source, scala]
----
apply(plan: LogicalPlan): Seq[SparkPlan]
----

`apply`...FIXME

NOTE: `apply` is a part of link:spark-sql-catalyst-GenericStrategy.adoc#apply[GenericStrategy Contract] to execute a planning strategy.

=== [[PhysicalAggregation]][[PhysicalAggregation-unapply]] Destructuring Logical Plan -- `PhysicalAggregation.unapply` Method

[source, scala]
----
unapply(a: Any): Option[ReturnType]
----

`unapply` destructures a link:spark-sql-LogicalPlan.adoc[logical plan], i.e. breaks a `LogicalPlan` into pieces.

CAUTION: FIXME What pieces?

[NOTE]
====
`ReturnType` is a type alias (aka _type synonym_) for a 4-element tuple with grouping, aggregate and result link:spark-sql-Expression.adoc[expressions], and child link:spark-sql-LogicalPlan.adoc[logical operator].

[source, scala]
----
type ReturnType =
  (Seq[NamedExpression], Seq[AggregateExpression], Seq[NamedExpression], LogicalPlan)
----
====

NOTE: `PhysicalAggregation` is a Scala http://docs.scala-lang.org/tutorials/tour/extractor-objects.html[extractor object] with a single <<PhysicalAggregation-unapply, unapply>> method.

=== [[planAggregateWithOneDistinct]][[AggUtils-planAggregateWithOneDistinct]] `AggUtils.planAggregateWithOneDistinct` Method

CAUTION: FIXME

=== [[AggUtils-createAggregate]] Selecting Aggregate Physical Operator Given Aggregate Expressions -- `AggUtils.createAggregate` Internal Method

[source, scala]
----
createAggregate(
  requiredChildDistributionExpressions: Option[Seq[Expression]] = None,
  groupingExpressions: Seq[NamedExpression] = Nil,
  aggregateExpressions: Seq[AggregateExpression] = Nil,
  aggregateAttributes: Seq[Attribute] = Nil,
  initialInputBufferOffset: Int = 0,
  resultExpressions: Seq[NamedExpression] = Nil,
  child: SparkPlan): SparkPlan
----

Internally, `createAggregate` selects and creates a link:spark-sql-SparkPlan.adoc[physical operator] given the input `aggregateExpressions` link:spark-sql-Expression-AggregateExpression.adoc[aggregate expressions].

[[aggregate-physical-operator-selection-criteria]]
.createAggregate's Aggregate Physical Operator Selection Criteria (in execution order)
[cols="1,2",options="header",width="100%"]
|===
| Aggregate Physical Operator
| Selection Criteria

| link:spark-sql-SparkPlan-HashAggregateExec.adoc[HashAggregateExec]
a| `HashAggregateExec` link:spark-sql-SparkPlan-HashAggregateExec.adoc#supportsAggregate[supports] all `aggBufferAttributes` of the input `aggregateExpressions` link:spark-sql-Expression-AggregateExpression.adoc[aggregate expressions].

| link:spark-sql-SparkPlan-ObjectHashAggregateExec.adoc[ObjectHashAggregateExec]
a|

1. link:spark-sql-SQLConf.adoc#spark.sql.execution.useObjectHashAggregateExec[spark.sql.execution.useObjectHashAggregateExec] internal flag enabled (it is by default)

1. `ObjectHashAggregateExec` link:spark-sql-SparkPlan-ObjectHashAggregateExec.adoc#supportsAggregate[supports] the input `aggregateExpressions` link:spark-sql-Expression-AggregateExpression.adoc[aggregate expressions].

| link:spark-sql-SparkPlan-SortAggregateExec.adoc[SortAggregateExec]
| When all the above requirements could not be met.
|===

[NOTE]
====
`createAggregate` is used in:

* <<AggUtils-planAggregateWithoutDistinct, AggUtils.planAggregateWithoutDistinct>>

* <<AggUtils-planAggregateWithOneDistinct, AggUtils.planAggregateWithOneDistinct>>

* Structured Streaming's `StatefulAggregationStrategy` (`planStreamingAggregation`)
====

=== [[AggUtils]][[AggUtils-planAggregateWithoutDistinct]] `AggUtils.planAggregateWithoutDistinct` Method

[source, scala]
----
planAggregateWithoutDistinct(
  groupingExpressions: Seq[NamedExpression],
  aggregateExpressions: Seq[AggregateExpression],
  resultExpressions: Seq[NamedExpression],
  child: SparkPlan): Seq[SparkPlan]
----

`planAggregateWithoutDistinct` is a two-step physical operator generator.

`planAggregateWithoutDistinct` first <<AggUtils-createAggregate, creates an aggregate physical operator>> with `aggregateExpressions` in `Partial` mode.

In the end, `planAggregateWithoutDistinct` <<AggUtils-createAggregate, creates another aggregate physical operator>> with `aggregateExpressions` in `Final` mode and the first aggregate operator as the child.

NOTE: `planAggregateWithoutDistinct` is used exclusively when `Aggregation` execution planning strategy <<apply, is executed>> (with no `AggregateExpressions` being link:spark-sql-Expression-AggregateExpression.adoc#isDistinct[distinct]).
