== QueryExecution

`QueryExecution` requires link:spark-sql-sqlcontext.adoc[SQLContext] and link:spark-sql-logical-plan.adoc[LogicalPlan].

CAUTION: FIXME What's `planner`? `analyzed`? Why do we need `assertAnalyzed` and `assertSupported`?

It belongs to `org.apache.spark.sql.execution` package.

NOTE: `QueryExecution` is a transient feature of a link:spark-sql-dataset.adoc[Dataset], i.e. it is not preserved across serializations.

[source, scala]
----
val ds = spark.range(5)
scala> ds.queryExecution
res17: org.apache.spark.sql.execution.QueryExecution =
== Parsed Logical Plan ==
Range 0, 5, 1, 8, [id#39L]

== Analyzed Logical Plan ==
id: bigint
Range 0, 5, 1, 8, [id#39L]

== Optimized Logical Plan ==
Range 0, 5, 1, 8, [id#39L]

== Physical Plan ==
WholeStageCodegen
:  +- Range 0, 1, 8, 5, [id#39L]
----

=== [[IncrementalExecution]] IncrementalExecution

`IncrementalExecution` is a custom `QueryExecution` with `OutputMode`, `checkpointLocation`, and `currentBatchId`.

It lives in `org.apache.spark.sql.execution.streaming` package.

CAUTION: FIXME What is `stateStrategy`?

Stateful operators in the query plan are numbered using `operatorId` that starts with `0`.

`IncrementalExecution` adds one `Rule[SparkPlan]` called `state` to <<preparations, preparations>> sequence of rules as the first element.

CAUTION: FIXME What does `IncrementalExecution` do? Where is it used?

=== [[preparations]] preparations - Rules to apply before Query Execution

It contains a sequence of rules called `preparations` (of type `Seq[Rule[SparkPlan]]`) that will be applied in order to the physical plan before execution, i.e. generates `SparkPlan` by executing <<executedPlan, executedPlan>> lazy value.

`preparations` rules are meant to allow access to the intermediate phases of query execution for developers.

=== [[executedPlan]] executedPlan SparkPlan

`executedPlan` lazy value is a `SparkPlan` ready for execution after applying <<preparations, the rules in preparations>>.

=== [[debug]] Debugging Query Execution

`debug` package object contains methods for debugging query execution that you can apply to your queries as `Dataset` objects.

[source, scala]
----
debug()
debugCodegen()
----

The `debug` package object belongs to `org.apache.spark.sql.execution.debug` package

Import the package and do the full analysis using `debug` method.

[source, scala]
----
import org.apache.spark.sql.execution.debug._

scala> spark.range(10).where('id === 4).debug
Results returned: 1
== WholeStageCodegen ==
Tuples output: 1
 id LongType: {java.lang.Long}
== Filter (id#12L = 4) ==
Tuples output: 0
 id LongType: {}
== Range (0, 10, splits=8) ==
Tuples output: 0
 id LongType: {}
----

You can also perform `debugCodegen`.

[source, scala]
----
import org.apache.spark.sql.execution.debug._

scala> spark.range(10).where('id === 4).debugCodegen
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 ==
*Filter (id#8L = 4)
+- *Range (0, 10, splits=8)

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ /**
 * Codegend pipeline for
 * Filter (id#8L = 4)
 * +- Range (0, 10, splits=8)
 */
...
----

[source, scala]
----
scala> spark.range(1, 1000).select('id+1+2+3, 'id+4+5+6).queryExecution.debug.codegen()
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 ==
*Project [(((id#0L + 1) + 2) + 3) AS (((id + 1) + 2) + 3)#3L,(((id#0L + 4) + 5) + 6) AS (((id + 4) + 5) + 6)#4L]
+- *Range (1, 1000, splits=8)

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ /**
 * Codegend pipeline for
...
/* 111 */       if (shouldStop()) return;
/* 112 */     }
/* 113 */   }
/* 114 */ }
----
