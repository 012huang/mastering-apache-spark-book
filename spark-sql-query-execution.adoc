== QueryExecution

`QueryExecution` requires link:spark-sql-sqlcontext.adoc[SQLContext] and link:spark-sql-logical-plan.adoc[LogicalPlan].

CAUTION: FIXME What's `planner`? `analyzed`? Why do we need `assertAnalyzed` and `assertSupported`?

It belongs to `org.apache.spark.sql.execution` package.

NOTE: `QueryExecution` is a transient feature of a link:spark-sql-dataset.adoc[Dataset], i.e. it is not preserved across serializations.

[source, scala]
----
val ds = sqlContext.range(5)
scala> ds.queryExecution
res17: org.apache.spark.sql.execution.QueryExecution =
== Parsed Logical Plan ==
Range 0, 5, 1, 8, [id#39L]

== Analyzed Logical Plan ==
id: bigint
Range 0, 5, 1, 8, [id#39L]

== Optimized Logical Plan ==
Range 0, 5, 1, 8, [id#39L]

== Physical Plan ==
WholeStageCodegen
:  +- Range 0, 1, 8, 5, [id#39L]
----

=== [[IncrementalExecution]] IncrementalExecution

`IncrementalExecution` is a custom `QueryExecution` with `OutputMode`, `checkpointLocation`, and `currentBatchId`.

It lives in `org.apache.spark.sql.execution.streaming` package.

CAUTION: FIXME What is `stateStrategy`?

Stateful operators in the query plan are numbered using `operatorId` that starts with `0`.

`IncrementalExecution` adds one `Rule[SparkPlan]` called `state` to <<preparations, preparations>> sequence of rules as the first element.

CAUTION: FIXME What does `IncrementalExecution` do? Where is it used?

=== [[preparations]] preparations - Rules to apply before Query Execution

It contains a sequence of rules called `preparations` (of type `Seq[Rule[SparkPlan]]`) that will be applied in order to the physical plan before execution, i.e. generates `SparkPlan` by executing <<executedPlan, executedPlan>> lazy value.

`preparations` rules are meant to allow access to the intermediate phases of query execution for developers.

=== [[executedPlan]] executedPlan SparkPlan

`executedPlan` lazy value is a `SparkPlan` ready for execution after applying <<preparations, the rules in preparations>>.

=== [[debug]] Debugging Query Execution

`debug` package object contains methods for debugging query execution that you can apply to your queries as `Dataset` objects.

[source, scala]
----
debug()
debugCodegen()
----

The `debug` package object belongs to `org.apache.spark.sql.execution.debug` package

Import the package and do the full analysis using `debug` method.

[source, scala]
----
import org.apache.spark.sql.execution.debug._
scala> sqlContext.range(10).where('id === 4).debug
16/04/18 23:44:04 DEBUG CodeGenerator:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ /** Codegened pipeline for:
...
/* 133 */       if (shouldStop()) return;
/* 134 */     }
/* 135 */   }
/* 136 */ }

16/04/18 23:44:04 INFO CodeGenerator: Code generated in 63.1855 ms
Results returned: 1
== WholeStageCodegen ==
Tuples output: 1
 id LongType: {java.lang.Long}
== Filter (id#20L = 4) ==
Tuples output: 0
 id LongType: {}
== Range 0, 1, 8, 10, [id#20L] ==
Tuples output: 0
 id LongType: {}
----

You can also perform `debugCodegen`.

[source, scala]
----
import org.apache.spark.sql.execution.debug._
scala> sqlContext.range(10).where('id === 4).debugCodegen
16/04/18 23:45:33 DEBUG CodeGenerator:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
...
16/04/18 23:45:33 INFO CodeGenerator: Code generated in 24.757977 ms
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 ==
WholeStageCodegen
:  +- Filter (id#24L = 4)
:     +- Range 0, 1, 8, 10, [id#24L]

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ /** Codegened pipeline for:
/* 006 */ * Filter (id#24L = 4)
/* 007 */ +- Range 0, 1, 8, 10, [id#24L]
/* 008 */ */
...
----
