== Overview of Apache Spark

The goal is to be able to describe what Spark is and more importanly where and how to use it.

http://spark.apache.org/[Apache Spark] is an open-source cluster computing framework with in-memory processing. Because it runs computations in memory (as often as possible), it's faster than existing Map Reduce processing engines for iterative algorithms or interactive data mining. Spark aims at speed, ease of use, and analytics.

It supports Scala, Java, and Python APIs for development.

If you have large amounts of data that requires low latency processing that a typical Map Reduce program cannot provide, Spark is the alternative.

Apache Spark delivers 100x the performance of Apache Hadoop for certain MapReduce workloads with its advanced in-memory computing engine.

* Access any data type across any data source.
* Huge demand for storage and data processing.

The Apache Spark project is an umbrella for http://spark.apache.org/sql/[SQL] (with DataFrames), http://spark.apache.org/streaming/[streaming], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph] processing engines built atop Spark Core. You can run them all in a single application using a consistent API.

Spark runs locally as well as in large clusters and cloud. It runs on top of Hadoop YARN, Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix).

Spark can access data from http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html[Hadoop Distributed File System (HDFS)], http://cassandra.apache.org/[Cassandra], http://hbase.apache.org/[HBase], or https://aws.amazon.com/s3/[S3].

Apache Spark's Streaming and SQL programming models with MLlib and GraphX make it easier for developers and data scientists to build apps that exploit machine learning and graph analytics.
