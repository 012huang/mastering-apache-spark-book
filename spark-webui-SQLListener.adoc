== SQLListener

`SQLListener` is a custom link:spark-SparkListener.adoc[SparkListener] that collects information about SQL query executions for web UI to display (in link:spark-webui-sql.adoc[SQL tab]).

It listens to the following events:

* <<onJobStart, onJobStart>>
* <<onJobEnd, onJobEnd>>
* onExecutorMetricsUpdate
* onStageSubmitted
* onTaskEnd
* <<onOtherEvent, onOtherEvent>>

It uses <<SQLExecutionUIData, SQLExecutionUIData>> exclusively to represent all the necessary data for a single SQL query execution. `SQLExecutionUIData` is tracked in the internal registries, i.e. `activeExecutions`, `failedExecutions`, and `completedExecutions` as well as lookup tables, i.e. `_executionIdToData`, `_jobIdToExecutionId`, and `_stageIdToStageMetrics`.

=== [[onJobStart]] Registering Job and Stages under Active Execution (onJobStart callback)

[source, scala]
----
onJobStart(jobStart: SparkListenerJobStart): Unit
----

`onJobStart` reads the link:spark-sql-SQLExecution.adoc#spark.sql.execution.id[`spark.sql.execution.id` key], the identifiers of the job and the stages and then updates the <<SQLExecutionUIData, SQLExecutionUIData>> for the execution id in `activeExecutions` internal registry.

NOTE: When `onJobStart` is executed, it is assumed that <<SQLExecutionUIData, SQLExecutionUIData>> has already been created and available in the internal `activeExecutions` registry.

The job in <<SQLExecutionUIData, SQLExecutionUIData>> is marked as running with the stages added (to `stages`). For each stage, a `SQLStageMetrics` is created in the internal `_stageIdToStageMetrics` registry. At the end, the execution id is recorded for the job id in the internal `_jobIdToExecutionId`.

=== [[onOtherEvent]] onOtherEvent

In `onOtherEvent`, `SQLListener` listens to the following link:spark-SparkListener.adoc#SparkListenerEvent[SparkListenerEvent] events:

* <<SparkListenerSQLExecutionStart, SparkListenerSQLExecutionStart>>
* <<SparkListenerSQLExecutionEnd, SparkListenerSQLExecutionEnd>>
* <<SparkListenerDriverAccumUpdates, SparkListenerDriverAccumUpdates>>

==== [[SparkListenerSQLExecutionStart]] Registering Active Execution (SparkListenerSQLExecutionStart Event)

[source, scala]
----
case class SparkListenerSQLExecutionStart(
  executionId: Long,
  description: String,
  details: String,
  physicalPlanDescription: String,
  sparkPlanInfo: SparkPlanInfo,
  time: Long)
extends SparkListenerEvent
----

When `SparkListenerSQLExecutionStart` comes, a new <<SQLExecutionUIData, SQLExecutionUIData>> for the input `executionId` query execution is created.

It is stored in `activeExecutions` and `_executionIdToData` internal registries.

==== [[SparkListenerSQLExecutionEnd]] SparkListenerSQLExecutionEnd

[source, scala]
----
case class SparkListenerSQLExecutionEnd(
  executionId: Long,
  time: Long)
extends SparkListenerEvent
----

When `SparkListenerSQLExecutionEnd` comes, <<SQLExecutionUIData, SQLExecutionUIData>> for the input `executionId` is looked up (in `_executionIdToData`) and `completionTime` set to the input `time`.

CAUTION: FIXME Describe running jobs clause.

==== [[SparkListenerDriverAccumUpdates]] SparkListenerDriverAccumUpdates

[source, scala]
----
case class SparkListenerDriverAccumUpdates(
  executionId: Long,
  accumUpdates: Seq[(Long, Long)])
extends SparkListenerEvent
----

When `SparkListenerDriverAccumUpdates` comes, <<SQLExecutionUIData, SQLExecutionUIData>> for the input `executionId` is looked up (in `_executionIdToData`) and `SQLExecutionUIData.driverAccumUpdates` is updated with the input `accumUpdates`.

=== [[onJobEnd]] onJobEnd

[source, scala]
----
onJobEnd(jobEnd: SparkListenerJobEnd): Unit
----

CAUTION: FIXME

=== [[getExecution]] Getting SQL Execution Data (getExecution method)

[source, scala]
----
getExecution(executionId: Long): Option[SQLExecutionUIData]
----

=== [[getExecutionMetrics]] Getting Execution Metrics (getExecutionMetrics method)

[source, scala]
----
getExecutionMetrics(executionId: Long): Map[Long, String]
----

`getExecutionMetrics` gets the metrics (aka _accumulator updates_) for `executionId` (by which it collects all the tasks that were used for an execution).

It is exclusively used to render the link:spark-webui-sql.adoc#ExecutionPage[ExecutionPage] page in web UI.

=== [[mergeAccumulatorUpdates]] mergeAccumulatorUpdates method

`mergeAccumulatorUpdates` is a `private` helper method for...TK

It is used exclusively in <<getExecutionMetrics, getExecutionMetrics>> method.

=== [[SQLExecutionUIData]] SQLExecutionUIData

`SQLExecutionUIData` is the data abstraction of `SQLListener` to describe SQL query executions. It is a container for jobs, stages, and accumulator updates for a single query execution.

=== [[settings]] Settings

==== [[spark.sql.ui.retainedExecutions]] spark.sql.ui.retainedExecutions

`spark.sql.ui.retainedExecutions` (default: `1000`) is the number of `SQLExecutionUIData` elements to keep in `failedExecutions` and `completedExecutions` internal registries.

When a query execution finishes, the execution is removed from the internal `activeExecutions` registry and stored in `failedExecutions` or `completedExecutions` given the end execution status. It is when `SQLListener` makes sure that the number of `SQLExecutionUIData` entires does not exceed `spark.sql.ui.retainedExecutions` and removes the excess of the old entries.
