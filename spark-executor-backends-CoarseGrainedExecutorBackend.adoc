== [[CoarseGrainedExecutorBackend]] CoarseGrainedExecutorBackend

`CoarseGrainedExecutorBackend` is an link:spark-ExecutorBackend.adoc[ExecutorBackend] that controls the lifecycle of a single <<executor, executor>>.

NOTE: The internal <<executor, executor>> reference is created after a <<RegisteredExecutor, connection to the driver is established>>.

`CoarseGrainedExecutorBackend` is a <<main, standalone application>> that is used when:

. Spark Standalone's link:spark-standalone-StandaloneSchedulerBackend.adoc#start[`StandaloneSchedulerBackend` starts]

. Spark on YARN's link:yarn/spark-yarn-ExecutorRunnable.adoc#prepareCommand[`ExecutorRunnable` starts]

. Spark on Mesos's link:spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.adoc#createCommand[`MesosCoarseGrainedSchedulerBackend` launches Spark executors]

When <<run, started>>, `CoarseGrainedExecutorBackend` registers itself as the `Executor` RPC endpoint to communicate with the driver.

.CoarseGrainedExecutorBackend and Others
image::images/CoarseGrainedExecutorBackend.png[align="center"]

When <<main, launched>>, `CoarseGrainedExecutorBackend` immediately connects to the owning link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend] to inform that it is ready to launch tasks.

[[internal-properties]]
.CoarseGrainedExecutorBackend's Internal Properties
[cols="1,^1,3",options="header",width="100%"]
|===
| Name
| Initial Value
| Description

| [[ser]] `ser`
| link:spark-SerializerInstance.adoc[SerializerInstance]
| Initialized when <<creating-instance, `CoarseGrainedExecutorBackend` is created>>.

NOTE: `CoarseGrainedExecutorBackend` uses the input `env` to link:spark-sparkenv.adoc#closureSerializer[access `closureSerializer`].

| [[driver]] `driver`
| (empty)
| link:spark-RpcEndpointRef.adoc[RpcEndpointRef] of the driver

FIXME

| [[stopping]] `stopping`
| `false`
| Enabled when `CoarseGrainedExecutorBackend` gets notified to <<StopExecutor, stop itself>> or <<Shutdown, shut down the managed executor>>.

Used when `CoarseGrainedExecutorBackend` RPC Endpoint gets notified that <<onDisconnected, a remote RPC endpoint disconnected>>.

| [[executor]] `executor`
| (empty)
|  link:spark-executor.adoc#coarse-grained-executor[Executor] that is managed by the `CoarseGrainedExecutorBackend` to <<LaunchTask, launch>> and <<KillTask, kill>> tasks.

Initialized after `CoarseGrainedExecutorBackend` <<RegisteredExecutor, has registered with `CoarseGrainedSchedulerBackend`>> and stopped when `CoarseGrainedExecutorBackend` gets requested to <<Shutdown, shut down>>.

|===

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.executor.CoarseGrainedExecutorBackend` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.CoarseGrainedExecutorBackend=INFO
```
====

=== [[statusUpdate]] `statusUpdate` Method

CAUTION: FIXME

=== [[onDisconnected]] `onDisconnected` Callback

CAUTION: FIXME

=== [[start]] `start` Method

CAUTION: FIXME

=== [[stop]] `stop` Method

CAUTION: FIXME

=== [[requestTotalExecutors]] `requestTotalExecutors`

CAUTION: FIXME

=== [[exitExecutor]] `exitExecutor` Method

CAUTION: FIXME

=== [[extractLogUrls]] Extracting Log URLs -- `extractLogUrls` Method

CAUTION: FIXME

=== [[creating-instance]] Creating CoarseGrainedExecutorBackend Instance

`CoarseGrainedExecutorBackend` takes the following when created:

1. [[rpcEnv]] link:spark-rpc.adoc[RpcEnv]
2. `driverUrl`
3. `executorId`
4. `hostname`
5. `cores`
6. `userClassPath` -- collection of URLs
7. link:spark-sparkenv.adoc[SparkEnv]

`CoarseGrainedExecutorBackend` initializes the <<internal-properties, internal properties>>.

NOTE: `CoarseGrainedExecutorBackend` is created (to act as a RPC Endpoint) when <<run, `Executor` RPC endpoint is registered>>.

=== [[onStart]] Connecting to Driver -- `onStart` Method

NOTE: `onStart` is a part of link:spark-rpc-RpcEndpoint.adoc#onStart[RpcEndpoint contract] that is executed before a RPC endpoint starts accepting messages.

When executed, you should see the following INFO message in the logs:

```
INFO CoarseGrainedExecutorBackend: Connecting to driver: [driverUrl]
```

It then retrieves the link:spark-RpcEndpointRef.adoc[RpcEndpointRef] of the driver asynchronously (using the constructor's <<driverUrl, driverUrl>>) and initializes the internal <<driver, driver>> property that it will send a blocking link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#RegisterExecutor[RegisterExecutor] message to.

If there is an issue while registering the executor, you should see the following ERROR message in the logs and process exits (with the exit code `1`).

```
ERROR Cannot register with driver: [driverUrl]
```

NOTE: The `RegisterExecutor` message contains `executorId`, the `RpcEndpointRef` to itself, `cores`, and <<extractLogUrls, log URLs>> of the `CoarseGrainedExecutorBackend`.

=== [[driverURL]] Driver's URL

The driver's URL is of the format `spark://[RpcEndpoint name]@[hostname]:[port]`, e.g. `spark://CoarseGrainedScheduler@192.168.1.6:64859`.

=== [[main]] Launching CoarseGrainedExecutorBackend Standalone Application -- `main` Method

`CoarseGrainedExecutorBackend` is a standalone application (i.e. comes with `main` entry method) that takes <<command-line-arguments, command-line arguments>> and <<run, runs CoarseGrainedExecutorBackend's Executor RPC endpoint>>.

[[command-line-arguments]]
.CoarseGrainedExecutorBackend Command-Line Arguments
[cols="1,^1,2",options="header",width="100%"]
|===
| Argument
| Required?
| Description

| [[driver-url]] `--driver-url`
| yes
| Driver's URL. See <<driverURL, driver's URL>>

| [[executor-id]] `--executor-id`
| yes
| Executor id

| [[hostname]] `--hostname`
| yes
| Host name

| [[cores]] `--cores`
| yes
| Number of cores (that must be greater than `0`).

| [[app-id]] `--app-id`
| yes
| Application id

| [[worker-url]] `--worker-url`
| no
| Worker's URL, e.g. `spark://Worker@192.168.1.6:64557`

NOTE: `--worker-url` is only used in link:spark-standalone-StandaloneSchedulerBackend.adoc[Spark Standalone] to enforce fate-sharing with the worker.

| [[user-class-path]] `--user-class-path`
| no
| URL/path to a resource to be added to CLASSPATH; can be specified multiple times.

|===

When executed with unrecognized command-line arguments or required arguments are missing, `main` shows the usage help and exits (with exit status `1`).

[source]
----
$ ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend

Usage: CoarseGrainedExecutorBackend [options]

 Options are:
   --driver-url <driverUrl>
   --executor-id <executorId>
   --hostname <hostname>
   --cores <cores>
   --app-id <appid>
   --worker-url <workerUrl>
   --user-class-path <url>
----

[NOTE]
====
`main` is used when:

1. Spark Standalone's link:spark-standalone-StandaloneSchedulerBackend.adoc#start[`StandaloneSchedulerBackend` starts]

2. Spark on YARN's link:yarn/spark-yarn-ExecutorRunnable.adoc#prepareCommand[`ExecutorRunnable` starts]

3. Spark on Mesos's link:spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.adoc#createCommand[`MesosCoarseGrainedSchedulerBackend` launches Spark executors]
====

=== [[run]] Running CoarseGrainedExecutorBackend (and Registering Executor RPC Endpoint) -- `run` Internal Method

[source, scala]
----
run(
  driverUrl: String,
  executorId: String,
  hostname: String,
  cores: Int,
  appId: String,
  workerUrl: Option[String],
  userClassPath: scala.Seq[URL]): Unit
----

When executed, `run` executes `Utils.initDaemon(log)`.

CAUTION: FIXME What does `initDaemon` do?

NOTE: `run` link:spark-SparkHadoopUtil.adoc#runAsSparkUser[runs itself with a Hadoop `UserGroupInformation`] (as a thread local variable distributed to child threads for authenticating HDFS and YARN calls).

NOTE: `run` expects a clear `hostname` with no `:` included (for a port perhaps).

[[run-driverPropsFetcher]]
`run` uses link:spark-executor.adoc#spark_executor_port[spark.executor.port] Spark property (or `0` if not set) for the port to link:spark-rpc.adoc#create[create a `RpcEnv`] called *driverPropsFetcher* (together with the input `hostname` and `clientMode` enabled).

`run` link:spark-rpc.adoc#setupEndpointRefByURI[resolves `RpcEndpointRef` for the input `driverUrl`] and requests `SparkAppConfig` (by posting a blocking `RetrieveSparkAppConfig`).

IMPORTANT: This is the first moment when `CoarseGrainedExecutorBackend` initiates communication with the driver available at `driverUrl` through `RpcEnv`.

`run` uses `SparkAppConfig` to get the driver's `sparkProperties` and adds link:spark-configuration.adoc#spark_app_id[spark.app.id] Spark property with the value of the input `appId`.

`run` link:spark-rpc.adoc#shutdown[shuts `driverPropsFetcher` RPC Endpoint down].

`run` creates a link:spark-configuration.adoc[SparkConf] using the Spark properties fetched from the driver, i.e. with the link:spark-configuration.adoc#isExecutorStartupConf[executor-related Spark settings] if they link:spark-configuration.adoc#setIfMissing[were missing] and the link:spark-configuration.adoc#set[rest unconditionally].

If link:yarn/spark-yarn-settings.adoc#spark.yarn.credentials.file[spark.yarn.credentials.file] Spark property is defined in `SparkConf`, you should see the following INFO message in the logs:

```
INFO Will periodically update credentials from: [spark.yarn.credentials.file]
```

`run` link:spark-SparkHadoopUtil.adoc#startCredentialUpdater[requests the current `SparkHadoopUtil` to start start the credential updater].

NOTE: `run` uses link:spark-SparkHadoopUtil.adoc#get[SparkHadoopUtil.get] to access the current `SparkHadoopUtil`.

`run` link:spark-sparkenv.adoc#createExecutorEnv[creates `SparkEnv` for executors] (with the input `executorId`, `hostname` and `cores`, and `isLocal` disabled).

IMPORTANT: This is the moment when `SparkEnv` gets created with all the executor services.

`run` link:spark-rpc.adoc#setupEndpoint[sets up an RPC endpoint] with the name *Executor* and <<creating-instance, CoarseGrainedExecutorBackend>> as the endpoint.

(only in Spark Standalone) If the optional input `workerUrl` was defined, `run` sets up an RPC endpoint with the name *WorkerWatcher* and `WorkerWatcher` RPC endpoint.

[NOTE]
====
The optional input `workerUrl` is defined only when <<worker-url, `--worker-url` command-line argument>> was used to <<main, launch `CoarseGrainedExecutorBackend` standalone application>>.

`--worker-url` is only used in link:spark-standalone-StandaloneSchedulerBackend.adoc[Spark Standalone].
====

``run``'s main thread is blocked until link:spark-rpc.adoc#awaitTermination[`RpcEnv` terminates] and only the RPC endpoints process RPC messages.

Once `RpcEnv` has terminated, `run` link:spark-SparkHadoopUtil.adoc#stopCredentialUpdater[stops the credential updater].

CAUTION: FIXME Think of the place for `Utils.initDaemon`, `Utils.getProcessName` et al.

NOTE: `run` is used exclusively when <<main, `CoarseGrainedExecutorBackend` standalone application is launched>>.

=== [[RegisteredExecutor]] Creating Executor -- `RegisteredExecutor` Message Handler

[source, scala]
----
RegisteredExecutor
extends CoarseGrainedClusterMessage with RegisterExecutorResponse
----

When `RegisteredExecutor` comes in, you should see the following INFO in the logs:

```
INFO CoarseGrainedExecutorBackend: Successfully registered with driver
```

`CoarseGrainedExecutorBackend` link:spark-executor.adoc#creating-instance[creates a `Executor`] (with `isLocal` disabled) that in turn becomes <<executor, executor>> internal reference.

NOTE: `CoarseGrainedExecutorBackend` uses `executorId`, `hostname`, `env`, `userClassPath` to create the `Executor` that were specified when <<creating-instance, `CoarseGrainedExecutorBackend` was created>>.

If creating the `Executor` fails with a non-fatal exception, `CoarseGrainedExecutorBackend` <<exitExecutor, exits executor>> with the reason:

```
Unable to create executor due to [message]
```

NOTE: `RegisteredExecutor` is sent exclusively when link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#RegisterExecutor[`CoarseGrainedSchedulerBackend` receives `RegisterExecutor`] (that is right before <<onStart, `CoarseGrainedExecutorBackend` RPC Endpoint starts accepting messages>> which happens when <<run, `CoarseGrainedExecutorBackend` is started>>).

=== [[RegisterExecutorFailed]] RegisterExecutorFailed

[source, scala]
----
RegisterExecutorFailed(message)
----

When a `RegisterExecutorFailed` message arrives, the following ERROR is printed out to the logs:

```
ERROR CoarseGrainedExecutorBackend: Slave registration failed: [message]
```

`CoarseGrainedExecutorBackend` then exits with the exit code `1`.

=== [[LaunchTask]] Launching Tasks -- `LaunchTask` Message Handler

[source, scala]
----
LaunchTask(data: SerializableBuffer)
----

`LaunchTask` handler deserializes `TaskDescription` from `data` (using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]).

NOTE: `LaunchTask` message is sent by link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#launchTasks[CoarseGrainedSchedulerBackend.launchTasks].

```
INFO CoarseGrainedExecutorBackend: Got assigned task [taskId]
```

`CoarseGrainedExecutorBackend` then link:spark-executor.adoc#launchTask[launches the task on the executor].

If however the internal `executor` field has not been created yet, it prints out the following ERROR to the logs:

```
ERROR CoarseGrainedExecutorBackend: Received LaunchTask command but executor was null
```

And it then exits.

=== [[KillTask]] Killing Tasks -- `KillTask` Message Handler

`KillTask(taskId, _, interruptThread)` message kills a task (calls `Executor.killTask`).

If an executor has not been initialized yet (FIXME: why?), the following ERROR message is printed out to the logs and CoarseGrainedExecutorBackend exits:

```
ERROR Received KillTask command but executor was null
```

=== [[StopExecutor]] StopExecutor Handler

[source, scala]
----
case object StopExecutor
extends CoarseGrainedClusterMessage
----

When `StopExecutor` is received, the handler turns <<stopping, stopping>> internal flag on. You should see the following INFO message in the logs:

```
INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
```

In the end, the handler sends a <<Shutdown, Shutdown>> message to itself.

NOTE: `StopExecutor` message is sent when `CoarseGrainedSchedulerBackend` RPC Endpoint (aka `DriverEndpoint`) processes link:spark-CoarseGrainedSchedulerBackend-DriverEndpoint.adoc#StopExecutors[StopExecutors] or link:spark-CoarseGrainedSchedulerBackend-DriverEndpoint.adoc#RemoveExecutor[RemoveExecutor] messages.

=== [[Shutdown]] Shutdown Handler

[source, scala]
----
case object Shutdown
extends CoarseGrainedClusterMessage
----

When `Shutdown` is received, the handler turns <<stopping, stopping>> internal flag on. It then starts the `CoarseGrainedExecutorBackend-stop-executor` thread that link:spark-executor.adoc#stop[stops the owned `Executor`] (using <<executor, executor>> reference).

NOTE: `Shutdown` message is sent exclusively when <<StopExecutor, `CoarseGrainedExecutorBackend` receives `StopExecutor`>>.
