== [[CoarseGrainedExecutorBackend]] CoarseGrainedExecutorBackend

`CoarseGrainedExecutorBackend` is an link:spark-ExecutorBackend.adoc[ExecutorBackend] to manage a single <<executor, coarse-grained executor>> (that lives as long as the owning executor backend).

`CoarseGrainedExecutorBackend` <<run, registers itself as a `ThreadSafeRpcEndpoint`>> under the name *Executor* to communicate with the driver.

NOTE: The internal <<executor, executor>> reference is created after a <<RegisteredExecutor, connection to the driver is established>>.

.CoarseGrainedExecutorBackend and Others
image::images/CoarseGrainedExecutorBackend.png[align="center"]

When <<main, launched>>, `CoarseGrainedExecutorBackend` immediately connects to the owning link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend] to inform that it can run tasks.

When it cannot connect to `driverUrl`, it terminates (with the exit code `1`).

CAUTION: What are `SPARK_LOG_URL_` env vars? Who sets them?

When the driver terminates, `CoarseGrainedExecutorBackend` exits (with exit code `1`).

```
ERROR Driver [remoteAddress] disassociated! Shutting down.
```

All task status updates are sent along to `driverRef` as `StatusUpdate` messages.

[[internal-properties]]
.CoarseGrainedExecutorBackend's Internal Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|===
| Name
| Initial Value
| Description

| [[driver]] `driver`
| (empty)
| link:spark-RpcEndpointRef.adoc[RpcEndpointRef] of the driver

| [[executor]] `executor`
| (empty)
|  link:spark-executor.adoc#coarse-grained-executor[Executor]...FIXME

Set when `CoarseGrainedExecutorBackend` <<RegisteredExecutor, registers with `CoarseGrainedSchedulerBackend`>>.

Used to <<LaunchTask, launch>> and <<KillTask, kill>> tasks as well as when `CoarseGrainedExecutorBackend` <<Shutdown, shuts down>>.

|===

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.executor.CoarseGrainedExecutorBackend` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.CoarseGrainedExecutorBackend=INFO
```
====

=== [[start]] `start` Method

CAUTION: FIXME

=== [[stop]] `stop` Method

CAUTION: FIXME

=== [[requestTotalExecutors]] `requestTotalExecutors`

CAUTION: FIXME

=== [[exitExecutor]] `exitExecutor` Method

CAUTION: FIXME

=== [[extractLogUrls]] Extracting Log URLs -- `extractLogUrls` Method

CAUTION: FIXME

=== [[creating-instance]] Creating CoarseGrainedExecutorBackend Instance

[source, scala]
----
CoarseGrainedExecutorBackend(
  override val rpcEnv: RpcEnv,
  driverUrl: String,
  executorId: String,
  hostname: String,
  cores: Int,
  userClassPath: Seq[URL],
  env: SparkEnv)
extends ThreadSafeRpcEndpoint with ExecutorBackend
----

While being created, `CoarseGrainedExecutorBackend` initializes the internal properties (e.g. <<executor, executor>> and <<driver, driver>>) and creates a link:spark-SerializerInstance.adoc[SerializerInstance] (using link:spark-sparkenv.adoc#closureSerializer[SparkEnv.closureSerializer]).

NOTE: `CoarseGrainedExecutorBackend` is created when...FIXME

=== [[onStart]] Starting RpcEndpoint -- `onStart` Method

NOTE: `onStart` is a link:spark-rpc.adoc[RpcEndpoint callback method] that is executed before a RPC endpoint starts to handle messages.

When executed, you should see the following INFO message in the logs:

```
INFO CoarseGrainedExecutorBackend: Connecting to driver: [driverUrl]
```

It then retrieves the link:spark-RpcEndpointRef.adoc[RpcEndpointRef] of the driver asynchronously (using the constructor's <<driverUrl, driverUrl>>) and initializes the internal <<driver, driver>> property that it will send a blocking link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#RegisterExecutor[RegisterExecutor] message to.

If there is an issue while registering the executor, you should see the following ERROR message in the logs and process exits (with the exit code `1`).

```
ERROR Cannot register with driver: [driverUrl]
```

NOTE: The `RegisterExecutor` message contains `executorId`, the `RpcEndpointRef` to itself, `cores`, and <<extractLogUrls, log URLs>> of the `CoarseGrainedExecutorBackend`.

=== [[driverURL]] Driver's URL

The driver's URL is of the format `spark://[RpcEndpoint name]@[hostname]:[port]`, e.g. `spark://CoarseGrainedScheduler@192.168.1.6:64859`.

=== [[main]] Launching CoarseGrainedExecutorBackend As Standalone Application -- `main` Method

`CoarseGrainedExecutorBackend` is a command-line application, i.e. it comes with `main` entry method.

`CoarseGrainedExecutorBackend` accepts the following command-line arguments:

* `--driver-url` (required) - the driver's URL. See <<driverURL, driver's URL>>.

[[executor-id]]
* `--executor-id` (required) - the executor's id
* `--hostname` (required) - the name of the host
* `--cores` (required) - the number of cores (must be greater than `0`)
* `--app-id` (required) - the id of the application
* `--worker-url` - the worker's URL, e.g. `spark://Worker@192.168.1.6:64557`
* `--user-class-path` - a URL/path to a resource to be added to CLASSPATH; can be specified multiple times.

Unrecognized options or required options missing cause displaying usage help and exit.

```
$ ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend

Usage: CoarseGrainedExecutorBackend [options]

 Options are:
   --driver-url <driverUrl>
   --executor-id <executorId>
   --hostname <hostname>
   --cores <cores>
   --app-id <appid>
   --worker-url <workerUrl>
   --user-class-path <url>
```

A `SparkEnv` is created using link:spark-sparkenv.adoc#createExecutorEnv[SparkEnv.createExecutorEnv] (with `isLocal` being `false`).

CAUTION: FIXME

[NOTE]
====
`main` is used for the following cluster managers:

1. Spark Standalone's link:spark-standalone-StandaloneSchedulerBackend.adoc#start[`StandaloneSchedulerBackend` starts]

2. Spark on YARN's link:yarn/spark-yarn-ExecutorRunnable.adoc#prepareCommand[`ExecutorRunnable` starts]

3. Spark on Mesos's link:spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.adoc#createCommand[MesosCoarseGrainedSchedulerBackend launches Spark executors]
====

=== [[run]] Running CoarseGrainedExecutorBackend (with Executor RPC Endpoint and WorkerWatcher) -- `run` Internal Method

[source, scala]
----
run(
  driverUrl: String,
  executorId: String,
  hostname: String,
  cores: Int,
  appId: String,
  workerUrl: Option[String],
  userClassPath: scala.Seq[URL]): Unit
----

When started, `run` executes `Utils.initDaemon(log)`.

CAUTION: FIXME What does `initDaemon` do?

NOTE: `run` link:spark-SparkHadoopUtil.adoc#runAsSparkUser[runs itself with a Hadoop `UserGroupInformation`] (as a thread local variable distributed to child threads for authenticating HDFS and YARN calls).

NOTE: `run` expects a clear `hostname` with no `:` included (for a port perhaps).

`run` uses link:spark-executor.adoc#spark_executor_port[spark.executor.port] Spark property (or `0` if not set) for the port to link:spark-rpc.adoc#create[create a `RpcEnv`] called *driverPropsFetcher* (together with the input `hostname` and `clientMode` enabled).

`run` link:spark-rpc.adoc#setupEndpointRefByURI[resolves `RpcEndpointRef` for the input `driverUrl`] and requests `SparkAppConfig` (by posting a blocking `RetrieveSparkAppConfig`).

IMPORTANT: This is the first moment when `CoarseGrainedExecutorBackend` initiates communication with the driver through `RpcEnv`.

`run` uses `SparkAppConfig` to get the driver's `sparkProperties` and adds link:spark-configuration.adoc#spark_app_id[spark.app.id] Spark property with the value of the input `appId`.

`run` link:spark-rpc.adoc#shutdown[shuts `driverPropsFetcher` RPC Endpoint down].

`run` creates a link:spark-configuration.adoc[SparkConf] using the Spark properties fetched from the driver, i.e. with the link:spark-configuration.adoc#isExecutorStartupConf[executor-related Spark settings] if they link:spark-configuration.adoc#setIfMissing[were missing] and the link:spark-configuration.adoc#set[rest unconditionally].

If link:yarn/spark-yarn-settings.adoc#spark.yarn.credentials.file[spark.yarn.credentials.file] Spark property is defined in `SparkConf`, you should see the following INFO message in the logs:

```
INFO Will periodically update credentials from: [spark.yarn.credentials.file]
```

`run` link:spark-SparkHadoopUtil.adoc#startCredentialUpdater[requests the current `SparkHadoopUtil` to start start the credential updater].

NOTE: `run` uses link:spark-SparkHadoopUtil.adoc#get[SparkHadoopUtil.get] to access the current `SparkHadoopUtil`.

`run` link:spark-sparkenv.adoc#createExecutorEnv[creates `SparkEnv` for executors] (with the input `executorId`, `hostname` and `cores`, and `isLocal` disabled).

IMPORTANT: This is the moment when `SparkEnv` gets created with all the executor services.

`run` link:spark-rpc.adoc#setupEndpoint[sets up an RPC endpoint] with the name *Executor* and <<creating-instance, CoarseGrainedExecutorBackend>> as the endpoint.

If the optional input `workerUrl` was defined, `run` sets up an RPC endpoint with the name *WorkerWatcher* and `WorkerWatcher` RPC endpoint.

CAUTION: FIXME When is `workerUrl` specified?

``run``'s main thread is blocked until link:spark-rpc.adoc#awaitTermination[`RpcEnv` terminates] and only the RPC endpoints process RPC messages.

Once `RpcEnv` has terminated, `run` link:spark-SparkHadoopUtil.adoc#stopCredentialUpdater[stops the credential updater].

CAUTION: FIXME Think of the place for `Utils.initDaemon`, `Utils.getProcessName` et al.

NOTE: `run` is executed when <<main, `CoarseGrainedExecutorBackend` command-line application is launched>>.

=== [[messages]] RPC Messages

==== [[RegisteredExecutor]] RegisteredExecutor

[source, scala]
----
RegisteredExecutor
extends CoarseGrainedClusterMessage with RegisterExecutorResponse
----

When `RegisteredExecutor` comes in, you should see the following INFO in the logs:

```
INFO CoarseGrainedExecutorBackend: Successfully registered with driver
```

`CoarseGrainedExecutorBackend` link:spark-executor.adoc#creating-instance[creates a `Executor`] (with `isLocal` disabled) that in turn becomes <<executor, executor>> internal reference.

NOTE: `CoarseGrainedExecutorBackend` uses `executorId`, `hostname`, `env`, `userClassPath` to create the `Executor` that were specified when <<creating-instance, `CoarseGrainedExecutorBackend` was created>>.

If creating the `Executor` fails with a non-fatal exception, `CoarseGrainedExecutorBackend` <<exitExecutor, exits executor>> with the reason:

```
Unable to create executor due to [message]
```

NOTE: `RegisteredExecutor` is sent exclusively when link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#RegisterExecutor[`CoarseGrainedSchedulerBackend` receives `RegisterExecutor`] (that is right before <<onStart, `CoarseGrainedExecutorBackend` RPC Endpoint starts accepting messages>> which happens when <<run, `CoarseGrainedExecutorBackend` is started>>).

==== [[RegisterExecutorFailed]] RegisterExecutorFailed

[source, scala]
----
RegisterExecutorFailed(message)
----

When a `RegisterExecutorFailed` message arrives, the following ERROR is printed out to the logs:

```
ERROR CoarseGrainedExecutorBackend: Slave registration failed: [message]
```

`CoarseGrainedExecutorBackend` then exits with the exit code `1`.

==== [[LaunchTask]] Launching Tasks -- `LaunchTask` Message Handler

[source, scala]
----
LaunchTask(data: SerializableBuffer)
----

`LaunchTask` handler deserializes `TaskDescription` from `data` (using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]).

NOTE: `LaunchTask` message is sent by link:spark-scheduler-backends-CoarseGrainedSchedulerBackend.adoc#launchTasks[CoarseGrainedSchedulerBackend.launchTasks].

```
INFO CoarseGrainedExecutorBackend: Got assigned task [taskId]
```

`CoarseGrainedExecutorBackend` then link:spark-executor.adoc#launchTask[launches the task on the executor].

If however the internal `executor` field has not been created yet, it prints out the following ERROR to the logs:

```
ERROR CoarseGrainedExecutorBackend: Received LaunchTask command but executor was null
```

And it then exits.

==== [[KillTask]] Killing Tasks -- `KillTask` Message Handler

`KillTask(taskId, _, interruptThread)` message kills a task (calls `Executor.killTask`).

If an executor has not been initialized yet (FIXME: why?), the following ERROR message is printed out to the logs and CoarseGrainedExecutorBackend exits:

```
ERROR Received KillTask command but executor was null
```

==== [[StopExecutor]] StopExecutor

`StopExecutor` message handler is receive-reply and blocking. When received, the handler prints the following INFO message to the logs:

```
INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
```

It then sends a `Shutdown` message to itself.

==== [[Shutdown]] Shutdown

`Shutdown` stops the executor, itself and RPC Environment.
