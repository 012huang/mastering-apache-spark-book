== Configuring Spark Applications

TODO

* Describe `SparkConfig` object for the application configuration.
* the default configs
* system properties

There are three ways to configure Spark and user programs:

* Spark Properties - use link:spark-webui.adoc[Web UI] to learn the current properties.
*

=== Spark Properties

Every user program starts with instantiating `SparkConf` that holds master, appName and other Spark properties required for proper runs. An instance of SparkConf is then used to create link:spark-sparkcontext.adoc[SparkContext].

[TIP]
====
Start link:spark-shell.adoc[Spark shell] with `--conf spark.logConf=true` to log the effective Spark configuration as INFO when SparkContext is started.

```
$ ./bin/spark-shell --conf spark.logConf=true
...
15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
15/10/19 17:13:49 INFO SparkContext: Spark configuration:
spark.app.name=Spark shell
spark.home=/Users/jacek/dev/oss/spark
spark.jars=
spark.logConf=true
spark.master=local[*]
spark.repl.class.uri=http://10.5.10.20:64055
spark.submit.deployMode=client
...
```
====

You can query for the values of Spark properties in link:spark-shell.adoc[Spark shell] as follows:

```
scala> sc.getConf.getOption("spark.local.dir")
res0: Option[String] = None

scala> sc.getConf.getOption("spark.app.name")
res1: Option[String] = Some(Spark shell)

scala> sc.getConf.get("spark.master")
res2: String = local[*]
```
