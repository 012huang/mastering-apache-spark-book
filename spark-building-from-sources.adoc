== Building Spark (using Scala 2.11)

You can download pre-packaged versions of Apache Spark from http://spark.apache.org/downloads.html[the project's web site]. The packages are built for a different Hadoop versions, but only for Scala 2.10.

If you want a *Scala 2.11* version of Apache Spark _"users should download the Spark source package and build with Scala 2.11 support"_ (quoted from the Note at http://spark.apache.org/downloads.html[Download Spark]).

The build process for Scala 2.11 takes around 15 mins (on a decent machine) and is so simple that it's unlikely to refuse the urge to do it yourself. It simply requires two steps:

1. Executing `./dev/change-scala-version.sh 2.11` in the source directory (so `pom.xml` use the version of Scala).
1. Executing the following build command:

    ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean install

The most important part is `-Dscala-2.11` for the Maven profile `scala-2.11` that triggers the build to use the latest and greatest Scala **2.11.7**.

After a couple of minutes your freshly baked distro is ready to fly!

I'm using Oracle Java 8 to build Spark.

```
➜  spark git:(master) ./dev/change-scala-version.sh 2.11
➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean install
Using `mvn` from path: /usr/local/bin/mvn
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Spark Project Parent POM
[INFO] Spark Project Launcher
[INFO] Spark Project Networking
[INFO] Spark Project Shuffle Streaming Service
[INFO] Spark Project Unsafe
[INFO] Spark Project Core
[INFO] Spark Project Bagel
[INFO] Spark Project GraphX
[INFO] Spark Project Streaming
[INFO] Spark Project Catalyst
[INFO] Spark Project SQL
[INFO] Spark Project ML Library
[INFO] Spark Project Tools
[INFO] Spark Project Hive
[INFO] Spark Project REPL
[INFO] Spark Project YARN Shuffle Service
[INFO] Spark Project YARN
[INFO] Spark Project Hive Thrift Server
[INFO] Spark Project Assembly
[INFO] Spark Project External Twitter
[INFO] Spark Project External Flume Sink
[INFO] Spark Project External Flume
[INFO] Spark Project External Flume Assembly
[INFO] Spark Project External MQTT
[INFO] Spark Project External MQTT Assembly
[INFO] Spark Project External ZeroMQ
[INFO] Spark Project External Kafka
[INFO] Spark Project Examples
[INFO] Spark Project External Kafka Assembly
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Parent POM 1.5.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] --- maven-clean-plugin:2.6.1:clean (default-clean) @ spark-parent_2.11 ---
...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [  3.256 s]
[INFO] Spark Project Launcher ............................. SUCCESS [ 12.027 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 10.117 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  6.428 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  7.646 s]
[INFO] Spark Project Core ................................. SUCCESS [01:50 min]
[INFO] Spark Project Bagel ................................ SUCCESS [  6.554 s]
[INFO] Spark Project GraphX ............................... SUCCESS [ 14.796 s]
[INFO] Spark Project Streaming ............................ SUCCESS [ 32.427 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [ 53.456 s]
[INFO] Spark Project SQL .................................. SUCCESS [ 58.727 s]
[INFO] Spark Project ML Library ........................... SUCCESS [01:10 min]
[INFO] Spark Project Tools ................................ SUCCESS [  5.314 s]
[INFO] Spark Project Hive ................................. SUCCESS [ 56.414 s]
[INFO] Spark Project REPL ................................. SUCCESS [  7.548 s]
[INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  8.957 s]
[INFO] Spark Project YARN ................................. SUCCESS [ 12.672 s]
[INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 10.274 s]
[INFO] Spark Project Assembly ............................. SUCCESS [01:30 min]
[INFO] Spark Project External Twitter ..................... SUCCESS [  9.062 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [  6.999 s]
[INFO] Spark Project External Flume ....................... SUCCESS [  9.741 s]
[INFO] Spark Project External Flume Assembly .............. SUCCESS [  2.248 s]
[INFO] Spark Project External MQTT ........................ SUCCESS [ 14.714 s]
[INFO] Spark Project External MQTT Assembly ............... SUCCESS [  3.952 s]
[INFO] Spark Project External ZeroMQ ...................... SUCCESS [  7.044 s]
[INFO] Spark Project External Kafka ....................... SUCCESS [ 11.701 s]
[INFO] Spark Project Examples ............................. SUCCESS [01:26 min]
[INFO] Spark Project External Kafka Assembly .............. SUCCESS [  5.883 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 12:16 min
[INFO] Finished at: 2015-09-12T10:22:03+02:00
[INFO] Final Memory: 432M/1895M
[INFO] ------------------------------------------------------------------------
```

Please note the messages that say the version of Spark (_Building Spark Project Parent POM 1.5.0-SNAPSHOT_), Scala version (_maven-clean-plugin:2.6.1:clean (default-clean) @ spark-parent_2.11_) and the Spark modules built.

The above command gives you the latest version of *Apache Spark 1.5.0-SNAPSHOT* built for *Scala 2.11.7* (see https://github.com/apache/spark/blob/master/pom.xml#L2436-L2445[the configuration of scala-2.11 profile]).

You can also check the version of Spark out using `./bin/spark-shell --version`:

```
➜  spark git:(master) ✗ ./bin/spark-shell --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.5.0-SNAPSHOT
      /_/

Type --help for more information.
```

=== Running Spark shell

Once built, the Spark shell can be run using `./bin/spark-shell` script.

```
➜  spark git:(master) ✗ ./bin/spark-shell
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/09/12 10:34:50 INFO SecurityManager: Changing view acls to: jacek
15/09/12 10:34:50 INFO SecurityManager: Changing modify acls to: jacek
15/09/12 10:34:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacek); users with modify permissions: Set(jacek)
15/09/12 10:34:50 INFO HttpServer: Starting HTTP Server
15/09/12 10:34:50 INFO Utils: Successfully started service 'HTTP server' on port 56649.
15/09/12 10:34:54 INFO Main: Spark class server started at http://192.168.1.4:56649
15/09/12 10:34:54 INFO SparkContext: Running Spark version 1.5.0-SNAPSHOT
15/09/12 10:34:54 INFO SecurityManager: Changing view acls to: jacek
15/09/12 10:34:54 INFO SecurityManager: Changing modify acls to: jacek
15/09/12 10:34:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacek); users with modify permissions: Set(jacek)
15/09/12 10:34:55 INFO Slf4jLogger: Slf4jLogger started
15/09/12 10:34:55 INFO Remoting: Starting remoting
15/09/12 10:34:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.4:56650]
15/09/12 10:34:55 INFO Utils: Successfully started service 'sparkDriver' on port 56650.
15/09/12 10:34:55 INFO SparkEnv: Registering MapOutputTracker
15/09/12 10:34:55 INFO SparkEnv: Registering BlockManagerMaster
15/09/12 10:34:55 INFO DiskBlockManager: Created local directory at /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/blockmgr-1af7c115-2cf3-4e1f-a0cc-ac47b93017e1
15/09/12 10:34:55 INFO MemoryStore: MemoryStore started with capacity 530.0 MB
15/09/12 10:34:55 INFO HttpFileServer: HTTP File server directory is /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-8e2fe432-b512-4d4a-9fc7-39c5292908b5/httpd-32eb8cc1-2263-46fd-b9e1-344bce75b889
15/09/12 10:34:55 INFO HttpServer: Starting HTTP Server
15/09/12 10:34:55 INFO Utils: Successfully started service 'HTTP file server' on port 56651.
15/09/12 10:34:55 INFO SparkEnv: Registering OutputCommitCoordinator
15/09/12 10:34:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/09/12 10:34:55 INFO SparkUI: Started SparkUI at http://192.168.1.4:4040
15/09/12 10:34:55 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
15/09/12 10:34:55 INFO Executor: Starting executor ID driver on host localhost
15/09/12 10:34:55 INFO Executor: Using REPL class URI: http://192.168.1.4:56649
15/09/12 10:34:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56652.
15/09/12 10:34:55 INFO NettyBlockTransferService: Server created on 56652
15/09/12 10:34:55 INFO BlockManagerMaster: Trying to register BlockManager
15/09/12 10:34:55 INFO BlockManagerMasterEndpoint: Registering block manager localhost:56652 with 530.0 MB RAM, BlockManagerId(driver, localhost, 56652)
15/09/12 10:34:55 INFO BlockManagerMaster: Registered BlockManager
15/09/12 10:34:55 INFO Main: Created spark context..
Spark context available as sc.
15/09/12 10:34:56 INFO HiveContext: Initializing execution hive, version 1.2.1
15/09/12 10:34:56 INFO ClientWrapper: Inspected Hadoop version: 2.7.1
15/09/12 10:34:56 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.7.1
15/09/12 10:34:56 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15/09/12 10:34:56 INFO ObjectStore: ObjectStore, initialize called
15/09/12 10:34:57 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
15/09/12 10:34:57 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
15/09/12 10:34:57 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/09/12 10:34:57 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/09/12 10:34:58 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15/09/12 10:34:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:34:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:34:59 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:34:59 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:34:59 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
15/09/12 10:34:59 INFO ObjectStore: Initialized ObjectStore
15/09/12 10:34:59 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
15/09/12 10:35:00 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
15/09/12 10:35:00 INFO HiveMetaStore: Added admin role in metastore
15/09/12 10:35:00 INFO HiveMetaStore: Added public role in metastore
15/09/12 10:35:00 INFO HiveMetaStore: No user is added in admin role, since config is empty
15/09/12 10:35:00 INFO HiveMetaStore: 0: get_all_databases
15/09/12 10:35:00 INFO audit: ugi=jacek	ip=unknown-ip-addr	cmd=get_all_databases
15/09/12 10:35:00 INFO HiveMetaStore: 0: get_functions: db=default pat=*
15/09/12 10:35:00 INFO audit: ugi=jacek	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*
15/09/12 10:35:00 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:35:00 INFO SessionState: Created local directory: /var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/cb22cb85-409a-4f1f-ab65-87932f4be6ae_resources
15/09/12 10:35:00 INFO SessionState: Created HDFS directory: /tmp/hive/jacek/cb22cb85-409a-4f1f-ab65-87932f4be6ae
15/09/12 10:35:00 INFO SessionState: Created local directory: /var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/jacek/cb22cb85-409a-4f1f-ab65-87932f4be6ae
15/09/12 10:35:00 INFO SessionState: Created HDFS directory: /tmp/hive/jacek/cb22cb85-409a-4f1f-ab65-87932f4be6ae/_tmp_space.db
15/09/12 10:35:00 INFO HiveContext: default warehouse location is /user/hive/warehouse
15/09/12 10:35:00 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15/09/12 10:35:00 INFO ClientWrapper: Inspected Hadoop version: 2.7.1
15/09/12 10:35:00 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.7.1
15/09/12 10:35:01 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15/09/12 10:35:01 INFO ObjectStore: ObjectStore, initialize called
15/09/12 10:35:01 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
15/09/12 10:35:01 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
15/09/12 10:35:01 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/09/12 10:35:01 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/09/12 10:35:01 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15/09/12 10:35:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:35:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:35:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:35:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:35:02 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
15/09/12 10:35:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
15/09/12 10:35:02 INFO ObjectStore: Initialized ObjectStore
15/09/12 10:35:02 INFO HiveMetaStore: Added admin role in metastore
15/09/12 10:35:02 INFO HiveMetaStore: Added public role in metastore
15/09/12 10:35:02 INFO HiveMetaStore: No user is added in admin role, since config is empty
15/09/12 10:35:02 INFO HiveMetaStore: 0: get_all_databases
15/09/12 10:35:02 INFO audit: ugi=jacek	ip=unknown-ip-addr	cmd=get_all_databases
15/09/12 10:35:02 INFO HiveMetaStore: 0: get_functions: db=default pat=*
15/09/12 10:35:02 INFO audit: ugi=jacek	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*
15/09/12 10:35:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
15/09/12 10:35:02 INFO SessionState: Created local directory: /var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/9ba4d3c9-94a0-4810-b47a-d2a863954219_resources
15/09/12 10:35:02 INFO SessionState: Created HDFS directory: /tmp/hive/jacek/9ba4d3c9-94a0-4810-b47a-d2a863954219
15/09/12 10:35:02 INFO SessionState: Created local directory: /var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/jacek/9ba4d3c9-94a0-4810-b47a-d2a863954219
15/09/12 10:35:02 INFO SessionState: Created HDFS directory: /tmp/hive/jacek/9ba4d3c9-94a0-4810-b47a-d2a863954219/_tmp_space.db
15/09/12 10:35:02 INFO Main: Created sql context (with Hive support)..
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.5.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

Spark is right under `sc` which is the http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[SparkContext] for the session.

```
scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2ac0cb64
```

Besides, there is also `sqlContext` which is an instance of https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext[org.apache.spark.sql.SQLContext] to use Spark SQL. Read link:spark-sql.adoc[Spark SQL] for more information on the module.

```
scala> sqlContext
res1: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@60ae950f
```

As if it weren't enough, you can also see the state of the Spark shell (which is Spark's driver in the Spark parlance) using Spark UI at http://localhost:4040.

.Spark UI
image::images/sparkui.png[]

Follow http://spark.apache.org/docs/latest/quick-start.html[Quick Start] to get started with Spark.

Close the Spark session using `Ctrl+D` or type `:quit`.

```
scala> :quit
15/08/31 14:15:38 INFO SparkUI: Stopped Spark web UI at http://192.168.99.1:4040
15/08/31 14:15:38 INFO DAGScheduler: Stopping DAGScheduler
15/08/31 14:15:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/31 14:15:38 INFO MemoryStore: MemoryStore cleared
15/08/31 14:15:38 INFO BlockManager: BlockManager stopped
15/08/31 14:15:38 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/31 14:15:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/31 14:15:38 INFO SparkContext: Successfully stopped SparkContext
15/08/31 14:15:38 INFO ShutdownHookManager: Shutdown hook called
15/08/31 14:15:38 INFO ShutdownHookManager: Deleting directory /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-4cfd9622-f495-4cd3-a07d-19591e640a61
15/08/31 14:15:38 INFO ShutdownHookManager: Deleting directory /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-5df5b0fd-a175-406c-985a-1686c9e0e95b
15/08/31 14:15:38 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/31 14:15:38 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
```
