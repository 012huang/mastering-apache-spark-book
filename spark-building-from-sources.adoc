== Building Spark (using Scala 2.11)

You can download pre-packaged versions of Apache Spark from http://spark.apache.org/downloads.html[the project's web site]. The packages are built for a different Hadoop versions, but only for Scala 2.10.

If you want a *Scala 2.11* version of Apache Spark _"users should download the Spark source package and build with Scala 2.11 support"_ (quoted from the Note at http://spark.apache.org/downloads.html[Download Spark]).

The build process for Scala 2.11 takes around 15 mins (on a decent machine) and is so simple that it's unlikely to refuse the urge to do it yourself. It simply requires two steps:

1. Executing `./dev/change-scala-version.sh 2.11` in the source directory (so `pom.xml` use the version of Scala).
1. Executing the following build command:

    ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean install

The most important part is `-Dscala-2.11` for the Maven profile `scala-2.11` that triggers the build to use the latest and greatest Scala **2.11.7**.

After a couple of minutes your freshly baked distro is ready to fly!

I'm using Oracle Java 8 to build Spark.

```
➜  spark git:(master) ./dev/change-scala-version.sh 2.11
➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean install
Using `mvn` from path: /usr/local/bin/mvn
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Spark Project Parent POM
[INFO] Spark Project Launcher
[INFO] Spark Project Networking
[INFO] Spark Project Shuffle Streaming Service
[INFO] Spark Project Unsafe
[INFO] Spark Project Core
[INFO] Spark Project Bagel
[INFO] Spark Project GraphX
[INFO] Spark Project Streaming
[INFO] Spark Project Catalyst
[INFO] Spark Project SQL
[INFO] Spark Project ML Library
[INFO] Spark Project Tools
[INFO] Spark Project Hive
[INFO] Spark Project REPL
[INFO] Spark Project YARN Shuffle Service
[INFO] Spark Project YARN
[INFO] Spark Project Hive Thrift Server
[INFO] Spark Project Assembly
[INFO] Spark Project External Twitter
[INFO] Spark Project External Flume Sink
[INFO] Spark Project External Flume
[INFO] Spark Project External Flume Assembly
[INFO] Spark Project External MQTT
[INFO] Spark Project External MQTT Assembly
[INFO] Spark Project External ZeroMQ
[INFO] Spark Project External Kafka
[INFO] Spark Project Examples
[INFO] Spark Project External Kafka Assembly
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Parent POM 1.6.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] --- maven-clean-plugin:2.6.1:clean (default-clean) @ spark-parent_2.11 ---
...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [  3.612 s]
[INFO] Spark Project Launcher ............................. SUCCESS [ 11.987 s]
[INFO] Spark Project Networking ........................... SUCCESS [  9.553 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  6.459 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  7.272 s]
[INFO] Spark Project Core ................................. SUCCESS [02:02 min]
[INFO] Spark Project Bagel ................................ SUCCESS [  8.658 s]
[INFO] Spark Project GraphX ............................... SUCCESS [ 16.949 s]
[INFO] Spark Project Streaming ............................ SUCCESS [ 34.541 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [ 59.716 s]
[INFO] Spark Project SQL .................................. SUCCESS [01:03 min]
[INFO] Spark Project ML Library ........................... SUCCESS [01:20 min]
[INFO] Spark Project Tools ................................ SUCCESS [  5.330 s]
[INFO] Spark Project Hive ................................. SUCCESS [ 50.157 s]
[INFO] Spark Project REPL ................................. SUCCESS [  6.829 s]
[INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  8.713 s]
[INFO] Spark Project YARN ................................. SUCCESS [ 12.686 s]
[INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 10.752 s]
[INFO] Spark Project Assembly ............................. SUCCESS [01:29 min]
[INFO] Spark Project External Twitter ..................... SUCCESS [  9.763 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [  7.922 s]
[INFO] Spark Project External Flume ....................... SUCCESS [  9.838 s]
[INFO] Spark Project External Flume Assembly .............. SUCCESS [  2.112 s]
[INFO] Spark Project External MQTT ........................ SUCCESS [ 16.728 s]
[INFO] Spark Project External MQTT Assembly ............... SUCCESS [  4.349 s]
[INFO] Spark Project External ZeroMQ ...................... SUCCESS [  9.052 s]
[INFO] Spark Project External Kafka ....................... SUCCESS [ 12.661 s]
[INFO] Spark Project Examples ............................. SUCCESS [01:24 min]
[INFO] Spark Project External Kafka Assembly .............. SUCCESS [  5.798 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 12:52 min
[INFO] Finished at: 2015-09-15T10:23:11+02:00
[INFO] Final Memory: 433M/1862M
[INFO] ------------------------------------------------------------------------
```

Please note the messages that say the version of Spark (_Building Spark Project Parent POM 1.6.0-SNAPSHOT_), Scala version (_maven-clean-plugin:2.6.1:clean (default-clean) @ spark-parent_2.11_) and the Spark modules built.

The above command gives you the latest version of *Apache Spark 1.6.0-SNAPSHOT* built for *Scala 2.11.7* (see https://github.com/apache/spark/blob/master/pom.xml#L2436-L2445[the configuration of scala-2.11 profile]).

You can also check the version of Spark out using `./bin/spark-shell --version`:

```
➜  spark git:(master) ✗ ./bin/spark-shell --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Type --help for more information.
```

=== Running Spark shell

Once built, the Spark shell can be run using `./bin/spark-shell` script.

```
➜  spark git:(master) ✗ ./bin/spark-shell
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")
15/09/15 21:18:54 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
Spark context available as sc.
15/09/15 21:18:55 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/09/15 21:18:55 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/09/15 21:18:57 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
15/09/15 21:18:58 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
15/09/15 21:18:59 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/09/15 21:18:59 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

Spark is right under `sc` which is the http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[SparkContext] for the session.

```
scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2ac0cb64
```

Besides, there is also `sqlContext` which is an instance of https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext[org.apache.spark.sql.SQLContext] to use Spark SQL. Read link:spark-sql.adoc[Spark SQL] for more information on the module.

```
scala> sqlContext
res1: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@60ae950f
```

As if it weren't enough, you can also see the state of the Spark shell (which is Spark's driver in the Spark parlance) using Spark UI at http://localhost:4040.

.Spark UI
image::images/sparkui.png[]

Follow http://spark.apache.org/docs/latest/quick-start.html[Quick Start] to get started with Spark.

Close the Spark session using `Ctrl+D` or type `:quit`.

```
scala> :quit
15/08/31 14:15:38 INFO SparkUI: Stopped Spark web UI at http://192.168.99.1:4040
15/08/31 14:15:38 INFO DAGScheduler: Stopping DAGScheduler
15/08/31 14:15:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/31 14:15:38 INFO MemoryStore: MemoryStore cleared
15/08/31 14:15:38 INFO BlockManager: BlockManager stopped
15/08/31 14:15:38 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/31 14:15:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/31 14:15:38 INFO SparkContext: Successfully stopped SparkContext
15/08/31 14:15:38 INFO ShutdownHookManager: Shutdown hook called
15/08/31 14:15:38 INFO ShutdownHookManager: Deleting directory /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-4cfd9622-f495-4cd3-a07d-19591e640a61
15/08/31 14:15:38 INFO ShutdownHookManager: Deleting directory /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-5df5b0fd-a175-406c-985a-1686c9e0e95b
15/08/31 14:15:38 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/31 14:15:38 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
```
