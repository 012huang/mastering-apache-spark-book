== Scheduler Backends

=== Introduction

Spark comes with a pluggable backend mechanism called *scheduler backend* (aka _backend scheduler_) to support various cluster managers, e.g. link:spark-mesos.adoc[Apache Mesos], link:spark-yarn.adoc[Hadoop YARN] or Spark's own link:spark-standalone.adoc[Spark Standalone] and link:spark-local.adoc#LocalBackend[Spark local]. These cluster managers differ by their custom task scheduling modes and resource offers mechanisms, and Spark's approach is to abstract the differences in <<contract, SchedulerBackend Contract>>.

A scheduler backend is created and started as part of SparkContext's initialization (when link:spark-taskscheduler.adoc[TaskSchedulerImpl] is started - see link:spark-sparkcontext.adoc#createTaskScheduler[Creating Scheduler Backend and Task Scheduler]).

CAUTION: FIXME Image how it gets created with SparkContext in play here or in SparkContext doc.

Scheduler backends are started and stopped as part of TaskSchedulerImpl's initialization and stopping.

Being a scheduler backend in Spark assumes a http://mesos.apache.org/[Apache Mesos]-like model in which "an application" gets *resource offers* as machines become available and can launch tasks on them. Once a scheduler backend obtains the resource allocation, it can start executors.

TIP: Understanding how http://mesos.apache.org/[Apache Mesos] works can greatly improve understanding Spark.

=== [[contract]] SchedulerBackend Contract

NOTE: `org.apache.spark.scheduler.SchedulerBackend` is a `private[spark]` Scala trait in Spark.

Every scheduler backend has to offer the following services:

* Can be started (using `start()`) and stopped (using `stop()`).
* Do <<reviveOffers, reviveOffers>>
* Calculate <<defaultParallelism, default level of parallelism>>.
* Be able to `killTask(taskId: Long, executorId: String, interruptThread: Boolean)`.
* Answers `isReady()` to tell about its started/stopped state. It returns `true` by default.
* Knows the application id for a job (using `applicationId()`).

CAUTION: FIXME `applicationId()` doesn't accept an input parameter. How is Scheduler Backend related to a job and an application?

* Knows an application attempt id (see <<applicationAttemptId,applicationAttemptId>>)
* Knows the URLs for the driver's logs (see <<getDriverLogUrls, getDriverLogUrls>>).

CAUTION: FIXME Screenshot the tab and the links

CAUTION: FIXME Have an exercise to create a SchedulerBackend.

==== [[reviveOffers]] reviveOffers

NOTE: It is used in `TaskSchedulerImpl` using `backend` internal reference.

Refer to link:spark-local.adoc#task-submission[Task Submission a.k.a. reviveOffers] for local mode.

Refer to link:spark-scheduler-backends-coarse-grained.adoc#reviveOffers[reviveOffers] for CoarseGrainedSchedulerBackend.

Refer to link:spark-mesos.adoc#reviveOffers[reviveOffers] for MesosSchedulerBackend.

No other custom implementations of `defaultParallelism()` exists.

==== [[defaultParallelism]] Default Level of Parallelism

*Default level of parallelism* is used by link:spark-taskscheduler.adoc[TaskScheduler] to use as a hint for sizing jobs.

NOTE: It is used in `TaskSchedulerImpl.defaultParallelism`.

Refer to link:spark-local.adoc#LocalBackend[LocalBackend] for local mode.

Refer to link:spark-scheduler-backends-coarse-grained.adoc#defaultParallelism[Default Level of Parallelism] for CoarseGrainedSchedulerBackend.

Refer to link:spark-mesos.adoc#defaultParallelism[Default Level of Parallelism] for CoarseMesosSchedulerBackend.

No other custom implementations of `defaultParallelism()` exists.

==== [[applicationAttemptId]] applicationAttemptId

`applicationAttemptId(): Option[String]` returns no application attempt id.

It is currently only supported by YARN cluster scheduler backend as the YARN cluster manager supports multiple attempts.

==== [[getDriverLogUrls]] getDriverLogUrls

`getDriverLogUrls: Option[Map[String, String]]` returns no URLs by default.

It is currently only supported by link:spark-yarn.adoc#YarnClusterSchedulerBackend[YarnClusterSchedulerBackend].

=== Available Implementations

Spark comes with the following scheduler backends:

* link:spark-local.adoc#LocalBackend[LocalBackend] (local mode)
* link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend]
** *SparkDeploySchedulerBackend* used in link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone] (and local-cluster - FIXME)
** YarnSchedulerBackend
*** YarnClientSchedulerBackend
*** *YarnClusterSchedulerBackend* used in link:spark-yarn.adoc#YarnClusterSchedulerBackend[Spark on YARN in cluster mode]
** link:spark-mesos.adoc#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend]
** SimrSchedulerBackend
* link:spark-mesos.adoc#MesosSchedulerBackend[MesosSchedulerBackend]
