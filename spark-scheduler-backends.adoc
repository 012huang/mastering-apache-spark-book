== Scheduler Backends

Spark comes with a pluggable backend mechanism called *scheduler backend* (aka _backend scheduler_) to support various backend schedulers, each with their own custom task scheduling modes and resource offers mechanism. It is the approach to plug in other task schedulers.

`SchedulerBackend` is the Spark interface to different task scheduling systems, i.e. link:spark-local.adoc#LocalBackend[Spark local], link:spark-standalone.adoc[Spark Standalone], link:spark-mesos.adoc[Mesos] or link:spark-yarn.adoc[YARN].

CAUTION: FIXME Have an exercise to create a SchedulerBackend.

Being a scheduler backend in Spark assumes a http://mesos.apache.org/[Apache Mesos]-like model in which "an application" gets *resource offers* as machines become available and can launch tasks on them. Once a scheduler backend obtains the resource allocation, it can start executors.

TIP: Understanding how Apache Mesos works can greatly improve understanding Spark.

Scheduler backends are started and stopped as part of Spark's initialization and stop.

A scheduler backend can `reviveOffers`, calculate `defaultParallelism`, kill tasks, return application attempt ids (supported only by `YarnClusterSchedulerBackend`) and URLs for the driver logs.

Spark comes with the following scheduler backends:

* link:spark-local.adoc#LocalBackend[LocalBackend] (local mode)
* link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend]
** *SparkDeploySchedulerBackend* used in link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone] (and local-cluster - FIXME)
** YarnSchedulerBackend
*** YarnClientSchedulerBackend
*** *YarnClusterSchedulerBackend* used in link:spark-yarn.adoc#YarnClusterSchedulerBackend[Spark on YARN in cluster mode]
** link:spark-mesos.adoc#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend]
** SimrSchedulerBackend
* link:spark-mesos.adoc#MesosSchedulerBackend[MesosSchedulerBackend]
