== Source

`Source` represents a continuous data stream for a streaming query. It generates batches of DataFrame for given offsets.

`Source` knows `schema` (as link:spark-sql-dataframe.adoc#StructType[StructType]) of the data, tracks the maximum *offset* (as `Offset`) and can create *batches* for start and end offsets.

It belongs to `org.apache.spark.sql.execution.streaming` package.

[source, scala]
----
import org.apache.spark.sql.execution.streaming.Source
----

There are two available `Source` implementations:

* <<FileStreamSource, FileStreamSource>>
* <<MemoryStream, MemoryStream>>

=== [[FileStreamSource]] FileStreamSource

`FileStreamSource` is a `Source` implementation that reads text files from the given directory as they appear.

It lives in `org.apache.spark.sql.execution.streaming` package.

It tracks already-processed files in `seenFiles` hash map.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.sql.execution.streaming.FileStreamSource` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSource=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[MemoryStream]] MemoryStream
