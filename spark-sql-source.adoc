== Source

`Source` represents a continuous data stream for a streaming query. It generates batches of link:spark-sql-dataframe.adoc[DataFrame] for given start and end offsets. For fault tolerance, a source must be able to replay data given a start offset.

`Source` trait has the following features:

* *schema* of the data (as link:spark-sql-dataframe.adoc#StructType[StructType] using `schema` method)
* the maximum *offset* (of type `Offset` using `getOffset` method)
* Returns a *batch* for start and end offsets (of type DataFrame).

It lives in `org.apache.spark.sql.execution.streaming` package.

[source, scala]
----
import org.apache.spark.sql.execution.streaming.Source
----

There are two available `Source` implementations:

* <<FileStreamSource, FileStreamSource>>
* <<MemoryStream, MemoryStream>>

=== [[FileStreamSource]] FileStreamSource

`FileStreamSource` is a `Source` implementation that reads text files from `path` directory as they appear. It uses `LongOffset` offsets.

NOTE: It is used by link:spark-sql-datasource.adoc#createSource[DataSource.createSource] for `FileFormat`.

You can provide the <<FileStreamSource-schema, schema>> of the data and `dataFrameBuilder` - the function to build a `DataFrame` in <<FileStreamSource-getBatch, getBatch>> at instantiation time.

[source, scala]
----
// NOTE The directory must exist
// mkdir hello

val textReader = spark.read.format("text")
val in = textReader.stream("hello")

scala> in.printSchema
root
 |-- value: string (nullable = true)

val textWriter = in.write.format("text")
val opts = Map("checkpointLocation" -> "hello-checkpoint")
val out = textWriter.options(opts).startStream("hello-out")

// NOTE Check out hello-out and hello-checkpoint directories
----

Batches are indexed.

It lives in `org.apache.spark.sql.execution.streaming` package.

It tracks already-processed files in `seenFiles` hash map.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.sql.execution.streaming.FileStreamSource` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSource=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

==== [[FileStreamSource-schema]] schema

If the schema is specified at instantiation time (using optional `dataSchema` constructor parameter) it is returned.

Otherwise, `fetchAllFiles` internal method is called to list all the files in a directory.

When there is at least one file the schema is calculated using `dataFrameBuilder` constructor parameter function. Else, an `IllegalArgumentException("No schema specified")` is thrown unless it is for *text* provider (as `providerName` constructor parameter) where the default schema with a single `value` column of type `StringType` is assumed.

NOTE: *text* as the value of `providerName` constructor parameter denotes *text file stream provider*.

==== [[FileStreamSource-getOffset]] getOffset

The maximum offset (`getOffset`) is calculated by fetching all the files in `path` excluding files that start with `_` (underscore).

When computing the maximum offset using `getOffset`, you should see the following DEBUG message in the logs:

```
DEBUG Listed ${files.size} in ${(endTime.toDouble - startTime) / 1000000}ms
```

When computing the maximum offset using `getOffset`, it also filters out the files that were already seen (tracked in `seenFiles` internal registry).

You should see the following DEBUG message in the logs (depending on the status of a file):

```
new file: $file
// or
old file: $file
```

==== [[FileStreamSource-getBatch]] getBatch

`FileStreamSource.getBatch` asks <<FileStreamSource-metadataLog, metadataLog>> for the batch.

You should see the following INFO and DEBUG messages in the logs:

```
INFO Processing ${files.length} files from ${startId + 1}:$endId
DEBUG Streaming ${files.mkString(", ")}
```

The method to create a result batch is given at instantiation time (as `dataFrameBuilder` constructor parameter).

==== [[FileStreamSource-metadataLog]] metadataLog

`metadataLog` is a metadata storage using `metadataPath` path (which is a constructor parameter).

NOTE: It extends `HDFSMetadataLog[Seq[String]]`.

CAUTION: FIXME Review `HDFSMetadataLog`

=== [[MemoryStream]] MemoryStream
