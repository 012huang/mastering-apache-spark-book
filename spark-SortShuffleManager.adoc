== [[SortShuffleManager]] SortShuffleManager -- The Default (And Only) Shuffle System

`SortShuffleManager` is the one and only link:spark-shuffle-manager.adoc[shuffle manager] in Spark with the short name `sort` or `tungsten-sort`.

`SortShuffleManager` uses `IndexShuffleBlockResolver` (as <<shuffleBlockResolver, `shuffleBlockResolver` internal registry>>).

[[internal-registries]]
.`SortShuffleManager` Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[numMapsForShuffle]] `numMapsForShuffle`
|

| [[shuffleBlockResolver]] `shuffleBlockResolver`
|

Used when ???
|===

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.shuffle.sort.SortShuffleManager$` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.SortShuffleManager$=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating `SortShuffleManager` Instance

`SortShuffleManager` takes a link:spark-configuration.adoc[SparkConf].

`SortShuffleManager` makes sure that <<spark_shuffle_spill, `spark.shuffle.spill` Spark property>> is enabled. If not you should see the following WARN message in the logs:

```
WARN SortShuffleManager: spark.shuffle.spill was set to false, but this configuration is ignored as of Spark 1.6+. Shuffle will continue to spill to disk when necessary.
```

`SortShuffleManager` <<internal-registries, initializes the internal registries and counters>>.

NOTE: `SortShuffleManager` is created when link:spark-sparkenv.adoc#ShuffleManager[`SparkEnv` is created] (per the driver and executors).

=== [[registerShuffle]] Getting `ShuffleHandle` -- `registerShuffle` Method

[source, scala]
----
registerShuffle[K, V, C](
  shuffleId: Int,
  numMaps: Int,
  dependency: ShuffleDependency[K, V, C]): ShuffleHandle
----

NOTE: `registerShuffle` is a part of link:spark-shuffle-manager.adoc#contract[ShuffleManager contract].

`registerShuffle` returns a new `ShuffleHandle` that can be one of the following:

1. `BypassMergeSortShuffleHandle` (with `ShuffleDependency[K, V, V]`) when <<shouldBypassMergeSort, `shouldBypassMergeSort` condition holds>>.

2. link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle] (with `ShuffleDependency[K, V, V]`) when <<canUseSerializedShuffle, `canUseSerializedShuffle` condition holds>>.

3. `BaseShuffleHandle`

=== [[getWriter]] Returning `ShuffleWriter` For `ShuffleHandle` -- `getWriter` Method

[source, scala]
----
getWriter[K, V](
  handle: ShuffleHandle,
  mapId: Int,
  context: TaskContext): ShuffleWriter[K, V]
----

NOTE: `getWriter` is a part of link:spark-shuffle-manager.adoc#contract[`ShuffleManager` contract].

Internally, `getWriter` makes sure that a `ShuffleHandle` is associated with its `numMaps` in <<numMapsForShuffle, `numMapsForShuffle` internal registry>>.

NOTE: `getWriter` expects that the input `handle` is of type `BaseShuffleHandle` (despite the signature that says that it can work with any `ShuffleHandle`).

`getWriter` then returns a new `ShuffleWriter` for the input `ShuffleHandle`:

1. link:spark-UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] for link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle].

2. `BypassMergeSortShuffleWriter` for `BypassMergeSortShuffleHandle`.

3. link:spark-SortShuffleWriter.adoc[SortShuffleWriter] for `BaseShuffleHandle`.

=== [[getReader]] Creating `BlockStoreShuffleReader` (For `ShuffleHandle`) -- `getReader` Method

[source, scala]
----
getReader[K, C](
  handle: ShuffleHandle,
  startPartition: Int,
  endPartition: Int,
  context: TaskContext): ShuffleReader[K, C]
----

NOTE: `getReader` is a part of link:spark-shuffle-manager.adoc#contract[`ShuffleManager` contract].

`getReader` returns a new link:spark-BlockStoreShuffleReader.adoc[BlockStoreShuffleReader] passing all the input parameters on.

NOTE: `getReader` assumes that the input `ShuffleHandle` is really a `BaseShuffleHandle`.

=== [[shouldBypassMergeSort]] `shouldBypassMergeSort` Method

[source, scala]
----
shouldBypassMergeSort(conf: SparkConf, dep: ShuffleDependency[_, _, _]): Boolean
----

`shouldBypassMergeSort` holds (i.e. is positive) when the input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] has link:spark-rdd-ShuffleDependency.adoc#mapSideCombine[`mapSideCombine` flag enabled] and link:spark-rdd-ShuffleDependency.adoc#aggregator[`aggregator` defined].

`shouldBypassMergeSort` holds when `mapSideCombine` flag is disabled but the link:spark-rdd-ShuffleDependency.adoc#partitioner[number of partitions (of the input `ShuffleDependency`)] is smaller than <<spark.shuffle.sort.bypassMergeThreshold, spark.shuffle.sort.bypassMergeThreshold>> Spark property.

Otherwise, `shouldBypassMergeSort` is negative (i.e. `false`).

NOTE: `shouldBypassMergeSort` is exclusively used when <<registerShuffle, `registerShuffle` selects a `ShuffleHandle`>>.

=== [[canUseSerializedShuffle]] Checking If `SerializedShuffleHandle` Can Be Used for `ShuffleHandle` -- `canUseSerializedShuffle` Method

[source, scala]
----
canUseSerializedShuffle(dependency: ShuffleDependency[_, _, _]): Boolean
----

`canUseSerializedShuffle` condition holds (i.e. is positive) when all of the following hold (checked in that order):

1. The link:spark-Serializer.adoc#supportsRelocationOfSerializedObjects[`Serializer` of the input `ShuffleDependency` supports relocation of serialized objects].

2. The link:spark-rdd-ShuffleDependency.adoc#aggregator[`Aggregator` of the input `ShuffleDependency` is _not_ defined].

3. The link:spark-rdd-ShuffleDependency.adoc#partitioner[number of shuffle output partitions of the input `ShuffleDependency`] is at most the supported maximum number (which is `(1 << 24) - 1`, i.e. `16777215`).

You should see the following DEBUG message in the logs when `canUseSerializedShuffle` holds:

```
DEBUG Can use serialized shuffle for shuffle [id]
```

Otherwise, `canUseSerializedShuffle` does not hold and you should see one of the following DEBUG messages:

```
DEBUG Can't use serialized shuffle for shuffle [id] because the serializer, [name], does not support object relocation

DEBUG Can't use serialized shuffle for shuffle [id] because an aggregator is defined

DEBUG Can't use serialized shuffle for shuffle [id] because it has more than [number] partitions
```

NOTE: `canUseSerializedShuffle` is exclusively used when <<registerShuffle, `registerShuffle` selects a `ShuffleHandle`>>.

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark.shuffle.sort.bypassMergeThreshold]] `spark.shuffle.sort.bypassMergeThreshold`
| `200`
|


| [[spark_shuffle_spill]] `spark.shuffle.spill`
| `true`
| No longer in use.

When `false` the following WARN shows in the logs when <<creating-instance, `SortShuffleManager` is created>>:

`WARN SortShuffleManager: spark.shuffle.spill was set to false, but this configuration is ignored as of Spark 1.6+. Shuffle will continue to spill to disk when necessary.`

|===
