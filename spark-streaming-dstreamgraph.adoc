== DStreamGraph

`DStreamGraph` (is a final helper class that) manages *input* and *output dstreams*.

It maintains the collections of link:spark-streaming-inputdstreams.adoc[InputDStream] instances (as `inputStreams`) and output link:spark-streaming-dstreams.adoc[DStream] instances (as `outputStreams`), but, more importantly, <<generateJobs, it generates streaming jobs for output streams for a batch>>.

[NOTE]
====
`DStreamGraph` holds `batchDuration` (using `setBatchDuration(duration: Duration)`) for other parts of the Streaming application.

It appears that it is _the_ place for the value since it must be set before link:spark-streaming-jobgenerator.adoc[JobGenerator] can be instantiated.

It _is_ set while link:spark-streaming-streamingcontext.adoc[StreamingContext] is being instantiated and is validated (using `validate()` method of `StreamingContext` and `DStreamGraph`) before `StreamingContext` is started.
====

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.DStreamGraph` logger to see what happens in `DStreamGraph`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.DStreamGraph=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[zeroTime]] Zero Time (aka zeroTime)

*Zero time* (internally `zeroTime`) is the time when <<start, DStreamGraph has been started>>.

=== [[start]] Start Method

[source, scala]
----
start(time: Time): Unit
----

When `DStreamGraph` is started (using `start` method), it sets <<zeroTime, zero time>> and *start time* (`startTime`).

NOTE: You can start `DStreamGraph` as many times until `time` is not `null` and <<zeroTime, zero time>> has been set.

(output streams) It then walks over the collection of output dstreams and for each output dstream, one at a time, calls their link:spark-streaming-dstreams.adoc#initialize[initialize(zeroTime)], link:spark-streaming-dstreams.adoc#remember[remember(rememberDuration)], and link:spark-streaming-dstreams.adoc#validateAtStart[validateAtStart] methods.

(input streams) When all the output streams are processed, it starts the input dstreams (in parallel) using `start` method.

=== [[batchDuration]] Batch Interval

CAUTION: FIXME

=== [[generateJobs]] Generating Streaming Jobs for Output Streams for Batch Time

`generateJobs(time: Time): Seq[Job]` generates a collection of streaming jobs for output streams for a given batch `time`. It walks over each link:spark-streaming-dstreams.adoc#register[registered output stream] (in `outputStreams` internal registry) and link:spark-streaming-dstreams.adoc#generateJob[requests the streams for a streaming job]

NOTE: `generateJobs` is called by link:spark-streaming-jobgenerator.adoc[JobGenerator] to link:spark-streaming-jobgenerator.adoc#GenerateJobs[generate jobs for a given batch time] or link:spark-streaming-jobgenerator.adoc#restarting[when restarted from checkpoint].

When `generateJobs` method executes, you should see the following DEBUG message in the logs:

```
DEBUG DStreamGraph: Generating jobs for time [time] ms
```

`generateJobs` then walks over each link:spark-streaming-dstreams.adoc#register[registered output stream] (in `outputStreams` internal registry) and link:spark-streaming-dstreams.adoc#generateJob[requests the streams for a streaming job].

Right before the method finishes, you should see the following DEBUG message with the number of streaming jobs generated (as `jobs.length`):

```
DEBUG DStreamGraph: Generated [jobs.length] jobs for time [time] ms
```

=== [[dstreamgraph-validation]] Validation Check (using validate method)

`validate()` method checks whether batch duration and at least one output stream have been set. It will throw `java.lang.IllegalArgumentException` when either is not.

NOTE: It is called when link:spark-streaming-streamingcontext.adoc#start[StreamingContext starts].

=== [[clearMetadata]] Metadata Cleanup

NOTE: It is called when  link:spark-streaming-jobgenerator.adoc#ClearMetadata[JobGenerator clears metadata].

When `clearMetadata(time: Time)` is called, you should see the following DEBUG message in the logs:

```
DEBUG DStreamGraph: Clearing metadata for time [time] ms
```

It merely walks over the collection of output streams and (synchronously, one by one) asks to do link:spark-streaming-dstreams.adoc#clearMetadata[its own metadata cleaning].

When finishes, you should see the following DEBUG message in the logs:

```
DEBUG DStreamGraph: Cleared old metadata for time [time] ms
```
