== Standard Functions for DataFrames (functions object)

`org.apache.spark.sql.functions` object comes with many functions for column manipulation in DataFrames.

NOTE: The `functions` object is an experimental feature of Spark since version 1.3.0.

You can access the functions using the following import statement:

[source, scala]
----
import org.apache.spark.sql.functions._
----

There are nearly 50 or more functions in the `functions` object. Some functions are transformations of `Column` objects (or column names) into other `Column` objects or transform `DataFrame` into `DataFrame`.

The functions are grouped by functional areas:

* <<udf, Defining UDFs>>
* <<string-functions, String functions>>
** <<split, split>>
** <<upper, upper>> (chained with `reverse`)
* Aggregate functions
* <<non-aggregate-functions, Non-aggregate functions>> (aka _normal functions_)
** <<struct, struct>>
** <<broadcast, broadcast>> (for `DataFrame`)
** <<expr, expr>>
* Date time functions
* ..._and others_

TIP: You should read the http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$[official documentation of the functions object].

=== [[udf]] Defining UDFs (udf factories)

[source, scala]
----
udf(f: FunctionN[...]): UserDefinedFunction
----

The `udf` family of functions allows you to create link:spark-sql-udfs.adoc[user-defined functions (UDFs)] based on a user-defined function in Scala. It accepts `f` function of 0 to 10 arguments and the input and output types are automatically inferred (given the types of the respective input and output types of the function `f`).

[source, scala]
----
import org.apache.spark.sql.functions._
val _length: String => Int = _.length
val _lengthUDF = udf(_length)

// define a dataframe
val df = sc.parallelize(0 to 3).toDF("num")

// apply the user-defined function to "num" column
scala> df.withColumn("len", _lengthUDF($"num")).show
+---+---+
|num|len|
+---+---+
|  0|  1|
|  1|  1|
|  2|  1|
|  3|  1|
+---+---+
----

Since Spark 2.0.0, there is another variant of `udf` function:

[source, scala]
----
udf(f: AnyRef, dataType: DataType): UserDefinedFunction
----

`udf(f: AnyRef, dataType: DataType)` allows you to use a Scala closure for the function argument (as `f`) and explicitly declaring the output data type (as `dataType`).

[source, scala]
----
// given the dataframe above

import org.apache.spark.sql.types.IntegerType
val byTwo = udf((n: Int) => n * 2, IntegerType)

scala> df.withColumn("len", byTwo($"num")).show
+---+---+
|num|len|
+---+---+
|  0|  0|
|  1|  2|
|  2|  4|
|  3|  6|
+---+---+
----

=== [[string-functions]] String functions

==== [[split]] split function

[source, scala]
----
split(str: Column, pattern: String): Column
----

`split` function splits `str` column using `pattern`. It returns a new `Column`.

NOTE: `split` UDF uses https://docs.oracle.com/javase/8/docs/api/java/lang/String.html#split-java.lang.String-int-[java.lang.String.split(String regex, int limit)] method.

[source, scala]
----
val df = Seq((0, "hello|world"), (1, "witaj|swiecie")).toDF("num", "input")
val withSplit = df.withColumn("split", split($"input", "[|]"))

scala> withSplit.show
+---+-------------+----------------+
|num|        input|           split|
+---+-------------+----------------+
|  0|  hello|world|  [hello, world]|
|  1|witaj|swiecie|[witaj, swiecie]|
+---+-------------+----------------+
----

NOTE: `.$|()[{^?*+\` are RegEx's meta characters and are considered special.

==== [[upper]] upper function

[source, scala]
----
upper(e: Column): Column
----

`upper` function converts a string column into one with all letter upper. It returns a new `Column`.

NOTE: The following example uses two functions that accept a `Column` and return another to showcase how to chain them.

[source, scala]
----
val df = Seq((0,1,"hello"), (2,3,"world"), (2,4, "ala")).toDF("id", "val", "name")
val withUpperReversed = df.withColumn("upper", reverse(upper($"name")))

scala> withUpperReversed.show
+---+---+-----+-----+
| id|val| name|upper|
+---+---+-----+-----+
|  0|  1|hello|OLLEH|
|  2|  3|world|DLROW|
|  2|  4|  ala|  ALA|
+---+---+-----+-----+
----

=== [[non-aggregate-functions]] Non-aggregate functions

They are also called *normal functions*.

==== [[struct]] struct functions

[source, scala]
----
struct(cols: Column*): Column
struct(colName: String, colNames: String*): Column
----

`struct` family of functions allows you to create a new struct column based on a collection of `Column` or their names.

NOTE: The difference between `struct` and another similar `array` function is that the types of the columns can be different (in `struct`).

[source, scala]
----
scala> df.withColumn("struct", struct($"name", $"val")).show
+---+---+-----+---------+
| id|val| name|   struct|
+---+---+-----+---------+
|  0|  1|hello|[hello,1]|
|  2|  3|world|[world,3]|
|  2|  4|  ala|  [ala,4]|
+---+---+-----+---------+
----

==== [[broadcast]] broadcast function

[source, scala]
----
broadcast(df: DataFrame): DataFrame
----

`broadcast` function creates a new `DataFrame` (out of the input `DataFrame`) and marks the logical plan to be broadcast when used in a `join` operator.

TIP: Consult link:spark-sql-joins.adoc#broadcast-join[Broadcast Join] document.

[source, scala]
----
val left = Seq((0, "aa"), (0, "bb")).toDF("id", "token")
val right = Seq(("aa", 0.99), ("bb", 0.57)).toDF("token", "prob")

scala> left.join(broadcast(right), "token").show
+-----+---+----+
|token| id|prob|
+-----+---+----+
|   aa|  0|0.99|
|   bb|  0|0.57|
+-----+---+----+
----

==== [[expr]] expr function

[source, scala]
----
expr(expr: String): Column
----

`expr` function parses the input `expr` string to a `Column` it represents.

NOTE: `expr` uses `SparkSqlParser.parseExpression` as `selectExpr`, `filter` and `where` functions of link:spark-sql-dataset.adoc[Dataset].

[source, scala]
----
scala> ds
res34: org.apache.spark.sql.Dataset[(String, Int)] = [text: string, id: int]

scala> ds.show
+------+---+
|  text| id|
+------+---+
| hello|  0|
|world!|  1|
+------+---+


scala> ds.filter(expr("text = 'hello'")).show
+-----+---+
| text| id|
+-----+---+
|hello|  0|
+-----+---+
----
