== Windows

* Window-based framework since 1.4
* Window function support in Spark SQL
* Define a (row) window to execute aggregations on
* Operate on a group of rows to return a single value for every input row
* Before 1.4, merely two kinds of functions in Spark SQL to calculate a single return value:
** *Built-in functions* or *UDFs* - `substr` or `round` - take values from a single row as input, and they generate a single return value for every input row
** *Aggregate functions*, such as `SUM` or `MAX`, operate on a group of rows and calculate a single return value for every group.

A window specification defines the *partitioning*, *ordering*, and *frame boundaries* (see https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.WindowSpec[org.apache.spark.sql.expressions.WindowSpec] API).

Spark SQL supports three kinds of window functions: *ranking* functions, *analytic* functions, and *aggregate* functions.

IMPORTANT: Where are they defined in the code?

.Window functions in Spark SQL (see https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL])
[align="center",width="80%",frame="topbot",options="header"]
|======================
| |SQL |DataFrame API
.5+^.^|*Ranking functions* |rank |rank
|dense_rank|denseRank
|percent_rank|percentRank
|ntile|ntile
|row_number|rowNumber
.5+^.^|*Analytic functions* |cume_dist |cumeDist
|first_value |firstValue
|last_value |lastValue
|lag |lag
|lead |lead
|======================

For aggregate functions, users can use any existing aggregate function as a window function.

=== [[window-object]] Window object

`Window` object defines functions to define windows.

CAUTION: FIXME What is a window? How is this different from groups like `groupBy`?

NOTE: It is an experimental feature since Spark *1.4.0*.

There are two families of the available functions in `Window` object that create <<WindowSpec, WindowSpec>> instance for one or many link:spark-sql-columns.adoc[Column] instances:

* <<partitionBy, partitionBy>>
* <<orderBy, orderBy>>

`Window` object lives in `org.apache.spark.sql.expressions` package. Import it to use `Window` functions.

[source, scala]
----
import org.apache.spark.sql.expressions.Window
----

==== [[partitionBy]] partitionBy

[source, scala]
----
partitionBy(colName: String, colNames: String*): WindowSpec
partitionBy(cols: Column*): WindowSpec
----

`partitionBy` creates an instance of `WindowSpec` with partition expression(s) defined for one or more columns.

[source, scala]
----
val byHTokens = Window.partitionBy('token startsWith "h")
val result = tokens.select('*, sum('id) over byHTokens as "sum over h tokens").orderBy('id)

scala> .show
+---+-----+-----------------+
| id|token|sum over h tokens|
+---+-----+-----------------+
|  0|hello|                4|
|  1|henry|                4|
|  2|  and|                2|
|  3|harry|                4|
+---+-----+-----------------+
----

==== [[orderBy]] orderBy

[source, scala]
----
orderBy(colName: String, colNames: String*): WindowSpec
orderBy(cols: Column*): WindowSpec
----

=== Window functions

You can mark a function _window_ by the `OVER` clause after a supported function in SQL, e.g. `avg(revenue) OVER (...);` or the `over` method on a supported function in the DataFrame API, e.g. `rank().over(...)`.

=== [[WindowSpec]] WindowSpec - Window Specification

Any window function needs a *window specification* which is an instance of `WindowSpec` class.

NOTE: `WindowSpec` class is marked as experimental since *1.4.0*.

A window specification defines which rows are included in a frame (associated with a given input row) by *partitioning* an entire data set and specifying *frame boundary* with *ordering*.

NOTE: Use static methods in <<window-object, Window object>> to create a `WindowSpec`.

[source, scala]
----

----

A window specification includes three parts:

1. *Partitioning Specification* controls which rows will be in the same partition with the given row. Also, the user might want to make sure all rows having the same value for  the category column are collected to the same machine before ordering and calculating the frame.  If no partitioning specification is given, then all data must be collected to a single machine.
2. *Ordering Specification* controls the way that rows in a partition are ordered, determining the position of the given row in its partition.
3. *Frame Specification* (unsupported in Hive; see http://stackoverflow.com/a/32379437/1305344[Why do Window functions fail with "Window function X does not take a frame specification"?]) states which rows are included in the frame for the current input row, based on their relative position to the current row.  For example, _“the three rows preceding the current row to the current row”_ describes a frame including the current input row and three rows appearing before the current row.

=== Frames

At its core, a window function calculates a return value for every input row of a table based on a group of rows, called the *frame*. Every input row can have a unique frame associated with it.

When you define a frame you have to specify three components of a frame specification - the *start and end boundaries*, and the *type*.

Types of boundaries (two positions and three offsets):

* `UNBOUNDED PRECEDING` - the first row of the partition
* `UNBOUNDED FOLLOWING` - the last row of the partition
* `CURRENT ROW`
* `<value> PRECEDING`
* `<value> FOLLOWING`

Offsets specify the offset from the current input row.

Types of frames:

* `ROW` - based on _physical offsets_ from the position of the current input row
* `RANGE` - based on _logical offsets_ from the position of the current input row

=== Examples

Two samples from https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window$[org.apache.spark.sql.expressions.Window] scaladoc:

```
// PARTITION BY country ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
Window.partitionBy("country").orderBy("date").rowsBetween(Long.MinValue, 0)
```

```
// PARTITION BY country ORDER BY date ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING
Window.partitionBy("country").orderBy("date").rowsBetween(-3, 3)
```

IMPORTANT: Present input data as a colorful table (headers, rows) and ask the question to be answered in an example.

==== Computing the first and second best-sellers in category

NOTE: This example is borrowed from an _excellent_ article  https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL].

.Table PRODUCT_REVENUE
[align="center",width="80%",frame="topbot",options="header,footer"]
|======================
|product |category |revenue
|      Thin|cell phone|   6000
|    Normal|    tablet|   1500
|      Mini|    tablet|   5500
|Ultra thin|cell phone|   5000
| Very thin|cell phone|   6000
|       Big|    tablet|   2500
|  Bendable|cell phone|   3000
|  Foldable|cell phone|   3000
|       Pro|    tablet|   4500
|      Pro2|    tablet|   6500
|======================

Question: What are the best-selling and the second best-selling products in every category?

```
scala> val schema = Seq("product", "category", "revenue")
schema: Seq[String] = List(product, category, revenue)

scala> val data = Seq(
     |   ("Thin",       "cell phone", 6000),
     |   ("Normal",     "tablet",     1500),
     |   ("Mini",       "tablet",     5500),
     |   ("Ultra thin", "cell phone", 5000),
     |   ("Very thin",  "cell phone", 6000),
     |   ("Big",        "tablet",     2500),
     |   ("Bendable",   "cell phone", 3000),
     |   ("Foldable",   "cell phone", 3000),
     |   ("Pro",        "tablet",     4500),
     |   ("Pro2",       "tablet",     6500)
     | )
data: Seq[(String, String, Int)] = List((Thin,cell phone,6000), (Normal,tablet,1500), (Mini,tablet,5500), (Ultra thin,cell phone,5000), (Very thin,cell phone,6000), (Big,tablet,2500), (Bendable,cell phone,3000), (Foldable,cell phone,3000), (Pro,tablet,4500), (Pro2,tablet,6500))

scala> val df = sc.parallelize(data).toDF(schema: _*)
df: org.apache.spark.sql.DataFrame = [product: string, category: string, revenue: int]

scala> df.select('*).show
+----------+----------+-------+
|   product|  category|revenue|
+----------+----------+-------+
|      Thin|cell phone|   6000|
|    Normal|    tablet|   1500|
|      Mini|    tablet|   5500|
|Ultra thin|cell phone|   5000|
| Very thin|cell phone|   6000|
|       Big|    tablet|   2500|
|  Bendable|cell phone|   3000|
|  Foldable|cell phone|   3000|
|       Pro|    tablet|   4500|
|      Pro2|    tablet|   6500|
+----------+----------+-------+

scala> df.where(df("category") === "tablet").show
+-------+--------+-------+
|product|category|revenue|
+-------+--------+-------+
| Normal|  tablet|   1500|
|   Mini|  tablet|   5500|
|    Big|  tablet|   2500|
|    Pro|  tablet|   4500|
|   Pro2|  tablet|   6500|
+-------+--------+-------+
```

The question boils down to ranking products in a category based on their revenue, and to pick the best selling and the second best-selling products based the ranking.

```
scala> import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.expressions.Window

scala> val overCategory = Window.partitionBy("category").orderBy(desc("revenue"))
overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@168cffde

// or alternatively using $
scala> val overCategory = Window.partitionBy("category").orderBy($"revenue".desc)
overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5c2a04b1
```

IMPORTANT: Explain the difference between `desc` and `$...desc`.

```
scala> val rank = denseRank.over(overCategory)
rank: org.apache.spark.sql.Column = 'dense_rank() WindowSpecDefinition UnspecifiedFrame

scala> val ranked = df.withColumn("rank", rank)

scala> ranked.show
+----------+----------+-------+----+
|   product|  category|revenue|rank|
+----------+----------+-------+----+
|      Pro2|    tablet|   6500|   1|
|      Mini|    tablet|   5500|   2|
|       Pro|    tablet|   4500|   3|
|       Big|    tablet|   2500|   4|
|    Normal|    tablet|   1500|   5|
|      Thin|cell phone|   6000|   1|
| Very thin|cell phone|   6000|   1|
|Ultra thin|cell phone|   5000|   2|
|  Bendable|cell phone|   3000|   3|
|  Foldable|cell phone|   3000|   3|
+----------+----------+-------+----+

scala> ranked.where(ranked("rank") <= 2).show
+----------+----------+-------+----+
|   product|  category|revenue|rank|
+----------+----------+-------+----+
|      Pro2|    tablet|   6500|   1|
|      Mini|    tablet|   5500|   2|
|      Thin|cell phone|   6000|   1|
| Very thin|cell phone|   6000|   1|
|Ultra thin|cell phone|   5000|   2|
+----------+----------+-------+----+
```

==== Computing the first and second best-sellers in category

NOTE: This example is the 2nd example from an _excellent_ article  https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL].

```
scala> import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.expressions.Window

scala> val overCategory = Window.partitionBy("category").orderBy($"revenue".desc)
overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5d699206

scala> val reveDiff = max(df("revenue")).over(overCategory) - df("revenue")
reveDiff: org.apache.spark.sql.Column = ('max(revenue) WindowSpecDefinition UnspecifiedFrame - revenue)

scala> df.select('*, reveDiff as "revenue_difference").show
+----------+----------+-------+------------------+
|   product|  category|revenue|revenue_difference|
+----------+----------+-------+------------------+
|      Pro2|    tablet|   6500|                 0|
|      Mini|    tablet|   5500|              1000|
|       Pro|    tablet|   4500|              2000|
|       Big|    tablet|   2500|              4000|
|    Normal|    tablet|   1500|              5000|
|      Thin|cell phone|   6000|                 0|
| Very thin|cell phone|   6000|                 0|
|Ultra thin|cell phone|   5000|              1000|
|  Bendable|cell phone|   3000|              3000|
|  Foldable|cell phone|   3000|              3000|
+----------+----------+-------+------------------+
```

==== Compute difference on column

Compute a difference between values in rows in a column.

```
scala> val pairs = (1 to 10).zip(10 to 100 by 10).flatMap(x => Seq(x,x))
pairs: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((1,10), (1,10), (2,20), (2,20), (3,30), (3,30), (4,40), (4,40), (5,50), (5,50), (6,60), (6,60), (7,70), (7,70), (8,80), (8,80), (9,90), (9,90), (10,100), (10,100))

scala> val df = sc.parallelize(pairs).toDF("ns", "tens")
df: org.apache.spark.sql.DataFrame = [ns: int, tens: int]

scala> df.show
+---+----+
| ns|tens|
+---+----+
|  1|  10|
|  1|  10|
|  2|  20|
|  2|  20|
|  3|  30|
|  3|  30|
|  4|  40|
|  4|  40|
|  5|  50|
|  5|  50|
|  6|  60|
|  6|  60|
|  7|  70|
|  7|  70|
|  8|  80|
|  8|  80|
|  9|  90|
|  9|  90|
| 10| 100|
| 10| 100|
+---+----+

scala> import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.expressions.Window

scala> val overNs = Window.partitionBy("ns").orderBy("tens")
overNs: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@35fc3588

scala> val diff = lead(df("tens"), 1).over(overNs)
diff: org.apache.spark.sql.Column = 'lead(tens,0,null) WindowSpecDefinition ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW

scala> df.withColumn("diff", diff - df("tens")).show
+---+----+----+
| ns|tens|diff|
+---+----+----+
|  1|  10|   0|
|  1|  10|null|
|  2|  20|   0|
|  2|  20|null|
|  3|  30|   0|
|  3|  30|null|
|  4|  40|   0|
|  4|  40|null|
|  5|  50|   0|
|  5|  50|null|
|  6|  60|   0|
|  6|  60|null|
|  7|  70|   0|
|  7|  70|null|
|  8|  80|   0|
|  8|  80|null|
|  9|  90|   0|
|  9|  90|null|
| 10| 100|   0|
| 10| 100|null|
+---+----+----+
```

Please note that http://stackoverflow.com/a/32379437/1305344[Why do Window functions fail with "Window function X does not take a frame specification"?]

The key here is to remember that DataFrames are RDDs under the covers and hence aggregation like grouping by a key in DataFrames is RDD's `groupBy` (or worse, `reduceByKey` or `aggregateByKey` transformations).

==== Accessing values of earlier rows

FIXME What's the value of rows before current one?

==== Calculate rank of row

==== Calculate moving average

==== Calculate cumulative sum

=== Interval data type for Date and Timestamp types

See https://issues.apache.org/jira/browse/SPARK-8943[[SPARK-8943\] CalendarIntervalType for time intervals].

With the Interval data type, you could use intervals as values specified in `<value> PRECEDING` and `<value> FOLLOWING` for `RANGE` frame. It is specifically suited for time-series analysis with window functions.

=== User-defined aggregate functions

See https://issues.apache.org/jira/browse/SPARK-3947[[SPARK-3947\] Support Scala/Java UDAF].

With the window function support, you could use user-defined aggregate functions as window functions.
