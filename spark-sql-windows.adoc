== Windowed Aggregates (Windows)

*Windowed Aggregates* (aka *Windows*) operate on a group of rows (a row set) called a *window* to apply aggregation on. They calculate a value for every input row for its window.

NOTE: Window-based framework is available since Spark *1.4.0*.

A window specification defines the *partitioning*, *ordering*, and *frame boundaries*.

Spark SQL supports three kinds of <<functions, window functions>>: *ranking* functions, *analytic* functions, and *aggregate* functions.

=== [[window-object]] Window object

`Window` object defines functions to define windows.

NOTE: It is an experimental feature since Spark *1.4.0*.

`Window` object lives in `org.apache.spark.sql.expressions` package. Import it to use `Window` functions.

[source, scala]
----
import org.apache.spark.sql.expressions.Window
----

CAUTION: FIXME What is a window? How is this different from groups like `groupBy`?

There are two families of the functions available in `Window` object that create <<WindowSpec, WindowSpec>> instance for one or many link:spark-sql-columns.adoc[Column] instances:

* <<partitionBy, partitionBy>>
* <<orderBy, orderBy>>

==== [[partitionBy]] partitionBy

[source, scala]
----
partitionBy(colName: String, colNames: String*): WindowSpec
partitionBy(cols: Column*): WindowSpec
----

`partitionBy` creates an instance of `WindowSpec` with partition expression(s) defined for one or more columns.

[source, scala]
----
// partition records into two groups
// * tokens starting with "h"
// * others
val byHTokens = Window.partitionBy('token startsWith "h")

// count the sum of ids in each group
val result = tokens.select('*, sum('id) over byHTokens as "sum over h tokens").orderBy('id)

scala> .show
+---+-----+-----------------+
| id|token|sum over h tokens|
+---+-----+-----------------+
|  0|hello|                4|
|  1|henry|                4|
|  2|  and|                2|
|  3|harry|                4|
+---+-----+-----------------+
----

==== [[orderBy]] orderBy

[source, scala]
----
orderBy(colName: String, colNames: String*): WindowSpec
orderBy(cols: Column*): WindowSpec
----

=== [[functions]] Window functions

Spark SQL supports three kinds of window functions:

* *ranking* functions
* *analytic* functions
* *aggregate* functions

.Window functions in Spark SQL (see https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL])
[align="center",width="80%",frame="topbot",options="header"]
|======================
| |SQL |DataFrame API
.5+^.^|*Ranking functions* |rank |rank
|dense_rank|denseRank
|percent_rank|percentRank
|ntile|ntile
|row_number|rowNumber
.5+^.^|*Analytic functions* |cume_dist |cumeDist
|first_value |firstValue
|last_value |lastValue
|lag |lag
|lead |lead
|======================

For aggregate functions, you can use the existing link:spark-sql-aggregation.adoc[aggregate functions] as window functions.

You can mark a function _window_ by `OVER` clause after a function in SQL, e.g. `avg(revenue) OVER (...)` or link:spark-sql-columns.adoc#over[over method] on a function in the Dataset API, e.g. `rank().over(...)`.

When executed, a window function computes a value for each row in a window.

CAUTION: FIXME How to know what functions to use to `over` the frame? They need to create a frame/window. How to know what functions create it?

=== [[WindowSpec]] WindowSpec - Window Specification

A window function needs a window specification which is an instance of `WindowSpec` class.

NOTE: `WindowSpec` class is marked as experimental since *1.4.0*.

TIP: Consult https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.WindowSpec[org.apache.spark.sql.expressions.WindowSpec] API.

A *window specification* defines which rows are included in a *window* (aka a _frame_), i.e. set of rows, that is associated with a given input row. It does so by *partitioning* an entire data set and specifying *frame boundary* with *ordering*.

NOTE: Use static methods in <<window-object, Window object>> to create a `WindowSpec`.

[source, scala]
----
import org.apache.spark.sql.expressions.Window

scala> val byHTokens = Window.partitionBy('token startsWith "h")
byHTokens: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@574985d8
----

A window specification includes three parts:

1. *Partitioning Specification* defines which rows will be in the same partition for a given row.
2. *Ordering Specification* defines how rows in a partition are ordered, determining the position of the given row in its partition.
3. *Frame Specification* (unsupported in Hive; see http://stackoverflow.com/a/32379437/1305344[Why do Window functions fail with "Window function X does not take a frame specification"?]) defines the rows to be included in the frame for the current input row, based on their relative position to the current row. For example, _“the three rows preceding the current row to the current row”_ describes a frame including the current input row and three rows appearing before the current row.

==== [[WindowSpec-examples]] WindowSpec Examples

Two samples from https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window$[org.apache.spark.sql.expressions.Window] scaladoc:

```
// PARTITION BY country ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
Window.partitionBy("country").orderBy("date").rowsBetween(Long.MinValue, 0)
```

```
// PARTITION BY country ORDER BY date ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING
Window.partitionBy("country").orderBy("date").rowsBetween(-3, 3)
```

=== Frames

At its core, a window function calculates a return value for every input row of a table based on a group of rows, called the *frame*. Every input row can have a unique frame associated with it.

When you define a frame you have to specify three components of a frame specification - the *start and end boundaries*, and the *type*.

Types of boundaries (two positions and three offsets):

* `UNBOUNDED PRECEDING` - the first row of the partition
* `UNBOUNDED FOLLOWING` - the last row of the partition
* `CURRENT ROW`
* `<value> PRECEDING`
* `<value> FOLLOWING`

Offsets specify the offset from the current input row.

Types of frames:

* `ROW` - based on _physical offsets_ from the position of the current input row
* `RANGE` - based on _logical offsets_ from the position of the current input row

=== [[examples]] Examples

==== [[example-top-n]] Top N per Group

Top N per Group is useful when you need to compute the first and second best-sellers in category.

NOTE: This example is borrowed from an _excellent_ article  https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL].

.Table PRODUCT_REVENUE
[align="center",width="80%",frame="topbot",options="header,footer"]
|======================
|product |category |revenue
|      Thin|cell phone|   6000
|    Normal|    tablet|   1500
|      Mini|    tablet|   5500
|Ultra thin|cell phone|   5000
| Very thin|cell phone|   6000
|       Big|    tablet|   2500
|  Bendable|cell phone|   3000
|  Foldable|cell phone|   3000
|       Pro|    tablet|   4500
|      Pro2|    tablet|   6500
|======================

Question: What are the best-selling and the second best-selling products in every category?

```
val columns = Seq("product", "category", "revenue")
val data = Seq(
  ("Thin",       "cell phone", 6000),
  ("Normal",     "tablet",     1500),
  ("Mini",       "tablet",     5500),
  ("Ultra thin", "cell phone", 5000),
  ("Very thin",  "cell phone", 6000),
  ("Big",        "tablet",     2500),
  ("Bendable",   "cell phone", 3000),
  ("Foldable",   "cell phone", 3000),
  ("Pro",        "tablet",     4500),
  ("Pro2",       "tablet",     6500))
  .toDF(columns: _*)

scala> data.show
+----------+----------+-------+
|   product|  category|revenue|
+----------+----------+-------+
|      Thin|cell phone|   6000|
|    Normal|    tablet|   1500|
|      Mini|    tablet|   5500|
|Ultra thin|cell phone|   5000|
| Very thin|cell phone|   6000|
|       Big|    tablet|   2500|
|  Bendable|cell phone|   3000|
|  Foldable|cell phone|   3000|
|       Pro|    tablet|   4500|
|      Pro2|    tablet|   6500|
+----------+----------+-------+

scala> data.where($"category" === "tablet").show
+-------+--------+-------+
|product|category|revenue|
+-------+--------+-------+
| Normal|  tablet|   1500|
|   Mini|  tablet|   5500|
|    Big|  tablet|   2500|
|    Pro|  tablet|   4500|
|   Pro2|  tablet|   6500|
+-------+--------+-------+
```

The question boils down to ranking products in a category based on their revenue, and to pick the best selling and the second best-selling products based the ranking.

```
import org.apache.spark.sql.expressions.Window
val overCategory = Window.partitionBy("category").orderBy($"revenue".desc)
val rank = dense_rank.over(overCategory)

val ranked = data.withColumn("rank", dense_rank.over(overCategory))

scala> ranked.show
+----------+----------+-------+----+
|   product|  category|revenue|rank|
+----------+----------+-------+----+
|      Pro2|    tablet|   6500|   1|
|      Mini|    tablet|   5500|   2|
|       Pro|    tablet|   4500|   3|
|       Big|    tablet|   2500|   4|
|    Normal|    tablet|   1500|   5|
|      Thin|cell phone|   6000|   1|
| Very thin|cell phone|   6000|   1|
|Ultra thin|cell phone|   5000|   2|
|  Bendable|cell phone|   3000|   3|
|  Foldable|cell phone|   3000|   3|
+----------+----------+-------+----+

scala> ranked.where($"rank" <= 2).show
+----------+----------+-------+----+
|   product|  category|revenue|rank|
+----------+----------+-------+----+
|      Pro2|    tablet|   6500|   1|
|      Mini|    tablet|   5500|   2|
|      Thin|cell phone|   6000|   1|
| Very thin|cell phone|   6000|   1|
|Ultra thin|cell phone|   5000|   2|
+----------+----------+-------+----+
```

==== Revenue Difference per Category

NOTE: This example is the 2nd example from an _excellent_ article  https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html[Introducing Window Functions in Spark SQL].

```
import org.apache.spark.sql.expressions.Window
val reveDesc = Window.partitionBy('category).orderBy('revenue.desc)
val reveDiff = max('revenue).over(reveDesc) - $"revenue"

scala> data.select('*, reveDiff as "revenue_diff").show
+----------+----------+-------+------------+
|   product|  category|revenue|revenue_diff|
+----------+----------+-------+------------+
|      Pro2|    tablet|   6500|           0|
|      Mini|    tablet|   5500|        1000|
|       Pro|    tablet|   4500|        2000|
|       Big|    tablet|   2500|        4000|
|    Normal|    tablet|   1500|        5000|
|      Thin|cell phone|   6000|           0|
| Very thin|cell phone|   6000|           0|
|Ultra thin|cell phone|   5000|        1000|
|  Bendable|cell phone|   3000|        3000|
|  Foldable|cell phone|   3000|        3000|
+----------+----------+-------+------------+
```

==== Difference on Column

Compute a difference between values in rows in a column.

```
val pairs = for {
  x <- 1 to 5
  y <- 1 to 2
} yield (x, 10 * x * y)
val ds = pairs.toDF("ns", "tens")

scala> ds.show
+---+----+
| ns|tens|
+---+----+
|  1|  10|
|  1|  20|
|  2|  20|
|  2|  40|
|  3|  30|
|  3|  60|
|  4|  40|
|  4|  80|
|  5|  50|
|  5| 100|
+---+----+

import org.apache.spark.sql.expressions.Window
val overNs = Window.partitionBy("ns").orderBy("tens")
val diff = lead('tens, 1).over(overNs)

scala> ds.withColumn("diff", diff - 'tens).show
+---+----+----+
| ns|tens|diff|
+---+----+----+
|  1|  10|  10|
|  1|  20|null|
|  3|  30|  30|
|  3|  60|null|
|  5|  50|  50|
|  5| 100|null|
|  4|  40|  40|
|  4|  80|null|
|  2|  20|  20|
|  2|  40|null|
+---+----+----+
```

Please note that http://stackoverflow.com/a/32379437/1305344[Why do Window functions fail with "Window function X does not take a frame specification"?]

The key here is to remember that DataFrames are RDDs under the covers and hence aggregation like grouping by a key in DataFrames is RDD's `groupBy` (or worse, `reduceByKey` or `aggregateByKey` transformations).

==== Accessing values of earlier rows

FIXME What's the value of rows before current one?

==== Calculate rank of row

==== [[example-moving-average]] Moving Average

==== [[example-cumulative-aggregates]] Cumulative Aggregates

Eg. cumulative sum

==== [[example-running-totals]] Running Totals

=== Interval data type for Date and Timestamp types

See https://issues.apache.org/jira/browse/SPARK-8943[[SPARK-8943\] CalendarIntervalType for time intervals].

With the Interval data type, you could use intervals as values specified in `<value> PRECEDING` and `<value> FOLLOWING` for `RANGE` frame. It is specifically suited for time-series analysis with window functions.

=== User-defined aggregate functions

See https://issues.apache.org/jira/browse/SPARK-3947[[SPARK-3947\] Support Scala/Java UDAF].

With the window function support, you could use user-defined aggregate functions as window functions.
