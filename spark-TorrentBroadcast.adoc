== [[TorrentBroadcast]] `TorrentBroadcast` -- Default `Broadcast` Implementation

`TorrentBroadcast` is the default (and only) implementation of the link:spark-broadcast.adoc#contract[`Broadcast` Contract] that describes link:spark-broadcast.adoc[broadcast variables]. `TorrentBroadcast` uses a BitTorrent-like protocol for block distribution and only when tasks access broadcast variables (on executors).

.TorrentBroadcast - broadcasting using BitTorrent
image::images/sparkcontext-broadcast-bittorrent.png[align="center"]

When a link:spark-sparkcontext.adoc#broadcast[broadcast variable is created (using `SparkContext.broadcast`)] on the driver, a <<creating-instance, new instance of `TorrentBroadcast` is created>>.

[source, scala]
----
// On the driver
val sc: SparkContext = ???
val anyScalaValue = ???
val b = sc.broadcast(anyScalaValue)
----

A broadcast variable is stored on the driver's link:spark-blockmanager.adoc[BlockManager] as a single value and broadcast blocks (after it was <<blockifyObject, divided into broadcast blocks, i.e. blockified>>). The broadcast block size is the valu of link:spark-service-broadcastmanager.adoc#spark_broadcast_blockSize[`spark.broadcast.blockSize` Spark property].

.TorrentBroadcast puts broadcast chunks to driver's BlockManager
image::images/sparkcontext-broadcast-bittorrent-newBroadcast.png[align="center"]

NOTE: `TorrentBroadcast`-based broadcast variables are created using link:spark-TorrentBroadcastFactory.adoc[TorrentBroadcastFactory].

NOTE: `TorrentBroadcast` belongs to `org.apache.spark.broadcast` package.

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.broadcast.TorrentBroadcast` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.broadcast.TorrentBroadcast=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[unpersist]] `unpersist` Method

CAUTION: FIXME

=== [[unBlockifyObject]] `unBlockifyObject` Method

CAUTION: FIXME

=== [[readBlocks]] `readBlocks` Method

CAUTION: FIXME

=== [[releaseLock]] `releaseLock` Method

CAUTION: FIXME

=== [[setConf]] `setConf` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `TorrentBroadcast` Instance

[source, scala]
----
TorrentBroadcast[T](obj: T, id: Long)
extends Broadcast[T](id)
----

When created, `TorrentBroadcast` <<readBroadcastBlock, reads broadcast blocks>> (to the internal `_value`).

NOTE: The internal `_value` is transient so it is not serialized and sent over the wire to executors. It is later recreated lazily on executors when requested.

`TorrentBroadcast` then sets the internal optional link:spark-CompressionCodec.adoc#createCodec[CompressionCodec] and the size of broadcast block (as controlled by  link:spark-service-broadcastmanager.adoc#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property in link:spark-configuration.adoc[SparkConf] per driver and executors).

NOTE: Compression is controlled by link:spark-service-broadcastmanager.adoc#spark_broadcast_compress[spark.broadcast.compress] Spark property and is enabled by default.

The internal `broadcastId` is link:spark-blockdatamanager.adoc#BroadcastBlockId[BroadcastBlockId] for the input `id`.

The internal `numBlocks` is set to <<writeBlocks, the number of the pieces the broadcast was divided into>>.

NOTE: A broadcast's blocks are first stored in the local link:spark-blockmanager.adoc[BlockManager] on the driver.

=== [[readBroadcastBlock]] `readBroadcastBlock` Internal Method

[source, scala]
----
readBroadcastBlock(): T
----

Internally, `readBroadcastBlock` <<setConf, sets the `SparkConf`>>

NOTE: The current link:spark-configuration.adoc[SparkConf] is available using link:spark-sparkenv.adoc#conf[SparkEnv.get.conf].

`readBroadcastBlock` requests the link:spark-blockmanager.adoc#getLocalValues[local `BlockManager` for values of the broadcast].

NOTE: The current link:spark-blockmanager.adoc[BlockManager] is available using link:spark-sparkenv.adoc#blockManager[SparkEnv.get.blockManager].

If the broadcast was available locally, `readBroadcastBlock` <<releaseLock, releases a lock>> for the broadcast and returns it.

If however the broadcast was not found locally, you should see the following INFO message in the logs:

```
INFO Started reading broadcast variable [id]
```

`readBroadcastBlock` <<readBlocks, reads blocks (as chunks)>> of the broadcast.

You should see the following INFO message in the logs:

```
INFO Reading broadcast variable [id] took [usedTimeMs]
```

`readBroadcastBlock` <<unBlockifyObject, _unblockifies_ the collection of `ByteBuffer` blocks>>

NOTE: `readBroadcastBlock` uses the link:spark-sparkenv.adoc#serializer[current `Serializer`] and the internal link:spark-CompressionCodec.adoc[CompressionCodec] to bring all the blocks together as one single broadcast variable.

`readBroadcastBlock` link:spark-blockmanager.adoc#putSingle[stores the broadcast variable with `MEMORY_AND_DISK` storage level to the local `BlockManager`]. When storing the broadcast variable was unsuccessful, a `SparkException` is thrown.

```
Failed to store [broadcastId] in BlockManager
```

The broadcast variable is returned.

NOTE: `readBroadcastBlock` is exclusively used to <<creating-instance, recreate a broadcast variable on executors>>.

=== [[writeBlocks]] Storing Broadcast and Its Blocks in Local BlockManager -- `writeBlocks` Internal Method

[source, scala]
----
writeBlocks(value: T): Int
----

`writeBlocks` is an internal method to store the broadcast's `value` and blocks in the driver's link:spark-blockmanager.adoc[BlockManager]. It returns the number of the broadcast blocks the broadcast was divided into.

NOTE: `writeBlocks` is exclusively used when a <<creating-instance, `TorrentBroadcast` is created>> that happens on the driver only.

Internally, `writeBlocks` link:spark-blockmanager.adoc#putSingle[stores the block for `value` broadcast to the local `BlockManager`] (using a new link:spark-blockdatamanager.adoc#BroadcastBlockId[BroadcastBlockId], `value`, `MEMORY_AND_DISK` storage level and without telling the driver).

If storing the broadcast block fails, you should see the following `SparkException` in the logs:

```
Failed to store [broadcastId] in BlockManager
```

`writeBlocks` divides `value` into blocks (of link:spark-service-broadcastmanager.adoc#spark_broadcast_blockSize[spark.broadcast.blockSize] size) using the link:spark-sparkenv.adoc#serializer[Serializer] and an optional link:spark-CompressionCodec.adoc[CompressionCodec] (enabled by link:spark-service-broadcastmanager.adoc#spark_broadcast_compress[spark.broadcast.compress]). Every block gets its own `BroadcastBlockId` (with `piece` and an index) and wrapped inside a `ChunkedByteBuffer`. link:spark-blockmanager.adoc#putBytes[Blocks are stored in the local `BlockManager`] (using the `piece` block id, `MEMORY_AND_DISK_SER` storage level and informing the driver).

NOTE: The entire broadcast value is stored in the local `BlockManager` with `MEMORY_AND_DISK` storage level, and the pieces with `MEMORY_AND_DISK_SER` storage level.

If storing any of the broadcast pieces fails, you should see the following `SparkException` in the logs:

```
Failed to store [pieceId] of [broadcastId] in local BlockManager
```

=== [[blockifyObject]] Chunking Broadcast Into Blocks -- `blockifyObject` Method

[source, scala]
----
blockifyObject[T](
  obj: T,
  blockSize: Int,
  serializer: Serializer,
  compressionCodec: Option[CompressionCodec]): Array[ByteBuffer]
----

`blockifyObject` divides (aka _blockifies_) the input `obj` broadcast variable into blocks (of `ByteBuffer`). `blockifyObject` uses the input `serializer` `Serializer` to write `obj` in a serialized format to a `ChunkedByteBufferOutputStream` (of `blockSize` size) with the optional link:spark-CompressionCodec.adoc[CompressionCodec].

NOTE: `blockifyObject` is executed when <<writeBlocks, `TorrentBroadcast` stores a broadcast and its blocks to a local `BlockManager`>>.
