== [[TorrentBroadcast]] `TorrentBroadcast` -- Default `Broadcast` Implementation

`org.apache.spark.broadcast.TorrentBroadcast` is the default implementation of the link:spark-broadcast.adoc#contract[`Broadcast` Contract]. It uses a BitTorrent-like protocol for distribution.

.TorrentBroadcast - broadcasting using BitTorrent
image::images/sparkcontext-broadcast-bittorrent.png[align="center"]

NOTE: `TorrentBroadcast`-based broadcast variables are created using link:spark-TorrentBroadcastFactory.adoc[TorrentBroadcastFactory].

When a link:spark-sparkcontext.adoc#broadcast[new broadcast value is created (using `SparkContext.broadcast` method)], a <<creating-instance, new instance of `TorrentBroadcast` is created>> that is then divided into blocks which are stored in the local link:spark-blockmanager.adoc[BlockManager] (on the driver).

.TorrentBroadcast puts broadcast chunks to driver's BlockManager
image::images/sparkcontext-broadcast-bittorrent-newBroadcast.png[align="center"]

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.broadcast.TorrentBroadcast` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.broadcast.TorrentBroadcast=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[unpersist]] `unpersist` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `TorrentBroadcast` Instance

[source, scala]
----
TorrentBroadcast[T](obj: T, id: Long)
extends Broadcast[T](id)
----

When created, `TorrentBroadcast` <<readBroadcastBlock, computes the internal `_value`>>.

NOTE: The internal `_value` is transient so it is not serialized and sent over the wire to executors, but rather recreated on demand (and that is why it is also `lazy`).

`TorrentBroadcast` then sets the internal optional link:spark-CompressionCodec.adoc#createCodec[CompressionCodec] and the size of broadcast block (as controlled by  link:spark-service-broadcastmanager.adoc#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property in link:spark-configuration.adoc[SparkConf] per driver and executors).

NOTE: Compression is controlled by link:spark-service-broadcastmanager.adoc#spark_broadcast_compress[spark.broadcast.compress] Spark property and is enabled by default.

The internal `broadcastId` is link:spark-blockdatamanager.adoc#BroadcastBlockId[BroadcastBlockId] for the input `id`.

The internal `numBlocks` is set to <<writeBlocks, the number of the pieces the broadcast was divided into>>.

NOTE: A broadcast's blocks are first stored in the local link:spark-blockmanager.adoc[BlockManager] on the driver.

=== [[unBlockifyObject]] `unBlockifyObject` Method

CAUTION: FIXME

=== [[readBlocks]] `readBlocks` Method

CAUTION: FIXME

=== [[releaseLock]] `releaseLock` Method

CAUTION: FIXME

=== [[setConf]] `setConf` Method

CAUTION: FIXME

=== [[readBroadcastBlock]] `readBroadcastBlock` Internal Method

[source, scala]
----
readBroadcastBlock(): T
----

Internally, `readBroadcastBlock` <<setConf, sets the `SparkConf`>>

NOTE: The current link:spark-configuration.adoc[SparkConf] is available using link:spark-sparkenv.adoc#conf[SparkEnv.get.conf].

`readBroadcastBlock` requests the link:spark-blockmanager.adoc#getLocalValues[local `BlockManager` for values of the broadcast].

NOTE: The current link:spark-blockmanager.adoc[BlockManager] is available using link:spark-sparkenv.adoc#blockManager[SparkEnv.get.blockManager].

If the broadcast was available locally, `readBroadcastBlock` <<releaseLock, releases a lock>> for the broadcast and returns it.

If however the broadcast was not found locally, you should see the following INFO message in the logs:

```
INFO Started reading broadcast variable [id]
```

`readBroadcastBlock` <<readBlocks, reads blocks (as chunks)>> of the broadcast.

You should see the following INFO message in the logs:

```
INFO Reading broadcast variable [id] took [usedTimeMs]
```

`readBroadcastBlock` <<unBlockifyObject, _unblockifies_ the collection of `ByteBuffer` blocks>>

NOTE: `readBroadcastBlock` uses the link:spark-sparkenv.adoc#serializer[current `Serializer`] and the internal link:spark-CompressionCodec.adoc[CompressionCodec] to bring all the blocks together as one single broadcast variable.

`readBroadcastBlock` link:spark-blockmanager.adoc#putSingle[stores the broadcast variable with `MEMORY_AND_DISK` storage level to the local `BlockManager`]. When storing the broadcast variable was unsuccessful, a `SparkException` is thrown.

```
Failed to store [broadcastId] in BlockManager
```

The broadcast variable is returned.

NOTE: `readBroadcastBlock` is exclusively used to <<creating-instance, recreate a broadcast variable on executors>>.

=== [[writeBlocks]] Storing Broadcast and Broadcast's Pieces in Local BlockManager -- `writeBlocks` Internal Method

[source, scala]
----
writeBlocks(value: T): Int
----

`writeBlocks` is an internal method to store the broadcast's value and pieces to the local link:spark-blockmanager.adoc[BlockManager]. It returns the number of the broadcast's block pieces the broadcast was divided into.

Internally, `writeBlocks` link:spark-blockmanager.adoc#putSingle[stores the block for `value` broadcast to the local `BlockManager`] (using a new link:spark-blockdatamanager.adoc#BroadcastBlockId[BroadcastBlockId], `value`, `MEMORY_AND_DISK` storage level and without telling the driver).

If storing the broadcast block fails, you should see the following `SparkException` in the logs:

```
Failed to store [broadcastId] in BlockManager
```

`writeBlocks` divides `value` into blocks (of link:spark-service-broadcastmanager.adoc#spark_broadcast_blockSize[spark.broadcast.blockSize] size) using the link:spark-sparkenv.adoc#serializer[Serializer] and an optional link:spark-CompressionCodec.adoc[CompressionCodec] (enabled by link:spark-service-broadcastmanager.adoc#spark_broadcast_compress[spark.broadcast.compress]). Every block gets its own `BroadcastBlockId` (with `piece` and an index) and wrapped inside a `ChunkedByteBuffer`. link:spark-blockmanager.adoc#putBytes[Blocks are stored in the local `BlockManager`] (using the `piece` block id, `MEMORY_AND_DISK_SER` storage level and informing the driver).

NOTE: The entire broadcast value is stored in the local `BlockManager` with `MEMORY_AND_DISK` storage level, and the pieces with `MEMORY_AND_DISK_SER` storage level.

If storing any of the broadcast pieces fails, you should see the following `SparkException` in the logs:

```
Failed to store [pieceId] of [broadcastId] in local BlockManager
```

NOTE: `writeBlocks` is used when a <<creating-instance, `TorrentBroadcast` is created>>.

=== [[blockifyObject]] `blockifyObject` Method

CAUTION: FIXME
