== Dependencies

* `def getDependencies: Seq[Dependency[_]]` returns how this RDD depends on parent RDDs.
+
```
scala> lines.dependencies
res3: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@56d5a50f)
```

=== [[shuffle-dependency]] ShuffleDependency

A *ShuffleDependency* represents a dependency on the output of link:spark-scheduler.adoc#ShuffleMapStage[a shuffle map stage] in a job that is submitted to link:spark-scheduler.adoc[DAGScheduler] for execution.

A ShuffleDependency belongs to a single pair RDD (available as `rdd` of type `RDD[Product2[K, V]]`).

A ShuffleDependency has a *shuffleId* (FIXME from `SparkContext.newShuffleId`).

It uses link:spark-rdd-partitions.adoc#partitioner[partitioner] to partition the shuffle output. It also uses link:spark-shuffle-service.adoc[ShuffleManager] to register itself and link:spark-service-contextcleaner.adoc[ContextCleaner] to register itself for cleanup.

Every ShuffleDependency is registered to link:spark-service-mapoutputtracker.adoc[MapOutputTracker] by the shuffle's id and the number of the partitions of a RDD (using `mapOutputTracker.registerShuffle`).

The places where ShuffleDependency is used:

* `CoGroupedRDD` and `SubtractedRDD` when partitioner differs among RDDs
* link:spark-rdd-shuffledrdd.adoc[ShuffledRDD] and `ShuffledRowRDD` that are RDDs from a shuffle

The RDD operations that may or may not use the above RDDs and hence shuffling:

* link:spark-rdd-partitions.adoc#coalesce[coalesce]
** `repartition`
* `cogroup`
** `intersection`
* `subtractByKey`
** `subtract`
* `sortByKey`
** `sortBy`
* `repartitionAndSortWithinPartitions`
* `combineByKeyWithClassTag`
** `combineByKey`
** `aggregateByKey`
** `foldByKey`
** `reduceByKey`
** `countApproxDistinctByKey`
** `groupByKey`
* `partitionBy`

NOTE: There may be other dependent methods that use the above.

=== [[narow-dependency]] NarrowDependency

FIXME
