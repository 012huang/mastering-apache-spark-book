== ReceiverTracker

=== [[introduction]] Introduction

`ReceiverTracker` manages execution of all the link:spark-streaming-receivers.adoc[Receivers] of link:spark-streaming-receiverinputdstreams.adoc[ReceiverInputDStreams]. It uses link:spark-rpc.adoc[RPC environment] for communication with link:spark-streaming-receiversupervisors.adoc[ReceiverSupervisors].

NOTE: It is started when link:spark-streaming-jobscheduler.adoc[JobScheduler] starts.

It can only be started once and sets up the `ReceiverTracker` RPC endpoint (using <<ReceiverTrackerEndpoint, ReceiverTrackerEndpoint>>).

When `ReceiverTracker` starts, you should see the following INFO message in the logs:

```
INFO ReceiverTracker: Starting [receivers.length] receivers
```

`ReceiverTracker` can be in one of the following states:

* `Initialized` - it is in the state after having been instantiated.
* `Started` - <<ReceiverTrackerEndpoint, ReceiverTrackerEndpoint>> has been set up.
* `Stopping`
* `Stopped`

It posts <<ReceiverTrackerEndpoint-StartAllReceivers, StartAllReceivers>> to <<ReceiverTrackerEndpoint, ReceiverTracker RPC endpoint>>.

A successful startup of `ReceiverTracker` finishes with the following INFO message in the logs:

```
INFO ReceiverTracker: ReceiverTracker started
```

=== [[ReceiverTrackerEndpoint]] ReceiverTrackerEndpoint

CAUTION: FIXME

==== [[ReceiverTrackerEndpoint-StopAllReceivers]] StopAllReceivers

CAUTION: FIXME

==== [[ReceiverTrackerEndpoint-AllReceiverIds]] AllReceiverIds

CAUTION: FIXME

==== [[ReceiverTrackerEndpoint-StartAllReceivers]] StartAllReceivers

`StartAllReceivers(receivers)` is a local message sent by <<ReceiverTracker, ReceiverTracker>> when it starts (using `ReceiverTracker.launchReceivers()`).

It schedules receivers (using `ReceiverSchedulingPolicy.scheduleReceivers(receivers, getExecutors)`).

CAUTION: FIXME What does `ReceiverSchedulingPolicy.scheduleReceivers(receivers, getExecutors)` do?

It does _some_ bookkeeping.

CAUTION: FIXME What is _the_ bookkeeping?

It finally starts every receiver (using the helper method <<ReceiverTrackerEndpoint-startReceiver, ReceiverTrackerEndpoint.startReceiver>>).

===== [[ReceiverTrackerEndpoint-startReceiver]] ReceiverTrackerEndpoint.startReceiver

CAUTION: FIXME When is the method called?

`ReceiverTrackerEndpoint.startReceiver(receiver: Receiver[_], scheduledLocations: Seq[TaskLocation])` starts a `receiver` link:spark-streaming.adoc#Receiver[Receiver] at the given `Seq[TaskLocation]` locations.

CAUTION: FIXME When the scaladoc says https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala#L543[_"along with the scheduled executors"_], does it mean that the executors are already started and waiting for the receiver?!

It defines an internal function (`startReceiverFunc`) to start `receiver` on a worker (in Spark cluster).

Namely, the internal `startReceiverFunc` function checks that the task attempt is `0`.

CAUTION: FIXME When could `TaskContext.get().attemptNumber()` be different than `0`?

It then starts a link:spark-streaming-receiversupervisors.adoc[ReceiverSupervisor] for `receiver` and keeps awaiting termination, i.e. once the task is run it does so until _a termination message_ comes from _some_ other external source). The task is a long-running task for `receiver`.

CAUTION: FIXME When does `supervisor.awaitTermination()` finish?

Having the internal function, it creates `receiverRDD` - an instance of `RDD[Receiver[_]]` - that uses link:spark-sparkcontext.adoc#makeRDD[SparkContext.makeRDD] with a one-element collection with the only element being `receiver`. When the collection of `TaskLocation` is empty, it uses exactly one partition. Otherwise, it distributes the one-element collection across the nodes (and potentially even executors) for `receiver`. The RDD has the name `Receiver [receiverId]`.

The Spark job's description is set to `Streaming job running receiver [receiverId]`.

CAUTION: FIXME What does `sparkContext.setJobDescription` actually do and how does this influence Spark jobs? It uses `ThreadLocal` so it assumes that a single thread will do a job?

Having done so, it submits a job (using link:spark-sparkcontext.adoc#submitJob[SparkContext.submitJob]) on the instance of `RDD[Receiver[_]]` with the function `startReceiverFunc` that runs `receiver`. It has link:spark-rdd-operations.adoc#FutureAction[SimpleFutureAction] to monitor `receiver`.

[NOTE]
====
The method demonstrates how you could use Spark Core as the distributed computation platform to launch _any_ process on clusters and let Spark handle the distribution.

_Very clever indeed!_
====

When it completes (successfully or not), `onReceiverJobFinish(receiverId)` is called, but only for cases when the tracker is fully up and running, i.e. started. When the tracker is being stopped or has already stopped, the following INFO message appears in the logs:

```
INFO Restarting Receiver [receiverId]
```

And a `RestartReceiver(receiver)` message is sent.

When there was a failure submitting the job, you should also see the ERROR message in the logs:

```
ERROR Receiver has been stopped. Try to restart it.
```

Ultimately, right before the method exits, the following INFO message appears in the logs:

```
INFO Receiver [receiver.streamId] started
```

=== [[stopping]] Stopping ReceiverTracker (using stop method)

`ReceiverTracker.stop(graceful: Boolean)` stops `ReceiverTracker` only when it is in `Started` state. Otherwise, it does nothing and exits.

NOTE: The `stop` method is called while link:spark-streaming-jobscheduler.adoc#stopping[JobScheduler is being stopped].

The state of `ReceiverTracker` is marked `Stopping`.

It then sends the stop signal to all the receivers (using posts <<ReceiverTrackerEndpoint-StopAllReceivers, StopAllReceivers>> to <<ReceiverTrackerEndpoint, ReceiverTracker RPC endpoint>>) and waits *10 seconds* for all the receivers to quit gracefully unless `graceful` flag is set.

NOTE: The 10-second wait time for graceful quit is not configurable.

You should see the following INFO messages if the `graceful` flag is enabled which means that the receivers quit in a graceful manner:

```
INFO ReceiverTracker: Waiting for receiver job to terminate gracefully
INFO ReceiverTracker: Waited for receiver job to terminate gracefully
```

It then checks whether all the receivers have been deregistered or not by posting <<ReceiverTrackerEndpoint-AllReceiverIds, AllReceiverIds>> to <<ReceiverTrackerEndpoint, ReceiverTracker RPC endpoint>>.

You should see the following INFO message in the logs if they have:

```
INFO ReceiverTracker: All of the receivers have deregistered successfully
```

Otherwise, when there were receivers not having been deregistered properly, the following WARN message appears in the logs:

```
WARN ReceiverTracker: Not all of the receivers have deregistered, [receivers]
```

It stops <<ReceiverTrackerEndpoint, ReceiverTracker RPC endpoint>> as well as <<ReceivedBlockTracker, ReceivedBlockTracker>>.

You should see the following INFO message in the logs:

```
INFO ReceiverTracker: ReceiverTracker stopped
```

The state of `ReceiverTracker` is marked `Stopped`.

=== [[ReceivedBlockTracker]] ReceivedBlockTracker

CAUTION: FIXME
