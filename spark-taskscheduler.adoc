== Task Schedulers

A *Task Scheduler* schedules link:spark-taskscheduler-tasks.adoc[tasks] for a link:spark-anatomy-spark-application.adoc[single Spark application] according to <<scheduling-mode, scheduling mode>> (aka *order task policy*).

.TaskScheduler works for a single SparkContext
image::images/sparkstandalone-sparkcontext-taskscheduler-schedulerbackend.png[align="center"]

A TaskScheduler gets sets of tasks (as link:spark-taskscheduler-tasksets.adoc[TaskSets]) submitted to it from the link:spark-dagscheduler.adoc[DAGScheduler] for each stage, and is responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers.

=== [[contract]] TaskScheduler Contract

NOTE: `org.apache.spark.scheduler.TaskScheduler` is a `private[spark]` Scala trait in Spark.

Every task scheduler has to offer the following services:

* Return `Pool` (using `rootPool`)
* Return `SchedulingMode` (using `schedulingMode`)
* Can be started (using `start()`) and stopped (using `stop()`)
* (optionally) `postStartHook()` if needed for additional post-start initialization. It does nothing by default. It is called at the very end of link:spark-sparkcontext.adoc#initialization[SparkContext's initialization].
* `submitTasks(taskSet: TaskSet)`
* `cancelTasks(stageId: Int, interruptThread: Boolean)` to cancel all tasks in a stage.
* `setDAGScheduler(dagScheduler: DAGScheduler)` to set custom link:spark-dagscheduler.adoc[DAGScheduler].
* Return the default level of parallelism (using `defaultParallelism()`)
* `executorHeartbeatReceived`
* (optionally) Return an application id for the current job (using `applicationId()`). It returns `spark-application-[System.currentTimeMillis]` by default.
* Handle executor lost events (using `executorLost(executorId: String, reason: ExecutorLossReason)`)
* Return an application attempt ID associated with the job (using `applicationAttemptId`)

CAUTION: FIXME Have an exercise to create a SchedulerBackend.

=== Available Implementations

Spark comes with the following task schedulers:

* link:spark-taskschedulerimpl.adoc[TaskSchedulerImpl]
* https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnScheduler.scala[YarnScheduler] for link:spark-yarn.adoc[Spark on YARN]
* https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClusterScheduler.scala[YarnClusterScheduler] for link:spark-yarn.adoc[Spark on YARN]

A TaskScheduler emits events to the DAGScheduler.

.TaskScheduler uses SchedulerBackend for different clusters
image::diagrams/taskscheduler-uses-schedulerbackend.png[align="center"]

DAGScheduler uses <<submitTasks, submitTasks() operation >> to submit a TaskSet to Task Scheduler and can cancel tasks in a stage.

=== TaskContextImpl

CAUTION: FIXME

* stage
* partition
* task attempt
* attempt number
* runningLocally = false

=== TaskMemoryManager

CAUTION: FIXME

=== TaskMetrics

CAUTION: FIXME

=== [[TaskResultGetter]] TaskResultGetter

FIXME

=== [[scheduling-mode]] Scheduling Modes

Task Scheduler uses a scheduling mode that determines policy to order tasks across a Schedulable's sub-queues.

It is configured by <<settings, spark.scheduler.mode>> setting that can accept the following values:

* *FIFO* - no pools; one root pool with link:spark-tasksetmanager.adoc[TaskSetManager]; lower priority gets Schedulable sooner or earlier stage wins.
* *FAIR* - more advanced FIXME
* *NONE* means no sub-queues

=== TaskSet's priority field and FIFO scheduling

A TaskSet has `priority` field that turns into the *priority* field's value of link:spark-tasksetmanager.adoc[TaskSetManager] (which is a Schedulable).

The `priority` field is used in `FIFOSchedulingAlgorithm` in which equal priorities give stages an advantage (not to say _priority_).

`FIFOSchedulingAlgorithm` is only used for `FIFO` scheduling mode in a `Pool` which is a Schedulable collection of `Schedulable`'s.

Effectively, the `priority` field is the job's id of the first job this stage was part of (for FIFO scheduling).

=== [[speculative-execution]] Speculative execution of tasks

*Speculative tasks* (also *speculatable tasks* or *task strugglers*) are tasks that run slower than most of the all tasks in a job.

*Speculative execution of tasks* is a health-check procedure that checks for tasks to be *speculated*, i.e. running slower in a stage than the median of all successfully completed tasks in a taskset. Such slow tasks will be re-launched in another worker. It will not stop the slow tasks, but run a new copy in parallel.

It is executed periodically by the TaskScheduler for link:spark-cluster.adoc[clustered deployment modes], when link:spark-tasksetmanager.adoc#tasksetmanager-settings[spark.speculation] is enabled (`true`).

With `spark.speculation` enabled, the following INFO message appears in the logs:

```
INFO Starting speculative execution thread
```

It is scheduled using *task-scheduler-speculation* daemon thread pool using `j.u.c.ScheduledThreadPoolExecutor` with core pool size `1`.

It is executed for link:spark-tasksetmanager.adoc#zombie-state[non-zombie TaskSetManagers] with more than one task to execute.

The process computes link:spark-tasksetmanager.adoc#tasksetmanager-settings[spark.speculation.quantile] of all the tasks and checks whether the number is greater than the number of tasks completed successfully.

You can find the DEBUG message in the logs:

```
DEBUG Checking for speculative tasks: minFinished =
```

It then computes the median duration of all the completed task length threshold for speculation to have it multiplied by link:spark-tasksetmanager.adoc#tasksetmanager-settings[spark.speculation.multiplier]. It has to be at least `100`.

In the logs at DEBUG level:

```
DEBUG Task length threshold for speculation:
```

For each active task for which there is only one copy running and the task takes more time than the threshold, it gets marked as *speculatable*.

In the logs at INFO level:

```
INFO Marking task %d in stage %s (on %s) as speculatable because it ran more than %.0f ms
```

The job with speculatable tasks should finish while speculative tasks are running, and it will leave these tasks running - no KILL command yet.

The check procedure is in link:spark-tasksetmanager.adoc[TaskSetManager.checkSpeculatableTasks] method.

1. How does Spark handle repeated results of speculative tasks since there are copies launched?
