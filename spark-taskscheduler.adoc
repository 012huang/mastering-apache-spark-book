== Task Schedulers

A *Task Scheduler* schedules link:spark-taskscheduler-tasks.adoc[tasks] for a link:spark-anatomy-spark-application.adoc[single Spark application] according to <<scheduling-mode, scheduling mode>> (aka *order task policy*).

.TaskScheduler works for a single SparkContext
image::images/sparkstandalone-sparkcontext-taskscheduler-schedulerbackend.png[align="center"]

A TaskScheduler gets sets of tasks (as link:spark-taskscheduler-tasksets.adoc[TaskSets]) submitted to it from the link:spark-dagscheduler.adoc[DAGScheduler] for each stage, and is responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers.

=== [[contract]] TaskScheduler Contract

NOTE: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala[org.apache.spark.scheduler.TaskScheduler] is a `private[spark]` Scala trait.

Every `TaskScheduler` has to follow the following contract:

* It can be <<start, started>>.
* It can be <<stop, stopped>>.
* It can <<postStartHook, do post-start initialization>> if needed for additional post-start initialization.
* It <<submitTasks, submits TaskSets for execution>>.
* Return <<Pool, Pool>> (using `rootPool`)
* Return `SchedulingMode` (using `schedulingMode`) that determines policy to order tasks.
* `cancelTasks(stageId: Int, interruptThread: Boolean)` to cancel all tasks in a stage.
* Setting a custom link:spark-dagscheduler.adoc[DAGScheduler] (using `setDAGScheduler(dagScheduler: DAGScheduler)`).
* Return the default level of parallelism (using `defaultParallelism()`)
* `executorHeartbeatReceived`
* (optionally) Return an application id for the current job (using `applicationId()`). It returns `spark-application-[System.currentTimeMillis]` by default.
* Handle executor lost events (using `executorLost(executorId: String, reason: ExecutorLossReason)`)
* Return an application attempt ID associated with the job (using `applicationAttemptId`)

CAUTION: FIXME Have an exercise to create a SchedulerBackend.

=== [[lifecycle]] TaskScheduler's Lifecycle

A `TaskScheduler` is created while link:spark-sparkcontext.adoc#initialization[SparkContext is being created] (by calling link:spark-sparkcontext.adoc#createTaskScheduler[SparkContext.createTaskScheduler] for a given link:spark-deployment-environments.adoc[master URL] and link:spark-submit.adoc#deploy-mode[deploy mode]).

.TaskScheduler uses SchedulerBackend to support different clusters
image::diagrams/taskscheduler-uses-schedulerbackend.png[align="center"]

At this point in SparkContext's lifecycle, the internal `_taskScheduler` points at the `TaskScheduler` and it is "announced" by sending a blocking `TaskSchedulerIsSet` message to HeartbeatReceiver RPC endpoint.

The <<start, TaskScheduler is started>> right after the blocking `TaskSchedulerIsSet` message receives a response.

The application ID and attempt ID are set at this point (and SparkContext uses the application id to set up `spark.app.id`, SparkUI, and BlockManager).

CAUTION: FIXME The application id is described as "associated with the job." in TaskScheduler, but I think it is "associated with the application".

Right before SparkContext is fully initialized, <<postStartHook, TaskScheduler.postStartHook>> is called.

The internal `_taskScheduler` is cleared (i.e. set to `null`) while link:spark-sparkcontext.adoc#stop[SparkContext is being stopped].

<<stop, TaskScheduler is stopped>> while link:spark-dagscheduler.adoc#stop[DAGScheduler is being stopped].

WARNING: FIXME If it is SparkContext to start a TaskScheduler, shouldn't SparkContext stop it too? Why is this the way it is now?

=== [[start]] Starting TaskScheduler

[source, scala]
----
start(): Unit
----

A `TaskScheduler` is started while link:spark-sparkcontext.adoc#initialization[SparkContext is being created].

=== [[stop]] Stopping TaskScheduler

[source, scala]
----
stop(): Unit
----

A `TaskScheduler` is stopped while link:spark-dagscheduler.adoc#stop[DAGScheduler is being stopped].

=== [[postStartHook]] Post-Start Initialization (postStartHook method)

[source, scala]
----
postStartHook() {}
----

`postStartHook` is called right before link:spark-sparkcontext.adoc#initialization[SparkContext's initialization finishes].

`postStartHook` does nothing by default, but allows custom implementations to do some post-start initialization.

=== [[submitTasks]] Submitting TaskSets for Execution

[source, scala]
----
submitTasks(taskSet: TaskSet): Unit
----

`submitTasks` accepts a link:spark-taskscheduler-tasksets.adoc[TaskSet] for execution.

It is called by link:spark-dagscheduler.adoc#submitMissingTasks[DAGScheduler when there are tasks to be executed for a stage].

=== [[implementations]] Available Implementations

Spark comes with the following task schedulers:

* link:spark-taskschedulerimpl.adoc[TaskSchedulerImpl]
* https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnScheduler.scala[YarnScheduler] and https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClusterScheduler.scala[YarnClusterScheduler] (for link:spark-yarn.adoc[Spark on YARN])

=== [[Schedulable]] Schedulable Contract

`Schedulable` is an interface for schedulable entities.

It assumes that each Schedulable:

* Is identified by `name`
* Knows about its parent <<Pool, Pool>>
* Has a `schedulableQueue`, a `schedulingMode`, `weight`, `minShare`, `runningTasks`, `priority`, `stageId`
* Adds or removes Schedulables
* Returns a Schedulable by name
* Can be informed about lost executors (using `executorLost` method)
* Checks speculatable tasks (using `checkSpeculatableTasks`)
* Tracks link:spark-tasksetmanager.adoc[TaskSetManagers] (using `getSortedTaskSetQueue`)

=== [[Pool]] Pool

`Pool` is a <<Schedulable, Schedulable>>. It requires a name, a scheduling mode, initial `minShare` and weight.

NOTE: An instance of `Pool` is created when link:spark-taskschedulerimpl.adoc#initialize[TaskSchedulerImpl is initialized].
