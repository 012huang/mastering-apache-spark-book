== Task Scheduler

A *Task Scheduler* schedules <<tasks, tasks>> for a single link:spark-sparkcontext.adoc[SparkContext].

.TaskScheduler works for a single SparkContext
image::diagrams/taskscheduler-single-sparkcontext.png[align="center"]

A TaskScheduler gets sets of tasks (as <<taskset, TaskSets>>) submitted to it from the link:spark-scheduler.adoc[DAGScheduler] for each stage, and is responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers.

A TaskScheduler emits events to the DAGScheduler.

The base task scheduler implementation in Spark is https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala[org.apache.spark.scheduler.TaskSchedulerImpl]. There are two specialized implementations for link:spark-yarn.adoc[Spark on YARN cluster mode] - https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnScheduler.scala[YarnScheduler] and https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClusterScheduler.scala[YarnClusterScheduler].

The default implementation `TaskSchedulerImpl` can schedule tasks for multiple types of clusters by acting through a link:spark-execution-model.adoc#scheduler-backends[SchedulerBackend]. It handles common logic, like determining a scheduling order across jobs, waking up to launch speculative tasks, etc.

.TaskScheduler uses SchedulerBackend for different clusters
image::diagrams/taskscheduler-uses-schedulerbackend.png[align="center"]

Task Scheduler can work in one of two scheduling modes - *FAIR* and *FIFO* - that determine policy to order tasks across a Schedulable's sub-queues. It is controlled by <<settings, spark.scheduler.mode>> property.

* *FIFO* - no pools; one root pool with TaskSetManagers; lower priority gets Schedulable sooner or earlier stage wins.
* *FAIR* - more advanced
* *NONE* means no sub-queues

DAGScheduler uses <<submitTasks, submitTasks() operation >> to submit a TaskSet to Task Scheduler and can cancel tasks in a stage.

=== [[tasks]] Tasks

A *Task* is an individual unit of work that is sent to one machine to execute.

A task can also be considered a computation in a stage on a partition in a given job attempt.

A Task belongs to a single stage and operates on a single partition (a part of an RDD).

CAUTION: FIXME What are `stageAttemptId` and `taskAttemptId`?

There are two kinds of tasks:

* *ShuffleMapTask* that executes a task and divides the task's output to multiple buckets (based on the task's partitioner). See <<shufflemaptask, ShuffleMapTask>>.
* *ResultTask* that executes a task and sends the task's output back to the driver application.

The very last stage in a job consists of multiple `ResultTasks`, while earlier stages consist of `ShuffleMapTasks`.

TaskRunner calls `Task.run` to execute a task.

Task can be in one of the states:

* LAUNCHING
* RUNNING
* FINISHED
* FAILED
* KILLED
* LOST

==== [[shufflemaptask]] ShuffleMapTask

A ShuffleMapTask divides the elements of an RDD into multiple buckets (based on a partitioner specified in the ShuffleDependency).

==== TaskContextImpl

* stage
* partition
* task attempt
* attempt number
* runningLocally = false

==== TaskMemoryManager

==== TaskMetrics

=== [[taskset]] TaskSet

*TaskSet* is a set of tasks that belong to a single stage and an attempt. The pair of a stage and an attempt uniquely describes a TaskSet. TaskSet represents missing partitions of a stage.

TaskSet has *priority* and *properties*. Priority is used in FIFO scheduling mode. Properties are the first job in a stage's properties.

A TaskSet contains a fully-independent sequence of tasks that can run right away based on the data that is already on the cluster, e.g. map output files from previous stages, though it may fail if this data becomes unavailable.

TaskSet can be <<submitTasks, submitted>>, i.e. started.

=== [[tasksetmanager]] TaskSetManager

*TaskSetManager* manages execution of the tasks in a TaskSet. It schedules the tasks and keeps track of their execution, retries them upon failure (up to <<settings, spark.task.maxFailures>>), and handles locality-aware scheduling for this TaskSet via delay scheduling.

[CAUTION]
====
FIXME

1. When is this class called? By whom?
====

For each submitted <<taskset, TaskSet>>, a new *TaskSetManager* is created. A TaskSetManager completely and exclusively owns a TaskSet submitted for execution.

.TaskSetManager owns TaskSet
image::diagrams/tasksetmanager-taskset.png[align="center"]

It is an `Schedulable` that works with `TaskSchedulerImpl`. As an `Schedulable` it has a *priority* property (among others).

[TIP]
====
Add the following line to `conf/log4j.properties` to see what happens under the covers of `TaskSetManager`:

```
log4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG
```
====

When a task has finished, TaskSetManager sends link:spark-scheduler.adoc#completionevent[a CompletionEvent message] to DAGScheduler.

CAUTION: FIXME Make it less code-oriented

The following "events" trigger communication between TaskSetManager and DAGScheduler:

* `handleSuccessfulTask` - `Success` (`TaskEndReason`)
** `TaskSchedulerImpl` calls `taskSetManager.handleSuccessfulTask`
** `TaskResultGetter` calls `scheduler.handleSuccessfulTask`
** `TaskSchedulerImpl.statusUpdate` calls `TaskResultGetter.enqueueSuccessfulTask`
** ...FIXME Finish me...
* `handleFailedTask` with the reason for the failure
* `executorLost` - `Resubmitted` (`TaskFailedReason`)

==== [[tasksetmanager-settings]] Settings

* `spark.scheduler.executorTaskBlacklistTime` (default: `0L`) - time interval to pass after which a task can be re-launched on an executor where it has just failed. It can prevent repeated task failures.
* `spark.speculation` (default: `false`)
* `spark.speculation.interval` (default: `100ms`) - how often to check for speculative tasks.
* `spark.speculation.quantile` (default: `0.75`) - the percentage of tasks that has not finished yet.
* `spark.speculation.multiplier` (default: `1.5`)

==== Task retries and spark.task.maxFailures

CAUTION: FIXME Review `handleFailedTask`

When you start Spark program you set up <<settings, spark.task.maxFailures>> for the number of failures that are acceptable until TaskSetManager gives up and marks a job failed.

In Spark shell with local master, `spark.task.maxFailures` is fixed to `1` and you need to use link:spark-local.adoc[local-with-retries master] to change it to some other value.

In the following example, you are going to execute a job with two partitions and keep one failing at all times (by throwing an exception). The aim is to learn the behavior of retrying task execution in a stage in TaskSet. You will only look at a single task execution, namely `0.0`.

```
$ ./bin/spark-shell --master "local[*, 5]"
...
scala> sc.textFile("README.md", 2).mapPartitionsWithIndex((idx, it) => if (idx == 0) throw new Exception("Partition 2 marked failed") else it).count
...
15/10/27 17:24:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitionsWithIndex at <console>:25)
15/10/27 17:24:56 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
15/10/27 17:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
...
15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2062 bytes)
...
15/10/27 17:24:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
...
15/10/27 17:24:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2062 bytes)
15/10/27 17:24:56 INFO Executor: Running task 0.1 in stage 1.0 (TID 4)
15/10/27 17:24:56 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784
15/10/27 17:24:56 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 4)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 ERROR Executor: Exception in task 0.4 in stage 1.0 (TID 7)
java.lang.Exception: Partition 2 marked failed
...
15/10/27 17:24:56 INFO TaskSetManager: Lost task 0.4 in stage 1.0 (TID 7) on executor localhost: java.lang.Exception (Partition 2 marked failed) [duplicate 4]
15/10/27 17:24:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 5 times; aborting job
15/10/27 17:24:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
15/10/27 17:24:56 INFO TaskSchedulerImpl: Cancelling stage 1
15/10/27 17:24:56 INFO DAGScheduler: ResultStage 1 (count at <console>:25) failed in 0.058 s
15/10/27 17:24:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
15/10/27 17:24:56 INFO DAGScheduler: Job 1 failed: count at <console>:25, took 0.085810 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 5 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 7, localhost): java.lang.Exception: Partition 2 marked failed
```

==== [[zombie-state]] Zombie state

TaskSetManager enters *zombie* state when at least one attempt of each task has completed successfully, or if the task set is aborted, e.g. because it was killed. In this zombie state, no more tasks should be launched for this TaskSetManager.

TaskSetManager remains in the zombie state until all tasks have finished running, i.e. to continue to track and account for the running tasks.

=== TaskSet's priority field and FIFO scheduling

A TaskSet has `priority` field that turns into the *priority* field's value of TaskSetManager (which is a Schedulable).

The `priority` field is used in `FIFOSchedulingAlgorithm` in which equal priorities give stages an advantage (not to say _priority_).

`FIFOSchedulingAlgorithm` is only used for `FIFO` scheduling mode in a `Pool` which is a Schedulable collection of `Schedulable`'s.

Effectively, the `priority` field is the job's id of the first job this stage was part of (for FIFO scheduling).

=== [[speculative-execution]] Speculative execution of tasks

*Speculative tasks* (also *speculatable tasks* or *task strugglers*) are tasks that run slower than most of the all tasks in a job.

*Speculative execution of tasks* is a health-check procedure that checks for tasks to be *speculated*, i.e. running slower in a stage than the median of all successfully completed tasks in a taskset. Such slow tasks will be re-launched in another worker. It will not stop the slow tasks, but run a new copy in parallel.

It is executed periodically by the TaskScheduler for link:spark-cluster.adoc[clustered deployment modes], when <<tasksetmanager-settings, spark.speculation>> is enabled (`true`).

With `spark.speculation` enabled, the following INFO message appears in the logs:

```
INFO Starting speculative execution thread
```

It is scheduled using *task-scheduler-speculation* daemon thread pool using `j.u.c.ScheduledThreadPoolExecutor` with core pool size `1`.

It is executed for <<zombie-state,non-zombie TaskSetManagers>> with more than one task to execute.

The process computes <<tasksetmanager-settings, spark.speculation.quantile>> of all the tasks and checks whether the number is greater than the number of tasks completed successfully.

You can find the DEBUG message in the logs:

```
DEBUG Checking for speculative tasks: minFinished =
```

It then computes the median duration of all the completed task length threshold for speculation to have it multiplied by <<tasksetmanager-settings, spark.speculation.multiplier>>. It has to be at least `100`.

In the logs at DEBUG level:

```
DEBUG Task length threshold for speculation:
```

For each active task for which there is only one copy running and the task takes more than the threshold, it gets marked as *speculatable*.

In the logs at INFO level:

```
INFO Marking task %d in stage %s (on %s) as speculatable because it ran more than %.0f ms
```

The job with speculatable tasks should finish while speculative tasks are running, and it will leave these tasks running - no KILL command yet.

1. How does Spark handle repeated results of speculative tasks since there are copies launched?

=== [[submitTasks]] submitTasks

Tasks are submitted for execution using `submitTasks(taskSet: TaskSet)`.

You should see the following INFO in the logs:

```
INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks
```

=== [[settings]] Settings

* `spark.task.maxFailures` (default: `4` for link:spark-cluster.adoc[cluster mode] and `1` for link:spark-local.adoc[local] except link:spark-local.adoc[local-with-retries]) - The number of individual task failures before giving up on the entire TaskSet and then the job.
+
Internally, it is used in `org.apache.spark.scheduler.TaskSchedulerImpl` to initialize `org.apache.spark.scheduler.TaskSetManager`.
* `spark.task.cpus` (default: `1`) - how many CPUs to request per task.
* `spark.scheduler.mode` (default: `FIFO`) can be of any of `FAIR`, `FIFO`, or `NONE`.
