== Task Schedulers

A *Task Scheduler* schedules link:spark-taskscheduler-tasks.adoc[tasks] for a link:spark-anatomy-spark-application.adoc[single Spark application] according to <<scheduling-mode, scheduling mode>> (aka *order task policy*).

.TaskScheduler works for a single SparkContext
image::images/sparkstandalone-sparkcontext-taskscheduler-schedulerbackend.png[align="center"]

A TaskScheduler gets sets of tasks (as link:spark-taskscheduler-tasksets.adoc[TaskSets]) submitted to it from the link:spark-dagscheduler.adoc[DAGScheduler] for each stage, and is responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers.

=== [[contract]] TaskScheduler Contract

NOTE: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala[org.apache.spark.scheduler.TaskScheduler] is a `private[spark]` Scala trait.

Every task scheduler has to offer the following services:

* Return <<Pool, Pool>> (using `rootPool`)
* Return `SchedulingMode` (using `schedulingMode`) that determines policy to order tasks.
* Can be started (using `start()`) and stopped (using `stop()`)
* (optionally) `postStartHook()` if needed for additional post-start initialization. It does nothing by default. It is called at the very end of link:spark-sparkcontext.adoc#initialization[SparkContext's initialization].
* `submitTasks(taskSet: TaskSet)`
* `cancelTasks(stageId: Int, interruptThread: Boolean)` to cancel all tasks in a stage.
* Setting a custom link:spark-dagscheduler.adoc[DAGScheduler] (using `setDAGScheduler(dagScheduler: DAGScheduler)`).
* Return the default level of parallelism (using `defaultParallelism()`)
* `executorHeartbeatReceived`
* (optionally) Return an application id for the current job (using `applicationId()`). It returns `spark-application-[System.currentTimeMillis]` by default.
* Handle executor lost events (using `executorLost(executorId: String, reason: ExecutorLossReason)`)
* Return an application attempt ID associated with the job (using `applicationAttemptId`)

CAUTION: FIXME Have an exercise to create a SchedulerBackend.

=== Available Implementations

Spark comes with the following task schedulers:

* link:spark-taskschedulerimpl.adoc[TaskSchedulerImpl]
* https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnScheduler.scala[YarnScheduler] for link:spark-yarn.adoc[Spark on YARN]
* https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClusterScheduler.scala[YarnClusterScheduler] for link:spark-yarn.adoc[Spark on YARN]

A TaskScheduler emits events to the DAGScheduler.

.TaskScheduler uses SchedulerBackend for different clusters
image::diagrams/taskscheduler-uses-schedulerbackend.png[align="center"]

=== [[Schedulable]] Schedulable Contract

`Schedulable` is an interface for schedulable entities.

It assumes that each Schedulable:

* Is identified by `name`
* Knows about its parent <<Pool, Pool>>
* Has a `schedulableQueue`, a `schedulingMode`, `weight`, `minShare`, `runningTasks`, `priority`, `stageId`
* Adds or removes Schedulables
* Returns a Schedulable by name
* Can be informed about lost executors (using `executorLost` method)
* Checks speculatable tasks (using `checkSpeculatableTasks`)
* Tracks link:spark-tasksetmanager.adoc[TaskSetManagers] (using `getSortedTaskSetQueue`)

=== [[Pool]] Pool

`Pool` is a <<Schedulable, Schedulable>>. It requires a name, a scheduling mode, initial `minShare` and weight.

=== [[TaskContext]] TaskContext

`TaskContext` is an abstract class that holds contextual information about a task.

* `stageId` is the id of the stage the task belongs to.
* `partitionId` is the id of the partition computed by the task.
* `attemptNumber` is to denote how many times the task has been attempted (starting from 0).
* `taskAttemptId` is the id of the attempt of the task.
* `isCompleted` returns `true` when a task is completed.
* `isInterrupted` returns `true` when a task was killed.

It is also an interface to define listeners:

* `addTaskCompletionListener`
* `addTaskFailureListener`

Using `TaskContext` you can:

* `getLocalProperty` that was set by the driver using `SparkContext.setLocalProperty`.
* `taskMetrics` to access the `TaskMetrics` of the task.
* `getMetricsSources(sourceName: String)` to access all metrics sources for `sourceName` name which are associated with the instance which runs the task.

You can read and change the `TaskContext` instance for a task using `org.apache.spark.TaskContext.get()`.

`TaskContext` belongs to `org.apache.spark` package.

[source, scala]
----
import org.apache.spark.TaskContext
----

==== [[TaskContextImpl]] TaskContextImpl

`TaskContextImpl` is the sole implementation of <<TaskContext, TaskContext>> abstract class.

CAUTION: FIXME

* stage
* partition
* task attempt
* attempt number
* runningLocally = false

=== [[TaskMemoryManager]] TaskMemoryManager

CAUTION: FIXME

=== TaskMetrics

CAUTION: FIXME
