== JobScheduler

`JobScheduler` schedules jobs to be run on Spark. It is created as part of link:spark-streaming-streamingcontext.adoc#creating-instance[creating a StreamingContext] and starts with it.

.JobScheduler with Dependencies
image::images/streaming-jobscheduler.png[align="center"]

It tracks jobs submitted for execution (as <<JobSet, JobSets>> via <<submitJobSet, submitJobSet>> method) in <<internal-registries, jobSets>> internal map.

NOTE: JobSets are submitted by link:spark-streaming-jobgenerator.adoc[JobGenerator].

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobScheduler` logger to see what happens in JobScheduler.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobScheduler=DEBUG
```
====

=== [[starting]] Starting JobScheduler (using start method)

When `JobScheduler` starts, it prints out following DEBUG message in the logs:

```
DEBUG JobScheduler: Starting JobScheduler
```

It then starts <<eventLoop, eventLoop - JobSchedulerEvent Handler>>, <<StreamingListenerBus, StreamingListenerBus>>, link:spark-streaming-receivertracker.adoc[ReceiverTracker], and link:spark-streaming-jobgenerator.adoc[JobGenerator]. It instantiates <<InputInfoTracker, InputInfoTracker>>.

It finishes `start` by printing the following INFO message to the logs:

```
INFO JobScheduler: Started JobScheduler
```

=== [[stopping]] Stopping JobScheduler (using stop method)

`JobScheduler.stop(processAllReceivedData: Boolean)` stops `JobScheduler`.

NOTE: It is called when link:spark-streaming-streamingcontext.adoc#stopping[StreamingContext is being stopped].

You should see the following DEBUG message in the logs:

```
DEBUG JobScheduler: Stopping JobScheduler
```

link:spark-streaming-receivertracker.adoc#stopping[ReceiverTracker is stopped] (unless it is not assigned yet).

NOTE: link:spark-streaming-receivertracker.adoc[ReceiverTracker] is only assigned (and started) while JobScheduler is starting.

It stops generating jobs (using link:spark-streaming-jobgenerator.adoc#stop[JobGenerator.stop(processAllReceivedData)]).

You should see the following DEBUG message in the logs:

```
DEBUG JobScheduler: Stopping job executor
```

<<streaming-job-executor, jobExecutor Thread Pool>> is shut down (using `jobExecutor.shutdown()`).

If the stop should wait for all received data to be processed (the input parameter `processAllReceivedData` is `true`), `stop` awaits termination of <<streaming-job-executor, jobExecutor Thread Pool>> for *1 hour* (it is assumed that it is enough and is not configurable). Otherwise, it waits for *2 seconds*.

<<streaming-job-executor, jobExecutor Thread Pool>>  is forcefully shut down (using `jobExecutor.shutdownNow()`) unless it has terminated already.

You should see the following DEBUG message in the logs:

```
DEBUG JobScheduler: Stopped job executor
```

<<StreamingListenerBus, StreamingListenerBus>> and <<eventLoop, eventLoop - JobSchedulerEvent Handler>> are stopped.

You should see the following INFO message in the logs:

```
INFO JobScheduler: Stopped JobScheduler
```

=== [[JobHandler]] JobHandler

`JobHandler` is a thread of execution for link:spark-streaming.adoc#Job[jobs] (it simply executes `Job.run`).

It runs when a new <<JobSet, JobSet>> is submitted (see <<submitJobSet, submitJobSet>> in this document).

=== [[submitJobSet]] submitJobSet

When `submitJobSet(jobSet: JobSet)` is called, it behaves differently per given `jobSet` <<JobSet, JobSet>>.

NOTE: The method is called as part of link:spark-streaming-jobgenerator.adoc#GenerateJobs[JobGenerator.generateJobs] and `JobGenerator.restart`.

When no jobs are inside the JobSet, it simply prints out the following INFO to the logs:

```
INFO JobScheduler: No jobs added for time [jobSet.time]
```

Otherwise, when there is at least one job inside the JobSet, `StreamingListenerBatchSubmitted` is posted to <<StreamingListenerBus, StreamingListenerBus>>.

The JobSet is added to <<internal-registries, jobSets>>.

It then goes over every job in the JobSet and executes a <<JobHandler, JobHandler>> (using <<streaming-job-executor, jobExecutor Thread Pool>>).

At the end, you should see the following INFO message in the logs:

```
INFO JobScheduler: Added jobs for time [jobSet.time] ms
```

=== [[streaming-job-executor]] jobExecutor Thread Pool

While `JobScheduler` is instantiated, the daemon thread pool `streaming-job-executor-ID` with link:spark-streaming-settings.adoc[spark.streaming.concurrentJobs] threads is created.

It is used to execute <<JobHandler, JobHandler>> for jobs in JobSet (see <<submitJobSet, submitJobSet>> in this document).

It shuts down when link:spark-streaming-streamingcontext.adoc#stop[StreamingContext] stops.

=== [[eventLoop]] eventLoop - JobSchedulerEvent Handler

JobScheduler uses `EventLoop` for `JobSchedulerEvent` events. It accepts <<JobStarted,JobStarted>> and <<JobCompleted, JobCompleted>> events. It also processes `ErrorReported` events.

==== [[JobStarted]] JobStarted and JobScheduler.handleJobStart

When `JobStarted` event is received, `JobScheduler.handleJobStart` is called.

NOTE: It is <<JobHandler, JobHandler>> to post `JobStarted`.

`handleJobStart(job: Job, startTime: Long)` takes a `JobSet` (from `jobSets`) and checks whether it has already been started.

It posts `StreamingListenerBatchStarted` to <<StreamingListenerBus, StreamingListenerBus>> when the JobSet is about to start.

It posts `StreamingListenerOutputOperationStarted` to <<StreamingListenerBus, StreamingListenerBus>>.

You should see the following INFO message in the logs:

```
INFO JobScheduler: Starting job [job.id] from job set of time [jobSet.time] ms
```

==== [[JobCompleted]] JobCompleted and JobScheduler.handleJobCompletion

When `JobCompleted` event is received, `JobScheduler.handleJobCompletion` is called.

NOTE: It is <<JobHandler, JobHandler>> to post `JobCompleted`.

`handleJobCompletion(job: Job, completedTime: Long)` takes the JobSet (using <<internal-registries, jobSets>>) and calls `jobSet.handleJobCompletion(job)` upon it. It also calls `job.setEndTime(completedTime)`.

It posts `StreamingListenerOutputOperationCompleted` to <<StreamingListenerBus, StreamingListenerBus>>.

You should see the following INFO message in the logs:

```
INFO JobScheduler: Finished job [job.id] from job set of time [jobSet.time] ms
```

If the entire JobSet is completed, it removes it from <<internal-registries, jobSets>>, and calls link:spark-streaming-jobgenerator.adoc#onBatchCompletion[JobGenerator.onBatchCompletion].

You should see the following INFO message in the logs:

```
INFO JobScheduler: Total delay: [totalDelay] s for time [time] ms (execution: [processingDelay] s)
```

It posts `StreamingListenerBatchCompleted` to <<StreamingListenerBus, StreamingListenerBus>>.

It reports an error if the job's result is a failure.

=== [[RecurringTimer]] RecurringTimer

CAUTION: FIXME

`RecurringTimer` uses a daemon thread prefixed `RecurringTimer - [name]` that executes `callback` every batch duration. The sleeping is achieved by `Clock.waitTillTime`.

When it starts (as part of JobGenerator start), you should see the following INFO message in the logs:

```
INFO RecurringTimer: Started timer for JobGenerator at time [nextTime]
```

=== [[StreamingListenerBus]] StreamingListenerBus and StreamingListenerEvents

`StreamingListenerBus` is a asynchronous listener bus for `StreamingListener` to receive `StreamingListenerEvent`:

* `StreamingListenerBatchStarted` triggers `StreamingListener.onBatchStarted`

* `StreamingListenerBatchSubmitted` triggers `StreamingListener.onBatchSubmitted`

=== [[StreamingJobProgressListener]] StreamingJobProgressListener

`StreamingJobProgressListener` is a `StreamingListener` to listen to `StreamingListenerEvent` events from <<StreamingListenerBus, StreamingListenerBus>>.

It is created while link:spark-streaming-streamingcontext.adoc#creating-instance[StreamingContext is created] and later registered as a `StreamingListener` and `SparkListener` when link:spark-streaming.adoc#StreamingTab[Streaming tab] is created.

CAUTION: FIXME How does this contribute to the result shown in the tab?

=== [[JobSet]] JobSet

A `JobSet` represents a collection of link:spark-streaming.adoc#Job[jobs] that belong to a batch.

It can be in created, started (after `JobSet.handleJobStart` is called), or completed (after `JobSet.handleJobCompletion`) state.

The time it takes to process all the jobs in a JobSet is called *processing delay*. The time from when the JobSet was created up to when it was completed is *total delay*.

NOTE: Total delay is greater than or equal to processing delay.

`JobSet` is used in:

* <<submitJobSet, submitJobSet(jobSet: JobSet)>>
* <<JobStarted, handleJobStart(job: Job, startTime: Long)>>
* <<JobCompleted, handleJobCompletion(job: Job, completedTime: Long)>>

=== [[internal-registries]] Internal Registries

`JobScheduler` maintains the following information in internal registries:

* `jobSets` - a mapping between time and JobSets. See <<JobSet, JobSet>>.

=== [[InputInfoTracker]] InputInfoTracker

`InputInfoTracker` is the class to track batch times and input data statistics (as `StreamInputInfo`) for link:spark-streaming-inputdstreams.adoc[input streams] (when they `compute` RDDs).

NOTE: It is created when <<starting, JobScheduler starts>>.
