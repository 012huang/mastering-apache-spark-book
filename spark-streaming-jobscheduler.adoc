== JobScheduler

`JobScheduler` schedules jobs to be run on Spark. It is created as part of link:spark-streaming-streamingcontext.adoc#creating-instance[creating a StreamingContext] and starts with it.

It tracks jobs submitted for execution (as <<JobSet, JobSets>> via <<submitJobSet, submitJobSet>> method) in <<internal-registries, jobSets>> internal map.

NOTE: JobSets are submitted by link:spark-streaming-jobgenerator.adoc[JobGenerator].

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.scheduler.JobScheduler` logger to see what happens in JobScheduler.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.scheduler.JobScheduler=DEBUG
```
====

With DEBUG logging level you should see the following message in the logs:

```
DEBUG Starting JobScheduler
```

When `JobScheduler` starts, it starts <<eventLoop, eventLoop - JobSchedulerEvent Handler>> and link:spark-streaming.adoc#ReceiverTracker[ReceiverTracker]. It also starts the link:spark-streaming-jobgenerator.adoc[JobGenerator].

At the end, it prints the following INFO message to the logs:

```
INFO JobScheduler: Started JobScheduler
```

=== [[JobHandler]] JobHandler

`JobHandler` is a thread of execution for link:spark-streaming.adoc#Job[jobs] (it simply executes `Job.run`).

It is run when a new <<JobSet, JobSet>> is submitted (see <<submitJobSet, submitJobSet>> in this document).

=== [[submitJobSet]] submitJobSet

When `submitJobSet(jobSet: JobSet)` is called, it behaves differently per the given `jobSet` <<JobSet, JobSet>>.

When no jobs are inside the JobSet, it simply prints out the following INFO to the logs:

```
INFO No jobs added for time [jobSet.time]
```

Otherwise, when there is at least one job inside the JobSet, `StreamingListenerBatchSubmitted` is posted to <<StreamingListenerBus, StreamingListenerBus>>.

The JobSet is added to <<internal-registries, jobSets>>.

It then goes over every job in the JobSet and executes a <<JobHandler, JobHandler>> (using <<streaming-job-executor, jobExecutor Thread Pool>>).

At the end, you should see the following INFO message in the logs:

```
INFO JobScheduler: Added jobs for time [jobSet.time] ms
```

The method is called as part of link:spark-streaming-jobgenerator.adoc#GenerateJobs[JobGenerator.generateJobs] and `JobGenerator.restart`.

=== [[streaming-job-executor]] jobExecutor Thread Pool

While `JobScheduler` is instantiated, the daemon thread pool `streaming-job-executor-ID` with <<settings, spark.streaming.concurrentJobs>> threads is created.

It is used to execute <<JobHandler, JobHandler>> for jobs in JobSet (see <<submitJobSet, submitJobSet>> in this document).

It is shut down when a link:spark-streaming-streamingcontext.adoc#stop[StreamingContext] stops.

=== [[eventLoop]] eventLoop - JobSchedulerEvent Handler

JobScheduler uses `EventLoop` for `JobSchedulerEvent` events. It accepts <<JobStarted,JobStarted>> and <<JobCompleted, JobCompleted>> events. It also processes `ErrorReported` events.

==== [[JobStarted]] JobStarted and JobScheduler.handleJobStart

When `JobStarted` event is received, `JobScheduler.handleJobStart` is called.

NOTE: It is <<JobHandler, JobHandler>> to post `JobStarted`.

`handleJobStart(job: Job, startTime: Long)` takes a `JobSet` (from `jobSets`) and checks whether it has already been started.

It posts `StreamingListenerBatchStarted` to <<StreamingListenerBus, StreamingListenerBus>> when the JobSet is about to start.

It posts `StreamingListenerOutputOperationStarted` to <<StreamingListenerBus, StreamingListenerBus>>.

You should see the following INFO message in the logs:

```
INFO JobScheduler: Starting job [job.id] from job set of time [jobSet.time] ms
```

==== [[JobCompleted]] JobCompleted and JobScheduler.handleJobCompletion

When `JobCompleted` event is received, `JobScheduler.handleJobCompletion` is called.

NOTE: It is <<JobHandler, JobHandler>> to post `JobCompleted`.

`handleJobCompletion(job: Job, completedTime: Long)` takes the JobSet (using <<internal-registries, jobSets>>) and calls `jobSet.handleJobCompletion(job)` upon it. It also calls `job.setEndTime(completedTime)`.

It posts `StreamingListenerOutputOperationCompleted` to <<StreamingListenerBus, StreamingListenerBus>>.

You should see the following INFO message in the logs:

```
INFO JobScheduler: Finished job [job.id] from job set of time [jobSet.time] ms
```

If the entire JobSet is completed, it removes it from <<internal-registries, jobSets>>, and calls link:spark-streaming-jobgenerator.adoc#onBatchCompletion[JobGenerator.onBatchCompletion].

You should see the following INFO message in the logs:

```
INFO JobScheduler: Total delay: [totalDelay] s for time [time] ms (execution: [processingDelay] s)
```

It posts `StreamingListenerBatchCompleted` to <<StreamingListenerBus, StreamingListenerBus>>.

It reports an error if the job's result is a failure.

=== [[RecurringTimer]] RecurringTimer

CAUTION: FIXME

`RecurringTimer` uses a daemon thread prefixed `RecurringTimer - [name]` that executes `callback` every batch duration. The sleeping is achieved by `Clock.waitTillTime`.

When it starts (as part of JobGenerator start), you should see the following INFO message in the logs:

```
INFO RecurringTimer: Started timer for JobGenerator at time [nextTime]
```

=== [[StreamingListenerBus]] StreamingListenerBus and StreamingListenerEvents

`StreamingListenerBus` is a asynchronous listener bus for `StreamingListener` to receive `StreamingListenerEvent`.

* `StreamingListenerBatchStarted` triggers `StreamingListener.onBatchStarted`

* `StreamingListenerBatchSubmitted` triggers ...FIXME

=== [[StreamingJobProgressListener]] StreamingJobProgressListener

`StreamingJobProgressListener` is a `StreamingListener` to listen to `StreamingListenerEvent` events from <<StreamingListenerBus, StreamingListenerBus>>.

It is created while link:spark-streaming-streamingcontext.adoc#creating-instance[StreamingContext is created] and later registered as a `StreamingListener` and `SparkListener` when link:spark-streaming.adoc#StreamingTab[Streaming tab] is created.

CAUTION: FIXME How does this contribute to the result shown in the tab?

=== [[JobSet]] JobSet

A `JobSet` represents a collection of link:spark-streaming.adoc#Job[jobs] that belong to a batch.

It can be in created, started (after `JobSet.handleJobStart` is called), or completed (after `JobSet.handleJobCompletion`) state.

The time it takes to process all the jobs in a JobSet is called *processing delay*. The time from when the JobSet was created up to when it was completed is *total delay*.

NOTE: Total delay is greater than or equal to processing delay.

`JobSet` is used in:

* <<submitJobSet, submitJobSet(jobSet: JobSet)>>
* <<JobStarted, handleJobStart(job: Job, startTime: Long)>>
* <<JobCompleted, handleJobCompletion(job: Job, completedTime: Long)>>

=== [[internal-registries]] Internal Registries

`JobScheduler` maintains the following information in internal registries:

* `jobSets` - a mapping between time and JobSets. See <<JobSet, JobSet>>.

=== [[settings]] Settings

* `spark.streaming.concurrentJobs` (default: `1`) is the number of concurrent jobs, i.e. threads in <<streaming-job-executor, streaming-job-executor thread pool>>.
