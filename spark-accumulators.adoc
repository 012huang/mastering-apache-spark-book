== Accumulators

TIP: Read the official documentation about http://spark.apache.org/docs/latest/programming-guide.html#accumulators[Accumulators] before looking for anything useful here. This document is not ready for prime time yet.

Accumulators are variables that are "added" to through an associative and commutative operation. They can therefore be used safely and efficiently in parallel and distributed Spark computations.

They can be used to implement counters or sums.

You can create accumulators using the link:spark-sparkcontext.adoc#accumulator[SparkContext.accumulator] methods. You can create accumulators with or without a name, but only *named accumulators* are displayed in Spark's webUI (under Stages tab for a given stage).

.Accumulators in the Spark UI
image::images/spark-webui-accumulators.png[align="center"]

Accumulator are write-only variables for workers. They can be added to by workers and read by the driver only.

```
worker1: accumulator += incByWorker1
worker2: accumulator += incByWorker2

driver:  println(accumulator.value)
```

Accumulators are not thread-safe and they do not really have to. They are write-only data structures for workers while accessing the value of an accumulator is only allowed by the driver.

Accumulators are serializable so they can safely be referenced in the code executed on workers.

[source, scala]
----
val accum = sc.accumulator(0)
sc.parallelize(1 to 9).foreach(x => accum += x)
----

Internally, link:spark-sparkcontext.adoc#accumulator[SparkContext.accumulator] methods create an instance of <<Accumulator, Accumulator>> class and register it to link:spark-service-contextcleaner.adoc[ContextCleaner] for cleanup (using link:spark-service-contextcleaner.adoc#registerAccumulatorForCleanup[registerAccumulatorForCleanup] method).

Each task creates its own local accumulator.

Noticed on the user@spark mailing list that using an external key-value store (like HBase, Redis, Cassandra) and performing lookups/updates inside of your mappers (creating a connection within a link:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] code block to avoid the connection setup/teardown overhead) might be a better solution.

If hbase is used as the external key value store, atomicity is guaranteed

=== [[Accumulator]] Accumulator

`Accumulator[T]` parameterized class is a simpler variant of <<Accumulable, Accumulable>> that assumes that the types of the result value and the elements added are the same, i.e. `T`.

`Accumulator[T]` parameterized type is `Accumulable[T, T]`.

It belongs to `org.apache.spark` package.

NOTE: `Accumulator` is a `private[spark]` class so the _right_ way to create `Accumulator` instances is through link:spark-sparkcontext.adoc#accumulator[SparkContext.accumulator] methods.

NOTE: `countFailedValues` is `false` by default.

=== [[Accumulable]] Accumulable

`Accumulable[R, T]` is a private parameterized class that belongs to `org.apache.spark` package. It allows for the result type of the commutative and associative "add" operation, i.e. `R`, to be different from the type of elements being added, i.e. `T`.

TIP: `Accumulable` comes with `countFailedValues` constructor parameter that allows to specify whether to accumulate values from failed tasks or not.

=== [[AccumulatorParam]] AccumulatorParam

=== When are Accumulators Updated?

=== [[examples]] Examples

==== [[example1]] Example: Using Accumulators in Transformations and Guarantee Exactly-Once Update

CAUTION: FIXME Code with failing transformations (tasks) that update accumulator (`Map`) with `TaskContext` info.

==== [[example2]] Example: Custom Accumulator

CAUTION: FIXME Improve the earlier example

=== [[i-want-more]] Further reading or watching

* http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf[Performance and Scalability of Broadcast in Spark]
