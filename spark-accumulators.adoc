== Accumulators

TIP: Read the official documentation about http://spark.apache.org/docs/latest/programming-guide.html#accumulators[Accumulators]. This document is not ready for prime time yet.

Accumulators are variables that are "added" to through an associative and commutative "add" operation. They can therefore be used safely and efficiently in parallel and distributed Spark computations. Accumulators are a solid foundation for distributed counters and sums.

You can create accumulators using the link:spark-sparkcontext.adoc#accumulator[SparkContext.accumulator] methods. You can create accumulators with or without a name, but only *named accumulators* are displayed in Spark's webUI (under Stages tab for a given stage).

.Accumulators in the Spark UI
image::images/spark-webui-accumulators.png[align="center"]

Accumulator are write-only variables for workers. They can be added to by workers and read by the driver only.

```
worker1: accumulator += incByWorker1
worker2: accumulator += incByWorker2

driver:  println(accumulator.value)
```

Accumulators are not thread-safe and they do not really have to -- the link:spark-dagscheduler.adoc#updateAccumulators[DAGScheduler.updateAccumulators] method that the driver uses to update the values of accumulators after a task completes (successfully or with a failure) is only executed on a link:spark-dagscheduler.adoc#eventProcessLoop[single thread that runs scheduling loop]. Beside that, they are write-only data structures for workers that have their own local accumulator reference whereas accessing the value of an accumulator is only allowed by the driver.

Accumulators are serializable so they can safely be referenced in the code executed on workers and then safely send over the wire for execution.

[source, scala]
----
val accum = sc.accumulator(0)
sc.parallelize(1 to 9).foreach(x => accum += x)
----

Internally, link:spark-sparkcontext.adoc#accumulator[SparkContext.accumulator] methods create an instance of <<Accumulator, Accumulator>> class and register it to link:spark-service-contextcleaner.adoc[ContextCleaner] for cleanup (using link:spark-service-contextcleaner.adoc#registerAccumulatorForCleanup[registerAccumulatorForCleanup] method).

=== [[Accumulator]] Accumulator

`Accumulator[T]` parameterized class is a simpler variant of <<Accumulable, Accumulable>> that assumes that the types of the result value and the elements added are the same, i.e. `T`.

In other words, `Accumulator[T]` parameterized type is `Accumulable[T, T]`.

`Accumulator[T]` belongs to `org.apache.spark` package.

NOTE: `Accumulator` is a `private[spark]` class so the _right_ way to create `Accumulator` instances is through link:spark-sparkcontext.adoc#accumulator[SparkContext.accumulator] methods.

NOTE: `countFailedValues` is `false` by default.

=== [[Accumulable]] Accumulable

`Accumulable[R, T]` makes for the result type of the "add" operation, `R`, to be different from the type of elements being added, `T`.

It is a private parameterized class that belongs to `org.apache.spark` package.

TIP: `Accumulable` comes with `countFailedValues` constructor parameter that allows to specify whether to accumulate values from failed tasks or not.

=== [[AccumulableInfo]] AccumulableInfo

`AccumulableInfo` contains information about a task's local updates to an <<Accumulable, Accumulable>>.

* `id` of the accumulator
* optional `name` of the accumulator
* optional partial `update` to the accumulator from a task
* `value`
* whether or not it is `internal`
* whether or not to `countFailedValues` to the final value of the accumulator for failed tasks
* optional `metadata`

=== [[AccumulatorParam]] AccumulatorParam[T]

`AccumulatorParam[T]` is ...FIXME

=== When are Accumulators Updated?

=== [[examples]] Examples

==== [[example1]] Example: Using Accumulators in Transformations and Guarantee Exactly-Once Update

CAUTION: FIXME Code with failing transformations (tasks) that update accumulator (`Map`) with `TaskContext` info.

==== [[example2]] Example: Custom Accumulator

CAUTION: FIXME Improve the earlier example

==== [[example3]] Example: Distributed Stopwatch

NOTE: This is _almost_ a raw copy of org.apache.spark.ml.util.DistributedStopwatch.

[source, scala]
----
class DistributedStopwatch(sc: SparkContext, val name: String) {

  val elapsedTime: Accumulator[Long] = sc.accumulator(0L, s"DistributedStopwatch($name)")

  override def elapsed(): Long = elapsedTime.value

  override protected def add(duration: Long): Unit = {
    elapsedTime += duration
  }
}
----

=== [[i-want-more]] Further reading or watching

* http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf[Performance and Scalability of Broadcast in Spark]
