== Structured Streaming (aka Streaming Datasets)

*Structured Streaming* is a new computation model introduced in Spark 2.0.0. It has a high-level streaming API built on top of link:spark-sql-dataset.adoc[Datasets] (inside Spark SQL engine) for continuous incremental execution of link:spark-sql-StreamingQuery.adoc[structured queries].

Structured streaming is an attempt to unify streaming, interactive, and batch queries that paves the way for *continuous applications* (e.g. continuous aggregations using link:spark-sql-dataset.adoc#groupBy[groupBy] operator or continuous windowed aggregations using `groupBy` operator with link:spark-sql-functions.adoc#window[window] function). Spark 2.0 aims at simplifying *streaming analytics* without having to reason about streaming at all.

The new model introduces the *streaming datasets* that are _infinite datasets_ with primitives like input link:spark-sql-source.adoc[sources] and output link:spark-sql-sink.adoc[sinks], *event time*, *windowing*, and *sessions*.

It lives in `org.apache.spark.sql.streaming` package with the following main data abstractions:

* link:spark-sql-StreamingQueryManager.adoc[StreamingQueryManager]
* link:spark-sql-StreamingQuery.adoc[StreamingQuery]
* link:spark-sql-source.adoc[Source]
* link:spark-sql-sink.adoc[Sink]

With Datasets being Spark SQL's view of structured data, structured streaming checks input sources for new data every link:spark-sql-trigger.adoc[trigger] (time) and executes the (continuous) queries.

NOTE: The feature has also been called *Streaming Spark SQL Query*, *Streaming DataFrames*, *Continuous DataFrames* or *Continuous Queries*. There have been lots of names before Structured Streaming was chosen.

TIP: Watch https://issues.apache.org/jira/browse/SPARK-8360[SPARK-8360 Streaming DataFrames] to track progress of the feature.

=== [[example]] Example

Below is a complete example of a streaming query in a form of `DataFrame` of data from `hello` cvs files of a given schema into a link:spark-sql-streaming-ConsoleSink.adoc[ConsoleSink] every 5 seconds.

[source, scala]
----
import org.apache.spark.sql.types._
val schema = StructType(
  StructField("f1", IntegerType, true) ::
  StructField("f2", LongType, false) ::
  StructField("f3", BooleanType, false) :: Nil
)

val in = spark.readStream
  .schema(schema)
  .format("csv")
  .load("hello")

scala> in.isStreaming
res0: Boolean = true

import scala.concurrent.duration._
import org.apache.spark.sql.streaming.ProcessingTime
val out = in.writeStream
  .format("console")
  .trigger(ProcessingTime(5.seconds))
  .queryName("textStream")
  .start()
----

=== [[i-want-more]] Further reading or watching

* (video) https://youtu.be/oXkxXDG0gNk[The Future of Real Time in Spark] from Spark Summit East 2016 in which Reynold Xin presents the concept of *Streaming DataFrames* to the public.
* (video) https://youtu.be/i7l3JQRx7Qw?t=19m15s[Structuring Spark: DataFrames, Datasets, and Streaming]
* http://www.infoworld.com/article/3052924/analytics/what-sparks-structured-streaming-really-means.html[What Spark's Structured Streaming really means]
* (video) https://youtu.be/rl8dIzTpxrI[A Deep Dive Into Structured Streaming] by Tathagata "TD" Das from Spark Summit 2016
