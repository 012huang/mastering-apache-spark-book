== Settings

The following list are the settings used to configure Spark Streaming applications.

* `spark.streaming.kafka.maxRetries` (default: `1`) sets up the number of connection attempts to Kafka brokers.

* `spark.streaming.receiver.writeAheadLog.enable` (default: `false`) controls what link:spark-streaming-receivedblockhandlers.adoc[ReceivedBlockHandler] to use: `WriteAheadLogBasedBlockHandler` or `BlockManagerBasedBlockHandler`.

* `spark.streaming.receiver.blockStoreTimeout` (default: `30`) time in seconds to wait until both writes to a write-ahead log and BlockManager complete successfully.

* `spark.streaming.clock` (default: `org.apache.spark.util.SystemClock`) specifies a fully-qualified class name that extends `org.apache.spark.util.Clock` to represent time. It is used in link:spark-streaming-jobgenerator.adoc[JobGenerator].

* `spark.streaming.ui.retainedBatches` (default: `1000`)
* `spark.streaming.checkpoint.directory`
* `spark.streaming.receiverRestartDelay` (default: `2000`) - the time interval between a receiver is stopped and started again.

* `spark.streaming.concurrentJobs` (default: `1`) is the number of concurrent jobs, i.e. threads in link:spark-streaming-jobscheduler.adoc#streaming-job-executor[streaming-job-executor thread pool].

* `spark.streaming.stopSparkContextByDefault` (default: `true`) controls whether (`true`) or not (`false`) to stop the underlying SparkContext (regardless of whether this `StreamingContext` has been started).

* `spark.streaming.backpressure.enabled` (default: `false`) controls whether or not backpressure is used.

* `spark.streaming.kafka.maxRatePerPartition` (default: `0`) if non-`0` sets maximum number of messages per partition.

* `spark.streaming.manualClock.jump` (default: `0`) offsets (aka _jumps_) the system time, i.e. adds its value to checkpoint time, when used with the clock being a subclass of `org.apache.spark.util.ManualClock`. It is used when link:spark-streaming-jobgenerator.adoc[JobGenerator] is restarted from checkpoint.

* `spark.streaming.unpersist` (default: `true`) is a flag to control whether link:spark-streaming-dstreams.adoc#clearMetadata[output streams should unpersist old RDDs].
