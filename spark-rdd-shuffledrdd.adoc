== [[ShuffledRDD]] ShuffledRDD

`ShuffledRDD` is an RDD of `(key, value)` pairs with the number of <<ShuffledRDDPartition, ShuffledRDDPartition>> partitions. It is a shuffle step (the result RDD) for transformations that trigger link:spark-rdd-shuffle.adoc[shuffle] at execution.

`ShuffledRDD` requires a parent RDD and a link:spark-rdd-Partitioner.adoc[Partitioner] when created.

NOTE: `ShuffledRDD` is a result of link:spark-rdd-partitions.adoc#coalesce[`coalesce` transformation] (when `shuffle` is enabled), link:spark-rdd-pairrdd-functions.adoc#combineByKeyWithClassTag[combineByKeyWithClassTag] (when partitioners are different), link:spark-rdd-pairrdd-functions.adoc#partitionBy[partitionBy] (using a different `Partitioner`), link:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[sortByKey] and link:spark-rdd-OrderedRDDFunctions.adoc#repartitionAndSortWithinPartitions[repartitionAndSortWithinPartitions] ordered operators.

By default, the map-side combining flag (`mapSideCombine`) is disabled (i.e. `false`). It can however be changed using `ShuffledRDD.setMapSideCombine(mapSideCombine: Boolean)` method (and is used in link:spark-rdd-pairrdd-functions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] that sets it `true` by default).

`getDependencies` returns a single-element collection of link:spark-rdd-dependencies.adoc#ShuffleDependency[ShuffleDependency].

Let's have a look at the below example with `groupBy` transformation:

```
scala> val r = sc.parallelize(0 to 9, 3).groupBy(_ / 3)
r: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:18

scala> r.toDebugString
res0: String =
(3) ShuffledRDD[2] at groupBy at <console>:18 []
 +-(3) MapPartitionsRDD[1] at groupBy at <console>:18 []
    |  ParallelCollectionRDD[0] at parallelize at <console>:18 []
```

As you may have noticed, `groupBy` transformation adds `ShuffledRDD` RDD that will execute shuffling at execution time (as depicted in the following screenshot).

.Two stages in a job due to shuffling
image::images/spark-webui-job-two-stages.png[align="center"]

It uses link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] to get preferred locations for a shuffle, i.e. a `ShuffleDependency`.

=== [[getPreferredLocations]] `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(partition: Partition): Seq[String]
----

NOTE: `getPreferredLocations` is a part of link:spark-rdd.adoc#contract[RDD contract] to specify placement preferences (aka _preferred task locations_), i.e. where tasks should be executed to be as close to the data as possible.

CAUTION: FIXME

=== [[ShuffledRDDPartition]] ShuffledRDDPartition

`ShuffledRDDPartition` gets an `index` when it is created (that in turn is the index of partitions as calculated by the link:spark-rdd-Partitioner.adoc[Partitioner] of a <<ShuffledRDD, ShuffledRDD>>).
