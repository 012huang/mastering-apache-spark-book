== [[ShuffledRDD]] ShuffledRDD

`ShuffledRDD` is an link:spark-rdd.adoc[RDD] of key-value pairs in <<ShuffledRDDPartition, ShuffledRDDPartition>> partitions. It is the output of transformations that trigger link:spark-rdd-shuffle.adoc[shuffle] at execution and is hence often called a *shuffle step*.

CAUTION: FIXME What transformations?

`ShuffledRDD` requires a parent RDD and a link:spark-rdd-Partitioner.adoc[Partitioner] when created.

NOTE: `ShuffledRDD` is a result of link:spark-rdd-partitions.adoc#coalesce[`coalesce` transformation] (when `shuffle` is enabled), link:spark-rdd-pairrdd-functions.adoc#combineByKeyWithClassTag[combineByKeyWithClassTag] (when partitioners are different), link:spark-rdd-pairrdd-functions.adoc#partitionBy[partitionBy] (using a different `Partitioner`), link:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[sortByKey] and link:spark-rdd-OrderedRDDFunctions.adoc#repartitionAndSortWithinPartitions[repartitionAndSortWithinPartitions] ordered operators.

The <<mapSideCombine, map-side combine internal flag>> (`mapSideCombine`) is used to set up the `Serializer` for shuffling. It is disabled (i.e. `false`) by default and can be changed using `setMapSideCombine` method.

[[getDependencies]]
`getDependencies` returns a single-element collection of link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] (with the `Serializer` given <<mapSideCombine, `mapSideCombine` internal flag>>).

Let's have a look at the below example with `groupBy` transformation:

```
scala> val r = sc.parallelize(0 to 9, 3).groupBy(_ / 3)
r: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:18

scala> r.toDebugString
res0: String =
(3) ShuffledRDD[2] at groupBy at <console>:18 []
 +-(3) MapPartitionsRDD[1] at groupBy at <console>:18 []
    |  ParallelCollectionRDD[0] at parallelize at <console>:18 []
```

As you may have noticed, `groupBy` transformation adds `ShuffledRDD` RDD that will execute shuffling at execution time (as depicted in the following screenshot).

.Two stages in a job due to shuffling
image::images/spark-webui-job-two-stages.png[align="center"]

It uses link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] to get preferred locations for a shuffle, i.e. a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency].

=== [[mapSideCombine]] `mapSideCombine` Internal Flag

[source, scala]
----
mapSideCombine: Boolean
----

`mapSideCombine` internal flag is used to select the `Serializer` (for shuffling) when link:spark-rdd-ShuffleDependency.adoc#creating-instance[`ShuffleDependency` is created] (which is the <<getDependencies, one and only `Dependency`>> of a `ShuffledRDD`).

NOTE: `mapSideCombine` is only used when `userSpecifiedSerializer` optional `Serializer` is not specified explicitly (which is the default).

NOTE: `mapSideCombine` uses link:spark-sparkenv.adoc#serializerManager[`SparkEnv` to access the current `SerializerManager`].

If enabled (i.e. `true`), `mapSideCombine` directs to link:spark-SerializerManager.adoc#getSerializer[find the `Serializer`] for the types `K` and `C`. Otherwise, `getDependencies` finds the `Serializer` for the types `K` and `V`.

NOTE: The types `K`, `C` and `V` are specified when `ShuffledRDD` is created.

[NOTE]
====
`mapSideCombine` is disabled (i.e. `false`) when `ShuffledRDD` is created and can be set using `setMapSideCombine` method.

`setMapSideCombine` method is only used in the experimental link:spark-rdd-pairrdd-functions.adoc#combineByKeyWithClassTag[`PairRDDFunctions.combineByKeyWithClassTag` transformations].
====

=== [[compute]] Computing Partition (in `TaskContext`) -- `compute` Method

[source, scala]
----
compute(split: Partition, context: TaskContext): Iterator[(K, C)]
----

NOTE: `compute` is a part of link:spark-rdd.adoc#contract[RDD contract] to compute a given partition in a link:spark-taskscheduler-taskcontext.adoc[TaskContext].

Internally, `compute` makes sure that the input `split` is a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]. It then link:spark-shuffle-manager.adoc#contract[requests `ShuffleManager` for a `ShuffleReader`] to read key-value pairs (as `Iterator[(K, C)]`) for the `split`.

NOTE: `compute` uses link:spark-sparkenv.adoc#shuffleManager[`SparkEnv` to access `ShuffleManager`].

NOTE: A Partition has the `index` property to specify `startPartition` and `endPartition` partition offsets.

=== [[getPreferredLocations]] `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(partition: Partition): Seq[String]
----

NOTE: `getPreferredLocations` is a part of link:spark-rdd.adoc#contract[RDD contract] to specify placement preferences (aka _preferred task locations_), i.e. where tasks should be executed to be as close to the data as possible.

Internally, `getPreferredLocations` requests link:spark-service-MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[`MapOutputTrackerMaster` for the preferred locations] of the input `partition` (for the only link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]).

NOTE: `getPreferredLocations` uses link:spark-sparkenv.adoc#mapOutputTracker[`SparkEnv` to access `MapOutputTrackerMaster`] (which runs on the driver).

=== [[ShuffledRDDPartition]] ShuffledRDDPartition

`ShuffledRDDPartition` gets an `index` when it is created (that in turn is the index of partitions as calculated by the link:spark-rdd-Partitioner.adoc[Partitioner] of a <<ShuffledRDD, ShuffledRDD>>).
