== Transformers

A *transformer* is a function that maps (aka _transforms_) a `DataFrame` into another `DataFrame`. It simply prepares a dataset for an machine learning algorithm to work with.

Transformers are instances of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Transformer[org.apache.spark.ml.Transformer] abstract class that offers `transform` family of methods:

[source, scala]
----
transform(dataset: DataFrame): DataFrame
transform(dataset: DataFrame, paramMap: ParamMap): DataFrame
transform(dataset: DataFrame, firstParamPair: ParamPair[_], otherParamPairs: ParamPair[_]*): DataFrame
----

A `Transformer` is a <<PipelineStage, PipelineStage>> (so it can be a part of a <<Pipeline, Pipeline>>).

The direct specialized extensions of the `Transformer` abstract class are:

* link:spark-mllib-models.adoc[Model]
* <<UnaryTransformer, UnaryTransformer>>

==== [[UnaryTransformer]] UnaryTransformers

The https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.UnaryTransformer[UnaryTransformer] abstract class is a specialized `Transformer` that applies transformation to one input column and writes results to another (by appending a new column).

Each `UnaryTransformer` defines the input and output columns using the following "chain" methods (they return the transformer on which they were executed):

* `setInputCol(value: String)`
* `setOutputCol(value: String)`

Each `UnaryTransformer` calls `validateInputType` while executing `transformSchema(schema: StructType)` (that is part of <<PipelineStage, PipelineStage>> contract).

NOTE: A `UnaryTransformer` is a `PipelineStage`.

When `transform` is called, it first calls `transformSchema` (with DEBUG logging enabled) and then adds the column as a result of calling a protected abstract `createTransformFunc`.

NOTE: `createTransformFunc` function is abstract and defined by concrete `UnaryTransformer` objects.

Internally, `transform` methods uses Spark SQL's link:spark-sql-udfs.adoc#udf-function[udf] to define a function (based on `createTransformFunc` function described above) that will create the new output column (with appropriate `outputDataType`). The UDF is later applied to the input column of the input DataFrame and the result becomes the output column (using link:spark-sql-dataframe.adoc#withColumn[DataFrame.withColumn] method).

NOTE: Using `udf` and `withColumn` methods from Spark SQL demonstrates integration between the Spark modules: MLlib and SQL.

==== [[Transformer-examples]] Examples of Transformers

One example of a transformer is http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer[org.apache.spark.ml.feature.RegexTokenizer].

[source, scala]
----
import org.apache.spark.ml.feature.RegexTokenizer
val regexTok = new RegexTokenizer()
  .setInputCol("text")
  .setOutputCol("words")
  .setPattern("\\s+")

// Create DataFrame with rows with tabs and spaces
val df = Seq((0, s"""hello\tworld"""), (1, "two  spaces inside")).toDF("id", "text")

// Use RegexTokenizer
val regexedDF = regexTok.transform(df)

scala> regexedDF.show(false)
+---+------------------+---------------------+
|id |text              |words                |
+---+------------------+---------------------+
|0  |hello	world       |[hello, world]       |
|1  |two  spaces inside|[two, spaces, inside]|
+---+------------------+---------------------+
----

Another example of a transformer could be http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.HashingTF[org.apache.spark.ml.feature.HashingTF] that works on a `Column` of `ArrayType`.

[source, scala]
----
import org.apache.spark.ml.feature.HashingTF
val hashingTF = new HashingTF()
  .setInputCol("words")
  .setOutputCol("features")
  .setNumFeatures(5000)

// see above for regexTok transformer
val regexedDF = regexTok.transform(df)

// Use HashingTF
val hashedDF = hashingTF.transform(regexedDF)

scala> hashedDF.show(false)
+---+------------------+---------------------+-----------------------------------+
|id |text              |words                |features                           |
+---+------------------+---------------------+-----------------------------------+
|0  |hello	world       |[hello, world]       |(5000,[2322,3802],[1.0,1.0])       |
|1  |two  spaces inside|[two, spaces, inside]|(5000,[276,940,2533],[1.0,1.0,1.0])|
+---+------------------+---------------------+-----------------------------------+
----

The name of the output column is optional, and if not specified, it becomes the identifier of a `HashingTF` object with the `__output` suffix.

[source, scala]
----
scala> hashingTF.uid
res7: String = hashingTF_fe3554836819

scala> hashingTF.transform(regexDF).show(false)
+---+------------------+---------------------+-------------------------------------------+
|id |text              |words                |hashingTF_fe3554836819__output             |
+---+------------------+---------------------+-------------------------------------------+
|0  |hello	world       |[hello, world]       |(262144,[71890,72594],[1.0,1.0])           |
|1  |two  spaces inside|[two, spaces, inside]|(262144,[53244,77869,115276],[1.0,1.0,1.0])|
+---+------------------+---------------------+-------------------------------------------+
----

In this example you use https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.NGram[org.apache.spark.ml.feature.NGram] that converts the input collection of strings into a collection of n-grams (of `n` words).

[source, scala]
----
import org.apache.spark.ml.feature.NGram

val bigram = new NGram("bigrams")
val df = Seq((0, Seq("hello", "world"))).toDF("id", "tokens")
bigram.setInputCol("tokens").transform(df).show

+---+--------------+---------------+
| id|        tokens|bigrams__output|
+---+--------------+---------------+
|  0|[hello, world]|  [hello world]|
+---+--------------+---------------+
----
