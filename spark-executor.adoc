== Executors

*Executors* are distributed agents running on hosts that run tasks. They typically (?) run for the entire lifetime of a Spark application. Executors communicate with the driver to send computation results back.

CAUTION: FIXME image - see the images above

Executor offers are described by executor id and the host the executor runs on (see <<resource-offers, Resource Offers>> in this document).

Executors provide in-memory storage for RDDs that are cached in Spark applications (via link:spark-blockmanager.adoc[Block Manager]).

When executors are started they register themselves with the driver and communicate directly to launch jobs (as tasks).

Internally, a Spark executor is backed by a thread pool to run tasks.

Each executor can run multiple tasks over its lifetime, both parallel and sequentially.

It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster.

An executor is described by id, hostname, environment (as `SparkEnv`), classpath, and whether it runs in link:spark-local.adoc[local] or link:spark-cluster.adoc[cluster mode].

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.executor.Executor` logger to see what happens under the covers in executors.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.Executor=INFO
```
====

When an executor is started the following INFO messages are printed out in the logs:

```
INFO Executor: Starting executor ID [executorId] on host [executorHostname]
INFO Executor: Using REPL class URI: http://192.168.1.4:56131
```

Executors use a daemon cached thread pool `Executor task launch worker-ID` for <<metrics,sending metrics>> and execute tasks.

(only for non-local mode) Executors initialize the application using `BlockManager.initialize()`.

CAUTION: FIXME What does `BlockManager.initialize()`?

Executors maintain a list of running tasks (in `runningTasks` as instances of `TaskRunner`).

When an executor starts you should see the following INFO message in the logs:

```
INFO Executor: Running task 0.0 in stage 2.0 (TID 8)
```

`TID` is the task's id being executed in `Executor task launch worker-8`.

You can later see the INFO log:

```
INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 2082 bytes result sent to driver
```

* Distributed workers
* Responsible for executing link:spark-taskscheduler.adoc#tasks[tasks]
* Responsible for storing any data that the user chooses to cache
* Can run many tasks in parallel

A worker requires the additional services (beside the common ones like ...):

* executorActorSystemName
* link:spark-rpc.adoc[RPC Environment] (for Akka only)
* link:spark-service-mapoutputtracker.adoc#MapOutputTrackerWorker[MapOutputTrackerWorker]
* link:spark-metrics.adoc[MetricsSystem] with the name `executor`

CAUTION: FIXME How many cores are assigned per executor?

=== [[resource-offers]] Resource Offers

Read link:spark-taskscheduler.adoc#resourceOffers[resourceOffers] in TaskSchedulerImpl and link:spark-tasksetmanager.adoc##resourceOffers[resourceOffer] in TaskSetManager.


=== [[metrics]] Metrics

Executors use link:spark-metrics.adoc[Metrics System] (via `ExecutorSource`) to report metrics about internal status.

NOTE: Metrics are only available for non-local mode.

The name of the source is *executor*.

It emits the following numbers:

* *threadpool.activeTasks* - the approximate number of threads that are actively executing tasks (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getActiveCount()])
* *threadpool.completeTasks* - the approximate total number of tasks that have completed execution (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getCompletedTaskCount()])
* *threadpool.currentPool_size* - the current number of threads in the pool (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getPoolSize()])
* *threadpool.maxPool_size* - the maximum allowed number of threads that have ever simultaneously been in the pool (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getMaximumPoolSize()])
* *filesystem.hdfs* / *read_bytes* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getBytesRead()`
* *filesystem.hdfs.write_bytes* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getBytesWritten()`
* *filesystem.hdfs.read_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getReadOps()`
* *filesystem.hdfs.largeRead_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getLargeReadOps()`
* *filesystem.hdfs.write_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getWriteOps()`
* *filesystem.file.read_bytes*
* *filesystem.file.write_bytes*
* *filesystem.file.read_ops*
* *filesystem.file.largeRead_ops*
* *filesystem.file.write_ops*

=== [[heartbeats-and-partial-metrics]] Sending heartbeats and partial metrics for active tasks

`driver-heartbeater` is a daemon single-thread scheduled pool executor, i.e. `ScheduledThreadPoolExecutor`, used by executor to send heartbeats and partial task metrics to a driver. They are sent to the driver every <<settings, spark.executor.heartbeatInterval>>.

.Executors use HeartbeatReceiver endpoint to report heartbeats and task metrics
image::images/executor-heartbeatReceiver-endpoint.png[align="center"]

The structure with the information is an array of `(Long, TaskMetrics)`.

CAUTION: FIXME `startDriverHeartbeater()` might be called too early in intialization of Executor before `heartbeater` internal field is initialized.

[CAUTION]
====
FIXME

* What's in `taskRunner.task.metrics`?
* What's in `Heartbeat`? Why is `blockManagerId` sent?
* What's in `RpcUtils.makeDriverRef`?
====

It creates an RPC endpoint for receiving RPCs from the driver.

=== [[settings]] Settings

* `spark.executor.cores` - the number of cores for an executor
* `spark.executor.extraClassPath` - a list of URLs representing the user classpath. Each entry is separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.
* `spark.executor.extraJavaOptions` - extra Java options for executors
* `spark.executor.extraLibraryPath` - a list of additional library paths separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.
* `spark.executor.userClassPathFirst` (default: `false`) controls whether to load classes in user jars before those in Spark jars.
* `spark.executor.heartbeatInterval` (default: `10s`) - the interval after which an executor reports heartbeat and metrics for active tasks to the driver. Refer to <<heartbeats-and-partial-metrics, Sending heartbeats and partial metrics for active tasks>>.
* `spark.executor.id`
* `spark.executor.instances` - the number of executors. When greater than `0`, it disables link:spark-dynamic-allocation.adoc[Dynamic Allocation].
* `spark.executor.logs.rolling.maxSize`
* `spark.executor.logs.rolling.maxRetainedFiles`
* `spark.executor.logs.rolling.strategy`
* `spark.executor.logs.rolling.time.interval`
* `spark.executor.memory` (default: `1024` mebibytes) - equivalent to `SPARK_EXECUTOR_MEMORY` (formerly and now deprecated `SPARK_MEM`).
* `spark.executor.port`
* `spark.executor.uri` - equivalent to `SPARK_EXECUTOR_URI`
* `spark.repl.class.uri` (default: `null`) used when in `spark-shell` to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code.
+
Enable `INFO` logging level for `org.apache.spark.executor.Executor` logger to have the value printed out to the logs:
+
```
INFO Using REPL class URI: [classUri]
```
* `spark.akka.frameSize` (default: `128` MB, maximum: `2047` MB) - the configured max frame size for Akka messages. If a task result is bigger, executors use link:spark-blockmanager.adoc[block manager] to send results back.
* `spark.driver.maxResultSize` (default: `1g`)

CAUTION: FIXME `spark.driver.maxResultSize` is used in few other pages so decide where it should belong to and link the other places.
