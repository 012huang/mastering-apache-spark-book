== Executors

*Executors* are distributed agents responsible for executing link:spark-taskscheduler-tasks.adoc[tasks]. They typically (?) run for the entire lifetime of a Spark application. Executors communicate with the driver to send computation results back.

CAUTION: FIXME image - see the images above

Executor offers are described by executor id and the host on which an executor runs (see <<resource-offers, Resource Offers>> in this document).

Executors provide in-memory storage for RDDs that are cached in Spark applications (via link:spark-blockmanager.adoc[Block Manager]).

When executors are started they register themselves with the driver and communicate directly to execute tasks.

Executors use a <<thread-pool, thread pool>> for <<metrics, sending metrics>> and <<launching-tasks, launching tasks>> (by means of <<TaskRunner, TaskRunner>>).

Each executor can run multiple tasks over its lifetime, both parallel and sequentially.

It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster.

An executor is described by id, hostname, environment (as `SparkEnv`), classpath, and whether it runs in link:spark-local.adoc[local] or link:spark-cluster.adoc[cluster mode].

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.executor.Executor` logger to see what happens in executors.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.Executor=INFO
```
====

When an executor is started the following INFO messages are printed out in the logs:

```
INFO Executor: Starting executor ID [executorId] on host [executorHostname]
INFO Executor: Using REPL class URI: http://192.168.1.4:56131
```

(only for non-local mode) Executors initialize the application using `BlockManager.initialize()`.

CAUTION: FIXME What does `BlockManager.initialize()`?

Executors maintain a list of running tasks (in `runningTasks` as instances of <<TaskRunner,TaskRunner>>).

A worker requires the additional services (beside the common ones like ...):

* executorActorSystemName
* link:spark-rpc.adoc[RPC Environment] (for Akka only)
* link:spark-service-mapoutputtracker.adoc#MapOutputTrackerWorker[MapOutputTrackerWorker]
* link:spark-metrics.adoc[MetricsSystem] with the name `executor`

CAUTION: FIXME How many cores are assigned per executor?

=== [[thread-pool]] Executor task launch worker Thread Pool

Executors use a daemon cached thread pool *Executor task launch worker-ID* for <<launching-tasks, launching tasks>>.

=== [[launching-tasks]] Launching Tasks

`Executor.launchTask` is the method to launch tasks. It accepts the context (as link:spark-executor-backends.adoc[ExecutorBackend]), attempt number, and the id, name and serialized form of the task.

It creates a <<TaskRunner,TaskRunner>> that is executed on <<thread-pool, Executor task launch worker Thread Pool>>.

TaskRunner is registered in `runningTasks` map.

=== [[TaskRunner]] TaskRunner

*TaskRunner* is a thread of execution that runs a task. It requires a link:spark-executor-backends.adoc[ExecutorBackend], task and attempt ids, task name, and serialized version of the task (as `ByteBuffer`). It then keeps sending updates about task execution to the executor backend.

When a `TaskRunner` starts running, it prints the following INFO message to the logs:

```
INFO Executor: Running [taskName] (TID [taskId])
```

`taskId` is the id of the task being executed in `Executor task launch worker-[taskId]`.

TaskRunner calls `Task.run` to execute a task.

CAUTION: FIXME Review `task.run()`

CAUTION: FIXME What's the cardinality between TaskRunner and Task?

CAUTION: FIXME TaskMemoryManager and task serialization and deserialization

It deserializes the task (using globally-configured `Serializer`) and sets the `TaskMemoryManager` for the task.

```
DEBUG Task [taskId]'s epoch is [task.epoch]
```

TaskRunner sends update of the epoch of the task to `MapOutputTracker`.

CAUTION: FIXME Why is `MapOutputTracker.updateEpoch` needed?

Task runs (with `taskId`, `attemptNumber`, and the globally-configured `MetricsSystem`). When a task finishes, it returns a value and `accumUpdates`.

CAUTION: FIXME What are `accumUpdates`?

The result value is serialized (using the other instance of `Serializer`, i.e. `serializer` - there are two `Serializer` instances in `SparkContext.env`).

A `DirectTaskResult` that contains the serialized result and `accumUpdates` is serialized.

If `maxResultSize` is set and the size of the serialized result exceeds the value, a SparkException is reported.

```
scala> sc.getConf.get("spark.driver.maxResultSize")
res5: String = 1m

scala> sc.parallelize(0 to 1024*1024+10, 1).collect
...
INFO DAGScheduler: Job 3 failed: collect at <console>:25, took 0.075073 s
org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1 tasks (4.0 MB) is bigger than spark.driver.maxResultSize (1024.0 KB)
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1430)
```

If however the size exceeds `akkaFrameSize`, ...FIXME.

A successful execution is "announced" as INFO to the logs:

```
INFO Executor: Finished [taskName] (TID [taskId]). [resultSize] bytes result sent to driver
```

The result is sent to the driver using `execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)`. FIXME

=== [[FetchFailedException]] FetchFailedException

`FetchFailedException` exception is thrown when an executor failed to fetch a shuffle block.

It contains the following:

* an unique identifier for a BlockManager (as `BlockManagerId`)
* `shuffleId`
* `mapId`
* `reduceId`
* `message` - a short exception message
* `cause` - a `Throwable` object

=== [[resource-offers]] Resource Offers

Read link:spark-taskscheduler.adoc#resourceOffers[resourceOffers] in TaskSchedulerImpl and link:spark-tasksetmanager.adoc##resourceOffers[resourceOffer] in TaskSetManager.

=== [[metrics]] Metrics

Executors use link:spark-metrics.adoc[Metrics System] (via `ExecutorSource`) to report metrics about internal status.

NOTE: Metrics are only available for cluster modes, i.e. `local` mode turns metrics off.

The name of the source is *executor*.

It emits the following numbers:

* *threadpool.activeTasks* - the approximate number of threads that are actively executing tasks (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getActiveCount()])
* *threadpool.completeTasks* - the approximate total number of tasks that have completed execution (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getCompletedTaskCount()])
* *threadpool.currentPool_size* - the current number of threads in the pool (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getPoolSize()])
* *threadpool.maxPool_size* - the maximum allowed number of threads that have ever simultaneously been in the pool (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getMaximumPoolSize()])
* *filesystem.hdfs* / *read_bytes* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getBytesRead()`
* *filesystem.hdfs.write_bytes* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getBytesWritten()`
* *filesystem.hdfs.read_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getReadOps()`
* *filesystem.hdfs.largeRead_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getLargeReadOps()`
* *filesystem.hdfs.write_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getWriteOps()`
* *filesystem.file.read_bytes*
* *filesystem.file.write_bytes*
* *filesystem.file.read_ops*
* *filesystem.file.largeRead_ops*
* *filesystem.file.write_ops*

=== [[heartbeats-and-partial-metrics]] Sending heartbeats and partial metrics for active tasks

`driver-heartbeater` is a daemon single-thread scheduled pool executor, i.e. `ScheduledThreadPoolExecutor`, used by executor to send heartbeats and partial task metrics to a driver. They are sent to the driver every <<settings, spark.executor.heartbeatInterval>>.

.Executors use HeartbeatReceiver endpoint to report heartbeats and task metrics
image::images/executor-heartbeatReceiver-endpoint.png[align="center"]

The structure with the information is an array of `(Long, TaskMetrics)`.

CAUTION: FIXME `startDriverHeartbeater()` might be called too early in intialization of Executor before `heartbeater` internal field is initialized.

[CAUTION]
====
FIXME

* What's in `taskRunner.task.metrics`?
* What's in `Heartbeat`? Why is `blockManagerId` sent?
* What's in `RpcUtils.makeDriverRef`?
====

It creates an RPC endpoint for receiving RPCs from the driver.

=== [[settings]] Settings

* `spark.executor.cores` - the number of cores for an executor
* `spark.executor.extraClassPath` - a list of URLs representing the user classpath. Each entry is separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.
* `spark.executor.extraJavaOptions` - extra Java options for executors
* `spark.executor.extraLibraryPath` - a list of additional library paths separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.
* `spark.executor.userClassPathFirst` (default: `false`) controls whether to load classes in user jars before those in Spark jars.
* `spark.executor.heartbeatInterval` (default: `10s`) - the interval after which an executor reports heartbeat and metrics for active tasks to the driver. Refer to <<heartbeats-and-partial-metrics, Sending heartbeats and partial metrics for active tasks>>.
* `spark.executor.id`
* `spark.executor.instances` - the number of executors. When greater than `0`, it disables link:spark-dynamic-allocation.adoc[Dynamic Allocation].
* `spark.executor.logs.rolling.maxSize`
* `spark.executor.logs.rolling.maxRetainedFiles`
* `spark.executor.logs.rolling.strategy`
* `spark.executor.logs.rolling.time.interval`
* `spark.executor.memory` (default: `1024` mebibytes) - equivalent to `SPARK_EXECUTOR_MEMORY` (formerly and now deprecated `SPARK_MEM`).
* `spark.executor.port`
* `spark.executor.uri` - equivalent to `SPARK_EXECUTOR_URI`
* `spark.repl.class.uri` (default: `null`) used when in `spark-shell` to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code.
+
Enable `INFO` logging level for `org.apache.spark.executor.Executor` logger to have the value printed out to the logs:
+
```
INFO Using REPL class URI: [classUri]
```
* `spark.akka.frameSize` (default: `128` MB, maximum: `2047` MB) - the configured max frame size for Akka messages. If a task result is bigger, executors use link:spark-blockmanager.adoc[block manager] to send results back.
* `spark.driver.maxResultSize` (default: `1g`)

CAUTION: FIXME `spark.driver.maxResultSize` is used in few other pages so decide where it should belong to and link the other places.
