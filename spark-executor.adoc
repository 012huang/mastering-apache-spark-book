== Executors

*Executors* are distributed agents responsible for link:spark-taskscheduler-tasks.adoc#execution[executing tasks]. They _typically_ (i.e. not always) run for the entire lifetime of a Spark application. Executors send <<heartbeats-and-active-task-metrics, active task metrics>> to a link:spark-driver.adoc[driver]. They also inform link:spark-executor-backends.adoc[executor backends] about task status updates (task results including).

Executors provide in-memory storage for RDDs that are cached in Spark applications (via link:spark-blockmanager.adoc[Block Manager]).

When executors are started they register themselves with the driver and communicate directly to execute tasks.

*Executor offers* are described by executor id and the host on which an executor runs (see <<resource-offers, Resource Offers>> in this document).

Executors use a <<thread-pool, thread pool>> for <<metrics, sending metrics>> and <<launching-tasks, launching tasks>> (by means of <<TaskRunner, TaskRunner>>).

Each executor can run multiple tasks over its lifetime, both in parallel and sequentially.

It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster.

Executors are described by their *id*, *hostname*, *environment* (as `SparkEnv`), and *classpath* (and, less importantly, and more for internal optimization, whether they run in link:spark-local.adoc[local] or link:spark-cluster.adoc[cluster mode]).

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.executor.Executor` logger to see what happens in executors.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.Executor=INFO
```
====

When an executor is started you should see the following INFO messages in the logs:

```
INFO Executor: Starting executor ID [executorId] on host [executorHostname]
INFO Executor: Using REPL class URI: http://[executorHostname]:56131
```

It creates an RPC endpoint for communication with the driver.

(only for non-local mode) Executors initialize the application using `BlockManager.initialize()`.

CAUTION: FIXME What does `BlockManager.initialize()`?

Executors maintain a mapping between task ids and running tasks (as instances of <<TaskRunner,TaskRunner>>) in `runningTasks` internal map.

A worker requires the additional services (beside the common ones like ...):

* executorActorSystemName
* link:spark-rpc.adoc[RPC Environment] (for Akka only)
* link:spark-service-mapoutputtracker.adoc#MapOutputTrackerWorker[MapOutputTrackerWorker]
* link:spark-metrics.adoc[MetricsSystem] with the name `executor`

CAUTION: FIXME How many cores are assigned per executor?

=== [[coarse-grained-executor]] Coarse-Grained Executors

*Coarse-grained executors* are executors that use link:spark-executor-backends-coarse-grained.adoc[CoarseGrainedExecutorBackend] for task scheduling.

=== [[launching-tasks]] Launching Tasks

`Executor.launchTask` creates a <<TaskRunner,TaskRunner>> that is executed on <<thread-pool, Executor task launch worker Thread Pool>>.

.Launching tasks on executor using TaskRunners
image::images/executor-taskrunner-executorbackend.png[align="center"]

It accepts the context (as link:spark-executor-backends.adoc[ExecutorBackend]), attempt number, and the id, name and serialized form of the task.

TaskRunner is registered in the internal `runningTasks` map.

=== [[heartbeats-and-active-task-metrics]] Heartbeats with Active Task Metrics

Executors keep sending <<metrics, active task metrics>> to a driver every <<settings, spark.executor.heartbeatInterval>>.

.Executors use HeartbeatReceiver endpoint to report task metrics
image::images/executor-heartbeatReceiver-endpoint.png[align="center"]

The structure sent is an array of `(Long, TaskMetrics)`.

[CAUTION]
====
FIXME

* What's in `taskRunner.task.metrics`?
* What's in `Heartbeat`? Why is `blockManagerId` sent?
* What's in `RpcUtils.makeDriverRef`?
====

Executors use internal `driver-heartbeater` daemon single-thread scheduled pool executor, i.e. `ScheduledThreadPoolExecutor`.

=== [[TaskRunner]] TaskRunner

*TaskRunner* is a thread of execution that runs a task. It requires a link:spark-executor-backends.adoc[ExecutorBackend], task and attempt ids, task name, and serialized version of the task (as `ByteBuffer`). It sends updates about task execution to the executor backend.

Sending task status updates to the executor backend is a mechanism to inform the executor backend about task being started (`TaskState.RUNNING`), task finish together with the serialized result (`TaskState.FINISHED`), task being killed (`TaskState.KILLED`) and task failures (`TaskState.FAILED`).

When a `TaskRunner` starts running, it prints the following INFO message to the logs:

```
INFO Executor: Running [taskName] (TID [taskId])
```

`taskId` is the id of the task being executed in `Executor task launch worker-[taskId]`.

TaskRunner link:spark-taskscheduler-tasks.adoc#execution[executes a task].

TaskRunner can run a single task only. When TaskRunner finishes, it is removed from `runningTasks`.

CAUTION: FIXME TaskMemoryManager and task serialization and deserialization

It deserializes the task (using globally-configured `Serializer`) and sets the `TaskMemoryManager` for the task.

```
DEBUG Task [taskId]'s epoch is [task.epoch]
```

TaskRunner sends update of the epoch of the task to `MapOutputTracker`.

CAUTION: FIXME Why is `MapOutputTracker.updateEpoch` needed?

Task runs (with `taskId`, `attemptNumber`, and the globally-configured `MetricsSystem`). When a task finishes, it returns a value and `accumUpdates`.

CAUTION: FIXME What are `accumUpdates`?

The result value is serialized (using the other instance of `Serializer`, i.e. `serializer` - there are two `Serializer` instances in `SparkContext.env`).

A `DirectTaskResult` that contains the serialized result and `accumUpdates` is serialized.

If `maxResultSize` is set and the size of the serialized result exceeds the value, a SparkException is reported.

```
scala> sc.getConf.get("spark.driver.maxResultSize")
res5: String = 1m

scala> sc.parallelize(0 to 1024*1024+10, 1).collect
...
INFO DAGScheduler: Job 3 failed: collect at <console>:25, took 0.075073 s
org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1 tasks (4.0 MB) is bigger than spark.driver.maxResultSize (1024.0 KB)
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1430)
```

If however the size exceeds `akkaFrameSize`, ...FIXME.

A successful execution is "announced" as INFO to the logs:

```
INFO Executor: Finished [taskName] (TID [taskId]). [resultSize] bytes result sent to driver
```

The serialized result is sent to the driver using `execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)`.

=== [[FetchFailedException]] FetchFailedException

CAUTION: FIXME

`FetchFailedException` exception is thrown when an executor (more specifically <<TaskRunner, TaskRunner>>) has failed to fetch a shuffle block.

It contains the following:

* the unique identifier for a BlockManager (as `BlockManagerId`)
* `shuffleId`
* `mapId`
* `reduceId`
* `message` - a short exception message
* `cause` - a `Throwable` object

TaskRunner catches it and informs link:spark-executor-backends.adoc[ExecutorBackend] about the case (using `statusUpdate` with `TaskState.FAILED` task state).

CAUTION: FIXME Image with the call to ExecutorBackend.

=== [[resource-offers]] Resource Offers

Read link:spark-taskschedulerimpl.adoc#resourceOffers[resourceOffers] in TaskSchedulerImpl and link:spark-tasksetmanager.adoc##resourceOffers[resourceOffer] in TaskSetManager.

=== [[thread-pool]] Executor task launch worker Thread Pool

Executors use a daemon cached thread pool *Executor task launch worker-ID* for <<launching-tasks, launching tasks>>.

=== [[memory]] Executor Memory

You can control the amount of memory per executor using <<settings, spark.executor.memory>> setting. It sets the available memory equally for all executors for the application.

NOTE: `spark.executor.memory` is looked up when link:spark-sparkcontext.adoc#initialization[creating a SparkContext].

You can however change the assigned memory per executor per node for link:spark-standalone.adoc[standalone cluster mode] using link:spark-sparkcontext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable.

You can find the value displayed as *Memory per Node* in link:spark-standalone-master.adoc[web UI for standalone Master].

CAUTION: FIXME screenshot

=== [[metrics]] Metrics

Executors use link:spark-metrics.adoc[Metrics System] (via `ExecutorSource`) to report metrics about internal status.

NOTE: Metrics are only available for cluster modes, i.e. `local` mode turns metrics off.

The name of the source is *executor*.

It emits the following numbers:

* *threadpool.activeTasks* - the approximate number of threads that are actively executing tasks (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getActiveCount()])
* *threadpool.completeTasks* - the approximate total number of tasks that have completed execution (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getCompletedTaskCount()])
* *threadpool.currentPool_size* - the current number of threads in the pool (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getPoolSize()])
* *threadpool.maxPool_size* - the maximum allowed number of threads that have ever simultaneously been in the pool (using http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getMaximumPoolSize()])
* *filesystem.hdfs* / *read_bytes* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getBytesRead()`
* *filesystem.hdfs.write_bytes* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getBytesWritten()`
* *filesystem.hdfs.read_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getReadOps()`
* *filesystem.hdfs.largeRead_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getLargeReadOps()`
* *filesystem.hdfs.write_ops* using https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics()] and `getWriteOps()`
* *filesystem.file.read_bytes*
* *filesystem.file.write_bytes*
* *filesystem.file.read_ops*
* *filesystem.file.largeRead_ops*
* *filesystem.file.write_ops*

=== [[settings]] Settings

* `spark.executor.cores` - the number of cores for an executor
* `spark.executor.extraClassPath` - a list of URLs representing the user classpath. Each entry is separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.
* `spark.executor.extraJavaOptions` - extra Java options for executors
* `spark.executor.extraLibraryPath` - a list of additional library paths separated by system-dependent path separator, i.e. `:` on Unix/MacOS systems and `;` on Microsoft Windows.
* `spark.executor.userClassPathFirst` (default: `false`) controls whether to load classes in user jars before those in Spark jars.
* `spark.executor.heartbeatInterval` (default: `10s`) - the interval after which an executor reports heartbeat and metrics for active tasks to the driver. Refer to <<heartbeats-and-partial-metrics, Sending heartbeats and partial metrics for active tasks>>.
* `spark.executor.id`
* `spark.executor.instances` - the number of executors. When greater than `0`, it disables link:spark-dynamic-allocation.adoc[Dynamic Allocation].
* `spark.executor.logs.rolling.maxSize`
* `spark.executor.logs.rolling.maxRetainedFiles`
* `spark.executor.logs.rolling.strategy`
* `spark.executor.logs.rolling.time.interval`
* `spark.executor.memory` (default: `1024` mebibytes) - the amount of memory to use per executor process (equivalent to link:spark-sparkcontext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable).
* `spark.executor.port`
* `spark.executor.uri` - equivalent to `SPARK_EXECUTOR_URI`
* `spark.repl.class.uri` (default: `null`) used when in `spark-shell` to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code.
+
Enable `INFO` logging level for `org.apache.spark.executor.Executor` logger to have the value printed out to the logs:
+
```
INFO Using REPL class URI: [classUri]
```
* `spark.akka.frameSize` (default: `128` MB, maximum: `2047` MB) - the configured max frame size for Akka messages. If a task result is bigger, executors use link:spark-blockmanager.adoc[block manager] to send results back.
* `spark.driver.maxResultSize` (default: `1g`)

CAUTION: FIXME `spark.driver.maxResultSize` is used in few other pages so decide where it should belong to and link the other places.
