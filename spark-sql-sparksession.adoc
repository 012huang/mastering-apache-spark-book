== [[SparkSession]] SparkSession -- Entry Point to Datasets

`SparkSession` is the entry point to programming Spark using the link:spark-sql-dataset.adoc[Dataset] and link:spark-sql-dataframe.adoc[DataFrame] API.

You should use the <<builder, builder>> method to create an instance of `SparkSession`.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .master("local[*]")
  .appName("My Spark Application")
  .getOrCreate()
----

The `private` more direct API to create a `SparkSession` requires a link:spark-sparkcontext.adoc[SparkContext] and an optional <<SharedState, SharedState>> (that represents the shared state across `SparkSession` instances).

[NOTE]
====
`SparkSession` has replaced link:spark-sql-sqlcontext.adoc[SQLContext] as of Spark *2.0.0*.
====

=== [[newSession]] newSession method

[source, scala]
----
newSession(): SparkSession
----

`newSession` creates a new `SparkSession` with the current link:spark-sparkcontext.adoc[SparkContext] and <<SharedState, SharedState>>.

NOTE: It is available as of *Spark 2.0.0*.

[source, scala]
----
scala> println(sc.version)
2.0.0-SNAPSHOT

scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
----

=== [[sharedState]] sharedState Attribute

`sharedState` points at the current <<SharedState, SharedState>>.

=== [[SharedState]] SharedState

`SharedState` represents the shared state across all active SQL sessions (i.e. <<SparkSession, SparkSession>> instances) by sharing link:spark-cachemanager.adoc[CacheManager], link:spark-sql-sqlcontext.adoc#SQLListener[SQLListener], and `ExternalCatalog`.

There are two implementations of `SharedState`:

* `org.apache.spark.sql.internal.SharedState` (default)
* `org.apache.spark.sql.hive.HiveSharedState`

You can select `SharedState` for the active `SparkSession` using  link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] setting.

`SharedState` is created lazily, i.e. when first accessed after <<creating-instance, `SparkSession` is created>>. It can happen when a <<newSession, new session is created>> or when the shared services are accessed. It is created with a link:spark-sparkcontext.adoc[SparkContext].

=== [[creating-instance]] Creating SparkSession Instance

CAUTION: FIXME

=== [[createDataset]] Creating Datasets (createDataset methods)

[source, scala]
----
createDataset[T: Encoder](data: Seq[T]): Dataset[T]
createDataset[T: Encoder](data: RDD[T]): Dataset[T]

// For Java
createDataset[T: Encoder](data: java.util.List[T]): Dataset[T]
----

`createDataset` is an experimental API to create a link:spark-sql-dataset.adoc[Dataset] from a local Scala collection, i.e. `Seq[T]` or Java's `List[T]`, or an `RDD[T]`.

[source, scala]
----
val ints = spark.createDataset(0 to 9)
----

=== [[read]] Accessing DataFrameReader (read method)

[source, scala]
----
read: DataFrameReader
----

`read` method returns a link:spark-sql-dataframereader.adoc[DataFrameReader] that is used to read data from external storage systems and load it into a `DataFrame`.

[source, scala]
----
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
----

=== [[conf]] Runtime Configuration (conf attribute)

[source, scala]
----
conf: RuntimeConfig
----

`conf` returns the current runtime configuration (as `RuntimeConfig`) that wraps link:spark-sql-SQLConf.adoc[SQLConf].

CAUTION: FIXME

=== [[sessionState]] sessionState

`sessionState` is a transient lazy value that represents the current link:spark-sql-sessionstate.adoc[SessionState].

`sessionState` is a lazily-created value based on the internal <<spark.sql.catalogImplementation, spark.sql.catalogImplementation>> setting that can be:

* `org.apache.spark.sql.hive.HiveSessionState` when the setting is `hive`
* `org.apache.spark.sql.internal.SessionState` for `in-memory`.

=== [[sql]] Executing SQL (sql method)

[source, scala]
----
sql(sqlText: String): DataFrame
----

`sql` executes the `sqlText` SQL statement.

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

CAUTION: FIXME See link:spark-sql-sqlcontext.adoc#sql[Executing SQL Queries].

=== [[builder]] Creating SessionBuilder (builder method)

[source, scala]
----
builder(): Builder
----

`SessionBuilder.builder` method creates a new `SparkSession.Builder` to build a `SparkSession` off it using a fluent API.

NOTE: It is available since Spark *2.0.0*.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
----

=== [[settings]] Settings

[[spark.sql.catalogImplementation]]
* `spark.sql.catalogImplementation` (default: `in-memory`) - internal setting with two possible values: `hive` and `in-memory`.
