== SparkSession

[NOTE]
====
`SparkSession` has replaced link:spark-sql-sqlcontext.adoc[SQLContext] as of Spark *2.0.0*.
====

=== [[createDataset]] Creating Datasets (createDataset methods)

[source, scala]
----
createDataset[T: Encoder](data: Seq[T]): Dataset[T]
createDataset[T: Encoder](data: RDD[T]): Dataset[T]
createDataset[T: Encoder](data: java.util.List[T]): Dataset[T]
----

`createDataset` is an experimental API to create a link:spark-sql-dataset.adoc[Dataset] from a local collection, i.e. `Seq[T]` or Java's `List[T]`, or an `RDD[T]`.

[source, scala]
----
val ints = spark.createDataset(0 to 9)
----

=== [[read]][[accessing-DataFrameReader]] Accessing DataFrameReader (read method)

[source, scala]
----
read: DataFrameReader
----

`read` method returns a link:spark-sql-dataframereader.adoc[DataFrameReader] that is used to read data from external storage systems and load it into a `DataFrame`.

[source, scala]
----
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
----

=== [[conf]] Runtime Configuration (conf attribute)

[source, scala]
----
conf: RuntimeConfig
----

`conf` returns the current runtime configuration (as `RuntimeConfig`) that wraps link:spark-sql-SQLConf.adoc[SQLConf].

CAUTION: FIXME

=== [[sessionState]] sessionState

`sessionState` is a transient lazy value that represents the current link:spark-sql-sessionstate.adoc[SessionState].

`sessionState` is a lazily-created value based on the internal <<spark.sql.catalogImplementation, spark.sql.catalogImplementation>> setting that can be:

* `org.apache.spark.sql.hive.HiveSessionState` when the setting is `hive`
* `org.apache.spark.sql.internal.SessionState` for `in-memory`.

=== [[sql]] Executing SQL (sql method)

[source, scala]
----
sql(sqlText: String): DataFrame
----

`sql` executes the `sqlText` SQL statement.

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

CAUTION: FIXME See link:spark-sql-sqlcontext.adoc#sql[Executing SQL Queries].

=== [[settings]] Settings

[[spark.sql.catalogImplementation]]
* `spark.sql.catalogImplementation` (default: `in-memory`) - internal setting with two possible values: `hive` and `in-memory`.
