== [[ShuffleMapStage]] `ShuffleMapStage` -- Intermediate Stage in Job

A *ShuffleMapStage* (aka *shuffle map stage*, or simply *map stage*) is an intermediate stage in the execution DAG that produces data for link:spark-rdd-shuffle.adoc[shuffle operation]. It is an input for the other following stages in the DAG of stages. That is why it is also called a *shuffle dependency's map side*.

TIP: Read about link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency].

A `ShuffleMapStage` may contain multiple *pipelined operations*, e.g. `map` and `filter`, before shuffle operation.

CAUTION: FIXME: Show the example and the logs + figures

A `ShuffleMapStage` can be part of many jobs -- refer to the section <<stage-sharing, `ShuffleMapStage` sharing>>.

A `ShuffleMapStage` is a stage with a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] -- the shuffle that it is part of and `outputLocs` and `numAvailableOutputs` track how many map outputs are ready.

NOTE: ``ShuffleMapStage``s can also be submitted independently as jobs with `DAGScheduler.submitMapStage` for link:spark-dagscheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling].

[[isAvailable]]
When executed, a `ShuffleMapStage` saves *map output files* that can later be fetched by reduce tasks. When all map outputs are available, the `ShuffleMapStage` is considered *available* (or *ready*).

CAUTION: FIXME Figure with ShuffleMapStages saving files

The output locations (`outputLocs`) of a `ShuffleMapStage` are the same as used by its link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]. Output locations can be missing, i.e. partitions have not been cached or are lost.

A `ShuffleMapStage` is registered to DAGScheduler that tracks the mapping of shuffles (by their ids from SparkContext) to corresponding ShuffleMapStages that compute them, stored in `shuffleToMapStage`.

A `ShuffleMapStage` is created from an input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and a job's id (in `DAGScheduler#newOrUsedShuffleStage`).

CAUTION: FIXME Where's `shuffleToMapStage` used?

* getShuffleMapStage - see <<stage-sharing, Stage sharing>>
* getAncestorShuffleDependencies

When there is no `ShuffleMapStage` for a shuffle id (of a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]), one is created with the ancestor shuffle dependencies of the RDD (of a `ShuffleDependency`) that are registered to link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster].

FIXME Where is `ShuffleMapStage` used?

* newShuffleMapStage - the proper way to create shuffle map stages (with the additional setup steps)
* `getShuffleMapStage` - see <<stage-sharing, Stage sharing>>

[CAUTION]
====
FIXME

* `newShuffleMapStage`
====

[[internal-registries]]
.`ShuffleMapStage` Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[outputLocs]] `outputLocs`
| Tracks link:spark-MapStatus.adoc[MapStatuses] for each partition.

There could be many `MapStatus` entries per partition due to link:spark-taskschedulerimpl-speculative-execution.adoc[Speculative Execution of Tasks].

When <<creating-instance, `ShuffleMapStage` is created>>, `outputLocs` is empty, i.e. all elements are empty lists.

The size of `outputLocs` is exactly the number of partitions of the link:spark-dagscheduler-stages.adoc#rdd[RDD the stage runs on].

| [[_numAvailableOutputs]] `_numAvailableOutputs`
| The number of available outputs for the partitions.

`_numAvailableOutputs` increments when the <<addOutputLoc, first `MapStatus` is registered for a partition>> and decrements when the <<removeOutputLoc, last `MapStatus` is removed for a partition>>.

`_numAvailableOutputs` should not be greater than the number of partitions (and hence the number of `MapStatus` collections in <<outputLocs, `outputLocs` internal registry>>).

|===

=== [[removeOutputsOnExecutor]] `removeOutputsOnExecutor` Method

CAUTION: FIXME

=== [[outputLocInMapOutputTrackerFormat]] `outputLocInMapOutputTrackerFormat` Method

CAUTION: FIXME

=== [[addActiveJob]] `addActiveJob` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `ShuffleMapStage` Instance

CAUTION: FIXME

`ShuffleMapStage` <<internal-registries, initializes the internal registries and counters>>.

=== [[mapStageJobs]] `mapStageJobs` Method

CAUTION: FIXME

=== [[shuffleDep]] `shuffleDep` Property

CAUTION: FIXME

=== [[removeActiveJob]] `removeActiveJob` Method

CAUTION: FIXME

=== [[addOutputLoc]] Registering `MapStatus` For Partition -- `addOutputLoc` Method

[source, scala]
----
addOutputLoc(partition: Int, status: MapStatus): Unit
----

`addOutputLoc` adds the input `status` to the <<outputLocs, output locations>> for the input `partition`.

`addOutputLoc` increments <<_numAvailableOutputs, `_numAvailableOutputs` internal counter>> if the input `MapStatus` is the first result for the `partition`.

NOTE: `addOutputLoc` is used when link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a `ShuffleMapStage` for a `ShuffleDependency` and a `ActiveJob`] (and link:spark-service-MapOutputTrackerMaster.adoc#containsShuffle[`MapOutputTrackerMaster` tracks some output locations of the `ShuffleDependency`]) and when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ShuffleMapTask[`ShuffleMapTask` has finished].

=== [[removeOutputLoc]] Removing `MapStatus` For Partition And BlockManager -- `removeOutputLoc` Method

[source, scala]
----
removeOutputLoc(partition: Int, bmAddress: BlockManagerId): Unit
----

`removeOutputLoc` removes the `MapStatus` for the input `partition` and `bmAddress` link:spark-blockmanager.adoc[BlockManager] from the <<outputLocs, output locations>>.

`removeOutputLoc` decrements <<_numAvailableOutputs, `_numAvailableOutputs` internal counter>> if the the removed `MapStatus` was the last result for the `partition`.

NOTE: `removeOutputLoc` is exclusively used when a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[`Task` has finished with `FetchFailed` failure].

=== [[findMissingPartitions]] Finding Missing Partitions -- `findMissingPartitions` Method

[source, scala]
----
findMissingPartitions(): Seq[Int]
----

NOTE: `findMissingPartitions` is a part of link:spark-dagscheduler-stages.adoc#contract[`Stage` contract] that returns the partitions that are missing, i.e. are yet to be computed.

Internally, `findMissingPartitions` uses <<outputLocs, `outputLocs` internal registry>> to find indices with empty lists of `MapStatus`.

=== [[stage-sharing]] `ShuffleMapStage` Sharing

A `ShuffleMapStage` can be shared across multiple jobs, if these jobs reuse the same RDDs.

When a `ShuffleMapStage` is submitted to DAGScheduler to execute, `getShuffleMapStage` is called.

[source, scala]
----
scala> val rdd = sc.parallelize(0 to 5).map((_,1)).sortByKey()  // <1>

scala> rdd.count  // <2>

scala> rdd.count  // <3>
----
<1> Shuffle at `sortByKey()`
<2> Submits a job with two stages with two being executed
<3> Intentionally repeat the last action that submits a new job with two stages with one being shared as already-being-computed

.Skipped Stages are already-computed ShuffleMapStages
image::images/dagscheduler-webui-skipped-stages.png[align="center"]
