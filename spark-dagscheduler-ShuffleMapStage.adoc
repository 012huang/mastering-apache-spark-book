== [[ShuffleMapStage]] ShuffleMapStage -- Intermediate Stage in Execution DAG

`ShuffleMapStage` (aka *shuffle map stage* or simply *map stage*) is an link:spark-dagscheduler-stages.adoc[intermediate stage] in the *physical execution DAG* that produces data for link:spark-rdd-shuffle.adoc[shuffle operation]. `ShuffleMapStage` is an input for the other following stages in the DAG of stages and is also called a *shuffle dependency's map side*.

TIP: Read about link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency].

NOTE: The *logical DAG* or *logical execution plan* is the link:spark-rdd-lineage.adoc[RDD lineage].

A `ShuffleMapStage` may contain multiple *pipelined operations*, e.g. `map` and `filter`, before shuffle operation.

CAUTION: FIXME: Show the example and the logs + figures

A single `ShuffleMapStage` <<stage-sharing, can be part of many jobs>>.

A `ShuffleMapStage` is a stage with a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] -- the shuffle that it is part of and `outputLocs` and `numAvailableOutputs` track how many map outputs are ready.

NOTE: `ShuffleMapStage` can also be submitted independently as a Spark job with link:spark-dagscheduler.adoc#submitMapStage[DAGScheduler.submitMapStage] for link:spark-dagscheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling].

[[isAvailable]]
When executed, a `ShuffleMapStage` saves *map output files* that can later be fetched by reduce tasks. When all map outputs are available, the `ShuffleMapStage` is considered *available* (or *ready*).

CAUTION: FIXME Figure with ShuffleMapStages saving files

The output locations (`outputLocs`) of a `ShuffleMapStage` are the same as used by its link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]. Output locations can be missing, i.e. partitions have not been cached or are lost.

A `ShuffleMapStage` is registered to DAGScheduler that tracks the mapping of shuffles (by their ids from SparkContext) to corresponding ShuffleMapStages that compute them, stored in `shuffleToMapStage`.

A `ShuffleMapStage` is created from an input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and a job's id (in `DAGScheduler#newOrUsedShuffleStage`).

CAUTION: FIXME Where's `shuffleToMapStage` used?

* getShuffleMapStage - see <<stage-sharing, Stage sharing>>
* getAncestorShuffleDependencies

When there is no `ShuffleMapStage` for a shuffle id (of a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]), one is created with the ancestor shuffle dependencies of the RDD (of a `ShuffleDependency`) that are registered to link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster].

FIXME Where is `ShuffleMapStage` used?

* newShuffleMapStage - the proper way to create shuffle map stages (with the additional setup steps)
* `getShuffleMapStage` - see <<stage-sharing, Stage sharing>>

[[internal-registries]]
.`ShuffleMapStage` Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[_mapStageJobs]] `_mapStageJobs`
| FIXME

| [[outputLocs]] `outputLocs`
| Tracks link:spark-MapStatus.adoc[MapStatuses] for each partition.

There could be many `MapStatus` entries per partition due to link:spark-taskschedulerimpl-speculative-execution.adoc[Speculative Execution of Tasks].

When <<creating-instance, `ShuffleMapStage` is created>>, `outputLocs` is empty, i.e. all elements are empty lists.

The size of `outputLocs` is exactly the number of partitions of the link:spark-dagscheduler-stages.adoc#rdd[RDD the stage runs on].

| [[_numAvailableOutputs]] `_numAvailableOutputs`
| The number of available outputs for the partitions.

`_numAvailableOutputs` is incremented when the <<addOutputLoc, first `MapStatus` is registered for a partition>> (that could be more tasks per partition) and decrements when the <<removeOutputLoc, last `MapStatus` is removed for a partition>>.

`_numAvailableOutputs` should not be greater than the number of partitions (and hence the number of `MapStatus` collections in <<outputLocs, outputLocs>> internal registry).

|===

=== [[creating-instance]] Creating ShuffleMapStage Instance

`ShuffleMapStage` takes the following when created:

1. `id` identifier
2. [[rdd]] `rdd` -- the link:spark-rdd-ShuffleDependency.adoc#rdd[`RDD` of `ShuffleDependency`]
3. `numTasks` -- the number of tasks (that is exactly the link:spark-rdd.adoc#partitions[number of partitions in the `rdd`])
4. `parents` -- the collection of parent link:spark-dagscheduler-stages.adoc[Stages]
5. `firstJobId` -- the link:spark-dagscheduler-jobs.adoc[ActiveJob] that created it
6. `callSite` -- the creationSite of the <<rdd, RDD>>
7. [[shuffleDep]] `shuffleDep` -- link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] (from the link:spark-rdd-lineage.adoc[logical execution plan])

`ShuffleMapStage` initializes the <<internal-registries, internal registries and counters>>.

NOTE: `DAGScheduler` tracks the link:spark-dagscheduler.adoc#nextStageId[number of `ShuffleMapStage` created] so far.

NOTE: `ShuffleMapStage` is created only when link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` is requested for one given `ShuffleDependency`].

=== [[addOutputLoc]] Registering MapStatus For Partition -- `addOutputLoc` Method

[source, scala]
----
addOutputLoc(partition: Int, status: MapStatus): Unit
----

`addOutputLoc` adds the input `status` to the <<outputLocs, output locations>> for the input `partition`.

`addOutputLoc` increments <<_numAvailableOutputs, `_numAvailableOutputs` internal counter>> if the input `MapStatus` is the first result for the `partition`.

NOTE: `addOutputLoc` is used when link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a `ShuffleMapStage` for a `ShuffleDependency` and a `ActiveJob`] (and link:spark-service-MapOutputTrackerMaster.adoc#containsShuffle[`MapOutputTrackerMaster` tracks some output locations of the `ShuffleDependency`]) and when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ShuffleMapTask[`ShuffleMapTask` has finished].

=== [[removeOutputLoc]] Removing MapStatus For Partition And BlockManager -- `removeOutputLoc` Method

[source, scala]
----
removeOutputLoc(partition: Int, bmAddress: BlockManagerId): Unit
----

`removeOutputLoc` removes the `MapStatus` for the input `partition` and `bmAddress` link:spark-blockmanager.adoc[BlockManager] from the <<outputLocs, output locations>>.

`removeOutputLoc` decrements <<_numAvailableOutputs, `_numAvailableOutputs` internal counter>> if the the removed `MapStatus` was the last result for the `partition`.

NOTE: `removeOutputLoc` is exclusively used when a link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[`Task` has failed with `FetchFailed` exception].

=== [[findMissingPartitions]] Finding Missing Partitions -- `findMissingPartitions` Method

[source, scala]
----
findMissingPartitions(): Seq[Int]
----

NOTE: `findMissingPartitions` is a part of link:spark-dagscheduler-stages.adoc#contract[`Stage` contract] that returns the partitions that are missing, i.e. are yet to be computed.

Internally, `findMissingPartitions` uses <<outputLocs, `outputLocs` internal registry>> to find indices with empty lists of `MapStatus`.

=== [[stage-sharing]] ShuffleMapStage Sharing

A `ShuffleMapStage` can be shared across multiple jobs, if these jobs reuse the same RDDs.

When a `ShuffleMapStage` is submitted to DAGScheduler to execute, `getShuffleMapStage` is called.

[source, scala]
----
scala> val rdd = sc.parallelize(0 to 5).map((_,1)).sortByKey()  // <1>

scala> rdd.count  // <2>

scala> rdd.count  // <3>
----
<1> Shuffle at `sortByKey()`
<2> Submits a job with two stages with two being executed
<3> Intentionally repeat the last action that submits a new job with two stages with one being shared as already-being-computed

.Skipped Stages are already-computed ShuffleMapStages
image::images/dagscheduler-webui-skipped-stages.png[align="center"]

=== [[numAvailableOutputs]] Returning Number of Available Shuffle Map Outputs -- `numAvailableOutputs` Method

[source, scala]
----
numAvailableOutputs: Int
----

`numAvailableOutputs` returns <<_numAvailableOutputs, _numAvailableOutputs>> internal registry.

NOTE: `numAvailableOutputs` is used exclusively when link:spark-dagscheduler.adoc#submitMissingTasks[`DAGScheduler` submits missing tasks for `ShuffleMapStage`] (and only to print a DEBUG message when the `ShuffleMapStage` is finished).

=== [[mapStageJobs]] Returning Collection of Active Jobs -- `mapStageJobs` Method

[source, scala]
----
mapStageJobs: Seq[ActiveJob]
----

`mapStageJobs` returns <<_mapStageJobs, _mapStageJobs>> internal registry.

NOTE: `mapStageJobs` is used exclusively when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[`DAGScheduler` is notified that a `ShuffleMapTask` has finished successfully] (and the task made `ShuffleMapStage` completed and so marks any map-stage jobs waiting on this stage as finished).

=== [[addActiveJob]] Registering Job (that Computes ShuffleDependency) -- `addActiveJob` Method

[source, scala]
----
addActiveJob(job: ActiveJob): Unit
----

`addActiveJob` registers the input link:spark-dagscheduler-jobs.adoc[ActiveJob] in <<_mapStageJobs, _mapStageJobs>> internal registry.

NOTE: The `ActiveJob` is added as the first element in `_mapStageJobs`.

NOTE: `addActiveJob` is used exclusively when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[`DAGScheduler` is notified that a `ShuffleDependency` was submitted] (and so a new `ActiveJob` is created to compute it).

=== [[removeActiveJob]] `removeActiveJob` Method

[source, scala]
----
removeActiveJob(job: ActiveJob): Unit
----

`removeActiveJob` removes a `ActiveJob` from <<_mapStageJobs, _mapStageJobs>> internal registry.

NOTE: `removeActiveJob` is used exclusively when link:spark-dagscheduler.adoc#cleanupStateForJobAndIndependentStages[`DAGScheduler` cleans up after `ActiveJob` has finished] (regardless of the outcome).

=== [[removeOutputsOnExecutor]] Removing All Shuffle Outputs Registered for Lost Executor -- `removeOutputsOnExecutor` Method

[source, scala]
----
removeOutputsOnExecutor(execId: String): Unit
----

`removeOutputsOnExecutor` removes all `MapStatuses` with the input `execId` executor from the <<outputLocs, outputLocs>> internal registry (of `MapStatuses` per partition).

If the input `execId` had the last registered `MapStatus` for a partition, `removeOutputsOnExecutor` decrements <<_numAvailableOutputs, _numAvailableOutputs>> counter and you should see the following INFO message in the logs:

```
INFO [stage] is now unavailable on executor [execId] ([_numAvailableOutputs]/[numPartitions], [isAvailable])
```

NOTE: `removeOutputsOnExecutor` is used exclusively when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[`DAGScheduler` cleans up after a lost executor].

=== [[outputLocInMapOutputTrackerFormat]] Preparing Shuffle Map Outputs in MapOutputTracker Format -- `outputLocInMapOutputTrackerFormat` Method

[source, scala]
----
outputLocInMapOutputTrackerFormat(): Array[MapStatus]
----

`outputLocInMapOutputTrackerFormat` returns the first (if available) element for every partition from <<outputLocs, outputLocs>> internal registry. If there is no entry for a partition, that position is filled with `null`.

[NOTE]
====
`outputLocInMapOutputTrackerFormat` is used when `DAGScheduler` is link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[notified that a `ShuffleMapTask` has finished successfully] (and the corresponding `ShuffleMapStage` is complete) and link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[cleans up after a lost executor].

In both cases, `outputLocInMapOutputTrackerFormat` is used to link:spark-service-MapOutputTrackerMaster.adoc#registerMapOutputs[register the shuffle map outputs (of the `ShuffleDependency`) with `MapOutputTrackerMaster`].
====
