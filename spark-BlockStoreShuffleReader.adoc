== [[BlockStoreShuffleReader]] BlockStoreShuffleReader

`BlockStoreShuffleReader` is the one and only `ShuffleReader` that fetches and <<read, reads the partitions>> (in range [`startPartition`, `endPartition`)) from a shuffle by requesting them from other nodes' block stores.

`BlockStoreShuffleReader` is <<creating-instance, created>> when the link:spark-SortShuffleManager.adoc#getReader[default `SortShuffleManager` is requested for a `ShuffleReader`] (for a `ShuffleHandle`).

=== [[read]] Reading Combined Key-Values For Reduce Task -- `read` Method

[source, scala]
----
read(): Iterator[Product2[K, C]]
----

NOTE: `read` is a part of ShuffleReader contract.

CAUTION: FIXME

Internally, `read` creates a `ShuffleBlockFetcherIterator`, a `SerializerInstance` (as link:spark-rdd-ShuffleDependency.adoc#serializer[defined for `ShuffleDependency`])...FIXME

=== [[creating-instance]] Creating BlockStoreShuffleReader Instance

`BlockStoreShuffleReader` takes:

1. link:spark-BaseShuffleHandle.adoc[BaseShuffleHandle]
2. `startPartition` and `endPartition` partition indices
3. link:spark-taskscheduler-taskcontext.adoc[TaskContext]
4. (optional) link:spark-SerializerManager.adoc[SerializerManager]
5. (optional) link:spark-blockmanager.adoc[BlockManager]
6. (optional) link:spark-service-mapoutputtracker.adoc[MapOutputTracker]

NOTE: `BlockStoreShuffleReader` uses link:spark-sparkenv.adoc[`SparkEnv` to define the optional `SerializerManager`, `BlockManager` and `MapOutputTracker`].

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_reducer_maxSizeInFlight]] `spark.reducer.maxSizeInFlight`
| `48m` (MB)
|

| [[spark_reducer_maxReqsInFlight]] `spark.reducer.maxReqsInFlight`
| `Int.MaxValue`
|

| [[spark_shuffle_detectCorrupt]] `spark.shuffle.detectCorrupt`
| `true`
|

|===
