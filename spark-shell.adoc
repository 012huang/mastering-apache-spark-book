== Spark shell

*Spark shell* is an interactive shell for learning about Apache Spark, ad-hoc queries and developing Spark applications. It is based on Scala REPL with automatic instantiation of link:spark-sparkcontext.adoc[Spark context] as `sc` and link:spark-sql.adoc[SQL context] as `sqlContext`.

It is a very convenient tool for many things available in Spark and one of the many reasons why Spark is so helpful even for simple tasks (see link:spark-overview.adoc#why-spark[Why Spark]).

=== Using Spark shell

You can start Spark shell using `spark-shell` script (available in `bin` directory).

```
$ ./bin/spark-shell
Spark context available as sc.
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

To close Spark shell, you press `Ctrl+D` or type in `:q` (or any subset of `:quit`).

```
scala> :quit
```

=== Learning Spark interactively

One way to learn about a tool like *the Spark shell* is to read its error messages. Together with the source code it may be a viable tool to reach mastery.

Let's give it a try using `spark-shell`.

While trying it out using an incorrect value for the master's URL, you're told about `--help` and `--verbose` options.

```
➜  spark git:(master) ✗ ./bin/spark-shell --master mss
Error: Master must start with yarn, spark, mesos, or local
Run with --help for usage help or --verbose for debug output
```

You're also told about link:spark-deployment-modes.adoc#master-urls[the acceptable values for `--master`].

Let's see what `--verbose` gives us.

```
➜  spark git:(master) ✗ ./bin/spark-shell --verbose --master mss
Using properties file: null
Parsed arguments:
  master                  mss
  deployMode              null
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          null
  driverMemory            null
  driverCores             null
  driverExtraClassPath    null
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               org.apache.spark.repl.Main
  primaryResource         spark-shell
  name                    Spark shell
  childArgs               []
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file null:



Error: Master must start with yarn, spark, mesos, or local
Run with --help for usage help or --verbose for debug output
```

TIP: These `null`'s could instead be replaced with some other, more meaningful values.
