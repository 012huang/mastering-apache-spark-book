== Driver

A Spark *driver* is the process that creates and owns an instance of link:spark-sparkcontext.adoc[SparkContext]. It is your Spark application that launches the `main` method in which the instance of SparkContext is created. It is the cockpit of jobs and tasks execution (using link:spark-dagscheduler.adoc[DAGScheduler] and link:spark-taskscheduler.adoc[Task Scheduler]). It hosts link:spark-webui.adoc[Web UI] for the environment.

.Driver with the services
image::images/spark-driver.png[align="center"]

It splits a Spark application into tasks and schedules them to run on executors.

A driver is where the task scheduler lives and spawns tasks across workers.

A driver coordinates workers and overall execution of tasks.

Driver requires the additional services (beside the common ones like link:spark-shuffle-service.adoc[ShuffleManager], MemoryManager, BlockTransferService, link:spark-service-broadcastmanager.adoc[BroadcastManager], link:spark-cachemanager.adoc[CacheManager]):

* Listener Bus
* driverActorSystemName
* link:spark-rpc.adoc[RPC Environment] (for Netty and Akka)
* link:spark-service-mapoutputtracker.adoc#MapOutputTrackerMaster[MapOutputTrackerMaster] with the name *MapOutputTracker*
* link:spark-blockmanager.adoc#BlockManagerMaster[BlockManagerMaster] with the name *BlockManagerMaster*
* link:spark-http-file-server.adoc[HttpFileServer]
* link:spark-metrics.adoc[MetricsSystem] with the name *driver*
* link:spark-service-outputcommitcoordinator.adoc[OutputCommitCoordinator] with the endpoint's name *OutputCommitCoordinator*

CAUTION: FIXME Diagram of RpcEnv for a driver (and later executors). Perhaps it should be in the notes about RpcEnv?
