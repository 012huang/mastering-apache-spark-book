== Driver

A *Spark driver* (_aka_ *an application's driver process*) is the separate Java process (running on its own JVM) that manages a link:spark-sparkcontext.adoc[SparkContext] in a Spark application.

It can be your Spark application that executes the `main` method in which the `SparkContext` object is created (`client` deploy mode), but can also be a process in a cluster (if executed in `cluster` deploy mode).

It is the cockpit of jobs and tasks execution (using link:spark-dagscheduler.adoc[DAGScheduler] and link:spark-taskscheduler.adoc[Task Scheduler]). It hosts link:spark-webui.adoc[Web UI] for the environment.

.Driver with the services
image::images/spark-driver.png[align="center"]

It splits a Spark application into tasks and schedules them to run on executors.

A driver is where the task scheduler lives and spawns tasks across workers.

A driver coordinates workers and overall execution of tasks.

NOTE: link:spark-shell.adoc[Spark shell] is a Spark application and the driver. It creates a `SparkContext` that is available as `sc`.

Driver requires the additional services (beside the common ones like link:spark-shuffle-manager.adoc[ShuffleManager], link:spark-MemoryManager.adoc[MemoryManager], link:spark-blocktransferservice.adoc[BlockTransferService], link:spark-service-broadcastmanager.adoc[BroadcastManager], link:spark-cachemanager.adoc[CacheManager]):

* Listener Bus
* driverActorSystemName
* link:spark-rpc.adoc[RPC Environment] (for Netty and Akka)
* link:spark-service-mapoutputtracker.adoc#MapOutputTrackerMaster[MapOutputTrackerMaster] with the name *MapOutputTracker*
* spark-BlockManagerMaster.adoc[BlockManagerMaster] with the name *BlockManagerMaster*
* link:spark-http-file-server.adoc[HttpFileServer]
* link:spark-metrics.adoc[MetricsSystem] with the name *driver*
* link:spark-service-outputcommitcoordinator.adoc[OutputCommitCoordinator] with the endpoint's name *OutputCommitCoordinator*

CAUTION: FIXME Diagram of RpcEnv for a driver (and later executors). Perhaps it should be in the notes about RpcEnv?

* High-level control flow of work
* Your Spark application runs as long as the Spark driver.
** Once the driver terminates, so does your Spark application.
* Creates `SparkContext`, `RDD`'s, and executes transformations and actions
* Launches link:spark-taskscheduler-tasks.adoc[tasks]

=== [[settings]] Settings

==== [[spark.driver.libraryPath]] spark.driver.libraryPath

`spark.driver.libraryPath`

==== [[spark.driver.extraLibraryPath]] spark.driver.extraLibraryPath

`spark.driver.extraLibraryPath`

==== [[spark.driver.extraJavaOptions]] spark.driver.extraJavaOptions

`spark.driver.extraJavaOptions`

==== [[spark.driver.appUIAddress]] spark.driver.appUIAddress

`spark.driver.appUIAddress` is only used in link:yarn/README.adoc[Spark on YARN]. It is set when link:spark-yarn-client-yarnclientschedulerbackend.adoc#start[YarnClientSchedulerBackend starts] to link:spark-yarn-applicationmaster.adoc#runExecutorLauncher[run ExecutorLauncher] (and link:spark-yarn-applicationmaster.adoc#registerAM[register ApplicationMaster] for the Spark application).

==== [[spark.driver.extraClassPath]] spark.driver.extraClassPath

`spark.driver.extraClassPath` is an optional setting that is used to...FIXME

==== [[spark.driver.cores]] spark.driver.cores

`spark.driver.cores` (default: `1`) sets the number of CPU cores assigned for the driver in link:spark-deploy-mode.adoc#cluster[cluster deploy mode].

It can be set using spark-submit's link:spark-submit.adoc#driver-cores[--driver-cores] command-line option for link:spark-cluster.adoc[Spark on cluster].

NOTE: When link:yarn/spark-yarn-client.adoc#creating-instance[Client is created] (for Spark on YARN in cluster mode only), it sets the number of cores for ApplicationManager using `spark.driver.cores`.

==== [[spark.driver.memory]] spark.driver.memory

`spark.driver.memory` (default: `1g`) sets the driver's memory size (in MiBs).
