== Spark SQL

:toc: right

From http://spark.apache.org/sql/[Spark SQL] home page:

> Spark SQL is Spark's module for working with structured data (rows and columns) in Spark.

Spark SQL introduces a tabular data abstraction called link:spark-sql-dataframe.adoc[DataFrames]. It is designed to ease processing large amount of structured tabular data on Spark infrastructure.

[NOTE]
====
Found the following note about Apache Drill, but appears to apply to Spark SQL perfectly:

> A SQL query engine for relational and NoSQL databases with direct queries on self-describing and semi-structured data in files, e.g. JSON or Parquet, and HBase tables without needing to specify metadata definitions in a centralized store.
====

FIXME

* What does the following do?

  df.selectExpr("rand()", "randn()", "rand(5)", "randn(50)")

From user@spark:

> If you already loaded csv data into a dataframe, why not register it as a table, and use Spark SQL
to find max/min or any other aggregates? SELECT MAX(column_name) FROM dftable_name ... seems natural.

> you're more comfortable with SQL, it might worth registering this DataFrame as a table and generating SQL query to it (generate a string with a series of min-max calls)

Looks like something to transform into a working example.

=== Creating DataFrames

From http://stackoverflow.com/a/32514683/1305344:

```
val df = sc.parallelize(Seq(
   Tuple1("08/11/2015"), Tuple1("09/11/2015"), Tuple1("09/12/2015")
)).toDF("date_string")

df.registerTempTable("df")

sqlContext.sql(
  """SELECT date_string,
        from_unixtime(unix_timestamp(date_string,'MM/dd/yyyy'), 'EEEEE') AS dow
      FROM df"""
).show
```

The result:

```
+-----------+--------+
|date_string|     dow|
+-----------+--------+
| 08/11/2015| Tuesday|
| 09/11/2015|  Friday|
| 09/12/2015|Saturday|
+-----------+--------+
```

* Where do `from_unixtime` and `unix_timestamp` come from? `HiveContext` perhaps? How are they registered
* What other UDFs are available?

=== Reading JSON file

Execute the following using `./bin/spark-shell` (it provides `sc` for SparkContext and `sqlContext` for Spark SQL context):

```
val hello = sc.textFile("/Users/jacek/dev/sandbox/hello.json")
val helloDF = sqlContext.read.json(hello)
```

Depending on the content of `hello.json` you may see different schema. The point, however, is that you can parse JSON files and let the _schema inferencer_ to deduct the schema.

```
scala> helloDF.printSchema
root
```

Register temp table to use for queries:

```
helloDF.registerTempTable("helloT")
```

=== Reading data via JDBC

```
scala> sqlContext.read.format("jdbc")
res0: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@46baac0d
```

=== Handling data in Avro format

Use custom serializer using http://spark-packages.org/package/databricks/spark-avro[spark-avro].

Run Spark shell with `--packages com.databricks:spark-avro_2.11:2.0.0` (see https://github.com/databricks/spark-avro/issues/85[2.0.0 artifact is not in any public maven repo] why `--repositories` is required).

```
./bin/spark-shell --packages com.databricks:spark-avro_2.11:2.0.0 --repositories "http://dl.bintray.com/databricks/maven"
```

And then...

```
val fileRdd = sc.textFile("README.md")
val df = fileRdd.toDF

import org.apache.spark.sql.SaveMode

val outputF = "test.avro"
df.write.mode(SaveMode.Append).format("com.databricks.spark.avro").save(outputF)
```

See https://spark.apache.org/docs/latest/api/java/index.html#org.apache.spark.sql.SaveMode[org.apache.spark.sql.SaveMode] (and perhaps https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SaveMode[org.apache.spark.sql.SaveMode] from Scala's perspective).

```
val df = sqlContext.read.format("com.databricks.spark.avro").load("test.avro")
```

Show the result:

```
df.show
```

=== Group and aggregate

```
val df = sc.parallelize(Seq(
  (1441637160, 10.0),
  (1441637170, 20.0),
  (1441637180, 30.0),
  (1441637210, 40.0),
  (1441637220, 10.0),
  (1441637230, 0.0))).toDF("timestamp", "value")

import org.apache.spark.sql.types._

val tsGroup = (floor($"timestamp" / lit(60)) * lit(60)).cast(IntegerType).alias("timestamp")

df.groupBy(tsGroup).agg(mean($"value").alias("value")).show
```

The above example yields the following result:

```
+----------+-----+
| timestamp|value|
+----------+-----+
|1441637160| 25.0|
|1441637220|  5.0|
+----------+-----+
```

See http://stackoverflow.com/a/32443728/1305344[the answer on StackOverflow].

=== More examples

Another example:

```
val df = Seq(1 -> 2).toDF("i", "j")
val query = df.groupBy('i)
  .agg(max('j).as("aggOrdering"))
  .orderBy(sum('j))
query == Row(1, 2) // should return true
```

What does it do?

```
val df = Seq((1, 1), (-1, 1)).toDF("key", "value")
df.registerTempTable("src")
sql("SELECT IF(a > 0, a, 0) FROM (SELECT key a FROM src) temp")
```
