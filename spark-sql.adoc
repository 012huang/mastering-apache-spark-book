== Spark SQL

From http://spark.apache.org/sql/[Spark SQL] home page:

> Spark SQL is Spark's module for working with structured data (rows and columns) in Spark.

The quickest way to work with Spark SQL is to use link:spark-shell.adoc[Spark shell] and `sqlContext` (https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext[org.apache.spark.sql.SQLContext]).

With an instance of SQLContext, you can create *DataFrames* and execute SQL queries.

```
scala> sqlContext
res1: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@60ae950f
```

As you may have noticed, `sqlContext` in Spark shell is actually a  https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.hive.HiveContext[org.apache.spark.sql.hive.HiveContext] that integrates *the Spark SQL execution engine* with data stored in https://hive.apache.org/[Apache Hive].

> The Apache Hiveâ„¢ data warehouse software facilitates querying and managing large datasets residing in distributed storage.

A https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame[DataFrame] is a distributed collection of data organized into named columns. It is equivalent to a relational table in Spark SQL.

FIXME

* what formats does `sqlContext.read` support?
** `sqlContext.read.parquet`
* What does the following do?

```
df.selectExpr("rand()", "randn()", "rand(5)", "randn(50)")
```

=== Creating DataFrames

Use Spark shell as described in link:spark-shell.adoc[Spark shell].

There are few `SQLContext` functions to create a dataframe.

* `emptyDataFrame`

  scala> val df = sqlContext.emptyDataFrame
  df: org.apache.spark.sql.DataFrame = []

  scala> val adf = df.withColumn("a", lit(1))
  adf: org.apache.spark.sql.DataFrame = [a: int]

  scala> adf.registerTempTable("t")

  scala> sqlContext.sql("select a from t")
  15/09/12 12:07:54 INFO ParseDriver: Parsing command: select a from t
  15/09/12 12:07:54 INFO ParseDriver: Parse Completed
  res32: org.apache.spark.sql.DataFrame = [a: int]

From user@spark:

> If you already loaded csv data into a dataframe, why not register it as a table, and use Spark SQL
to find max/min or any other aggregates? SELECT MAX(column_name) FROM dftable_name ... seems natural.

> you're more comfortable with SQL, it might worth registering this DataFrame as a table and generating SQL query to it (generate a string with a series of min-max calls)

Looks like something to transform into a working example.

=== Creating DataFrames

From http://stackoverflow.com/a/32514683/1305344:

```
val df = sc.parallelize(Seq(
   Tuple1("08/11/2015"), Tuple1("09/11/2015"), Tuple1("09/12/2015")
)).toDF("date_string")

df.registerTempTable("df")

sqlContext.sql(
  """SELECT date_string,
        from_unixtime(unix_timestamp(date_string,'MM/dd/yyyy'), 'EEEEE') AS dow
      FROM df"""
).show
```

The result:

```
+-----------+--------+
|date_string|     dow|
+-----------+--------+
| 08/11/2015| Tuesday|
| 09/11/2015|  Friday|
| 09/12/2015|Saturday|
+-----------+--------+
```

* Where do `from_unixtime` and `unix_timestamp` come from? `HiveContext` perhaps? How are they registered
* What other UDFs are available?

=== Reading JSON file

Execute the following using `./bin/spark-shell` (it provides `sc` for SparkContext and `sqlContext` for Spark SQL context):

```
val hello = sc.textFile("/Users/jacek/dev/sandbox/hello.json")
val helloDF = sqlContext.read.json(hello)
```

Depending on the content of `hello.json` you may see different schema. The point, however, is that you can parse JSON files and let the _schema inferencer_ to deduct the schema.

```
scala> helloDF.printSchema
root
```

Register temp table to use for queries:

```
helloDF.registerTempTable("helloT")
```

=== Reading data via JDBC

```
scala> sqlContext.read.format("jdbc")
res0: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@46baac0d
```

=== Handling data in Avro format

Use custom serializer using http://spark-packages.org/package/databricks/spark-avro[spark-avro].

Run Spark shell with `--packages com.databricks:spark-avro_2.11:2.0.0` (see https://github.com/databricks/spark-avro/issues/85[2.0.0 artifact is not in any public maven repo] why `--repositories` is required).

```
./bin/spark-shell --packages com.databricks:spark-avro_2.11:2.0.0 --repositories "http://dl.bintray.com/databricks/maven"
```

And then...

```
val fileRdd = sc.textFile("README.md")
val df = fileRdd.toDF

import org.apache.spark.sql.SaveMode

val outputF = "test.avro"
df.write.mode(SaveMode.Append).format("com.databricks.spark.avro").save(outputF)
```

See https://spark.apache.org/docs/latest/api/java/index.html#org.apache.spark.sql.SaveMode[org.apache.spark.sql.SaveMode] (and perhaps https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SaveMode[org.apache.spark.sql.SaveMode] from Scala's perspective).

```
val df = sqlContext.read.format("com.databricks.spark.avro").load("test.avro")
```

Show the result:

```
df.show
```

=== Group and aggregate

```
val df = sc.parallelize(Seq(
  (1441637160, 10.0),
  (1441637170, 20.0),
  (1441637180, 30.0),
  (1441637210, 40.0),
  (1441637220, 10.0),
  (1441637230, 0.0))).toDF("timestamp", "value")

import org.apache.spark.sql.types._

val tsGroup = (floor($"timestamp" / lit(60)) * lit(60)).cast(IntegerType).alias("timestamp")

df.groupBy(tsGroup).agg(mean($"value").alias("value")).show
```

The above example yields the following result:

```
+----------+-----+
| timestamp|value|
+----------+-----+
|1441637160| 25.0|
|1441637220|  5.0|
+----------+-----+
```

See http://stackoverflow.com/a/32443728/1305344[the answer on StackOverflow].

=== More examples

Another example:

```
val df = Seq(1 -> 2).toDF("i", "j")
val query = df.groupBy('i)
  .agg(max('j).as("aggOrdering"))
  .orderBy(sum('j))
query == Row(1, 2) // should return true
```

What does it do?

```
val df = Seq((1, 1), (-1, 1)).toDF("key", "value")
df.registerTempTable("src")
sql("SELECT IF(a > 0, a, 0) FROM (SELECT key a FROM src) temp")
```
