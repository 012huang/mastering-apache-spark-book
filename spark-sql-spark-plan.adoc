== [[SparkPlan]] SparkPlan -- Spark Query Planner

`SparkPlan` is a base link:spark-sql-query-plan.adoc[QueryPlan] for physical operators. It can be executed to produce a `RDD[InternalRow]`.

NOTE: The naming convention for physical operators in Spark's source code is to have their names end with the _Exec_ prefix, e.g. `DebugExec`.

TIP: Read link:spark-sql-InternalRow.adoc[InternalRow] about the internal binary row format.

`SparkPlan` has the following attributes:

* `metadata`
* <<metrics, metrics>>
* `outputPartitioning`
* `outputOrdering`

`SparkPlan` has the following `final` methods that prepare environment and pass calls on to corresponding methods that constitute <<contract, SparkPlan Contract>>:

* `execute` calls `doExecute`
* `prepare` calls `doPrepare`
* `executeBroadcast` calls `doExecuteBroadcast`

=== [[contract]] SparkPlan Contract

The contract of `SparkPlan` requires that concrete implementations define the following method:

* `doExecute(): RDD[InternalRow]`

They may also define their own custom overrides:

* `doPrepare`
* `doExecuteBroadcast`

CAUTION: FIXME Why are there two executes?

=== [[executeCollect]] executeCollect

CAUTION: FIXME

=== [[SQLMetric]] SQLMetric

`SQLMetric` is an link:spark-accumulators.adoc[accumulator] that accumulate and produce long values.

There are three known `SQLMetrics`:

* `sum`
* `size`
* `timing`

=== [[metrics]] metrics Lookup Table

[source, scala]
----
metrics: Map[String, SQLMetric] = Map.empty
----

`metrics` is a `private[sql]` lookup table of supported <<SQLMetric, SQLMetrics>> by their names.
