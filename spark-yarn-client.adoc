== Client

`org.apache.spark.deploy.yarn.Client` is a standalone application used for link:spark-submit.adoc#submit[submitting Spark applications] to a YARN cluster.

Precisely, it is <<main, Client.main>> method that is invoked.

Depending on the link:spark-deploy-mode.adoc[deploy mode] it calls link:spark-yarn-applicationmaster.adoc[org.apache.spark.deploy.yarn.ApplicationMaster] or ApplicationMaster's wrapper link:spark-yarn-applicationmaster.adoc#ExecutorLauncher[org.apache.spark.deploy.yarn.ExecutorLauncher] by their class names (so the instantiation is implicit and dynamic, i.e. happens at runtime).

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.deploy.yarn.Client` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.yarn.Client=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating Client Instance

Creating an instance of `Client` does the following:

* Creates an internal instance of `YarnClient` (using `YarnClient.createYarnClient`) that becomes `yarnClient`.

* Creates an internal instance of `YarnConfiguration` (using `YarnConfiguration` and the input `hadoopConf`) that becomes `yarnConf`.

* Sets the internal `isClusterMode` that says whether link:spark-deploy-mode.adoc#spark.submit.deployMode[spark.submit.deployMode] is link:spark-deploy-mode.adoc#cluster[cluster deploy mode].

* Sets the internal `amMemory` to link:spark-driver.adoc#spark.driver.memory[spark.driver.memory] when `isClusterMode` is enabled or link:spark-yarn-settings.adoc#spark.yarn.am.memory[spark.yarn.am.memory] otherwise.

* Sets the internal `amMemoryOverhead` to link:spark-yarn-settings.adoc#spark.yarn.driver.memoryOverhead[spark.yarn.driver.memoryOverhead] when `isClusterMode` is enabled or link:spark-yarn-settings.adoc#spark.yarn.am.memoryOverhead[spark.yarn.am.memoryOverhead] otherwise. If neither is available, the maximum of 10% of `amMemory` and `384` is chosen.

* Sets the internal `amCores` to link:spark-driver.adoc#spark.driver.cores[spark.driver.cores] when `isClusterMode` is enabled or link:spark-yarn-settings.adoc#spark.yarn.am.cores[spark.yarn.am.cores] otherwise.

* Sets the internal `executorMemory` to link:spark-executor.adoc#spark.executor.memory[spark.executor.memory].

* Sets the internal `executorMemoryOverhead` to link:spark-yarn-settings.adoc#spark.yarn.executor.memoryOverhead[spark.yarn.executor.memoryOverhead]. If unavailable, it is set to the maximum of 10% of `executorMemory` and `384`.

* Creates an internal instance of `ClientDistributedCacheManager` (as `distCacheMgr`).

* Sets the variables: `loginFromKeytab` to `false` with `principal`, `keytab`, and `credentials` to `null`.

* Creates an internal instance of `LauncherBackend` (as <<launcherBackend, launcherBackend>>).

* Sets the internal `fireAndForget` flag to the result of `isClusterMode` and not link:spark-yarn-settings.adoc#spark.yarn.submit.waitAppCompletion[spark.yarn.submit.waitAppCompletion].

* Sets the internal variable `appId` to `null`.

* Sets the internal `appStagingBaseDir` to link:spark-yarn-settings.adoc#spark.yarn.stagingDir[spark.yarn.stagingDir] or the home directory of Hadoop.

=== [[launcherBackend]] launcherBackend value

`launcherBackend`...FIXME

=== [[SPARK_YARN_MODE]] SPARK_YARN_MODE flag

`SPARK_YARN_MODE` is a flag that says whether...FIXME.

NOTE: Any environment variable with the `SPARK_` prefix is propagated to all (remote) processes.

CAUTION: FIXME Where is `SPARK_` prefix rule enforced?

NOTE: `SPARK_YARN_MODE` is a system property (i.e. available using `System.getProperty`) and a environment variable (i.e. available using `System.getenv`). See link:spark-yarn.adoc#YarnSparkHadoopUtil[YarnSparkHadoopUtil].

It is enabled (i.e. `true`) when link:spark-sparkcontext-creating-instance-internals.adoc#SPARK_YARN_MODE[SparkContext is created for Spark on YARN in client deploy mode], when <<setupLaunchEnv, Client...FIXME>> and <<main, a Spark application is deployed to a YARN cluster>>.

CAUTION: FIXME Why is this needed? `git blame` it.

`SPARK_YARN_MODE` flag is checked when link:spark-yarn.adoc#get[YarnSparkHadoopUtil] or link:spark-hadoop.adoc#get[SparkHadoopUtil] are accessed.

It is cleared later when <<stop, Client is requested to stop>>.

=== [[setupLaunchEnv]] setupLaunchEnv method

CAUTION: FIXME

=== [[launcherBackend]] Internal LauncherBackend (launcherBackend value)

CAUTION: FIXME

=== [[yarnClient]] Internal Hadoop's YarnClient (yarnClient value)

CAUTION: FIXME

=== [[main]] main

`main` method is invoked while a Spark application is being deployed to a YARN cluster.

NOTE: It is executed by link:spark-submit.adoc#submit[spark-submit] with `--master yarn` command-line argument.

[NOTE]
====
When you start the `main` method when starting the `Client` standalone application, say using `./bin/spark-class org.apache.spark.deploy.yarn.Client`, you will see the following WARN message in the logs unless you set `SPARK_SUBMIT` system property.

```
WARN Client: WARNING: This client is deprecated and will be removed in a future version of Spark. Use ./bin/spark-submit with "--master yarn"
```
====

`main` turns <<SPARK_YARN_MODE, SPARK_YARN_MODE flag>> on.

It then instantiates link:spark-configuration.adoc[SparkConf], parses command-line arguments (using <<ClientArguments, ClientArguments>>) and passes the call on to <<run, Client.run>> method.

=== [[stop]] stop

[source, scala]
----
stop(): Unit
----

`stop` closes the internal <<launcherBackend, LauncherBackend>> and stops the internal <<yarnClient, yarnClient>>. It also clears <<SPARK_YARN_MODE, SPARK_YARN_MODE flag>> (to allow switching between cluster types).

=== [[run]] run

`run` <<submitApplication, submits a Spark application>> to a link:spark-yarn-introduction.adoc[YARN ResourceManager] (RM).

If `LauncherBackend` is not connected to a RM, i.e. `LauncherBackend.isConnected` returns `false`, and `fireAndForget` is enabled, ...FIXME

CAUTION: FIXME When could `LauncherBackend` lost the connection since it was connected in <<submitApplication, submitApplication>>?

CAUTION: FIXME What is `fireAndForget`?

Otherwise, when `LauncherBackend` is connected or `fireAndForget` is disabled, <<monitorApplication, monitorApplication>> is called. It returns a pair of `yarnApplicationState` and `finalApplicationStatus` that is checked against three different state pairs and throw a `SparkException`:

* `YarnApplicationState.KILLED` or `FinalApplicationStatus.KILLED` lead to `SparkException` with the message "Application [appId] is killed".

* `YarnApplicationState.FAILED` or `FinalApplicationStatus.FAILED` lead to `SparkException` with the message "Application [appId] finished with failed status".

* `FinalApplicationStatus.UNDEFINED` leads to `SparkException` with the message "The final status of application [appId] is undefined".

CAUTION: FIXME What are `YarnApplicationState` and `FinalApplicationStatus` statuses?

=== [[monitorApplication]] monitorApplication

[source, scala]
----
monitorApplication(
  appId: ApplicationId,
  returnOnRunning: Boolean = false,
  logApplicationReport: Boolean = true): (YarnApplicationState, FinalApplicationStatus)
----

`monitorApplication` continuously reports the status of an application `appId` every link:spark-yarn.adoc#spark.yarn.report.interval[spark.yarn.report.interval] (unless `returnOnRunning` is enabled).

NOTE: It is used in <<run, run>>, link:spark-yarn-client-yarnclientschedulerbackend.adoc#waitForApplication[YarnClientSchedulerBackend.waitForApplication] and `MonitorThread.run`.

It gets the application's report from the ResourceManager to access `YarnApplicationState`.

TIP: It uses Hadoop's `YarnClient.getApplicationReport(appId)`.

Unless `logApplicationReport` is disabled, it prints the following INFO message to the logs:

```
INFO Client: Application report for [appId] (state: [state])
```

If `logApplicationReport` and DEBUG log level are enabled, it prints report details every time interval to the logs:

```
16/04/23 13:21:36 INFO Client:
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1461410495109
	 final status: UNDEFINED
	 tracking URL: http://japila.local:8088/proxy/application_1461410200840_0001/
	 user: jacek
```

For INFO log level it prints report details only when the application state changes.

When the application state changes, `LauncherBackend` is notified (using `LauncherBackend.setState`).

NOTE: The application state is an instance of Hadoop's `YarnApplicationState`.

For states `FINISHED`, `FAILED` or `KILLED`, <<cleanupStagingDir, cleanupStagingDir>> is called and the method finishes by returning a pair of the current state and the final application status.

If `returnOnRunning` is enabled (it is disabled by default) and the application state turns `RUNNING`, the method returns a pair of the current state `RUNNING` and the final application status.

NOTE: <<cleanupStagingDir, cleanupStagingDir>> won't be called when `returnOnRunning` is enabled and an application turns RUNNING. _I guess it is likely a left-over since the Client is deprecated now_.

The current state is recorded for future checks (in the loop).

=== [[cleanupStagingDir]] cleanupStagingDir

`cleanupStagingDir` clears the staging directory of an application.

NOTE: It is used in <<submitApplication, submitApplication>> when there is an exception and <<monitorApplication, monitorApplication>> when an application finishes and the method quits.

It uses link:spark-yarn-settings.adoc#spark.yarn.stagingDir[spark.yarn.stagingDir] setting or falls back to a user's home directory for the staging directory. If link:spark-yarn-settings.adoc#spark.yarn.preserve.staging.files[cleanup is enabled], it deletes the entire staging directory for the application.

You should see the following INFO message in the logs:

```
INFO Deleting staging directory [stagingDirPath]
```

=== [[reportLauncherState]] reportLauncherState

[source, scala]
----
reportLauncherState(state: SparkAppHandle.State): Unit
----

`reportLauncherState` merely passes the call on to `LauncherBackend.setState`.

CAUTION: What does `setState` do?

=== [[submitApplication]] submitApplication

`submitApplication` submits a Spark application to a YARN ResourceManager. It waits until the application is running and eventually returns its unique https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/ApplicationId.html[ApplicationId].

NOTE: `submitApplication` is used in <<run, Client.run>> and link:spark-yarn-client-yarnclientschedulerbackend.adoc#YarnClientSchedulerBackend[YarnClientSchedulerBackend.start].

Internally, it executes `LauncherBackend.connect` first and then executes `Client.setupCredentials` to set up credentials for future calls.

It creates a YARN client (using Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/YarnClient.html#createYarnClient()[YarnClient.createYarnClient]), https://hadoop.apache.org/docs/current/api/org/apache/hadoop/service/AbstractService.html#init(org.apache.hadoop.conf.Configuration)[inits it] with a https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/conf/YarnConfiguration.html[YarnConfiguration] and https://hadoop.apache.org/docs/current/api/org/apache/hadoop/service/AbstractService.html#start()[starts it]. All this happens using Hadoop API.

CAUTION: FIXME How to configure `YarnClient`? What is `getYarnClusterMetrics`?

You should see the following INFO in the logs:

```
INFO Client: Requesting a new application from cluster with [yarnClient.getYarnClusterMetrics.getNumNodeManagers] NodeManagers
```

It then https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/YarnClient.html#createApplication()[YarnClient.createApplication()] to create a new application in YARN and obtains the application id.

The `LauncherBackend` instance changes state to SUBMITTED with the application id.

CAUTION: FIXME Why is this important?

`submitApplication` verifies whether the cluster has resources for the ApplicationManager (using <<verifyClusterResources, verifyClusterResources>>).

It then <<createContainerLaunchContext, createContainerLaunchContext>> and <<createApplicationSubmissionContext, createApplicationSubmissionContext>>.

It submits the application to YARN ResourceManager.

```
INFO Client: Submitting application [applicationId.getId] to ResourceManager
```

And finally submits a new application to YARN (using Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/YarnClient.html#submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)[YarnClient.submitApplication]) and waits until it is accepted by YARN ResourceManager.

=== [[verifyClusterResources]] verifyClusterResources

```
INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
```

=== [[createContainerLaunchContext]] createContainerLaunchContext

=== [[createApplicationSubmissionContext]] createApplicationSubmissionContext

=== [[ClientArguments]] ClientArguments
