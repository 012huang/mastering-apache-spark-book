== Persisting Machine Learning Components

<<MLWriter, MLWriter>> and <<MLReader, MLReader>> belong to `org.apache.spark.ml.util` package.

=== [[MLWriter]] MLWriter

`MLWriter` abstract class comes with `save(path: String)` method to save a machine learning component to a given `path`.

```
save(path: String): Unit
```

It comes with another (chainable) method `overwrite` to overwrite the output path if it already exists.

```
overwrite(): this.type
```

The component is saved into a JSON file (see <<MLWriter-Example, MLWriter Example>> section below).

[TIP]
====
Enable `INFO` logging level for the `MLWriter` implementation logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.ml.Pipeline$.PipelineWriter=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

CAUTION: FIXME The logging doesn't work and overwriting does not print out INFO message to the logs :(

==== [[MLWriter-Example]] MLWriter Example

[source, scala]
----
import org.apache.spark.ml._
val pipeline = new Pipeline().setStages(Array())
pipeline.write.overwrite().save("hello-pipeline")
----

The result of `save` is a JSON file (as shown below).

```
$ cat hello-pipeline/metadata/part-00000 | jq
{
  "class": "org.apache.spark.ml.Pipeline",
  "timestamp": 1457685293319,
  "sparkVersion": "2.0.0-SNAPSHOT",
  "uid": "pipeline_12424a3716b2",
  "paramMap": {
    "stageUids": []
  }
}
```

=== [[MLReader]] MLReader

`MLReader` abstract class comes with `load(path: String)` method to `load` a machine learning component from a given `path`.

[source, scala]
----
import org.apache.spark.ml._
val pipeline = Pipeline.read.load("hello-pipeline")
----
