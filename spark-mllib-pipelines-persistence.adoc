== [[MLWriter]][[MLReader]] ML Persistence -- Saving and Loading Models and Pipelines

<<MLWriter, MLWriter>> and <<MLReader, MLReader>> belong to `org.apache.spark.ml.util` package.

They allow you to save and load link:spark-mllib-models.adoc[models] despite the languages -- Scala, Java, Python or R -- they have been saved in and loaded later on.

=== [[MLWriter]] MLWriter

`MLWriter` abstract class comes with `save(path: String)` method to save a ML component to a given `path`.

```
save(path: String): Unit
```

It comes with another (chainable) method `overwrite` to overwrite the output path if it already exists.

```
overwrite(): this.type
```

The component is saved into a JSON file (see <<MLWriter-Example, MLWriter Example>> section below).

[TIP]
====
Enable `INFO` logging level for the `MLWriter` implementation logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.ml.Pipeline$.PipelineWriter=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

CAUTION: FIXME The logging doesn't work and overwriting does not print out INFO message to the logs :(

==== [[MLWriter-Example]] MLWriter Example

[source, scala]
----
import org.apache.spark.ml._
val pipeline = new Pipeline().setStages(Array.empty[PipelineStage])
pipeline.write.overwrite.save("sample-pipeline")
----

The result of `save` is a JSON file (as shown below).

```
$ cat sample-pipeline/metadata/part-00000 | jq
{
  "class": "org.apache.spark.ml.Pipeline",
  "timestamp": 1472747720477,
  "sparkVersion": "2.1.0-SNAPSHOT",
  "uid": "pipeline_181c90b15d65",
  "paramMap": {
    "stageUids": []
  }
}
```

=== [[MLReader]] MLReader

`MLReader` abstract class comes with `load(path: String)` method to `load` a ML component from a given `path`.

[source, scala]
----
import org.apache.spark.ml._
val pipeline = Pipeline.read.load("sample-pipeline")

scala> val stageCount = pipeline.getStages.size
stageCount: Int = 0
----
