== [[CollapseCodegenStages]] CollapseCodegenStages Physical Preparation Rule -- Collapsing Physical Operators with Whole-Stage CodeGen Support

`CollapseCodegenStages` is a link:spark-sql-QueryExecution-SparkPlan-Preparations.adoc[physical preparation rule] that <<apply, collapses physical operators with whole-stage codegen support into a WholeStageCodegenExec physical operator>>.

[NOTE]
====
`CollapseCodegenStages` does nothing (i.e. passes a physical plan through unchanged) when link:spark-sql-SQLConf.adoc#spark.sql.codegen.wholeStage[spark.sql.codegen.wholeStage] internal property is disabled.

`spark.sql.codegen.wholeStage` property is enabled by default.

[source, scala]
----
import org.apache.spark.sql.internal.SQLConf.WHOLESTAGE_CODEGEN_ENABLED
scala> spark.conf.get(WHOLESTAGE_CODEGEN_ENABLED)
res0: String = true
----

Use link:spark-sql-SQLConf.adoc#wholeStageEnabled[SQLConf.wholeStageEnabled] method to access the current value.

[source, scala]
----
scala> spark.sessionState.conf.wholeStageEnabled
res1: Boolean = true
----
====

NOTE: Only link:spark-sql-CodegenSupport.adoc[CodegenSupport] physical plans that have `supportCodegen` enabled support codegen.

NOTE: All link:spark-sql-Expression.adoc[Catalyst expressions] (but link:spark-sql-Expression.adoc#CodegenFallback[CodegenFallback]) support codegen.

`CollapseCodegenStages` is a part of link:spark-sql-QueryExecution.adoc#preparations[preparations] batch of physical plan rules and is executed in link:spark-sql-QueryExecution.adoc#executedPlan[executedPlan] phase of a link:spark-sql-QueryExecution.adoc[query execution].

[[conf]]
`CollapseCodegenStages` takes a link:spark-sql-SQLConf.adoc[SQLConf] when created.

`CollapseCodegenStages` uses the internal setting `spark.sql.codegen.maxFields` (default: `200`) to control the number of fields in input and output schemas before deactivating whole-stage codegen. It counts the fields included in complex types, i.e. link:spark-sql-StructType.adoc[StructType], `MapType`, `ArrayType`, `UserDefinedType`, and their combinations, recursively. See https://issues.apache.org/jira/browse/SPARK-14554[SPARK-14554].

`CollapseCodegenStages` inserts `InputAdapter` leaf nodes in a physical plan recursively that is then used to generate code that consumes an RDD iterator of link:spark-sql-InternalRow.adoc[InternalRow].

TIP: Import `CollapseCodegenStages` and apply the rule directly to a physical plan to learn how the rule works.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...
val query = spark.range(2).join(spark.range(2), "id")

// the final result (after CollapseCodegenStages among other rules)
scala> query.explain
== Physical Plan ==
*Project [id#9L]
+- *BroadcastHashJoin [id#9L], [id#12L], Inner, BuildRight
   :- *Range (0, 2, step=1, splits=8)
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))
      +- *Range (0, 2, step=1, splits=8)

val plan = query.queryExecution.sparkPlan

// wholeStageEnabled is enabled
scala> println(spark.sessionState.conf.wholeStageEnabled)
true

import org.apache.spark.sql.execution.CollapseCodegenStages
val ccs = CollapseCodegenStages(conf = spark.sessionState.conf)

scala> ccs.ruleName
res0: String = org.apache.spark.sql.execution.CollapseCodegenStages

// Before CollapseCodegenStages
scala> println(plan.numberedTreeString)
00 Project [id#9L]
01 +- BroadcastHashJoin [id#9L], [id#12L], Inner, BuildRight
02    :- Range (0, 2, step=1, splits=8)
03    +- Range (0, 2, step=1, splits=8)

// After CollapseCodegenStages
// Note the star
val executedPlan = ccs.apply(plan)
scala> println(executedPlan.numberedTreeString)
00 *Project [id#9L]
01 +- *BroadcastHashJoin [id#9L], [id#12L], Inner, BuildRight
02    :- *Range (0, 2, step=1, splits=8)
03    +- *Range (0, 2, step=1, splits=8)

import org.apache.spark.sql.execution.WholeStageCodegenExec
val wsc = executedPlan(0).asInstanceOf[WholeStageCodegenExec]
scala> println(wsc.numberedTreeString)
00 *Project [id#9L]
01 +- *BroadcastHashJoin [id#9L], [id#12L], Inner, BuildRight
02    :- *Range (0, 2, step=1, splits=8)
03    +- *Range (0, 2, step=1, splits=8)

scala> println(wsc.child.numberedTreeString)
00 Project [id#9L]
01 +- BroadcastHashJoin [id#9L], [id#12L], Inner, BuildRight
02    :- Range (0, 2, step=1, splits=8)
03    +- Range (0, 2, step=1, splits=8)

// Let's disable wholeStage codegen
// CollapseCodegenStages becomes a noop

val newSpark = spark.newSession()
import org.apache.spark.sql.internal.SQLConf.WHOLESTAGE_CODEGEN_ENABLED
newSpark.sessionState.conf.setConf(WHOLESTAGE_CODEGEN_ENABLED, false)

scala> println(newSpark.sessionState.conf.wholeStageEnabled)
false

val ccsWholeStageDisabled = CollapseCodegenStages(conf = newSpark.sessionState.conf)
scala> println(ccsWholeStageDisabled.apply(plan).numberedTreeString)
00 Project [id#9L]
01 +- BroadcastHashJoin [id#9L], [id#12L], Inner, BuildRight
02    :- Range (0, 2, step=1, splits=8)
03    +- Range (0, 2, step=1, splits=8)
----

=== [[insertInputAdapter]] `insertInputAdapter` Internal Method

CAUTION: FIXME

=== [[apply]] Inserting WholeStageCodegenExec to Physical Plan for Operators that Support CodeGen -- `apply` Method

[source, scala]
----
apply(plan: SparkPlan): SparkPlan
----

`apply` finds link:spark-sql-CodegenSupport.adoc[physical operators with optional Java code generation] that <<supportCodegen-SparkPlan, do support codegen>> and inserts a link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc[WholeStageCodegenExec] operator into the link:spark-sql-SparkPlan.adoc[physical plan] (wrapping the operators).

Internally, `apply` walks the input physical `plan` to find operators with link:spark-sql-CodegenSupport.adoc[CodegenSupport] and <<supportCodegen-SparkPlan, supportCodegen>> enabled. If found, `apply` inserts a link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc[WholeStageCodegenExec] with an <<insertInputAdapter, input adapter>> for child operators that do not support codegen.

[NOTE]
====
Input Adapters show themselves with no star in link:spark-sql-dataset-operators.adoc[explain].

[source, scala]
----
scala> spark.range(1).groupBy("id").count.explain
== Physical Plan ==
*HashAggregate(keys=[id#31L], functions=[count(1)])
+- Exchange hashpartitioning(id#31L, 200) // <-- no star here
   +- *HashAggregate(keys=[id#31L], functions=[partial_count(1)])
      +- *Range (0, 1, step=1, splits=8)
----
====

`apply` does nothing (i.e. passes a physical plan through unchanged) when link:spark-sql-SQLConf.adoc#spark.sql.codegen.wholeStage[spark.sql.codegen.wholeStage] internal property is disabled.

[NOTE]
====
link:spark-sql-SQLConf.adoc#spark.sql.codegen.wholeStage[spark.sql.codegen.wholeStage] property is enabled by default.

[source, scala]
----
import org.apache.spark.sql.internal.SQLConf.WHOLESTAGE_CODEGEN_ENABLED
scala> spark.conf.get(WHOLESTAGE_CODEGEN_ENABLED)
res0: String = true
----

Use link:spark-sql-SQLConf.adoc#wholeStageEnabled[SQLConf.wholeStageEnabled] method to access the current value.

[source, scala]
----
scala> spark.sessionState.conf.wholeStageEnabled
res1: Boolean = true
----
====

=== [[supportCodegen]][[supportCodegen-SparkPlan]] Physical Operators with Codegen Support -- `supportCodegen` Internal Predicate

[source, scala]
----
supportCodegen(plan: SparkPlan): Boolean
----

`supportCodegen` finds link:spark-sql-SparkPlan.adoc[physical operators] with link:spark-sql-CodegenSupport.adoc[CodegenSupport] and link:spark-sql-CodegenSupport.adoc#supportCodegen[supportCodegen] flag enabled.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...
// both where and select support codegen
val query = spark.range(2).where('id === 0).select('id)
scala> query.explain
== Physical Plan ==
*Filter (id#88L = 0)
+- *Range (0, 2, step=1, splits=8)
----

`supportCodegen` is positive when all of the following hold:

* link:spark-sql-Expression.adoc[Catalyst expressions] of the physical operator all <<supportCodegen-Expression, support codegen>>
* Number of nested fields of the link:spark-sql-catalyst-QueryPlan.adoc#schema[schema of the physical operator] is up to link:spark-sql-SQLConf.adoc#spark.sql.codegen.maxFields[spark.sql.codegen.maxFields] internal property (100 by default)
* Number of the nested fields in the schema of the children is up to `spark.sql.codegen.maxFields` (same as above)

Otherwise, `supportCodegen` is negative/disabled.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...
// both where and select support codegen
// let's break the requirement of having up to spark.sql.codegen.maxFields
val newSpark = spark.newSession()
import org.apache.spark.sql.internal.SQLConf.WHOLESTAGE_MAX_NUM_FIELDS
newSpark.sessionState.conf.setConf(WHOLESTAGE_MAX_NUM_FIELDS, 2)

scala> println(newSpark.sessionState.conf.wholeStageMaxNumFields)
2

import newSpark.implicits._
val query = Seq((1,2,3)).toDF("id", "c0", "c1").where('id === 0)
scala> query.explain
== Physical Plan ==
Project [_1#452 AS id#456, _2#453 AS c0#457, _3#454 AS c1#458]
+- Filter (_1#452 = 0)
   +- LocalTableScan [_1#452, _2#453, _3#454]
----

=== [[supportCodegen-Expression]] Expressions with Codegen Support -- `supportCodegen` Internal Predicate

[source, scala]
----
supportCodegen(e: Expression): Boolean
----

`supportCodegen` is positive when the link:spark-sql-Expression.adoc[Catalyst expression] `e` is (in the order of verification):

1. link:spark-sql-Expression.adoc#LeafExpression[LeafExpression]
1. non-link:spark-sql-Expression.adoc#CodegenFallback[CodegenFallback] expression

Otherwise, `supportCodegen` is negative.

NOTE: `supportCodegen` (for expressions) is used when <<supportCodegen, supportCodegen>> (for physical plans) finds operators that support codegen.
