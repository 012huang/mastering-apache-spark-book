== RDD Caching and Persistence

RDDs can be cached (using RDD's `cache()` operation) or persisted (using RDD's `persist(newLevel: StorageLevel)` operation).

The `cache()` operation is a synonym of `persist()` that uses the default storage level `MEMORY_ONLY`.

RDDs can be <<unpersist, unpersisted>>.

=== [[StorageLevel]][[storage-levels]] Storage Levels

`StorageLevel` describes how an RDD is persisted (and addresses the following concerns):

* Does RDD use disk?
* How much of RDD is in memory?
* Does RDD use off-heap memory?
* Should an RDD be serialized (while persisting)?
* How many replicas (default: `1`) to use (can only be less than `40`)?

There are the following `StorageLevel` (number `_2` in the name denotes 2 replicas):

* `NONE` (default)
* `DISK_ONLY`
* `DISK_ONLY_2`
* `MEMORY_ONLY` (default for `cache()` operation)
* `MEMORY_ONLY_2`
* `MEMORY_ONLY_SER`
* `MEMORY_ONLY_SER_2`
* `MEMORY_AND_DISK`
* `MEMORY_AND_DISK_2`
* `MEMORY_AND_DISK_SER`
* `MEMORY_AND_DISK_SER_2`
* `OFF_HEAP`

You can check out the storage level using `getStorageLevel()` operation.

```
scala> val lines = sc.textFile("README.md")
lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at textFile at <console>:24

scala> lines.getStorageLevel
res2: org.apache.spark.storage.StorageLevel = StorageLevel(false, false, false, false, 1)
```

=== [[unpersist]] Unpersisting RDDs (Clearing Blocks)

When `unpersist(blocking: Boolean = true)` method is called, you should see the following INFO message in the logs:

```
INFO [RddName]: Removing RDD [id] from persistence list
```

It then calls link:spark-sparkcontext.adoc#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets <<StorageLevel, StorageLevel.NONE>> as the storage level.
