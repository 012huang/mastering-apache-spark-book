== Spark on Mesos

=== Running Spark on Mesos

A Mesos cluster needs at least one Mesos Master to coordinate and dispatch tasks onto Mesos Slaves.

```
$ mesos-master --registry=in_memory --ip=127.0.0.1
I1119 12:30:22.153122 2107527936 main.cpp:229] Build: 2015-11-15 11:52:29 by brew
I1119 12:30:22.153700 2107527936 main.cpp:231] Version: 0.25.0
I1119 12:30:22.153736 2107527936 main.cpp:252] Using 'HierarchicalDRF' allocator
I1119 12:30:22.155131 2107527936 main.cpp:465] Starting Mesos master
I1119 12:30:22.161689 2107527936 master.cpp:376] Master 91979713-a32d-4e08-aaea-5dffbfd44e5d (localhost) started on 127.0.0.1:5050
...
```

Visit the management console at http://localhost:5050.

.Mesos Management Console
image::images/mesos-console.png[align="center"]

Run Mesos Slave onto which Master will dispatch jobs.

```
$ mesos-slave --master=127.0.0.1:5050
I1119 12:35:18.750463 2107527936 main.cpp:185] Build: 2015-11-15 11:52:29 by brew
I1119 12:35:18.750747 2107527936 main.cpp:187] Version: 0.25.0
I1119 12:35:18.753484 2107527936 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I1119 12:35:18.763685 2107527936 main.cpp:272] Starting Mesos slave
I1119 12:35:18.766768 219140096 slave.cpp:190] Slave started on 1)@192.168.1.4:5051
...
I1119 12:35:18.787820 217530368 slave.cpp:741] Detecting new master
I1119 12:35:19.639030 218066944 slave.cpp:880] Registered with master master@127.0.0.1:5050; given slave ID 91979713-a32d-4e08-aaea-5dffbfd44e5d-S0
...
```

Switch to the management console at http://localhost:5050/#/slaves to see the slaves available.

.Mesos Management Console (Slaves tab) with one slave running
image::images/mesos-console-slaves.png[align="center"]

[IMPORTANT]
====
You have to export `MESOS_NATIVE_JAVA_LIBRARY` environment variable before connecting to the Mesos cluster.

```
$ export MESOS_NATIVE_JAVA_LIBRARY=/usr/local/lib/libmesos.dylib
```
====

[NOTE]
====
The preferred approach to launch Spark on Mesos and to give the location of Spark binaries is through <<settings, spark.executor.uri>> setting.

```
--conf spark.executor.uri=/Users/jacek/Downloads/spark-1.5.2-bin-hadoop2.6.tgz
```

For us, on a bleeding edge of Spark development, it is very convenient to use <<settings, spark.mesos.executor.home>> setting, instead.

```
-c spark.mesos.executor.home=`pwd`
```
====

```
$ ./bin/spark-shell --master mesos://127.0.0.1:5050 -c spark.mesos.executor.home=`pwd`
...
I1119 13:02:55.468960 747294720 sched.cpp:164] Version: 0.25.0
I1119 13:02:55.470788 698224640 sched.cpp:262] New master detected at master@127.0.0.1:5050
I1119 13:02:55.470947 698224640 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1119 13:02:55.471592 734216192 sched.cpp:641] Framework registered with 91979713-a32d-4e08-aaea-5dffbfd44e5d-0002
...
```

In http://localhost:5050/#/frameworks[Frameworks tab] you should see a single active framework for `spark-shell`.

.Mesos Management Console (Frameworks tab) with Spark shell active
image::images/spark-mesos-frameworks-one-active.png[align="center"]

TIP: Consult slave logs under `/tmp/mesos/slaves` when facing troubles.

IMPORTANT: Ensure that the versions of Spark of `spark-shell` and as pointed out by `spark.executor.uri` are the same or compatible.

```
scala> sc.parallelize(0 to 10, 8).count
res0: Long = 11
```

.Completed tasks in Mesos Management Console
image::images/spark-mesos-completed-tasks.png[align="center"]

Stop Spark shell.

```
scala> Stopping spark context.
I1119 16:01:37.831179 206073856 sched.cpp:1771] Asked to stop the driver
I1119 16:01:37.831310 698224640 sched.cpp:1040] Stopping framework '91979713-a32d-4e08-aaea-5dffbfd44e5d-0002'
```

=== [[CoarseMesosSchedulerBackend]] CoarseMesosSchedulerBackend

`CoarseMesosSchedulerBackend` is the link:spark-scheduler-backends.adoc[scheduler backend] for Spark on Mesos.

It requires a link:spark-taskscheduler.adoc[Task Scheduler], link:spark-sparkcontext.adoc[Spark context], `mesos://` master URL, and link:spark-security.adoc[Security Manager].

It is a specialized link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend] and implements Mesos's http://mesos.apache.org/api/latest/java/org/apache/mesos/Scheduler.html[org.apache.mesos.Scheduler] interface.

It accepts only two failures before blacklisting a Mesos slave (it is hardcoded and not configurable).

It tracks:

* the number of tasks already submitted (`nextMesosTaskId`)
* the number of cores per task (`coresByTaskId`)
* the total number of cores acquired (`totalCoresAcquired`)
* slave ids with executors (`slaveIdsWithExecutors`)
* slave ids per host (`slaveIdToHost`)
* task ids per slave (`taskIdToSlaveId`)
* How many times tasks on each slave failed (`failuresBySlaveId`)

TIP: `createSchedulerDriver` instantiates Mesos's org.apache.mesos.MesosSchedulerDriver

CoarseMesosSchedulerBackend starts the *MesosSchedulerUtils-mesos-driver* daemon thread with Mesos's http://mesos.apache.org/api/latest/java/org/apache/mesos/MesosSchedulerDriver.html[org.apache.mesos.MesosSchedulerDriver].

==== [[defaultParallelism]] Default Level of Parallelism

The default parallelism is controlled by <<settings, spark.default.parallelism>>.

==== [[settings]] Settings

* `spark.default.parallelism` (default: `8`) - the number of cores to use as <<defaultParallelism,  Default Level of Parallelism>>.
* `spark.cores.max` (default: `Int.MaxValue`) - maximum number of cores to acquire
* `spark.mesos.extra.cores` (default: `0`) - extra cores per slave (`extraCoresPerSlave`) FIXME
* `spark.mesos.constraints` (default: (empty)) - offer constraints FIXME `slaveOfferConstraints`
* `spark.mesos.rejectOfferDurationForUnmetConstraints` (default: `120s`) - reject offers with mismatched constraints in seconds
* `spark.mesos.executor.home` (default: `SPARK_HOME`) - the home directory of Spark for executors. It is only required when no `spark.executor.uri` is set.

=== [[MesosExternalShuffleClient]] MesosExternalShuffleClient

FIXME

=== [[MesosSchedulerBackend]] (Fine)MesosSchedulerBackend

When <<settings, spark.mesos.coarse>> is `false`, Spark on Mesos uses `MesosSchedulerBackend`

==== [[reviveOffers]] reviveOffers

It calls `mesosDriver.reviveOffers()`.

CAUTION: FIXME

=== [[settings]] Settings

* `spark.mesos.coarse` (default: `true`) controls whether the scheduler backend for Mesos works in coarse- (`CoarseMesosSchedulerBackend`) or fine-grained mode (`MesosSchedulerBackend`).

[CAUTION]
====
FIXME Review

*  https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala[MesosClusterScheduler.scala]
* MesosExternalShuffleService
====

=== Introduction to Mesos

* Mesos is _a distributed system kernel_
* Mesos essentially uses a container architecture but is abstracted enough to allow seamless execution of multiple, sometimes identical, distributed systems on the same architecture, minus the resource overhead of virtualization systems. This includes appropriate resource isolation while still allowing for data locality needed for frameworks like MapReduce.
* Mesos' computation-agnostic approach makes it suitable for
+
> Program against your datacenter as a single pool of resources.
* Concepts in Mesos:
** *(Resource) Offers*, i.e. CPU cores, memory, ports, disk
*** Mesos _offers resources_ to frameworks
** *Frameworks*
*** Frameworks _accept_ or _reject offers_
*** (Mesos-specific) Chronos, Marathon
*** Spark, HDFS, YARN (Myriad), Jenkins, Cassandra
** Mesos master
** Mesos API
** Mesos agent

* Mesos is _a scheduler of schedulers_
** Mesos is really an additional layer of (resource) scheduling on top of application frameworks that each bring their own brand of scheduling. Application schedulers interface with a Mesos master setup in a familiar Zookeeper-coordinated active-passive architecture, which passes jobs down to compute slaves to run the application of choice.
* Mesos assigns jobs
* Mesos typically runs with an agent on every virtual machine or bare metal server under management (https://www.joyent.com/blog/mesos-by-the-pound)
* Mesos uses Zookeeper for master election and discovery. Apache Auroa is a scheduler that runs on Mesos.
* Mesos slaves, masters, schedulers, executors, tasks
* Mesos makes use of event-driven message passing.
* Mesos is written in C++, not Java, and includes support for Docker along with other frameworks. Mesos, then, is the core of the Mesos Data Center Operating System, or DCOS, as it was coined by Mesosphere.
* This Operating System includes other handy components such as Marathon and Chronos. Marathon provides cluster-wide “init” capabilities for application in containers like Docker or cgroups. This allows one to programmatically automate the launching of large cluster-based applications. Chronos acts as a Mesos API for longer-running batch type jobs while the core Mesos SDK provides an entry point for other applications like Hadoop and Spark.
* The true goal is a full shared, generic and reusable on demand distributed architecture.
* https://mesosphere.com/infinity/[Infinity] to package and integrate the deployment of clusters
** Out of the box it will include Cassandra, Kafka, Spark, and Akka.
** an early access project
* Apache Myriad = Integrate YARN with Mesos
** making the execution of YARN work on Mesos scheduled systems transparent, multi-tenant, and smoothly managed
** to allow Mesos to centrally schedule YARN work via a Mesos based framework, including a REST API for scaling up or down
** includes a Mesos executor for launching the node manager

=== Schedulers in Mesos

Available scheduler modes:

* *fine-grained mode*
* *coarse-grained mode* - `spark.mesos.coarse=true`

The main difference between these two scheduler modes is the number of tasks per Spark executor per single Mesos executor. In fine-grained mode, there is a single task in a single Spark executor that shares a single Mesos executor with the other Spark executors. In coarse-grained mode, there is a single Spark executor per Mesos executor with many Spark tasks.

*Coarse-grained mode* pre-starts all the executor backends, e.g. link:spark-executor-backends.adoc[Executor Backends], so it has the least overhead comparing to *fine-grain mode*. Since the executors are up before tasks get launched, it is better for interactive sessions. It also means that the resources are locked up in a task.

Spark on Mesos supports link:spark-dynamic-allocation.adoc[dynamic allocation] in the Mesos coarse-grained scheduler since Spark 1.5. It can add/remove executors based on load, i.e. kills idle executors and adds executors when tasks queue up. It needs an link:spark-shuffle-manager.adoc[external shuffle service] on each node.

Mesos Fine-Grained Mode offers a better resource utilization. It has a slower startup for tasks and hence  it is fine for batch and relatively static streaming.

=== Commands

The following command is how you could execute a Spark application on Mesos:

```
./bin/spark-submit --master mesos://iq-cluster-master:5050 --total-executor-cores 2 --executor-memory 3G --conf spark.mesos.role=dev ./examples/src/main/python/pi.py 100
```

=== Other Findings

From https://developer.ibm.com/bluemix/2015/09/09/four-reasons-pay-attention-to-apache-mesos/[Four reasons to pay attention to Apache Mesos]:

> Spark workloads can also be sensitive to the physical characteristics of the infrastructure, such as memory size of the node, access to fast solid state disk, or proximity to the data source.

> to run Spark workloads well you need a resource manager that not only can handle the rapid swings in load inherent in analytics processing, but one that can do to smartly. Matching of the task to the RIGHT resources is crucial and awareness of the physical environment is a must. Mesos is designed to manage this problem on behalf of workloads like Spark.
