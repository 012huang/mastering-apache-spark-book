== [[ApplicationMaster]] ApplicationMaster

`ApplicationMaster` is a standalone application to...FIXME...for link:spark-deploy-mode.adoc#cluster[cluster deploy mode].

<<ExecutorLauncher, ExecutorLauncher>> is a custom `ApplicationMaster` for link:spark-deploy-mode.adoc#client[client deploy mode] (so you can easily distinguish between client and cluster deploy modes for Spark ApplicationMasters using `ps` or `jps` -- see <<ExecutorLauncher, ExecutorLauncher>>).

`ApplicationMaster` and `ExecutorLauncher` are both created by link:spark-yarn-client.adoc[Client] to...FIXME

=== [[sparkContextInitialized]] Setting Internal SparkContext Reference (sparkContextInitialized methods)

[source, scala]
----
sparkContextInitialized(sc: SparkContext): Unit
----

`sparkContextInitialized` passes the call on to the `ApplicationMaster.sparkContextInitialized` that sets the internal `sparkContextRef` reference (to be `sc`).

=== [[sparkContextStopped]] Clearing Internal SparkContext Reference (sparkContextStopped methods)

[source, scala]
----
sparkContextStopped(sc: SparkContext): Boolean
----

`sparkContextStopped` passes the call on to the `ApplicationMaster.sparkContextStopped` that clears the internal `sparkContextRef` reference (i.e. sets it to `null`).

=== [[main]] main

`main` defines `ApplicationMaster` as a standalone application.

It parses command-line arguments using `ApplicationMasterArguments` class.

CAUTION: FIXME What does `ApplicationMasterArguments` do?

It then uses `SparkHadoopUtil.get.runAsSparkUser` to run the main code with a Hadoop `UserGroupInformation` as a thread local variable (distributed to child threads) for authenticating HDFS and YARN calls.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.deploy.SparkHadoopUtil` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.SparkHadoopUtil=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

When running with `DEBUG` log level set for `org.apache.spark.deploy.SparkHadoopUtil` (see above TIP) you should see the following message in the logs:

```
DEBUG running as user: [user]
```

`SparkHadoopUtil.get.runAsSparkUser` function executes a block that instantiates a `ApplicationMaster` instance first to call <<run, run>> method upon it afterwards.

=== [[run]] run

When `run` executes you should see the following INFO in the logs:

```
INFO ApplicationAttemptId: [appAttemptId]
```

=== [[ExecutorLauncher]] ExecutorLauncher

`ExecutorLauncher` comes with no extra functionality when compared to `ApplicationMaster`. It serves as a helper class to run `ApplicationMaster` under another class name in link:spark-deploy-mode.adoc#client[client deploy mode].

With the two different class names (pointing at the same class `ApplicationMaster`) you should be more successful to distinguish between `ExecutorLauncher` (which is really a `ApplicationMaster`) in link:spark-deploy-mode.adoc#client[client deploy mode] and the `ApplicationMaster` in link:spark-deploy-mode.adoc#cluster[cluster deploy mode] using tools like `ps` or `jps`.

NOTE: Consider `ExecutorLauncher` a `ApplicationMaster` for client deploy mode.

=== [[getAttemptId]] Obtain Attempt Id (getAttemptId method)

CAUTION: FIXME

=== [[AMEndpoint]] AMEndpoint RPC Endpoint

==== [[AMEndpoint-onStart]] onStart Callback

When `onStart` is called, `AMEndpoint` communicates with the driver (the `driver` remote RPC Endpoint reference) by sending a one-way `RegisterClusterManager` message with a reference to itself.

After `RegisterClusterManager` has been sent (and received by link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc#RegisterClusterManager[YarnSchedulerEndpoint]) the communication between the RPC endpoints of <<ApplicationMaster, ApplicationMaster>> (YARN) and link:spark-yarn-yarnschedulerbackend.adoc[YarnSchedulerBackend] (the Spark driver) is considered established.
