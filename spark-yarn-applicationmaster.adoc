== [[ApplicationMaster]] ApplicationMaster

`ApplicationMaster` is a <<main, standalone application>> to...FIXME...for link:spark-deploy-mode.adoc#cluster[cluster deploy mode].

.ApplicationMaster's Dependencies
image::images/spark-yarn-ApplicationMaster.png[align="center"]

<<ExecutorLauncher, ExecutorLauncher>> is a custom `ApplicationMaster` for link:spark-deploy-mode.adoc#client[client deploy mode] (so you can easily distinguish between client and cluster deploy modes for Spark's `ApplicationMasters` using `ps` or `jps`).

`ApplicationMaster` and `ExecutorLauncher` are both created when link:spark-yarn-client.adoc#createContainerLaunchContext[`Client` creates a `ContainerLaunchContext`] to launch Spark on YARN.

.Launching ApplicationMaster
image::images/spark-yarn-ApplicationMaster-main.png[align="center"]

NOTE: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/ContainerLaunchContext.html[ContainerLaunchContext] represents all of the information needed by the NodeManager to launch a container.

=== [[registerAM]] registerAM

[source, scala]
----
registerAM(
  _rpcEnv: RpcEnv,
  driverRef: RpcEndpointRef,
  uiAddress: String,
  securityMgr: SecurityManager): Unit
----

`registerAM` is a private helper procedure to...FIXME

It reads link:spark-yarn-settings.adoc#spark.yarn.historyServer.address[spark.yarn.historyServer.address] setting and substitute Hadoop variables to create a complete address of the History Server, i.e. `[address]/history/[appId]/[attemptId]`.

CAUTION: FIXME substitute Hadoop variables?

`registerAM` then creates a link:spark-rpc.adoc#RpcEndpointAddress[RpcEndpointAddress] for link:spark-scheduler-backends-coarse-grained.adoc#CoarseGrainedScheduler[CoarseGrainedScheduler RPC Endpoint] on the driver available on link:spark-sparkenv.adoc#spark.driver.host[spark.driver.host] and link:spark-sparkenv.adoc#spark.driver.port[spark.driver.port].

It link:spark-yarn-yarnrmclient.adoc#register[registers the application master] and link:spark-yarn-yarnrmclient.adoc#allocateResources[requests resources]. Ultimately, `registerAM` <<launchReporterThread, launches reporter thread>>.

It is used in <<runDriver, runDriver>> and <<runExecutorLauncher, runExecutorLauncher>>.

=== [[runDriver]] runDriver

=== [[runExecutorLauncher]] runExecutorLauncher

=== [[launchReporterThread]] launchReporterThread

CAUTION: FIXME

=== [[sparkContextInitialized]] Setting Internal SparkContext Reference (sparkContextInitialized methods)

[source, scala]
----
sparkContextInitialized(sc: SparkContext): Unit
----

`sparkContextInitialized` passes the call on to the `ApplicationMaster.sparkContextInitialized` that sets the internal `sparkContextRef` reference (to be `sc`).

=== [[sparkContextStopped]] Clearing Internal SparkContext Reference (sparkContextStopped methods)

[source, scala]
----
sparkContextStopped(sc: SparkContext): Boolean
----

`sparkContextStopped` passes the call on to the `ApplicationMaster.sparkContextStopped` that clears the internal `sparkContextRef` reference (i.e. sets it to `null`).

=== [[creating-instance]] Creating ApplicationMaster Instance

CAUTION: FIXME

=== [[main]] main

`ApplicationMaster` is a standalone application. When executed, `main` parses <<command-arguments, command-line arguments>> (using `ApplicationMasterArguments` class).

CAUTION: FIXME What does `ApplicationMasterArguments` do?

It then uses `SparkHadoopUtil.get.runAsSparkUser` to run the main code with a Hadoop `UserGroupInformation` as a thread local variable (distributed to child threads) for authenticating HDFS and YARN calls.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.deploy.SparkHadoopUtil` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.SparkHadoopUtil=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

When running with `DEBUG` log level set for `org.apache.spark.deploy.SparkHadoopUtil` (see above TIP) you should see the following message in the logs:

```
DEBUG running as user: [user]
```

`SparkHadoopUtil.get.runAsSparkUser` function executes a block that <<creating-instance, creates a `ApplicationMaster`>> and then <<run, runs>> it.

=== [[command-arguments]] Command-Line Arguments

CAUTION: FIXME

=== [[run]] run

When `run` executes you should see the following INFO in the logs:

```
INFO ApplicationAttemptId: [appAttemptId]
```



=== [[ExecutorLauncher]] ExecutorLauncher

`ExecutorLauncher` comes with no extra functionality when compared to `ApplicationMaster`. It serves as a helper class to run `ApplicationMaster` under another class name in link:spark-deploy-mode.adoc#client[client deploy mode].

With the two different class names (pointing at the same class `ApplicationMaster`) you should be more successful to distinguish between `ExecutorLauncher` (which is really a `ApplicationMaster`) in link:spark-deploy-mode.adoc#client[client deploy mode] and the `ApplicationMaster` in link:spark-deploy-mode.adoc#cluster[cluster deploy mode] using tools like `ps` or `jps`.

NOTE: Consider `ExecutorLauncher` a `ApplicationMaster` for client deploy mode.

=== [[getAttemptId]] Obtain Attempt Id (getAttemptId method)

CAUTION: FIXME
