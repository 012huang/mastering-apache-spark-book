== [[ApplicationMaster]] ApplicationMaster

`ApplicationMaster` represents the YARN ApplicationMaster for a Spark application.

It is a <<main, standalone application>> that YARN NodeManager is going to run inside a YARN container to manage a Spark application's execution on YARN (that accepts <<command-line-parameters, command-line parameters>>, e.g. `--jar`, `--class`, `--arg`, `--properties-file`).

<<ExecutorLauncher, ExecutorLauncher>> is a custom `ApplicationMaster` for link:spark-deploy-mode.adoc#client[client deploy mode] (so you can easily distinguish between client and cluster deploy modes for Spark's `ApplicationMasters` using `ps` or `jps`).

```
$ jps -lm

71253 org.apache.spark.deploy.yarn.ExecutorLauncher --arg 192.168.99.1:50188 --properties-file /tmp/hadoop-jacek/nm-local-dir/usercache/jacek/appcache/application_1468961163409_0001/container_1468961163409_0001_01_000001/__spark_conf__/__spark_conf__.properties

70631 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
70934 org.apache.spark.deploy.SparkSubmit --master yarn --class org.apache.spark.repl.Main --name Spark shell spark-shell
71320 sun.tools.jps.Jps -lm
70731 org.apache.hadoop.yarn.server.nodemanager.NodeManager
```

`ApplicationMaster` and `ExecutorLauncher` are both created when link:spark-yarn-client.adoc#createContainerLaunchContext[`Client` creates a `ContainerLaunchContext`] to launch Spark on YARN.

.Launching ApplicationMaster
image::images/spark-yarn-ApplicationMaster-main.png[align="center"]

NOTE: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/ContainerLaunchContext.html[ContainerLaunchContext] represents all of the information needed by the YARN NodeManager to launch a container.

=== [[main]] main

`ApplicationMaster` is started as a standalone command-line application inside a YARN container on a node.

NOTE: The command-line application is executed as a result of sending a `ContainerLaunchContext` request to launch `ApplicationMaster` to YARN ResourceManager (after link:spark-yarn-client.adoc#createContainerLaunchContext[creating the request for `ApplicationMaster`])

.Submitting ApplicationMaster to YARN NodeManager
image::images/spark-yarn-ApplicationMaster-client-submitApplication.png[align="center"]

When executed, `main` first parses <<command-line-parameters, command-line parameters>> and then uses `SparkHadoopUtil.runAsSparkUser` to run the main code with a Hadoop `UserGroupInformation` as a thread local variable (distributed to child threads) for authenticating HDFS and YARN calls.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.deploy.SparkHadoopUtil` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.SparkHadoopUtil=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

You should see the following message in the logs:

```
DEBUG running as user: [user]
```

`SparkHadoopUtil.runAsSparkUser` function executes a block that <<creating-instance, creates a `ApplicationMaster`>> (passing the <<ApplicationMasterArguments, ApplicationMasterArguments>> instance and a brand new link:spark-yarn-yarnrmclient.adoc[YarnRMClient]) and then <<run, runs>> it.

=== [[command-line-parameters]][[ApplicationMasterArguments]] ApplicationMasterArguments -- Command-Line Parameters Handler

`ApplicationMaster` uses `ApplicationMasterArguments` class to handle command-line parameters.

`ApplicationMasterArguments` is created right after <<main, main>> method has been executed for `args` command-line parameters.

It accepts the following command-line parameters:

* `--jar JAR_PATH` -- the path to the Spark application's JAR file
* `--class CLASS_NAME` -- the name of the Spark application's main class
* `--arg ARG` -- an argument to be passed to the Spark application's main class. There can be multiple `--arg` arguments that are passed in order.
* `--properties-file FILE` -- the path to a custom Spark properties file.
* `--primary-py-file FILE` -- the main Python file to run.
* `--primary-r-file FILE` -- the main R file to run.

When an unsupported parameter is found the following message is printed out to standard error output and `ApplicationMaster` exits with the exit code `1`.

```
Unknown/unsupported param [unknownParam]

Usage: org.apache.spark.deploy.yarn.ApplicationMaster [options]
Options:
  --jar JAR_PATH       Path to your application's JAR file
  --class CLASS_NAME   Name of your application's main class
  --primary-py-file    A main Python file
  --primary-r-file     A main R file
  --arg ARG            Argument to be passed to your application's main class.
                       Multiple invocations are possible, each will be passed in order.
  --properties-file FILE Path to a custom Spark properties file.
```

=== [[registerAM]] Registering ApplicationMaster with YARN ResourceManager and Requesting Resources (registerAM method)

[source, scala]
----
registerAM(
  _rpcEnv: RpcEnv,
  driverRef: RpcEndpointRef,
  uiAddress: String,
  securityMgr: SecurityManager): Unit
----

`registerAM` is a private helper procedure to link:spark-yarn-yarnrmclient.adoc#register[register the `ApplicationMaster`] (with the YARN ResourceManager) and link:spark-yarn-yarnrmclient.adoc#allocateResources[request resources] (given hints about where to allocate containers to be as close to the data as possible).

Internally, it reads link:spark-yarn-settings.adoc#spark.yarn.historyServer.address[spark.yarn.historyServer.address] setting and substitute Hadoop variables to create a complete address of the History Server, i.e. `[address]/history/[appId]/[attemptId]`.

CAUTION: FIXME substitute Hadoop variables?

`registerAM` then creates a link:spark-rpc.adoc#RpcEndpointAddress[RpcEndpointAddress] for link:spark-scheduler-backends-coarse-grained.adoc#CoarseGrainedScheduler[CoarseGrainedScheduler RPC Endpoint] on the driver available on link:spark-sparkenv.adoc#spark.driver.host[spark.driver.host] and link:spark-sparkenv.adoc#spark.driver.port[spark.driver.port].

Ultimately, `registerAM` <<launchReporterThread, launches reporter thread>>.

It is used in <<runDriver, runDriver>> and <<runExecutorLauncher, runExecutorLauncher>>.

=== [[runDriver]] Running Driver in Cluster Mode (runDriver method)

[source, scala]
----
runDriver(securityMgr: SecurityManager): Unit
----

`runDriver` is a private procedure to...???

It starts by registering Web UI security filters.

CAUTION: FIXME Why is this needed? `addAmIpFilter`

It then starts the user class (with the driver) in a separate thread. You should see the following INFO message in the logs:

```
INFO Starting the user application in a separate Thread
```

CAUTION: FIXME Review `startUserApplication`.

You should see the following INFO message in the logs:

```
INFO Waiting for spark context initialization
```

CAUTION: FIXME Review `waitForSparkContextInitialized`

CAUTION: FIXME Finish...

=== [[runExecutorLauncher]] Running Executor Launcher (runExecutorLauncher method)

[source, scala]
----
runExecutorLauncher(securityMgr: SecurityManager): Unit
----

`runExecutorLauncher` reads link:spark-yarn-settings.adoc#spark.yarn.am.port[spark.yarn.am.port] (or assume `0`) and starts the `sparkYarnAM` RPC Environment (in client mode).

CAUTION: FIXME What's client mode?

It then waits for the driver to be available.

CAUTION: FIXME Review `waitForSparkDriver`

It registers Web UI security filters.

CAUTION: FIXME Why is this needed? `addAmIpFilter`

Ultimately, `runExecutorLauncher` <<registerAM, registers the `ApplicationMaster` and requests resources>> and waits until the <<reporterThread, reporterThread>> dies.

CAUTION: FIXME Describe `registerAM`

=== [[reporterThread]] reporterThread

CAUTION: FIXME

=== [[launchReporterThread]] launchReporterThread

CAUTION: FIXME

=== [[sparkContextInitialized]] Setting Internal SparkContext Reference (sparkContextInitialized methods)

[source, scala]
----
sparkContextInitialized(sc: SparkContext): Unit
----

`sparkContextInitialized` passes the call on to the `ApplicationMaster.sparkContextInitialized` that sets the internal `sparkContextRef` reference (to be `sc`).

=== [[sparkContextStopped]] Clearing Internal SparkContext Reference (sparkContextStopped methods)

[source, scala]
----
sparkContextStopped(sc: SparkContext): Boolean
----

`sparkContextStopped` passes the call on to the `ApplicationMaster.sparkContextStopped` that clears the internal `sparkContextRef` reference (i.e. sets it to `null`).

=== [[creating-instance]] Creating ApplicationMaster Instance

CAUTION: FIXME

.ApplicationMaster's Dependencies
image::images/spark-yarn-ApplicationMaster.png[align="center"]

=== [[run]] run

When `ApplicationMaster` is started as a standalone command-line application (using <<main, main>> method), ultimately it calls `run`. The result of calling `run` is the final result of the `ApplicationMaster` command-line application.

[source, scala]
----
run(): Int
----

It sets cluster mode settings, registers a cleanup shutdown hook, schedules `AMDelegationTokenRenewer` and finally registers `ApplicationMaster` for the Spark application (either calling <<runDriver, runDriver>> for cluster mode or <<runExecutorLauncher, runExecutorLauncher>> for client mode).

When `run` runs you should see the following INFO in the logs:

```
INFO ApplicationAttemptId: [appAttemptId]
```

CAUTION: FIXME Explain what `appAttemptId` is and how to change it.

When executed in `cluster` deploy mode, it sets the following system properties:

* link:spark-webui.adoc#spark.ui.port[spark.ui.port] as `0`
* link:spark-configuration.adoc#spark.master[spark.master] as `yarn`
* link:spark-deploy-mode.adoc#spark.submit.deployMode[spark.submit.deployMode] as `cluster`
* link:spark-yarn-settings.adoc#spark.yarn.app.id[spark.yarn.app.id] as application id

CAUTION: FIXME Link to the page about yarn deploy modes (not the general ones).

The cleanup shutdown hook is registered.

CAUTION: FIXME Describe the shutdown hook.

`SecurityManager` is created. If the link:spark-yarn-settings.adoc#spark.yarn.credentials.file[credentials file config] is present, a `AMDelegationTokenRenewer` is started.

CAUTION: FIXME Describe `AMDelegationTokenRenewer#scheduleLoginFromKeytab`

It finally registers `ApplicationMaster` for the Spark application (either calling <<runDriver, runDriver>> for cluster mode or <<runExecutorLauncher, runExecutorLauncher>> for client mode).

Any exceptions in `run` are caught and reported to the logs as ERROR message:

```
ERROR Uncaught exception: [exception]
```

And the application run attempt is <<finish, finished>> with `FAILED` status and `EXIT_UNCAUGHT_EXCEPTION` (10) exit code.

=== [[finish]] finish

CAUTION: FIXME

=== [[ExecutorLauncher]] ExecutorLauncher

`ExecutorLauncher` comes with no extra functionality when compared to `ApplicationMaster`. It serves as a helper class to run `ApplicationMaster` under another class name in link:spark-deploy-mode.adoc#client[client deploy mode].

With the two different class names (pointing at the same class `ApplicationMaster`) you should be more successful to distinguish between `ExecutorLauncher` (which is really a `ApplicationMaster`) in link:spark-deploy-mode.adoc#client[client deploy mode] and the `ApplicationMaster` in link:spark-deploy-mode.adoc#cluster[cluster deploy mode] using tools like `ps` or `jps`.

NOTE: Consider `ExecutorLauncher` a `ApplicationMaster` for client deploy mode.

=== [[getAttemptId]] Obtain Attempt Id (getAttemptId method)

CAUTION: FIXME
