== [[ApplicationMaster]] ApplicationMaster

`ApplicationMaster` is a <<main, standalone application>> to...FIXME...for link:spark-deploy-mode.adoc#cluster[cluster deploy mode].

.ApplicationMaster's Dependencies
image::images/spark-yarn-ApplicationMaster.png[align="center"]

<<ExecutorLauncher, ExecutorLauncher>> is a custom `ApplicationMaster` for link:spark-deploy-mode.adoc#client[client deploy mode] (so you can easily distinguish between client and cluster deploy modes for Spark's `ApplicationMasters` using `ps` or `jps`).

`ApplicationMaster` and `ExecutorLauncher` are both created when link:spark-yarn-client.adoc#createContainerLaunchContext[`Client` creates a `ContainerLaunchContext`] to launch Spark on YARN.

.Launching ApplicationMaster
image::images/spark-yarn-ApplicationMaster-main.png[align="center"]

NOTE: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/ContainerLaunchContext.html[ContainerLaunchContext] represents all of the information needed by the YARN NodeManager to launch a container.

.Submitting ApplicationMaster to YARN NodeManager
image::images/spark-yarn-ApplicationMaster-client-submitApplication.png[align="center"]

=== [[registerAM]] Registering ApplicationMaster with YARN ResourceManager and Requesting Resources (registerAM method)

[source, scala]
----
registerAM(
  _rpcEnv: RpcEnv,
  driverRef: RpcEndpointRef,
  uiAddress: String,
  securityMgr: SecurityManager): Unit
----

`registerAM` is a private helper procedure to link:spark-yarn-yarnrmclient.adoc#register[register the `ApplicationMaster`] (with the YARN ResourceManager) and link:spark-yarn-yarnrmclient.adoc#allocateResources[requests resources] (given hints about where to allocate containers to be as close to the data as possible).

Internally, it reads link:spark-yarn-settings.adoc#spark.yarn.historyServer.address[spark.yarn.historyServer.address] setting and substitute Hadoop variables to create a complete address of the History Server, i.e. `[address]/history/[appId]/[attemptId]`.

CAUTION: FIXME substitute Hadoop variables?

`registerAM` then creates a link:spark-rpc.adoc#RpcEndpointAddress[RpcEndpointAddress] for link:spark-scheduler-backends-coarse-grained.adoc#CoarseGrainedScheduler[CoarseGrainedScheduler RPC Endpoint] on the driver available on link:spark-sparkenv.adoc#spark.driver.host[spark.driver.host] and link:spark-sparkenv.adoc#spark.driver.port[spark.driver.port].

Ultimately, `registerAM` <<launchReporterThread, launches reporter thread>>.

It is used in <<runDriver, runDriver>> and <<runExecutorLauncher, runExecutorLauncher>>.

=== [[runDriver]] Running Driver in Cluster Mode (runDriver method)

[source, scala]
----
runDriver(securityMgr: SecurityManager): Unit
----

`runDriver` is a private procedure to...???

It starts by registering Web UI security filters.

CAUTION: FIXME Why is this needed? `addAmIpFilter`

It then starts the user class (with the driver) in a separate thread. You should see the following INFO message in the logs:

```
INFO Starting the user application in a separate Thread
```

CAUTION: FIXME Review `startUserApplication`.

You should see the following INFO message in the logs:

```
INFO Waiting for spark context initialization
```

CAUTION: FIXME Review `waitForSparkContextInitialized`

CAUTION: FIXME Finish...

=== [[runExecutorLauncher]] Running Executor Launcher (runExecutorLauncher method)

[source, scala]
----
runExecutorLauncher(securityMgr: SecurityManager): Unit
----

`runExecutorLauncher` reads link:spark-yarn-settings.adoc#spark.yarn.am.port[spark.yarn.am.port] (or assume `0`) and starts the `sparkYarnAM` RPC Environment (in client mode).

CAUTION: FIXME What's client mode?

It then waits for the driver to be available.

CAUTION: FIXME Review `waitForSparkDriver`

It registers Web UI security filters.

CAUTION: FIXME Why is this needed? `addAmIpFilter`

Ultimately, `runExecutorLauncher` <<registerAM, registers the `ApplicationMaster` and requests resources>> and waits until the <<reporterThread, reporterThread>> dies.

CAUTION: FIXME Describe `registerAM`

=== [[reporterThread]] reporterThread

CAUTION: FIXME

=== [[launchReporterThread]] launchReporterThread

CAUTION: FIXME

=== [[sparkContextInitialized]] Setting Internal SparkContext Reference (sparkContextInitialized methods)

[source, scala]
----
sparkContextInitialized(sc: SparkContext): Unit
----

`sparkContextInitialized` passes the call on to the `ApplicationMaster.sparkContextInitialized` that sets the internal `sparkContextRef` reference (to be `sc`).

=== [[sparkContextStopped]] Clearing Internal SparkContext Reference (sparkContextStopped methods)

[source, scala]
----
sparkContextStopped(sc: SparkContext): Boolean
----

`sparkContextStopped` passes the call on to the `ApplicationMaster.sparkContextStopped` that clears the internal `sparkContextRef` reference (i.e. sets it to `null`).

=== [[creating-instance]] Creating ApplicationMaster Instance

CAUTION: FIXME

=== [[main]] main

`ApplicationMaster` is a standalone command-line application.

NOTE: The command-line application is executed as a result of sending a `ContainerLaunchContext` request to launch `ApplicationMaster` to YARN ResourceManager (after link:spark-yarn-client.adoc#createContainerLaunchContext[creating the request for `ApplicationMaster`])

When executed, `main` parses <<command-arguments, command-line arguments>> (using `ApplicationMasterArguments` class).

CAUTION: FIXME What does `ApplicationMasterArguments` do?

It then uses `SparkHadoopUtil.get.runAsSparkUser` to run the main code with a Hadoop `UserGroupInformation` as a thread local variable (distributed to child threads) for authenticating HDFS and YARN calls.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.deploy.SparkHadoopUtil` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.SparkHadoopUtil=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

You should see the following message in the logs:

```
DEBUG running as user: [user]
```

`SparkHadoopUtil.get.runAsSparkUser` function executes a block that <<creating-instance, creates a `ApplicationMaster`>> and then <<run, runs>> it.

=== [[command-arguments]] Command-Line Arguments

CAUTION: FIXME

=== [[run]] run

When `ApplicationMaster` is started as a standalone command-line application (using <<main, main>> method), ultimately it calls `run`. The result of calling `run` is the final result of the `ApplicationMaster` command-line application.

[source, scala]
----
run(): Int
----

It sets cluster mode settings, registers a cleanup shutdown hook, schedules `AMDelegationTokenRenewer` and finally registers `ApplicationMaster` for the Spark application (either calling <<runDriver, runDriver>> for cluster mode or <<runExecutorLauncher, runExecutorLauncher>> for client mode).

When `run` runs you should see the following INFO in the logs:

```
INFO ApplicationAttemptId: [appAttemptId]
```

CAUTION: FIXME Explain what `appAttemptId` is and how to change it.

When executed in `cluster` deploy mode, it sets the following system properties:

* link:spark-webui.adoc#spark.ui.port[spark.ui.port] as `0`
* link:spark-configuration.adoc#spark.master[spark.master] as `yarn`
* link:spark-deploy-mode.adoc#spark.submit.deployMode[spark.submit.deployMode] as `cluster`
* link:spark-yarn-settings.adoc#spark.yarn.app.id[spark.yarn.app.id] as application id

CAUTION: FIXME Link to the page about yarn deploy modes (not the general ones).

The cleanup shutdown hook is registered.

CAUTION: FIXME Describe the shutdown hook.

`SecurityManager` is created. If the link:spark-yarn-settings.adoc#spark.yarn.credentials.file[credentials file config] is present, a `AMDelegationTokenRenewer` is started.

CAUTION: FIXME Describe `AMDelegationTokenRenewer#scheduleLoginFromKeytab`

It finally registers `ApplicationMaster` for the Spark application (either calling <<runDriver, runDriver>> for cluster mode or <<runExecutorLauncher, runExecutorLauncher>> for client mode).

Any exceptions in `run` are caught and reported to the logs as ERROR message:

```
ERROR Uncaught exception: [exception]
```

And the application run attempt is <<finish, finished>> with `FAILED` status and `EXIT_UNCAUGHT_EXCEPTION` (10) exit code.

=== [[finish]] finish

CAUTION: FIXME

=== [[ExecutorLauncher]] ExecutorLauncher

`ExecutorLauncher` comes with no extra functionality when compared to `ApplicationMaster`. It serves as a helper class to run `ApplicationMaster` under another class name in link:spark-deploy-mode.adoc#client[client deploy mode].

With the two different class names (pointing at the same class `ApplicationMaster`) you should be more successful to distinguish between `ExecutorLauncher` (which is really a `ApplicationMaster`) in link:spark-deploy-mode.adoc#client[client deploy mode] and the `ApplicationMaster` in link:spark-deploy-mode.adoc#cluster[cluster deploy mode] using tools like `ps` or `jps`.

NOTE: Consider `ExecutorLauncher` a `ApplicationMaster` for client deploy mode.

=== [[getAttemptId]] Obtain Attempt Id (getAttemptId method)

CAUTION: FIXME
