== [[SparkPlanner]] SparkPlanner -- Default Query Planner (no Hive Support)

`SparkPlanner` is a Catalyst link:spark-sql-catalyst-QueryPlanner.adoc[query planner] that converts a link:spark-sql-LogicalPlan.adoc[logical plan] to a collection of link:spark-sql-SparkPlan.adoc[physical plans] using <<strategies, execution planning strategies>> with support for <<extraStrategies, extra strategies>> (by means of <<experimentalMethods, ExperimentalMethods>>).

`SparkPlanner` is <<creating-instance, created>> in:

1. `BaseSessionStateBuilder`
1. `HiveSessionStateBuilder`
1. Structured Streaming's `IncrementalExecution`

`SparkPlanner` is available as link:spark-sql-SessionState.adoc#planner[planner] of a `SessionState`.

[source, scala]
----
val spark: SparkSession = ...
spark.sessionState.planner
----

[[strategies]]
.SparkPlanner's Execution Planning Strategies (in alphabetic order)
[cols="1,2",options="header",width="100%"]
|===
| SparkStrategy
| Description

| link:spark-sql-SparkStrategy-Aggregation.adoc[Aggregation]
|

| link:spark-sql-SparkStrategy-BasicOperators.adoc[BasicOperators]
|

| link:spark-sql-SparkStrategy-DataSourceStrategy.adoc[DataSourceStrategy]
|

| link:spark-sql-SparkStrategy-DDLStrategy.adoc[DDLStrategy]
|

| link:spark-sql-SparkStrategy-FileSourceStrategy.adoc[FileSourceStrategy]
|

| `InMemoryScans`
|

| link:spark-sql-SparkStrategy-JoinSelection.adoc[JoinSelection]
|

| `SpecialLimits`
|
|===

`SparkPlanner` defines `numPartitions` method that returns the value of link:spark-sql-SQLConf.adoc#spark.sql.shuffle.partitions[spark.sql.shuffle.partitions] for the number of partitions to use for link:spark-sql-joins.adoc[joins] and link:spark-sql-basic-aggregation.adoc[aggregations]. It is later used in link:spark-sql-SparkStrategy-BasicOperators.adoc[BasicOperators] execution planning strategy with link:spark-sql-LogicalPlan-Repartition-RepartitionByExpression.adoc[RepartitionByExpression] logical operator.

The required `strategies` collection uses `extraStrategies` extension point (defined as the argument to the constructor) and the predefined collection of `Strategy` objects.

`collectPlaceholders` required method returns a collection of link:spark-sql-SparkStrategy.adoc#PlanLater[PlanLater] physical operators and the corresponding link:spark-sql-LogicalPlan.adoc[logical plans].

`prunePlans` required method does nothing, i.e. it returns what it gets directly.

[NOTE]
====
The order of the `SparkStrategy` execution planning strategies in `SparkPlanner` is as follows:

1. [[extraStrategies]] ``ExperimentalMethods``'s link:spark-sql-ExperimentalMethods.adoc#extraStrategies[extraStrategies]
2. link:spark-sql-SparkStrategy-FileSourceStrategy.adoc[FileSourceStrategy]
3. link:spark-sql-SparkStrategy-DataSourceStrategy.adoc[DataSourceStrategy]
4. link:spark-sql-SparkStrategy-DDLStrategy.adoc[DDLStrategy]
5. SpecialLimits
6. link:spark-sql-SparkStrategy-Aggregation.adoc[Aggregation]
7. link:spark-sql-SparkStrategy-JoinSelection.adoc[JoinSelection]
8. InMemoryScans
9. link:spark-sql-SparkStrategy-BasicOperators.adoc[BasicOperators]
====

NOTE: `SparkPlanner` extends link:spark-sql-catalyst-QueryPlanner.adoc#SparkStrategies[SparkStrategies] abstract class.

=== [[creating-instance]] Creating SparkPlanner Instance

`SparkPlanner` takes the following when created:

* [[sparkContext]] link:spark-sparkcontext.adoc[SparkContext],
* [[conf]] link:spark-sql-SQLConf.adoc[SQLConf]
* [[experimentalMethods]] link:spark-sql-ExperimentalMethods.adoc[ExperimentalMethods]
