== Anatomy of Spark Application

Every Spark application starts at instantiating a link:spark-sparkcontext.adoc[Spark context]. For it to work, you have to link:spark-configuration.adoc[create a Spark configuration using SparkConf] or use a link:spark-sparkcontext.adoc#creating-sparkcontext[custom SparkContext constructor].

[source, scala]
----
package pl.japila.spark

import org.apache.spark.{SparkContext, SparkConf}

object SparkMeApp {
  def main(args: Array[String]) {

    val masterURL = "local[*]"  // <1>

    val conf = new SparkConf()  // <2>
      .setAppName("SparkMe Application")
      .setMaster(masterURL)

    val sc = new SparkContext(conf) // <3>

    val fileName = util.Try(args(0)).getOrElse("build.sbt")

    val lines = sc.textFile(fileName).cache() // <4>

    val c = lines.count() // <5>
    println(s"There are $c lines in $fileName")
  }
}
----
<1> link:spark-deployment-modes.adoc#master-urls[Master URL] to connect the application to
<2> Create Spark configuration
<3> Create Spark context
<4> Create `lines` RDD
<5> Execute `count` action

TIP: link:spark-shell.adoc[Spark shell] creates a Spark context and SQL context for you at startup.

When a Spark application starts (using link:spark-submit.adoc[spark-submit script] or as a standalone application), it connects to link:spark-master.adoc[Spark master] as described by link:spark-deployment-modes.adoc#master-urls[master URL]. It is part of link:spark-sparkcontext.adoc#initialization[Spark context's initialization].

.Submitting Spark application to master using master URL
image::diagrams/spark-submit-master-workers.png[align="center"]

NOTE: Your Spark application can run locally or on the cluster which is based on the cluster manager and the deploy mode (`--deploy-mode`). Refer to link:spark-deployment-modes.adoc[Deployment Modes].

You can then link:spark-rdd.adoc#creating-rdds[create RDDs], link:spark-rdd-operations.adoc#transformations[transform them to other RDDs] and ultimately link:spark-rdd-operations.adoc#actions[execute actions]. You can also link:spark-rdd-caching.adoc[cache interim RDDs] to speed up data processing.

After all the data processing is completed, the Spark application finishes by link:spark-sparkcontext.adoc#stopping-spark-context[stopping the Spark context].
