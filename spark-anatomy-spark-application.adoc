== Anatomy of Spark Application

Every Spark application begins at instantiating a link:spark-sparkcontext.adoc[Spark context]. For it to work, you have to link:spark-configuration.adoc[create a Spark configuration using SparkConfig].

[source, scala]
----
package pl.japila.spark

import org.apache.spark.{SparkContext, SparkConf}

object SparkMeApp {
  def main(args: Array[String]) {

    val conf = new SparkConf()  // <1>
      .setAppName("SparkMe Application")
      .setMaster("local[*]")

    val sc = new SparkContext(conf) // <2>

    val fileName = util.Try(args(0)).getOrElse("build.sbt")

    val lines = sc.textFile(fileName).cache() // <3>

    val c = lines.count() // <4>
    println(s"There are $c lines in $fileName")
  }
}
----
<1> Create Spark configuration
<2> Create Spark context
<3> Create `lines` RDD
<4> Execute `count` action

TIP: link:spark-shell.adoc[Spark shell] creates a Spark context and SQL context for you at startup.

When a Spark application starts using link:spark-submit.adoc[spark-submit script] or as a standalone application, it connects to link:spark-execution-model.adoc#master[Spark master] as described by link:spark-deployment-modes.adoc#master-urls[master URL]. It is part of Spark context's initialization.

.Submitting Spark application to master using master URL
image::diagrams/spark-submit-master-workers.png[align="center"]

You can then link:spark-rdd.adoc#creating-rdds[create RDDs], link:spark-rdd-operations.adoc#transformations[transform them to other RDDs] and ultimately link:spark-rdd-operations.adoc#actions[execute actions]. You could also link:spark-rdd-caching.adoc[cache interim RDDs] to speed up data processing.

After all the data processing is finished, the Spark application finishes by link:spark-sparkcontext.adoc#stopping-spark-context[stopping the Spark context] or simply exiting.
