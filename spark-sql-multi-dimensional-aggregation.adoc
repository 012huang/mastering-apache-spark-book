== Multi-Dimensional Aggregation

**Multi-dimensional aggregation operators** are enhanced variants of link:spark-sql-basic-aggregation.adoc#groupBy[groupBy] operator with higher efficiency (that work similarly to `groupBy` connected by `union`).

[[aggregate-operators]]
.Multi-dimensional Aggregate Operators (in alphabetical order)
[width="100%",cols="1,1,2",options="header"]
|===
| Operator
| Return Type
| Description

| <<cube, cube>>
| link:spark-sql-basic-aggregation.adoc#RelationalGroupedDatasets[RelationalGroupedDatasets]
| Calculates subtotals and totals over all combinations of groups

| <<rollup, rollup>>
| link:spark-sql-basic-aggregation.adoc#RelationalGroupedDatasets[RelationalGroupedDatasets]
| Calculates subtotals and totals over (ordered) combination of groups
|===

[NOTE]
====
Beside <<cube, cube>> and <<rollup, rollup>> multi-dimensional operators, Spark SQL supports <<grouping-sets, GROUPING SETS>> operator as a SQL clause in SQL mode only, i.e.

```
scala> sql("""
  SELECT grouping_id()
  FROM range(4)
  GROUP BY (id % 2)
  GROUPING SETS (id % 2)
  """).queryExecution.logical
res0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
'GroupingSets [ArrayBuffer(('id % 2))], [('id % 2)], [unresolvedalias('grouping_id(), None)]
+- 'UnresolvedTableValuedFunction range, [4]
```
====

=== [[cube]] `cube` Operator

[source, scala]
----
cube(cols: Column*): RelationalGroupedDataset
cube(col1: String, cols: String*): RelationalGroupedDataset
----

`cube` multi-dimensional aggregate operator is an extension of link:spark-sql-basic-aggregation.adoc#groupBy[groupBy] operator that allows calculating subtotals and totals across all combinations of specified group of `n + 1` dimensions (with `n` being the number of columns as `cols` and `col1` and `1` for where values become `null`, i.e. undefined).

`cube` returns link:spark-sql-basic-aggregation.adoc#RelationalGroupedDataset[RelationalGroupedDataset] that you can use to execute aggregate function on.

NOTE: `cube` is more than <<rollup, rollup>> operator, i.e. `cube` does `rollup` with aggregation over all the missing combinations given the columns.

[source, scala]
----
val sales = Seq(
  ("Warsaw", 2016, 100),
  ("Warsaw", 2017, 200),
  ("Boston", 2015, 50),
  ("Boston", 2016, 150),
  ("Toronto", 2017, 50)
).toDF("city", "year", "amount")

val q = sales.cube("city", "year")
  .agg(sum("amount") as "amount/year")
  .sort($"city".desc_nulls_last, $"year".asc_nulls_last)

scala> q.show
+-------+----+-----------+
|   city|year|amount/year|
+-------+----+-----------+
| Warsaw|2016|        100|  <-- total in Warsaw in 2016
| Warsaw|2017|        200|  <-- total in Warsaw in 2017
| Warsaw|null|        300|  <-- total in Warsaw (across all years)
|Toronto|2017|         50|
|Toronto|null|         50|
| Boston|2015|         50|
| Boston|2016|        150|
| Boston|null|        200|
|   null|2015|         50|  <-- total in 2015 (across all cities)
|   null|2016|        250|
|   null|2017|        250|
|   null|null|        550|  <-- total sales (all cities and years)
+-------+----+-----------+
----

=== [[grouping-sets]] GROUPING SETS SQL Clause

```
GROUP BY ... GROUPING SETS (...)
```

NOTE: `GROUPING SETS` SQL clause is parsed in link:spark-sql-AstBuilder.adoc#withAggregation[`withAggregation` parsing handler] in `AstBuilder`.

`GROUPING SETS` SQL clause generates a result set equivalent to a `UNION ALL` of multiple simple `GROUP BY` clauses.

```
scala> sql("""
     |   SELECT grouping_id()
     |   FROM range(4)
     |   GROUP BY (id % 2)
     |   GROUPING SETS (id % 2)
     |   """).explain(true)
== Parsed Logical Plan ==
'GroupingSets [ArrayBuffer(('id % 2))], [('id % 2)], [unresolvedalias('grouping_id(), None)]
+- 'UnresolvedTableValuedFunction range, [4]

== Analyzed Logical Plan ==
grouping_id(): int
Aggregate [(id#217L % cast(2 as bigint))#221L, spark_grouping_id#219], [spark_grouping_id#219 AS grouping_id()#218]
+- Expand [List(id#217L, (id#217L % cast(2 as bigint))#220L, 0)], [id#217L, (id#217L % cast(2 as bigint))#221L, spark_grouping_id#219]
   +- Project [id#217L, (id#217L % cast(2 as bigint)) AS (id#217L % cast(2 as bigint))#220L]
      +- Range (0, 4, step=1, splits=None)

== Optimized Logical Plan ==
Aggregate [(id#217L % cast(2 as bigint))#221L, spark_grouping_id#219], [spark_grouping_id#219 AS grouping_id()#218]
+- Expand [List((id#217L % cast(2 as bigint))#220L, 0)], [(id#217L % cast(2 as bigint))#221L, spark_grouping_id#219]
   +- Project [(id#217L % 2) AS (id#217L % cast(2 as bigint))#220L]
      +- Range (0, 4, step=1, splits=None)

== Physical Plan ==
*HashAggregate(keys=[(id#217L % cast(2 as bigint))#221L, spark_grouping_id#219], functions=[], output=[grouping_id()#218])
+- Exchange hashpartitioning((id#217L % cast(2 as bigint))#221L, spark_grouping_id#219, 200)
   +- *HashAggregate(keys=[(id#217L % cast(2 as bigint))#221L, spark_grouping_id#219], functions=[], output=[(id#217L % cast(2 as bigint))#221L, spark_grouping_id#219])
      +- *Expand [List((id#217L % cast(2 as bigint))#220L, 0)], [(id#217L % cast(2 as bigint))#221L, spark_grouping_id#219]
         +- *Project [(id#217L % 2) AS (id#217L % cast(2 as bigint))#220L]
            +- *Range (0, 4, step=1, splits=None)
```

=== [[rollup]] `rollup` Operator

[source, scala]
----
rollup(cols: Column*): RelationalGroupedDataset
rollup(col1: String, cols: String*): RelationalGroupedDataset
----

`rollup` multi-dimensional aggregate operator is an extension of link:spark-sql-basic-aggregation.adoc#groupBy[groupBy] operator that calculates subtotals and totals across specified group of `n + 1` dimensions (with `n` being the number of columns as `cols` and `col1` and `1` for where values become `null`, i.e. undefined).

[NOTE]
====
`rollup` operator is commonly used for analysis over hierarchical data; e.g. total salary by department, division, and company-wide total.

See PostgreSQL's https://www.postgresql.org/docs/current/static/queries-table-expressions.html#QUERIES-GROUPING-SETS[7.2.4. GROUPING SETS, CUBE, and ROLLUP]
====

NOTE: `rollup` operator is equivalent to `GROUP BY \... WITH ROLLUP` in SQL (which in turn is equivalent to `GROUP BY \... GROUPING SETS \((a,b,c),(a,b),(a),())` when used with 3 columns: `a`, `b`, and `c`).

From https://technet.microsoft.com/en-us/library/bb522495(v=sql.105).aspx[Using GROUP BY with ROLLUP, CUBE, and GROUPING SETS] in Microsoft's TechNet:

> The ROLLUP, CUBE, and GROUPING SETS operators are extensions of the GROUP BY clause. The ROLLUP, CUBE, or GROUPING SETS operators can generate the same result set as when you use UNION ALL to combine single grouping queries; however, using one of the GROUP BY operators is usually more efficient.

From PostgreSQL's https://www.postgresql.org/docs/current/static/queries-table-expressions.html#QUERIES-GROUPING-SETS[7.2.4. GROUPING SETS, CUBE, and ROLLUP]:

> References to the grouping columns or expressions are replaced by null values in result rows for grouping sets in which those columns do not appear.

From https://technet.microsoft.com/en-us/library/ms189305(v=sql.90).aspx[Summarizing Data Using ROLLUP] in Microsoft's TechNet:

> The ROLLUP operator is useful in generating reports that contain subtotals and totals. (...)
> ROLLUP generates a result set that shows aggregates for a hierarchy of values in the selected columns.

[[rollup-example-inventory]]
[source, scala]
----
// Borrowed from Microsoft's "Summarizing Data Using ROLLUP" article
val inventory = Seq(
  ("table", "blue", 124),
  ("table", "red", 223),
  ("chair", "blue", 101),
  ("chair", "red", 210)).toDF("item", "color", "quantity")

scala> inventory.show
+-----+-----+--------+
| item|color|quantity|
+-----+-----+--------+
|chair| blue|     101|
|chair|  red|     210|
|table| blue|     124|
|table|  red|     223|
+-----+-----+--------+

// ordering and empty rows done manually for demo purposes
scala> inventory.rollup("item", "color").sum().show
+-----+-----+-------------+
| item|color|sum(quantity)|
+-----+-----+-------------+
|chair| blue|          101|
|chair|  red|          210|
|chair| null|          311|
|     |     |             |
|table| blue|          124|
|table|  red|          223|
|table| null|          347|
|     |     |             |
| null| null|          658|
+-----+-----+-------------+
----

From Hive's https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation,+Cube,+Grouping+and+Rollup#EnhancedAggregation,Cube,GroupingandRollup-CubesandRollups[Cubes and Rollups]:

> WITH ROLLUP is used with the GROUP BY only. ROLLUP clause is used with GROUP BY to compute the aggregate at the hierarchy levels of a dimension.

> GROUP BY a, b, c with ROLLUP assumes that the hierarchy is "a" drilling down to "b" drilling down to "c".

> GROUP BY a, b, c, WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ( (a, b, c), (a, b), (a), ( )).

NOTE: Read up on ROLLUP in Hive's LanguageManual in link:++https://cwiki.apache.org/confluence/display/Hive/LanguageManual+GroupBy#LanguageManualGroupBy-GroupingSets,Cubes,Rollups,andtheGROUPING__IDFunction++[Grouping Sets, Cubes, Rollups, and the GROUPING__ID Function].

[[rollup-example-quarterly-scores]]
[source, scala]
----
// Borrowed from http://stackoverflow.com/a/27222655/1305344
val quarterlyScores = Seq(
  ("winter2014", "Agata", 99),
  ("winter2014", "Jacek", 97),
  ("summer2015", "Agata", 100),
  ("summer2015", "Jacek", 63),
  ("winter2015", "Agata", 97),
  ("winter2015", "Jacek", 55),
  ("summer2016", "Agata", 98),
  ("summer2016", "Jacek", 97)).toDF("period", "student", "score")

scala> quarterlyScores.show
+----------+-------+-----+
|    period|student|score|
+----------+-------+-----+
|winter2014|  Agata|   99|
|winter2014|  Jacek|   97|
|summer2015|  Agata|  100|
|summer2015|  Jacek|   63|
|winter2015|  Agata|   97|
|winter2015|  Jacek|   55|
|summer2016|  Agata|   98|
|summer2016|  Jacek|   97|
+----------+-------+-----+

// ordering and empty rows done manually for demo purposes
scala> quarterlyScores.rollup("period", "student").sum("score").show
+----------+-------+----------+
|    period|student|sum(score)|
+----------+-------+----------+
|winter2014|  Agata|        99|
|winter2014|  Jacek|        97|
|winter2014|   null|       196|
|          |       |          |
|summer2015|  Agata|       100|
|summer2015|  Jacek|        63|
|summer2015|   null|       163|
|          |       |          |
|winter2015|  Agata|        97|
|winter2015|  Jacek|        55|
|winter2015|   null|       152|
|          |       |          |
|summer2016|  Agata|        98|
|summer2016|  Jacek|        97|
|summer2016|   null|       195|
|          |       |          |
|      null|   null|       706|
+----------+-------+----------+
----

From PostgreSQL's https://www.postgresql.org/docs/current/static/queries-table-expressions.html#QUERIES-GROUPING-SETS[7.2.4. GROUPING SETS, CUBE, and ROLLUP]:

> The individual elements of a CUBE or ROLLUP clause may be either individual expressions, or sublists of elements in parentheses. In the latter case, the sublists are treated as single units for the purposes of generating the individual grouping sets.

[[rollup-example-sublists]]
[source, scala]
----
// given the above inventory dataset

// using struct function
scala> inventory.rollup(struct("item", "color") as "(item,color)").sum().show
+------------+-------------+
|(item,color)|sum(quantity)|
+------------+-------------+
| [table,red]|          223|
|[chair,blue]|          101|
|        null|          658|
| [chair,red]|          210|
|[table,blue]|          124|
+------------+-------------+

// using expr function
scala> inventory.rollup(expr("(item, color)") as "(item, color)").sum().show
+-------------+-------------+
|(item, color)|sum(quantity)|
+-------------+-------------+
|  [table,red]|          223|
| [chair,blue]|          101|
|         null|          658|
|  [chair,red]|          210|
| [table,blue]|          124|
+-------------+-------------+
----

Internally, `rollup` link:spark-sql-dataset-operators.adoc#toDF[converts the `Dataset` into a `DataFrame`] (i.e. uses link:spark-sql-RowEncoder.adoc[RowEncoder] as the encoder) and then creates a <<RelationalGroupedDataset, RelationalGroupedDataset>> (with `RollupType` group type).

NOTE: <<Rollup, Rollup>> expression represents `GROUP BY \... WITH ROLLUP` in SQL in Spark's Catalyst Expression tree (after `AstBuilder` link:spark-sql-AstBuilder.adoc#withAggregation[parses a structured query with aggregation]).

TIP: Read up on `rollup` in https://www.compose.com/articles/deeper-into-postgres-9-5-new-group-by-options-for-aggregation/[Deeper into Postgres 9.5 - New Group By Options for Aggregation].

=== [[Rollup]] `Rollup` GroupingSet with CodegenFallback Expression (for `rollup` Operator)

[source, scala]
----
Rollup(groupByExprs: Seq[Expression])
extends GroupingSet
----

`Rollup` expression represents <<rollup, rollup>> operator in Spark's Catalyst Expression tree (after `AstBuilder` link:spark-sql-AstBuilder.adoc#withAggregation[parses a structured query with aggregation]).

NOTE: `GroupingSet` is an link:spark-sql-catalyst-Expression.adoc[Expression] with link:spark-sql-catalyst-Expression.adoc#CodegenFallback[CodegenFallback] support.
