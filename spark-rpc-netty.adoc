== Netty-based RpcEnv

TIP: Read link:spark-rpc.adoc[RPC Environment (RpcEnv)] about the concept of RpcEnv in Spark.

The class https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala[org.apache.spark.rpc.netty.NettyRpcEnv] is the implementation of link:spark-rpc.adoc[RpcEnv] using http://netty.io/[Netty] - _"an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients"_.

It uses Java's built-in serialization (the implementation of `JavaSerializerInstance`).

CAUTION: FIXME What other choices of JavaSerializerInstance are available in Spark?

NettyRpcEnv is only started on the driver.

When NettyRpcEnv starts, the following INFO message is printed out in the logs:

```
INFO Utils: Successfully started service 'NettyRpcEnv' on port 0.
```

=== [[thread-pools]] Thread Pools

==== dispatcher-event-loop-ID

NettyRpcEnv's Dispatcher uses the daemon fixed thread pool with <<settings, spark.rpc.netty.dispatcher.numThreads>> threads.

Thread names are formatted as `dispatcher-event-loop-ID`, where `ID` is a unique, sequentially assigned integer.

It starts the message processing loop on all of the threads.

==== netty-rpc-env-timeout

NettyRpcEnv uses the daemon single-thread scheduled thread pool `netty-rpc-env-timeout`.

```
"netty-rpc-env-timeout" #87 daemon prio=5 os_prio=31 tid=0x00007f887775a000 nid=0xc503 waiting on condition [0x0000000123397000]
```

==== netty-rpc-connection-ID

NettyRpcEnv uses the daemon cached thread pool with up to <<settings, spark.rpc.connect.threads>> threads.

Thread names are formatted as `netty-rpc-connection-ID`, where `ID` is a unique, sequentially assigned integer.

=== [[settings]] Settings

The Netty-based implementation uses the following properties:

* `spark.shuffle.io.numConnectionsPerPeer` always equals `1`
* `spark.rpc.io.threads` (default: `0`; maximum: `8`) - the number of threads to use for the Netty client and server thread pools.
** `spark.shuffle.io.serverThreads` (default: the value of `spark.rpc.io.threads`)
** `spark.shuffle.io.clientThreads` (default: the value of `spark.rpc.io.threads`)
* `spark.rpc.netty.dispatcher.numThreads` (default: the number of processors available to JVM)
* `spark.rpc.connect.threads` (default: `64`) - used in cluster mode to communicate with a remote RPC endpoint
* `spark.port.maxRetries` (default: `16` or `100` for testing when `spark.testing` is set) controls the maximum number of retries when binding to a port before giving up.

=== [[endpoints]] Endpoints

* `endpoint-verifier`

=== Message Dispatcher

A message dispatcher is responsible for routing RPC messages to the appropriate endpoint(s).

It uses the daemon fixed thread pool `dispatcher-event-loop` with `spark.rpc.netty.dispatcher.numThreads` threads for dispatching messages.

```
"dispatcher-event-loop-0" #26 daemon prio=5 os_prio=31 tid=0x00007f8877153800 nid=0x7103 waiting on condition [0x000000011f78b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000784ce81c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:218)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```
