== ML Pipelines - High-Level API for MLlib

NOTE: Both http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html[scikit-learn] and http://graphlab.com/learn/userguide/index.html#Deployment[GraphLab] have the concept of *pipelines* built into their system.

Use of a machine learning algorithm is only one component of *a predictive analytic workflow*. There can also be *pre-processing steps* for the machine learning algorithm to work.

A typical standard machine learning workflow is to:

1. Load the data
2. Extract features (aka _feature extraction_)
3. Train model
4. Evaluate (or _predictionize_)

The goal of the *Pipeline API* (_aka_ *spark.ml*) is to let users quickly and easily assemble and configure practical machine learning pipelines (_aka_ workflows).

NOTE: The Pipeline API lives under https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.package[org.apache.spark.ml] package.

Concepts:

* <<pipelines, Pipelines>> and <<PipelineStage, PipelineStages>>
* <<transformers, Transformers>>
** <<UnaryTransformer, UnaryTransformers>>
* <<Estimator, Estimators>>
* <<evaluators, Evaluators>>
* <<Model, Models>>

The beauty of using Spark MLlib is that the ML dataset is simply a link:spark-sql-dataframe.adoc[DataFrame].

Example: In text classification, preprocessing steps like n-gram extraction, and TF-IDF feature weighting are often necessary before training of a classification model like an SVM.

Upon deploying a model, your system must not only know the SVM weights to apply to input features, but also transform raw data into the format the model is trained on.

* Pipeline for text categorization
* Pipeline for image classification

Pipelines are like a query plan in a database system.

Components of ML Pipeline:

* *Pipeline Construction Framework* â€“ A DSL for the construction of pipelines that includes concepts of *Nodes* and *Pipelines*.
** Nodes are data transformation steps (*transformers*)
** Pipelines are a DAG of Nodes.
+
Pipelines become objects that can be saved out and applied in real-time to new data.

It can help creating domain-specific feature transformers, general purpose transformers, statistical utilities and nodes.

You could eventually `save` or `load` machine learning components as described in link:spark-mllib-pipelines-persistence.adoc[Persisting Machine Learning Components].

NOTE: A *machine learning component* is any object that belongs to Pipeline API, e.g. link:spark-mllib-pipelines.adoc#Pipeline[Pipeline], link:spark-mllib-pipelines.adoc#LinearRegressionModel[LinearRegressionModel], etc.

=== Features of Pipeline API

The features of the Pipeline API in Spark MLlib:

* link:spark-sql-dataframe.adoc[DataFrame] as a dataset format
* ML Pipelines API is similar to http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html[scikit-learn]
* Easy debugging (via inspecting columns added during execution)
* Parameter tuning
* Compositions (to build more complex pipelines out of existing ones)

=== [[pipelines]][[Pipeline]] Pipelines

A *ML pipeline* (or a *ML workflow*) is a sequence of <<transformers, Transformers>> and <<estimators, Estimators>> to build a model out of input dataset.

A pipeline is represented by https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline[Pipeline] class.

```
import org.apache.spark.ml.Pipeline
```

`Pipeline` is an <<Estimator, Estimator>> (so it is acceptable to set up a `Pipeline` with other `Pipeline` instances).

The `Pipeline` object can `read` or `load` pipelines (refer to link:spark-mllib-pipelines-persistence.adoc[Persisting Machine Learning Components] page).

[source, scala]
----
read: MLReader[Pipeline]
load(path: String): Pipeline
----

You can create a `Pipeline` with an optional `uid` identifier. It is of the format `pipeline_[randomUid]` when unspecified.

[source, scala]
----
val pipeline = new Pipeline()

scala> println(pipeline.uid)
pipeline_94be47c3b709

val pipeline = new Pipeline("my_pipeline")

scala> println(pipeline.uid)
my_pipeline
----

The identifier `uid` is used to create an instance of <<PipelineModel, PipelineModel>> to return from `fit(dataset: DataFrame): PipelineModel` method.

[source, scala]
----
scala> val pipeline = new Pipeline("my_pipeline")
pipeline: org.apache.spark.ml.Pipeline = my_pipeline

scala> val df = sc.parallelize(0 to 9).toDF("num")
df: org.apache.spark.sql.DataFrame = [num: int]

scala> val model = pipeline.setStages(Array()).fit(df)
model: org.apache.spark.ml.PipelineModel = my_pipeline
----

The `stages` mandatory parameter can be set using `setStages(value: Array[PipelineStage]): this.type` method.

==== [[Pipeline-fit]] Pipeline Fitting (fit method)

[source, scala]
----
fit(dataset: DataFrame): PipelineModel
----

The `fit` method returns a <<PipelineModel, PipelineModel>> that holds a collection of `Transformer` objects that are results of  `Estimator.fit` method for every `Estimator` in the Pipeline (with possibly-modified `dataset`) or simply input `Transformer` objects. The input `dataset` DataFrame is passed to `transform` for every `Transformer` instance in the Pipeline.

It first transforms the schema of the input `dataset` DataFrame.

It then searches for the index of the last <<Estimator, Estimator>> to calculate <<Transformer, Transformers>> for <<Estimator, Estimator>> and simply return <<Transformer, Transformers>> back up to the index in the pipeline. For each <<Estimator, Estimator>> the `fit` method is called with the input `dataset`. The result DataFrame is passed to the next `Transformer` in the chain.

NOTE: An `IllegalArgumentException` exception is thrown when a stage is neither `Estimator` or `Transformer`.

`transform` method is called for every `Transformer` calculated but the last one (that is the result of executing `fit` on the last `Estimator`).

The calculated Transformers are collected.

After the last `Estimator` there can only be `Transformer` stages.

The method returns a `PipelineModel` with `uid` and transformers. The parent `Estimator` is the `Pipeline` itself.

=== [[PipelineStage]] PipelineStage

The https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage[PipelineStage] abstract class represents a single stage in a <<Pipeline, Pipeline>>.

`PipelineStage` has the following direct implementations (of which few are abstract classes, too):

* <<Estimator, Estimator>>
* <<Model, Model>>
* <<Pipeline, Pipeline>>
* Predictor
* <<Transformer, Transformer>>

Each `PipelineStage` transforms schema using `transformSchema` family of methods:

```
transformSchema(schema: StructType): StructType
transformSchema(schema: StructType, logging: Boolean): StructType
```

NOTE: `StructType` is a Spark SQL type. Read up on it in link:spark-sql-dataframe.adoc#traits[Traits of DataFrame].

[TIP]
====
Enable `DEBUG` logging level for the respective `PipelineStage` implementations to see what happens beneath.
====

=== [[transformers]][[Transformer]] Transformers

A *transformer* is a function that maps (aka _transforms_) a `DataFrame` into another `DataFrame`. It simply prepares a dataset for an machine learning algorithm to work with.

Transformers are instances of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Transformer[org.apache.spark.ml.Transformer] abstract class that offers `transform` family of methods:

[source, scala]
----
transform(dataset: DataFrame): DataFrame
transform(dataset: DataFrame, paramMap: ParamMap): DataFrame
transform(dataset: DataFrame, firstParamPair: ParamPair[_], otherParamPairs: ParamPair[_]*): DataFrame
----

A `Transformer` is a <<PipelineStage, PipelineStage>> (so it can be a part of a <<Pipeline, Pipeline>>).

The direct descendents of the `Transformer` abstract class are:

* <<Model, Model>>
* <<UnaryTransformer, UnaryTransformers>>

==== [[UnaryTransformer]] UnaryTransformers

The https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.UnaryTransformer[UnaryTransformer] abstract class is a specialized `Transformer` that applies transformation to one input column and writes results to another (by appending a new column).

Each `UnaryTransformer` defines the input and output columns using the following "chain" methods (they return the transformer on which they were executed):

* `setInputCol(value: String)`
* `setOutputCol(value: String)`

Each `UnaryTransformer` calls `validateInputType` while executing `transformSchema(schema: StructType)` (that is part of <<PipelineStage, PipelineStage>> contract).

NOTE: A `UnaryTransformer` is a `PipelineStage`.

When `transform` is called, it first calls `transformSchema` (with DEBUG logging enabled) and then adds the column as a result of calling a protected abstract `createTransformFunc`.

NOTE: `createTransformFunc` function is abstract and defined by concrete `UnaryTransformer` objects.

Internally, `transform` methods uses Spark SQL's link:spark-sql-udfs.adoc#udf-function[udf] to define a function (based on `createTransformFunc` function described above) that will create the new output column (with appropriate `outputDataType`). The UDF is later applied to the input column of the input DataFrame and the result becomes the output column (using link:spark-sql-dataframe.adoc#withColumn[DataFrame.withColumn] method).

NOTE: Using `udf` and `withColumn` methods from Spark SQL demonstrates integration between the Spark modules: MLlib and SQL.

==== [[Transformer-examples]] Examples of Transformers

One example of a transformer is http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer[org.apache.spark.ml.feature.RegexTokenizer].

[source, scala]
----
import org.apache.spark.ml.feature.RegexTokenizer
val regexTok = new RegexTokenizer()
  .setInputCol("text")
  .setOutputCol("words")
  .setPattern("\\s+")

// Create DataFrame with rows with tabs and spaces
val df = Seq((0, s"""hello\tworld"""), (1, "two  spaces inside")).toDF("id", "text")

// Use RegexTokenizer
scala> regexTok.transform(df).show(false)
+---+------------------+---------------------+
|id |text              |words                |
+---+------------------+---------------------+
|0  |hello	world       |[hello, world]       |
|1  |two  spaces inside|[two, spaces, inside]|
+---+------------------+---------------------+
----

Another example of a transformer could be http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.HashingTF[org.apache.spark.ml.feature.HashingTF] that works on a `Column` of `ArrayType`.

[source, scala]
----
import org.apache.spark.ml.feature.HashingTF
val hashingTF = new HashingTF()
  .setInputCol("words")
  .setOutputCol("features")
  .setNumFeatures(5000)

// see above for regexTok transformer
val regexDF = regexTok.transform(df)

scala> hashingTF.transform(regexDF).show(false)
+---+------------------+---------------------+-----------------------------------+
|id |text              |words                |features                           |
+---+------------------+---------------------+-----------------------------------+
|0  |hello	world       |[hello, world]       |(5000,[2322,3802],[1.0,1.0])       |
|1  |two  spaces inside|[two, spaces, inside]|(5000,[276,940,2533],[1.0,1.0,1.0])|
+---+------------------+---------------------+-----------------------------------+
----

The name of the output column is optional, and if not specified, it becomes the identifier of a `HashingTF` object with the `__output` suffix.

[source, scala]
----
scala> hashingTF.uid
res7: String = hashingTF_fe3554836819

scala> hashingTF.transform(regexDF).show(false)
+---+------------------+---------------------+-------------------------------------------+
|id |text              |words                |hashingTF_fe3554836819__output             |
+---+------------------+---------------------+-------------------------------------------+
|0  |hello	world       |[hello, world]       |(262144,[71890,72594],[1.0,1.0])           |
|1  |two  spaces inside|[two, spaces, inside]|(262144,[53244,77869,115276],[1.0,1.0,1.0])|
+---+------------------+---------------------+-------------------------------------------+
----

In this example you use https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.NGram[org.apache.spark.ml.feature.NGram] that converts the input collection of strings into a collection of n-grams (of `n` words).

[source, scala]
----
import org.apache.spark.ml.feature.NGram

val bigram = new NGram("bigrams")
val df = Seq((0, Seq("hello", "world"))).toDF("id", "tokens")
bigram.setInputCol("tokens").transform(df).show

+---+--------------+---------------+
| id|        tokens|bigrams__output|
+---+--------------+---------------+
|  0|[hello, world]|  [hello world]|
+---+--------------+---------------+
----

=== [[Estimator]] Estimators

An *estimator* takes a `DataFrame` and parameters (as `ParamMap`) and fits a model. It is a function that maps a `DataFrame` into a `Model` that takes a `DataFrame`, trains on it and produces a `Model`.

CAUTION: FIXME What does _fitting a model_ mean?

It is a <<PipelineStage, PipelineStage>> (and so can be a part of <<Pipeline, Pipeline>>).

=== [[evaluators]] Evaluators

A *evaluator* is a function that maps a `DataFrame` into a metric indicating how well the model is.

=== [[Model]] Models

`Model` abstract class is a <<Transformer, Transformer>> with the optional <<Estimator, Estimator>> that has produced it (as a transient `parent` field).

NOTE: <<Estimator, Estimator>> is optional.

CAUTION: FIXME What does it mean when a Estimator is not known? When could an Estimator be missing?

CAUTION: FIXME What does `a fitted model` mean? What are the other kinds of models?

There are two direct implementations of the `Model` class that are not directly related to a ML algorithm:

* <<PipelineModel, PipelineModel>>
* <<PredictionModel, PredictionModel>>

==== [[PipelineModel]] PipelineModel

CAUTION: `PipelineModel` is a `private[ml]` class so _perhaps_ of less interest to end users like me (as of today).

CAUTION: FIXME

==== [[PredictionModel]] PredictionModel

`PredictionModel` is an abstract model for prediction algorithms like regression and classification (that have their own specialized models).

The direct non-algorithm-specific extensions of `PredictionModel` are:

* `ClassificationModel`
* `RegressionModel`

==== [[LinearRegressionModel]] LinearRegressionModel

CAUTION: FIXME

=== Further reading or watching

* https://amplab.cs.berkeley.edu/ml-pipelines/[ML Pipelines]
* https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html[ML Pipelines: A New High-Level API for MLlib]
* (video) https://youtu.be/OednhGRp938[Building, Debugging, and Tuning Spark Machine Learning Pipelines - Joseph Bradley (Databricks)]
