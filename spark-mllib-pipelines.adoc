== ML Pipelines - High-Level API for MLlib

NOTE: Both http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html[scikit-learn] and http://graphlab.com/learn/userguide/index.html#Deployment[GraphLab] have the concept of *pipelines* built into their system.

The beauty of using Spark MLlib is that the ML dataset is simply a DataFrame (from Spark SQL).

NOTE: The Pipeline API lives under https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.package[spark.ml] package.

The goal of the Pipeline API is to let users quickly assemble and configure practical machine learning pipelines.

Concepts:

* <<transformers, Transformers>>
* <<estimators, Estimators>>
* <<evaluators, Evaluators>>
* <<pipelines, Pipelines>>

Use of a machine learning algorithm is only one component of *a predictive analytic workflow*. There can also be *pre-processing steps* for the machine learning algorithm to work.

A typical standard machine learning workflow is to:

1. Load the data
2. Extract features (aka _feature extraction_)
3. Train model
4. Evaluate (or _predictionize_)

Example: In text classification, preprocessing steps like n-gram extraction, and TF-IDF feature weighting are often necessary before training of a classification model like an SVM.

Upon deploying a model, your system must not only know the SVM weights to apply to input features, but also transform raw data into the format the model is trained on.

* Pipeline for text categorization
* Pipeline for image classification

Pipelines are like a query plan in a database system.

Components of ML Pipeline:

* *Pipeline Construction Framework* â€“ A DSL for the construction of pipelines that includes concepts of *Nodes* and *Pipelines*.
** Nodes are data transformation steps (*transformers*)
** Pipelines are a DAG of Nodes.
+
Pipelines become objects that can be saved out and applied in real-time to new data.

It can help creating domain-specific feature transformers, general purpose transformers, statistical utilities and nodes.

=== Features of Pipeline API

The features of the Pipeline API in Spark MLlib:

* link:spark-sql-dataframe.adoc[DataFrame] as a dataset format
* ML Pipelines API is similar to http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html[scikit-learn]
* Easy debugging (via inspecting columns added during execution)
* Parameter tuning
* Compositions (to build more complex pipelines out of existing ones)

=== [[Pipeline]] Pipelines

A *ML pipeline* (or a *ML workflow*) is a sequence of <<transformers, Transformers>> and <<estimators, Estimators>> to build a model out of input data.

A pipeline is represented by https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline[Pipeline] class.

```
import org.apache.spark.ml.Pipeline
```

It is an <<Estimator, Estimator>>.

=== [[PipelineStage]] PipelineStage

The https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage[PipelineStage] abstract class represents a single stage in a <<Pipeline, Pipeline>>.

`PipelineStage` has the following direct implementations (of which few are abstract classes, too):

* <<Estimator, Estimator>>
* Model
* Pipeline
* Predictor
* <<Transformer, Transformer>>

Each `PipelineStage` transforms schema using `transformSchema` family of methods:

* `transformSchema(schema: StructType): StructType`
* `transformSchema(schema: StructType, logging: Boolean): StructType`

NOTE: `StructType` is a Spark SQL type. Read up on it in link:spark-sql-dataframe.adoc#traits[Traits of DataFrame].

[TIP]
====
Enable `DEBUG` logging level for the respective `PipelineStage` implementations to see what happens beneath.
====

=== [[Transformer]] Transformers

A *transformer* is a function that maps a `DataFrame` into another `DataFrame`.

Transformers are instances of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Transformer[org.apache.spark.ml.Transformer] abstract class that offers `transform` family of methods:

[source, scala]
----
transform(dataset: DataFrame): DataFrame
transform(dataset: DataFrame, paramMap: ParamMap): DataFrame
transform(dataset: DataFrame, firstParamPair: ParamPair[_], otherParamPairs: ParamPair[_]*): DataFrame
----

The direct descendents of the `Transformer` abstract class are:

* Model
* <<UnaryTransformer, UnaryTransformers>>

==== [[UnaryTransformer]] UnaryTransformers

The https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.UnaryTransformer[UnaryTransformer] abstract class is a specialized `Transformer` that applies transformation to one input column and writes results to another (by appending a new column).

Each `UnaryTransformer` defines the input and output columns using the following "chain" methods (they return the transformer on which they were executed):

* `setInputCol(value: String)`
* `setOutputCol(value: String)`

Each `UnaryTransformer` calls `validateInputType` while executing `transformSchema(schema: StructType)` (that is part of <<PipelineStage, PipelineStage>> contract).

NOTE: A `UnaryTransformer` is a `PipelineStage`.

When `transform` is called, it first calls `transformSchema` (with DEBUG logging enabled) and then adds the column as a result of calling a protected abstract `createTransformFunc`.

NOTE: `createTransformFunc` function is abstract and defined by concrete `UnaryTransformer` objects.

Internally, `transform` methods uses Spark SQL's link:spark-sql-udfs.adoc#udf-function[udf] to define a function (based on `createTransformFunc` function described above) that will create the new output column (with appropriate `outputDataType`). The UDF is later applied to the input column of the input DataFrame and the result becomes the output column (using link:spark-sql-dataframe.adoc#withColumn[DataFrame.withColumn] method).

NOTE: Using `udf` and `withColumn` methods from Spark SQL demonstrates integration between the Spark modules: MLlib and SQL.

==== [[Transformer-examples]] Examples of Transformers

One example of a transformer is http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer[org.apache.spark.ml.feature.RegexTokenizer].

[source, scala]
----
scala> import org.apache.spark.ml.feature.RegexTokenizer

scala> val regexTok = new RegexTokenizer().setInputCol("text").setOutputCol("words").setPattern("s")
regexTok: org.apache.spark.ml.feature.RegexTokenizer = regexTok_31b044abd10c
----

Another example of a transformer could be http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.HashingTF[org.apache.spark.ml.feature.HashingTF].

[source, scala]
----
scala> val hashingTF = new HashingTF().setInputCol("text").setOutputCol("features").setNumFeatures(5000)
hashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_16ecd3b7e333
----

In this example you use https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.NGram[org.apache.spark.ml.feature.NGram] that converts the input collection of strings into a collection of n-grams (of `n` words).

[source, scala]
----
import org.apache.spark.ml.feature.NGram

val bigram = new NGram("bigrams")
val df = Seq((0, Seq("hello", "world"))).toDF("id", "tokens")
bigram.setInputCol("tokens").transform(df).show

+---+--------------+---------------+
| id|        tokens|bigrams__output|
+---+--------------+---------------+
|  0|[hello, world]|  [hello world]|
+---+--------------+---------------+
----

=== [[Estimator]] Estimators

A *estimator* is a function that maps a `DataFrame` into a `Model`. It takes a DataFrame, trains on it and produces a Model.

=== [[evaluators]] Evaluators

A *evaluator* is a function that maps a `DataFrame` into a metric indicating how well the model is.

=== Example: Text Classification

NOTE: It comes from the video https://youtu.be/OednhGRp938[Building, Debugging, and Tuning Spark Machine Learning Pipelines - Joseph Bradley (Databricks)].

Problem: Given a text document, classify it as a scientific or non-scientific one.

When loading the input data it usually becomes a link:spark-sql-dataframe.adoc[DataFrame].

NOTE: The example uses a case class `LabeledText` to have the schema described nicely.

```
import sqlContext.implicits._

sealed trait Category
case object Scientific extends Category
case object NonScientific extends Category

// FIXME: Define schema for Category

case class LabeledText(id: Long, category: Category, text: String)

val data = Seq(LabeledText(0, Scientific, "hello world"), LabeledText(1, NonScientific, "witaj swiecie")).toDF

scala> data.show
+-----+-------------+
|label|         text|
+-----+-------------+
|    0|  hello world|
|    1|witaj swiecie|
+-----+-------------+
```

It is then _tokenized_ and transformed into another DataFrame with an additional column called features that is a `Vector` of numerical values.

NOTE: Paste the code below into Spark Shell using `:paste` mode.

[source, scala]
----
import sqlContext.implicits._

case class Article(id: Long, topic: String, text: String)
val articles = Seq(Article(0, "sci.math", "Hello, Math!"),
  Article(1, "alt.religion", "Hello, Religion!"),
  Article(2, "sci.physics", "Hello, Physics!")).toDF

org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)  // <1>

val papers = articles.as[Article]
----
<1> The line is required due to the way Spark Shell and Datasets interact. See link:spark-sql-dataset.adoc[Dataset] for more coverage.

Now, the tokenization part comes that maps the input text of each text document into tokens (a `Seq[String]`) and then into a `Vector` of numerical values that can only then be understood by a machine learning algorithm (that operates on `Vector` instances).

```
scala> papers.show
+---+------------+----------------+
| id|       topic|            text|
+---+------------+----------------+
|  0|    sci.math|    Hello, Math!|
|  1|alt.religion|Hello, Religion!|
|  2| sci.physics| Hello, Physics!|
+---+------------+----------------+

// FIXME Use Dataset API (not DataFrame API)
val labelled = papers.toDF.withColumn("label", $"topic".like("sci%")).cache

val topic2Label: Boolean => Double = isSci => if (isSci) 1 else 0
val toLabel = udf(topic2Label)

val training = papers.toDF.withColumn("label", toLabel($"topic".like("sci%"))).cache

scala> training.show
+---+------------+----------------+-----+
| id|       topic|            text|label|
+---+------------+----------------+-----+
|  0|    sci.math|    Hello, Math!|  1.0|
|  1|alt.religion|Hello, Religion!|  0.0|
|  2| sci.physics| Hello, Physics!|  1.0|
+---+------------+----------------+-----+

scala> training.groupBy("label").count.show
+-----+-----+
|label|count|
+-----+-----+
|  0.0|    1|
|  1.0|    2|
+-----+-----+
```

The _train a model_ phase uses the logistic regression machine learning algorithm to build a model and predict `label` for future input text documents (and hence classify them as scientific or non-scientific).

[source, scala]
----
scala> import org.apache.spark.ml.feature.RegexTokenizer

scala> val tokenizer = new RegexTokenizer().setInputCol("text").setOutputCol("words").setPattern("\\s+")
tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_f5a01fb6646a

scala> import org.apache.spark.ml.feature.HashingTF
import org.apache.spark.ml.feature.HashingTF

scala> val hashingTF = new HashingTF().setInputCol(tokenizer.getOutputCol).setOutputCol("features").setNumFeatures(5000)
hashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_152427802099

scala> import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.classification.LogisticRegression

scala> val lr = new LogisticRegression().setMaxIter(20).setRegParam(0.01)
lr: org.apache.spark.ml.classification.LogisticRegression = logreg_c346fddce901

scala> import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.Pipeline

scala> val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))
pipeline: org.apache.spark.ml.Pipeline = pipeline_c1feff10b9bb
----

It uses two columns, namely `label` and `features` vector to build a logistic regression model to make predictions.

[source, scala]
----
scala> val model = pipeline.fit(training)
model: org.apache.spark.ml.PipelineModel = pipeline_88711f477c1b

val predictions = model.transform(training)

scala> predictions.show
+---+------------+----------------+-----+-------------------+--------------------+--------------------+--------------------+----------+
| id|       topic|            text|label|              words|            features|       rawPrediction|         probability|prediction|
+---+------------+----------------+-----+-------------------+--------------------+--------------------+--------------------+----------+
|  0|    sci.math|    Hello, Math!|  1.0|     [hello, math!]|  (5000,[563],[1.0])|[-3.5586272181164...|[0.02768935730464...|       1.0|
|  1|alt.religion|Hello, Religion!|  0.0| [hello, religion!]| (5000,[4298],[1.0])|[3.18473454618966...|[0.96025575257636...|       0.0|
|  2| sci.physics| Hello, Physics!|  1.0|[hello, phy, ic, !]|(5000,[33,2499,33...|[-4.4061570147914...|[0.01205488687952...|       1.0|
+---+------------+----------------+-----+-------------------+--------------------+--------------------+--------------------+----------+

// Notice that the computations add new columns
scala> predictions.printSchema
root
 |-- id: long (nullable = false)
 |-- topic: string (nullable = true)
 |-- text: string (nullable = true)
 |-- label: double (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- features: vector (nullable = true)
 |-- rawPrediction: vector (nullable = true)
 |-- probability: vector (nullable = true)
 |-- prediction: double (nullable = true)

scala> import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

scala> val evaluator = new BinaryClassificationEvaluator().setMetricName("areaUnderROC")
evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_bd8ba11bc44a

scala> evaluator.evaluate(predictions)
res42: Double = 1.0
----

Let's tune the model (using "tools" from https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.tuning.package[org.apache.spark.ml.tuning] package).

CAUTION: FIXME Review the available classes in the org.apache.spark.ml.tuning package.

[source, scala]
----
import org.apache.spark.ml.tuning.ParamGridBuilder
val paramGrid = new ParamGridBuilder()
  .addGrid(hashingTF.numFeatures, Array(1000, 10000))
  .addGrid(lr.regParam, Array(0.05, 0.2))
  .build

import org.apache.spark.ml.tuning.CrossValidator
import org.apache.spark.ml.param._
val cv = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(evaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(2)

val cvModel = cv.fit(training)
----

CAUTION: FIXME Review https://github.com/apache/spark/blob/master/mllib/src/test/scala/org/apache/spark/ml/tuning/CrossValidatorSuite.scala

You can eventually save the model for later use (using `DataFrame.write`).

[source, scala]
----
cvModel.transform(test).select("id", "prediction")
  .write
  .json("/demo/predictions")
----

=== Further reading or watching

* https://amplab.cs.berkeley.edu/ml-pipelines/[ML Pipelines]
* https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html[ML Pipelines: A New High-Level API for MLlib]
* (video) https://youtu.be/OednhGRp938[Building, Debugging, and Tuning Spark Machine Learning Pipelines - Joseph Bradley (Databricks)]
