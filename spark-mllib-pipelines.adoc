== ML Pipelines - High-Level API for MLlib

NOTE: Both http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html[scikit-learn] and http://graphlab.com/learn/userguide/index.html#Deployment[GraphLab] have the concept of *pipelines* built into their system.

Use of a machine learning algorithm is only one component of *a predictive analytic workflow*. There can also be *pre-processing steps* for the machine learning algorithm to work.

A typical standard machine learning workflow is to:

1. Load the data
2. Extract features (aka _feature extraction_)
3. Train model
4. Evaluate (or _predictionize_)

The goal of the *Pipeline API* (_aka_ *spark.ml*) is to let users quickly and easily assemble and configure practical machine learning pipelines (_aka_ workflows).

NOTE: The Pipeline API lives under https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.package[org.apache.spark.ml] package.

Concepts:

* <<pipelines, Pipelines>> and <<PipelineStage, PipelineStages>>
* link:spark-mllib-transformers.adoc[Transformers]
* <<Estimator, Estimators>>
* <<evaluators, Evaluators>>
* <<Model, Models>>

The beauty of using Spark MLlib is that the ML dataset is simply a link:spark-sql-dataframe.adoc[DataFrame].

Example: In text classification, preprocessing steps like n-gram extraction, and TF-IDF feature weighting are often necessary before training of a classification model like an SVM.

Upon deploying a model, your system must not only know the SVM weights to apply to input features, but also transform raw data into the format the model is trained on.

* Pipeline for text categorization
* Pipeline for image classification

Pipelines are like a query plan in a database system.

Components of ML Pipeline:

* *Pipeline Construction Framework* â€“ A DSL for the construction of pipelines that includes concepts of *Nodes* and *Pipelines*.
** Nodes are data transformation steps (link:spark-mllib-transformers.adoc[Transformers])
** Pipelines are a DAG of Nodes.
+
Pipelines become objects that can be saved out and applied in real-time to new data.

It can help creating domain-specific feature transformers, general purpose transformers, statistical utilities and nodes.

You could eventually `save` or `load` machine learning components as described in link:spark-mllib-pipelines-persistence.adoc[Persisting Machine Learning Components].

NOTE: A *machine learning component* is any object that belongs to Pipeline API, e.g. link:spark-mllib-pipelines.adoc#Pipeline[Pipeline], link:spark-mllib-pipelines.adoc#LinearRegressionModel[LinearRegressionModel], etc.

=== Features of Pipeline API

The features of the Pipeline API in Spark MLlib:

* link:spark-sql-dataframe.adoc[DataFrame] as a dataset format
* ML Pipelines API is similar to http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html[scikit-learn]
* Easy debugging (via inspecting columns added during execution)
* Parameter tuning
* Compositions (to build more complex pipelines out of existing ones)

=== [[pipelines]][[Pipeline]] Pipelines

A *ML pipeline* (or a *ML workflow*) is a sequence of link:spark-mllib-transformers.adoc[Transformers] and <<estimators, Estimators>> to build a model out of input dataset.

A pipeline is represented by https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline[Pipeline] class.

```
import org.apache.spark.ml.Pipeline
```

`Pipeline` is an <<Estimator, Estimator>> (so it is acceptable to set up a `Pipeline` with other `Pipeline` instances).

The `Pipeline` object can `read` or `load` pipelines (refer to link:spark-mllib-pipelines-persistence.adoc[Persisting Machine Learning Components] page).

[source, scala]
----
read: MLReader[Pipeline]
load(path: String): Pipeline
----

You can create a `Pipeline` with an optional `uid` identifier. It is of the format `pipeline_[randomUid]` when unspecified.

[source, scala]
----
val pipeline = new Pipeline()

scala> println(pipeline.uid)
pipeline_94be47c3b709

val pipeline = new Pipeline("my_pipeline")

scala> println(pipeline.uid)
my_pipeline
----

The identifier `uid` is used to create an instance of <<PipelineModel, PipelineModel>> to return from `fit(dataset: DataFrame): PipelineModel` method.

[source, scala]
----
scala> val pipeline = new Pipeline("my_pipeline")
pipeline: org.apache.spark.ml.Pipeline = my_pipeline

scala> val df = sc.parallelize(0 to 9).toDF("num")
df: org.apache.spark.sql.DataFrame = [num: int]

scala> val model = pipeline.setStages(Array()).fit(df)
model: org.apache.spark.ml.PipelineModel = my_pipeline
----

The `stages` mandatory parameter can be set using `setStages(value: Array[PipelineStage]): this.type` method.

==== [[Pipeline-fit]] Pipeline Fitting (fit method)

[source, scala]
----
fit(dataset: DataFrame): PipelineModel
----

The `fit` method returns a <<PipelineModel, PipelineModel>> that holds a collection of `Transformer` objects that are results of  `Estimator.fit` method for every `Estimator` in the Pipeline (with possibly-modified `dataset`) or simply input `Transformer` objects. The input `dataset` DataFrame is passed to `transform` for every `Transformer` instance in the Pipeline.

It first transforms the schema of the input `dataset` DataFrame.

It then searches for the index of the last <<Estimator, Estimator>> to calculate link:spark-mllib-transformers.adoc[Transformers] for <<Estimator, Estimator>> and simply return `Transformer` back up to the index in the pipeline. For each <<Estimator, Estimator>> the `fit` method is called with the input `dataset`. The result DataFrame is passed to the next `Transformer` in the chain.

NOTE: An `IllegalArgumentException` exception is thrown when a stage is neither `Estimator` or `Transformer`.

`transform` method is called for every `Transformer` calculated but the last one (that is the result of executing `fit` on the last `Estimator`).

The calculated Transformers are collected.

After the last `Estimator` there can only be `Transformer` stages.

The method returns a `PipelineModel` with `uid` and transformers. The parent `Estimator` is the `Pipeline` itself.

=== [[PipelineStage]] PipelineStage

The https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage[PipelineStage] abstract class represents a single stage in a <<Pipeline, Pipeline>>.

`PipelineStage` has the following direct implementations (of which few are abstract classes, too):

* <<Estimator, Estimator>>
* <<Model, Model>>
* <<Pipeline, Pipeline>>
* Predictor
* link:spark-mllib-transformers.adoc[Transformer]

Each `PipelineStage` transforms schema using `transformSchema` family of methods:

```
transformSchema(schema: StructType): StructType
transformSchema(schema: StructType, logging: Boolean): StructType
```

NOTE: `StructType` is a Spark SQL type. Read up on it in link:spark-sql-dataframe.adoc#traits[Traits of DataFrame].

[TIP]
====
Enable `DEBUG` logging level for the respective `PipelineStage` implementations to see what happens beneath.
====

=== [[Estimator]] Estimators

An *estimator* takes a `DataFrame` and parameters (as `ParamMap`) and fits a model. It is a function that maps a `DataFrame` into a `Model` that takes a `DataFrame`, trains on it and produces a `Model`.

CAUTION: FIXME What does _fitting a model_ mean?

It is a <<PipelineStage, PipelineStage>> (and so can be a part of <<Pipeline, Pipeline>>).

=== [[evaluators]] Evaluators

A *evaluator* is a function that maps a `DataFrame` into a metric indicating how well the model is.

=== [[Model]] Models

`Model` abstract class is a link:spark-mllib-transformers.adoc[Transformers] with the optional <<Estimator, Estimator>> that has produced it (as a transient `parent` field).

NOTE: <<Estimator, Estimator>> is optional.

CAUTION: FIXME What does it mean when a Estimator is not known? When could an Estimator be missing?

CAUTION: FIXME What does `a fitted model` mean? What are the other kinds of models?

There are two direct implementations of the `Model` class that are not directly related to a ML algorithm:

* <<PipelineModel, PipelineModel>>
* <<PredictionModel, PredictionModel>>

==== [[PipelineModel]] PipelineModel

CAUTION: `PipelineModel` is a `private[ml]` class so _perhaps_ of less interest to end users like me (as of today).

CAUTION: FIXME

==== [[PredictionModel]] PredictionModel

`PredictionModel` is an abstract model for prediction algorithms like regression and classification (that have their own specialized models).

The direct non-algorithm-specific extensions of `PredictionModel` are:

* `ClassificationModel`
* `RegressionModel`

==== [[LinearRegressionModel]] LinearRegressionModel

CAUTION: FIXME

=== Further reading or watching

* https://amplab.cs.berkeley.edu/ml-pipelines/[ML Pipelines]
* https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html[ML Pipelines: A New High-Level API for MLlib]
* (video) https://youtu.be/OednhGRp938[Building, Debugging, and Tuning Spark Machine Learning Pipelines - Joseph Bradley (Databricks)]
