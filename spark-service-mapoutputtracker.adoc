== [[MapOutputTracker]] MapOutputTracker -- Shuffle Map Output Registry

`MapOutputTracker` is a Spark service that <<mapStatuses, tracks the locations of the shuffle map outputs>> (i.e. ``MapStatus``es per shuffle).

There are two custom `MapOutputTrackers`, i.e. one for the driver and another for executors:

* link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] for the driver
* link:spark-service-MapOutputTrackerWorker.adoc[MapOutputTrackerWorker] for executors

Given different runtime environments of the driver and executors, accessing the current `MapOutputTracker` is possible using link:spark-sparkenv.adoc#get[SparkEnv].

[source, scala]
----
SparkEnv.get.mapOutputTracker
----

`MapOutputTracker` is also registered as `MapOutputTracker` in the driver's RPC Environment.

[[internal-registries]]
.`MapOutputTracker` Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[mapStatuses]] `mapStatuses`
| Lookup table (aka _cache_) for collection of link:spark-MapStatus.adoc[MapStatus]es per shuffle id.

Used when `MapOutputTracker` <<getStatuses, finds map outputs for a `ShuffleDependency`>>, <<updateEpoch, updateEpoch>> and <<unregisterShuffle, unregisters a shuffle>>.
|===

It works with link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD] when it asks for *preferred locations for a shuffle* using `tracker.getPreferredLocationsForShuffle`.

It is also used for `mapOutputTracker.containsShuffle` and link:spark-service-MapOutputTrackerMaster.adoc#registerShuffle[MapOutputTrackerMaster.registerShuffle] when a new link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] is created.

CAUTION: FIXME `DAGScheduler.mapOutputTracker`

link:spark-service-MapOutputTrackerMaster.adoc#getStatistics[MapOutputTrackerMaster.getStatistics(dependency)] returns `MapOutputStatistics` that becomes the result of link:spark-dagscheduler-JobWaiter.adoc[JobWaiter.taskSucceeded] for link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] if it's the final stage in a job.

link:spark-service-MapOutputTrackerMaster.adoc#registerMapOutputs[MapOutputTrackerMaster.registerMapOutputs] for a shuffle id and a list of `MapStatus` when a link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] is finished.

NOTE: `MapOutputTracker` is used in `BlockStoreShuffleReader` and when creating link:spark-blockmanager.adoc[BlockManager] and link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint].

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.MapOutputTracker` logger to see what happens in `MapOutputTracker`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.MapOutputTracker=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[getMapSizesByExecutorId]] `getMapSizesByExecutorId` Method

CAUTION: FIXME

=== [[updateEpoch]] `updateEpoch` Method

CAUTION: FIXME

=== [[deserializeMapStatuses]] `deserializeMapStatuses` Method

CAUTION: FIXME

=== [[serializeMapStatuses]] `serializeMapStatuses` Method

CAUTION: FIXME

=== [[unregisterShuffle]] `unregisterShuffle` Method

CAUTION: FIXME

=== [[epoch]] Epoch Number

CAUTION: FIXME

=== [[getStatistics]] Computing Statistics for `ShuffleDependency` -- `getStatistics` Method

[source, scala]
----
getStatistics(dep: ShuffleDependency[_, _, _]): MapOutputStatistics
----

`getStatistics` returns a `MapOutputStatistics` which is simply a pair of the shuffle id (of the input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]) and the total sums of estimated sizes of the reduce blocks from all the link:spark-blockmanager.adoc[BlockManager]s.

Internally, `getStatistics` <<getStatuses, finds map outputs>> for the input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and calculates the total sizes for the link:spark-MapStatus.adoc#getSizeForBlock[estimated sizes of the reduce block (in bytes)] for every `MapStatus` and partition.

NOTE: The internal `totalSizes` array has the number of elements as specified by the link:spark-rdd-Partitioner.adoc#numPartitions[number of partitions of the `Partitioner`] of the input `ShuffleDependency`. `totalSizes` contains elements as a sum of the estimated size of the block for partition in a link:spark-blockmanager.adoc[BlockManager] (given link:spark-MapStatus.adoc[MapStatus]).

NOTE: `getStatistics` is used when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[`DAGScheduler` accepts a `ShuffleDependency` for execution] (and the link:spark-dagscheduler-ShuffleMapStage.adoc#isAvailable[corresponding `ShuffleMapStage` has already been computed]) and link:#handleTaskCompletion-Success-ShuffleMapTask[gets notified that a `ShuffleMapTask` has completed] (and map-stage jobs waiting for the stage are then marked as finished).

=== [[getStatuses]] Finding Map Outputs For `ShuffleDependency` -- `getStatuses` Internal Method

[source, scala]
----
getStatuses(shuffleId: Int): Array[MapStatus]
----

`getStatuses` finds link:spark-MapStatus.adoc[MapStatuses] for the input `shuffleId`, possibly requesting them from a remote link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] (using RPC).

Internally, `getStatuses` first queries the <<mapStatuses, `mapStatuses` internal cache>> and returns the map outputs if found.

If not found (in the `mapStatuses` internal cache), you should see the following INFO message in the logs:

```
INFO Don't have map outputs for shuffle [id], fetching them
```

If some other process fetches the map outputs for the `shuffleId` (as recorded in `fetching` internal registry), `getStatuses` waits until it is done.

When no other process fetches the map outputs, `getStatuses` registers the input `shuffleId` in `fetching` internal registry (of shuffle map outputs being fetched).

You should see the following INFO message in the logs:

```
INFO Doing the fetch; tracker endpoint = [trackerEndpoint]
```

`getStatuses` sends a `GetMapOutputStatuses` RPC remote message for the input `shuffleId` to the `trackerEndpoint` expecting a `Array[Byte]`.

NOTE: `getStatuses` requests shuffle map outputs remotely within a timeout and with retries. Refer to link:spark-rpc.adoc#RpcEndpointRef[RpcEndpointRef].

`getStatuses` <<deserializeMapStatuses, deserializes the map output statuses>> and records the result in the <<mapStatuses, `mapStatuses` internal cache>>.

You should see the following INFO message in the logs:

```
INFO Got the output locations
```

`getStatuses` removes the input `shuffleId` from `fetching` internal registry.

You should see the following DEBUG message in the logs:

```
DEBUG Fetching map output statuses for shuffle [id] took [time] ms
```

If, for some reason, `getStatuses` could not find the map output locations for the input `shuffleId`, you should see the following ERROR message in the logs and a `MetadataFetchFailedException` is thrown.

```
ERROR Missing all output locations for shuffle [id]
```

NOTE: `getStatuses` is used when `MapOutputTracker` <<getMapSizesByExecutorId, getMapSizesByExecutorId>> and <<getStatistics, computes statistics for `ShuffleDependency`>>.
