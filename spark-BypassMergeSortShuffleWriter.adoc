== [[BypassMergeSortShuffleWriter]] BypassMergeSortShuffleWriter

`BypassMergeSortShuffleWriter` is a link:spark-ShuffleWriter.adoc[ShuffleWriter] that link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTask] uses to <<write, write records>> (i.e. ``K``-key-``V``-value pairs) to a shuffle system when the link:spark-taskscheduler-ShuffleMapTask.adoc#runTask[task runs for a `ShuffleDependency`].

.BypassMergeSortShuffleWriter writing records (for ShuffleMapTask) using DiskBlockObjectWriters
image::images/spark-BypassMergeSortShuffleWriter-write.png[align="center"]

<<creating-instance, `BypassMergeSortShuffleWriter` is created>> exclusively when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` selects a `ShuffleWriter`] (for a link:spark-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle]).

TIP: Review the link:spark-SortShuffleManager.adoc#shouldBypassMergeSort[conditions `SortShuffleManager` uses to select `BypassMergeSortShuffleHandle` for a `ShuffleHandle`].

[[internal-registries]]
.BypassMergeSortShuffleWriter's Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[numPartitions]] `numPartitions`
| FIXME

| [[partitionWriters]] `partitionWriters`
| FIXME

| [[partitionWriterSegments]] `partitionWriterSegments`
| FIXME

| [[shuffleBlockResolver]] `shuffleBlockResolver`
| link:spark-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver].

Initialized when <<creating-instance, `BypassMergeSortShuffleWriter` is created>>.

Used when <<write, `BypassMergeSortShuffleWriter` writes records>>.

| [[mapStatus]] `mapStatus`
| link:spark-MapStatus.adoc[MapStatus] that <<stop, `BypassMergeSortShuffleWriter` returns when stopped>>

Initialized every time `BypassMergeSortShuffleWriter` <<write, writes records>>.

Used when <<stop, `BypassMergeSortShuffleWriter` stops>> (with `success` enabled) as a marker if <<write, any records were written>> and <<stop, returned if they did>>.

| [[partitionLengths]] `partitionLengths`
| Temporary array of partition lengths after records are <<write, written to a shuffle system>>.

Initialized every time `BypassMergeSortShuffleWriter` <<write, writes records>> before passing it in to link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[IndexShuffleBlockResolver]). After `IndexShuffleBlockResolver` finishes, it is used to initialize <<mapStatus, mapStatus>> internal property.

|===

[TIP]
====
Enable `ERROR` logging level for `org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter` logger to see what happens in `BypassMergeSortShuffleWriter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter=ERROR
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[writePartitionedFile]] Writing FIXME (and Tracking Write Time) -- `writePartitionedFile` Internal Method

[source, scala]
----
long[] writePartitionedFile(File outputFile) throws IOException
----

`writePartitionedFile` creates a file output stream for the input `outputFile` in append mode.

NOTE: `writePartitionedFile` uses Java's https://docs.oracle.com/javase/8/docs/api/java/io/FileOutputStream.html[java.io.FileOutputStream] to create a file output stream.

`writePartitionedFile` starts tracking write time (as `writeStartTime`).

For every <<numPartitions, numPartitions>> partition, `writePartitionedFile` takes the file from the `FileSegment` (from <<partitionWriterSegments, partitionWriterSegments>>) and creates a file input stream to read raw bytes.

NOTE: `writePartitionedFile` uses Java's https://docs.oracle.com/javase/8/docs/api/java/io/FileInputStream.html[java.io.FileInputStream] to create a file input stream.

`writePartitionedFile` copies all data from the input stream to the output stream (possibly in a NIO way depending on the `transferToEnabled` flag set when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>) and records the length of the shuffle data file (in `lengths` internal array).

CAUTION: FIXME Describe `Utils.copyStream` and make sure that it matches the description here.

NOTE: `transferToEnabled` is controlled by link:spark-UnsafeShuffleWriter.adoc#spark_file_transferTo[spark.file.transferTo] Spark property and is enabled (i.e. `true`) by default.

In the end, `writePartitionedFile` requests `ShuffleWriteMetrics` to `incWriteTime`, clears <<partitionWriters, partitionWriters>> array and returns the lengths of the shuffle data files per partition.

NOTE: `ShuffleWriteMetrics` was created when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

NOTE: `writePartitionedFile` is used exclusively when `BypassMergeSortShuffleWriter` <<write, writes records>>.

=== [[creating-instance]] Creating BypassMergeSortShuffleWriter Instance

`BypassMergeSortShuffleWriter` takes the following when created:

1. link:spark-blockmanager.adoc[BlockManager]
2. link:spark-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver]
3. link:spark-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle]
4. `mapId`
5. link:spark-taskscheduler-taskcontext.adoc[TaskContext]
6. link:spark-configuration.adoc[SparkConf]

[[fileBufferSize]]
`BypassMergeSortShuffleWriter` uses link:spark-ExternalSorter.adoc#spark_shuffle_file_buffer[spark.shuffle.file.buffer] (for `fileBufferSize` with the size of `32k` by default) and `spark.file.transferTo` (for `transferToEnabled` which is enabled by default) Spark properties.

`BypassMergeSortShuffleWriter` initializes the <<internal-registries, internal registries and counters>>.

=== [[write]] Writing Records -- `write` Method

[source, java]
----
void write(Iterator<Product2<K, V>> records) throws IOException
----

NOTE: `write` is a part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract] to write a sequence of records to a shuffle system.

Internally, when the input `records` iterator has no more records, `write` creates an empty <<partitionLengths, partitionLengths>> internal array of `numPartitions` size.

`write` then link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[requests the internal `IndexShuffleBlockResolver` to write shuffle index and data files] (with `dataTmp` as `null`) and sets the internal `mapStatus` (with the address of link:spark-blockmanager.adoc[BlockManager] in use and <<partitionLengths, partitionLengths>>).

However, when there are records to write, `write` creates a new link:spark-Serializer.adoc[Serializer].

NOTE: link:spark-Serializer.adoc[Serializer] was specified when <<creating-instance, `BypassMergeSortShuffleWriter` was created>> and is exactly the `Serializer` of the link:spark-rdd-ShuffleDependency.adoc#serializer[ShuffleDependency].

`write` initializes <<partitionWriters, partitionWriters>> with `DiskBlockObjectWriter` for <<numPartitions, every partition>>.

For <<numPartitions, every partition>>, `write` link:spark-DiskBlockManager.adoc#createTempShuffleBlock[requests `DiskBlockManager` for a temporary shuffle block and its file].

NOTE: `write` uses link:spark-blockmanager.adoc#diskBlockManager[`BlockManager` to access `DiskBlockManager`]. `BlockManager` was specified when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

`write` link:spark-blockmanager.adoc#getDiskWriter[requests `BlockManager` for a `DiskBlockObjectWriter`] (for the temporary `blockId` and `file`, the `SerializerInstance`, `fileBufferSize` and `writeMetrics`).

After `DiskBlockObjectWriter` writers were created, `write` requests `ShuffleWriteMetrics` to `incWriteTime`.

`write` initializes <<partitionWriterSegments, partitionWriterSegments>> with `FileSegment` for <<numPartitions, every partition>>.

`write` takes record by record and, after link:spark-rdd-Partitioner.adoc#getPartition[computing the partition for a key], writes the record using the corresponding `DiskBlockObjectWriter` (from <<partitionWriters, partitionWriters>>).

NOTE: `write` uses the link:spark-rdd-ShuffleDependency.adoc#partitioner[`Partitioner` from the `ShuffleDependency`] for which <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

NOTE: `write` initializes <<partitionWriters, partitionWriters>> with <<numPartitions, numPartitions>> number of `DiskBlockObjectWriters`.

After all the `records` have been written, `write` requests every `DiskBlockObjectWriter` to `commitAndGet` and saves the commit results in <<partitionWriterSegments, partitionWriterSegments>>. `write` closes every `DiskBlockObjectWriter`.

`write` link:spark-IndexShuffleBlockResolver.adoc#getDataFile[requests `IndexShuffleBlockResolver` for the shuffle block data file] for `shuffleId` and `mapId`.

NOTE: `IndexShuffleBlockResolver` was defined when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

`write` creates a temporary file for the shuffle block data file and <<writePartitionedFile, writePartitionedFile>> to it.

In the end, `write` link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[requests `IndexShuffleBlockResolver` to write shuffle index and data files] for the `shuffleId` and `mapId` (with `partitionLengths` and the temporary file) and creates a new <<mapStatus, mapStatus>> (with the link:spark-blockmanager.adoc#shuffleServerId[location of the `BlockManager`] and <<partitionLengths, partitionLengths>>).
