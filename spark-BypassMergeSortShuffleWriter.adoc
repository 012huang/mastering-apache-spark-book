== [[BypassMergeSortShuffleWriter]] BypassMergeSortShuffleWriter

`BypassMergeSortShuffleWriter` is a link:spark-ShuffleWriter.adoc[ShuffleWriter] that is used to <<write, write records>> (i.e. ``K``-key-``V``-value pairs).

[[internal-registries]]
.BypassMergeSortShuffleWriter's Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[numPartitions]] `numPartitions`
| FIXME

| [[partitionWriters]] `partitionWriters`
| FIXME

| [[partitionWriterSegments]] `partitionWriterSegments`
| FIXME

| [[shuffleBlockResolver]] `shuffleBlockResolver`
| link:spark-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver].

Initialized when <<creating-instance, `BypassMergeSortShuffleWriter` is created>>.

Used when <<write, `BypassMergeSortShuffleWriter` writes records>>.

| [[mapStatus]] `mapStatus`
| link:spark-MapStatus.adoc[MapStatus] that <<stop, `BypassMergeSortShuffleWriter` returns when stopped>>

Initialized every time `BypassMergeSortShuffleWriter` <<write, writes records>>.

Used when <<stop, `BypassMergeSortShuffleWriter` stops>> (with `success` enabled) as a marker if <<write, any records were written>> and <<stop, returned if they did>>.

| [[partitionLengths]] `partitionLengths`
| Temporary array of partition lengths after records are <<write, written to a shuffle system>>.

Initialized every time `BypassMergeSortShuffleWriter` <<write, writes records>> before passing it in to link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[IndexShuffleBlockResolver]). After `IndexShuffleBlockResolver` finishes, it is used to initialize <<mapStatus, mapStatus>> internal property.

|===

[TIP]
====
Enable `ERROR` logging level for `org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter` logger to see what happens in `BypassMergeSortShuffleWriter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter=ERROR
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[write]] Writing Records -- `write` Method

[source, java]
----
void write(Iterator<Product2<K, V>> records) throws IOException
----

NOTE: `write` is a part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract] to write a sequence of records to a shuffle system.

Internally, when the input `records` iterator has no more records, `write` creates an empty <<partitionLengths, partitionLengths>> internal array of `numPartitions` size.

`write` then link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[requests the internal `IndexShuffleBlockResolver` to write shuffle index and data files] (with `dataTmp` as `null`) and sets the internal `mapStatus` (with the address of link:spark-blockmanager.adoc[BlockManager] in use and <<partitionLengths, partitionLengths>>).

However, when there are records to write, `write` creates a new link:spark-Serializer.adoc[Serializer].

NOTE: link:spark-Serializer.adoc[Serializer] was specified when <<creating-instance, `BypassMergeSortShuffleWriter` was created>> and is exactly the `Serializer` of the link:spark-rdd-ShuffleDependency.adoc#serializer[ShuffleDependency].

`write` initializes <<partitionWriters, partitionWriters>> with `DiskBlockObjectWriter` for <<numPartitions, every partition>>.

For <<numPartitions, every partition>>, `write` link:spark-DiskBlockManager.adoc#createTempShuffleBlock[requests `DiskBlockManager` for a temporary shuffle block and its file].

NOTE: `write` uses link:spark-blockmanager.adoc#diskBlockManager[`BlockManager` to access `DiskBlockManager`]. `BlockManager` was specified when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

`write` link:spark-blockmanager.adoc#getDiskWriter[requests `BlockManager` for a `DiskBlockObjectWriter`] (for the temporary `blockId` and `file`, the `SerializerInstance`, `fileBufferSize` and `writeMetrics`).

After `DiskBlockObjectWriter` writers were created, `write` requests `ShuffleWriteMetrics` to `incWriteTime`.

`write` initializes <<partitionWriterSegments, partitionWriterSegments>> with `FileSegment` for <<numPartitions, every partition>>.

CAUTION: FIXME Finish me...

=== [[creating-instance]] Creating BypassMergeSortShuffleWriter Instance

`BypassMergeSortShuffleWriter` takes the following when created:

1. link:spark-blockmanager.adoc[BlockManager]
2. link:spark-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver]
3. link:spark-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle]
4. `mapId`
5. link:spark-taskscheduler-taskcontext.adoc[TaskContext]
6. link:spark-configuration.adoc[SparkConf]

[[fileBufferSize]]
`BypassMergeSortShuffleWriter` uses link:spark-ExternalSorter.adoc#spark_shuffle_file_buffer[spark.shuffle.file.buffer] (for `fileBufferSize` with the size of `32k` by default) and `spark.file.transferTo` (for `transferToEnabled` which is enabled by default) Spark properties.

`BypassMergeSortShuffleWriter` initializes the <<internal-registries, internal registries and counters>>.

NOTE: `BypassMergeSortShuffleWriter` is created exclusively when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` selects a `ShuffleWriter`] (for a link:spark-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle]).
