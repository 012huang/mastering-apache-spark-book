== Scheduler Listeners

A Spark *listener* is a class that listens to execution events from link:spark-dagscheduler.adoc[DAGScheduler]. It extends https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.SparkListener[org.apache.spark.scheduler.SparkListener].

TIP: Developing a custom SparkListener can be an excellent introduction to low-level details of link:spark-execution-model.adoc[Spark's Execution Model]. Check out the exercise link:exercises/spark-exercise-custom-scheduler-listener.adoc[Developing Custom SparkListener to monitor DAGScheduler in Scala].

=== [[events]] Events

A Spark listener can receive events about:

* when a stage completes successfully or fails
* when a stage is submitted
* when a task starts
* when a task begins remotely fetching its result
* when a task ends
* when a job starts
* when a job ends
* when environment properties have been updated
* when a new block manager has joined
* when an existing block manager has been removed
* when an RDD is manually unpersisted by the application
* when the application starts (as `SparkListenerApplicationStart`)
* when the application ends (as `SparkListenerApplicationEnd`)
* when the driver receives task metrics from an executor in a heartbeat.
* <<SparkListenerExecutorAdded, when the driver registers a new executor>> (FIXME or is this to let the driver know about the new executor?).
* when the driver removes an executor.
* when the driver receives a block update info.

CAUTION: FIXME What are SparkListenerEvents? Where and why are they posted? What do they cause?

* *SparkListenerEnvironmentUpdate*
* <<SparkListenerExecutorMetricsUpdate, SparkListenerExecutorMetricsUpdate>>

==== [[SparkListenerExecutorMetricsUpdate]] SparkListenerExecutorMetricsUpdate

`SparkListenerExecutorMetricsUpdate` triggers <<SparkListenerInterface, SparkListenerInterface.onExecutorMetricsUpdate>>.

==== [[SparkListenerApplicationStart]] SparkListenerApplicationStart

FIXME

==== [[SparkListenerExecutorAdded]] SparkListenerExecutorAdded

`SparkListenerExecutorAdded` is posted as a result of:

* A `RegisterExecutor` event having been received by link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend]

* Calling link:spark-mesos.adoc#MesosSchedulerBackend[MesosSchedulerBackend.resourceOffers].

* link:spark-local.adoc#LocalBackend[LocalBackend being started].

`SparkListenerExecutorAdded` is passed along to <<spark-listeners, SparkListeners>> using `SparkListener.onExecutorAdded(executorAdded)` method.

=== [[listenerBus]][[listener-bus]] Listener Bus

A *Listener Bus* asynchronously passes <<events, listener events>> to registered <<spark-listeners, Spark listeners>>.

An instance of Listener Bus runs on link:spark-driver.adoc[a driver] as an instance of `LiveListenerBus` (as `listenerBus`). It is created and started when link:spark-sparkcontext.adoc#creating-instance[a Spark context starts].

A Listener Bus is a daemon thread called *SparkListenerBus* that asynchronously processes events from a http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html[java.util.concurrent.LinkedBlockingQueue] capped at 10000 events.

=== [[SparkListener]][[spark-listeners]] Spark Listeners

CAUTION: FIXME What do the listeners do? Move them to appropriate sections.

* link:spark-scheduler-listeners-eventlogginglistener.adoc[EventLoggingListener]
* `ExecutorsListener` that prepares information to be displayed on the *Executors* tab in link:spark-webui.adoc[web UI].
* `SparkFirehoseListener` that allows users to receive all SparkListener events by overriding the `onEvent` method only.
* `ExecutorAllocationListener`
* link:spark-sparkcontext-HeartbeatReceiver.adoc[HeartbeatReceiver]

=== [[registering-listener]] Registering Spark Listener

You can register a Spark listener in a Spark application using `SparkContext.addSparkListener(listener: SparkListener)` method or <<settings, spark.extraListeners>> setting.

It is assumed that the listener comes with one of the following (in this order):

* a single-argument constructor that accepts `SparkConf`
* a zero-argument constructor

[TIP]
====
Set `INFO` on `org.apache.spark.SparkContext` logger to see the extra listeners being registered.

```
INFO SparkContext: Registered listener pl.japila.spark.CustomSparkListener
```
====

=== [[SparkListenerInterface]] SparkListenerInterface

CAUTION: FIXME

=== [[internal-listeners]] Internal Listeners

* web UI and link:spark-scheduler-listeners-eventlogginglistener.adoc[EventLoggingListener] listeners

=== [[settings]] Settings

* `spark.extraListeners` (default: empty) is a comma-separated list of listener class names that should be registered with Spark's listener bus when link:spark-sparkcontext.adoc#creating-instance[SparkContext is initialized].
+
```
$ ./bin/spark-shell --conf spark.extraListeners=pl.japila.spark.CustomSparkListener
```

=== Exercise

In link:exercises/spark-exercise-custom-scheduler-listener.adoc[Developing Custom SparkListener to monitor DAGScheduler in Scala] you can find a complete custom Scheduler Listener using Scala and sbt.
