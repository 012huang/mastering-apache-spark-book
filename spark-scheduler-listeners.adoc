== Scheduler Listeners

A Spark *listener* is a class that listens to execution events from link:spark-dagscheduler.adoc[DAGScheduler]. It extends https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.SparkListener[org.apache.spark.scheduler.SparkListener].

TIP: Developing a custom SparkListener can be an excellent introduction to low-level details of link:spark-execution-model.adoc[Spark's Execution Model]. Check out the exercise link:exercises/spark-exercise-custom-scheduler-listener.adoc[Developing Custom SparkListener to monitor DAGScheduler in Scala].

It can receive events about:

* when a stage completes successfully or fails
* when a stage is submitted
* when a task starts
* when a task begins remotely fetching its result
* when a task ends
* when a job starts
* when a job ends
* when environment properties have been updated
* when a new block manager has joined
* when an existing block manager has been removed
* when an RDD is manually unpersisted by the application
* when the application starts
* when the application ends
* when the driver receives task metrics from an executor in a heartbeat.
* when the driver registers a new executor.
* when the driver removes an executor.
* when the driver receives a block update info.

The `spark.extraListeners` (default: empty) setting is a comma-separated list of listener class names that are registered with Spark's listener bus when link:spark-sparkcontext.adoc[SparkContext is initialized].

```
$ ./bin/spark-shell --conf spark.extraListeners=pl.japila.spark.CustomSparkListener
```

You can also register a Spark listener in a Spark application using `SparkContext.addSparkListener(listener: SparkListener)` method.

It is assumed that the classes come with one of the following (in this order):

* a single-argument constructor that accepts `SparkConf`
* a zero-argument constructor

[TIP]
====
Set `INFO` on `org.apache.spark.SparkContext` logger to see the extra listeners being registered.

```
INFO SparkContext: Registered listener pl.japila.spark.CustomSparkListener
```
====

Internal listeners:

* web UI and <<event-logging, event logging>> listeners

=== [[SparkListenerBus]] ListenerBus

An instance of a *ListenerBus* runs on a driver only.

When a Spark context starts, it starts an instance of `LiveListenerBus`.

CAUTION: FIXME Review

=== [[event-logging]] Event Logging

Spark comes with its own `EventLoggingListener` listener that logs events to persistent storage.

When the listener starts, it prints out the INFO message `Logging events to [logPath]` and logs JSON-encoded events using `JsonProtocol` class.

`EventLoggingListener` uses the following properties:

* `spark.eventLog.enabled` (default: `false`) - whether to log Spark events that encode the information displayed in the UI to persisted storage. It is useful for reconstructing the Web UI after a Spark application has finished.
* `spark.eventLog.compress` (default: `false`) - whether to compress logged events.
* `spark.eventLog.overwrite` (default: `false`) - whether to overwrite any existing files.
* `spark.eventLog.dir` (default: `/tmp/spark-events`) - path to the directory in which events are logged, e.g. `hdfs://namenode:8021/directory`. The directory must exist before Spark starts up.
* `spark.eventLog.buffer.kb` (default: `100`) - buffer size to use when writing to output streams.
* `spark.eventLog.testing` (default: `false`)

=== StatsReportListener - Logging summary statistics

*org.apache.spark.scheduler.StatsReportListener* (see https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.StatsReportListener[the class' scaladoc]) is a SparkListener that logs a few summary statistics when each stage completes.

It listens to `SparkListenerTaskEnd`, `SparkListenerStageCompleted` events.

```
$ ./bin/spark-shell --conf \
      spark.extraListeners=org.apache.spark.scheduler.StatsReportListener
...
INFO SparkContext: Registered listener org.apache.spark.scheduler.StatsReportListener
...

scala> sc.parallelize(0 to 10).count
...
15/11/04 15:39:45 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4d3956a4
15/11/04 15:39:45 INFO StatsReportListener: task runtime:(count: 8, mean: 36.625000, stdev: 5.893588, max: 52.000000, min: 33.000000)
15/11/04 15:39:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/11/04 15:39:45 INFO StatsReportListener: 	33.0 ms	33.0 ms	33.0 ms	34.0 ms	35.0 ms	36.0 ms	52.0 ms	52.0 ms	52.0 ms
15/11/04 15:39:45 INFO StatsReportListener: task result size:(count: 8, mean: 953.000000, stdev: 0.000000, max: 953.000000, min: 953.000000)
15/11/04 15:39:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/11/04 15:39:45 INFO StatsReportListener: 	953.0 B	953.0 B	953.0 B	953.0 B	953.0 B	953.0 B	953.0 B	953.0 B	953.0 B
15/11/04 15:39:45 INFO StatsReportListener: executor (non-fetch) time pct: (count: 8, mean: 17.660220, stdev: 1.948627, max: 20.000000, min: 13.461538)
15/11/04 15:39:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/11/04 15:39:45 INFO StatsReportListener: 	13 %	13 %	13 %	17 %	18 %	20 %	20 %	20 %	20 %
15/11/04 15:39:45 INFO StatsReportListener: other time pct: (count: 8, mean: 82.339780, stdev: 1.948627, max: 86.538462, min: 80.000000)
15/11/04 15:39:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/11/04 15:39:45 INFO StatsReportListener: 	80 %	80 %	80 %	82 %	82 %	83 %	87 %	87 %	87 %
```

=== Exercise

In link:exercises/spark-exercise-custom-scheduler-listener.adoc[Developing Custom SparkListener to monitor DAGScheduler in Scala] you can find a complete custom Scheduler Listener using Scala and sbt.
