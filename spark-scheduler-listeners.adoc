== Scheduler Listeners

[CAUTION]
====
FIXME:

* Code review `StatsReportListener`

====

A Spark *listener* is a class that listening to events from link:spark-scheduler.adoc[Spark's DAGScheduler]. It extends https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.SparkListener[org.apache.spark.scheduler.SparkListener].

This is a Developer API and can change between Spark releases.

It can receive events about:

* when a stage completes successfully or fails
* when a stage is submitted
* when a task starts
* when a task begins remotely fetching its result
* when a task ends
* when a job starts
* when a job ends
* when environment properties have been updated
* when a new block manager has joined
* when an existing block manager has been removed
* when an RDD is manually unpersisted by the application
* when the application starts
* when the application ends
* when the driver receives task metrics from an executor in a heartbeat.
* when the driver registers a new executor.
* when the driver removes an executor.
* when the driver receives a block update info.

The `spark.extraListeners` (default: empty) setting is a comma-separated list of listener class names that are registered with Spark's listener bus when link:spark-sparkcontext.adoc[SparkContext is initialized].

```
$ ./bin/spark-shell --conf spark.extraListeners=i.do.not.exist.CustomSparkListener
...
org.apache.spark.SparkException: Exception when registering SparkListener
  at org.apache.spark.SparkContext.setupAndStartListenerBus(SparkContext.scala:2145)
  at org.apache.spark.SparkContext.<init>(SparkContext.scala:562)
  at org.apache.spark.repl.Main$.createSparkContext(Main.scala:79)
  ... 47 elided
Caused by: java.lang.ClassNotFoundException: i.do.not.exist.CustomSparkListener
  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at org.apache.spark.util.Utils$.classForName(Utils.scala:173)
  at org.apache.spark.SparkContext$$anonfun$setupAndStartListenerBus$1.apply(SparkContext.scala:2113)
  at org.apache.spark.SparkContext$$anonfun$setupAndStartListenerBus$1.apply(SparkContext.scala:2110)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
  at org.apache.spark.SparkContext.setupAndStartListenerBus(SparkContext.scala:2110)
  ... 49 more
```

It is assumed that the classes come with one of the following (in this order):

* a single-argument constructor that accepts `SparkConf`
* a zero-argument constructor

[TIP]
====
Set `INFO` on `org.apache.spark.SparkContext` logger to see the extra listeners being registered.
====

Internal listeners:

* web UI and <<event-logging, event logging>> listeners

=== [[event-logging]] Event Logging

* `spark.eventLog.enabled` (default: `false`)

=== Exercise

In link:exercises/spark-exercise-custom-scheduler-listener.adoc[Developing Custom DAGScheduler Listener to monitor DAGScheduler in Scala] you can find a complete custom Scheduler Listener using Scala and sbt.
