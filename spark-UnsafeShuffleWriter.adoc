== [[UnsafeShuffleWriter]] UnsafeShuffleWriter

`UnsafeShuffleWriter` is a link:spark-ShuffleWriter.adoc[ShuffleWriter] that is used when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` is requested for a `ShuffleWriter`] (for a link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle]).

`UnsafeShuffleWriter` can use a specialized NIO-based merge procedure that avoids extra serialization/deserialization.

[TIP]
====
Enable `ERROR` or `DEBUG` logging levels for `org.apache.spark.shuffle.sort.UnsafeShuffleWriter` logger to see what happens in `UnsafeShuffleWriter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[open]] `open` Method

CAUTION: FIXME

=== [[insertRecordIntoSorter]] `insertRecordIntoSorter` Method

CAUTION: FIXME

=== [[closeAndWriteOutput]] `closeAndWriteOutput` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `UnsafeShuffleWriter` Instance

[source, scala]
----
UnsafeShuffleWriter(
  BlockManager blockManager,
  IndexShuffleBlockResolver shuffleBlockResolver,
  TaskMemoryManager memoryManager,
  SerializedShuffleHandle<K, V> handle,
  int mapId,
  TaskContext taskContext,
  SparkConf sparkConf) throws IOException
----

When created, `UnsafeShuffleWriter` takes a link:spark-blockmanager.adoc[BlockManager], `IndexShuffleBlockResolver`, link:spark-taskscheduler-taskmemorymanager.adoc[TaskMemoryManager], link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle], `mapId`, link:spark-taskscheduler-taskcontext.adoc[TaskContext] and link:spark-configuration.adoc[SparkConf].

`UnsafeShuffleWriter` makes sure that the number of shuffle output partitions (of the `ShuffleDependency` of the input `SerializedShuffleHandle`) is at most `(1 << 24) - 1`, i.e. `16777215`.

NOTE: The number of shuffle output partitions is first enforced when link:spark-SortShuffleManager.adoc#canUseSerializedShuffle[`SortShuffleManager` checks if `SerializedShuffleHandle` can be used for `ShuffleHandle`] (that eventually leads to `UnsafeShuffleWriter`).

`UnsafeShuffleWriter` uses <<spark_file_transferTo, spark.file.transferTo>> and <<spark_shuffle_sort_initialBufferSize, spark.shuffle.sort.initialBufferSize>> Spark properties to initialize `transferToEnabled` and `initialSortBufferSize` attributes, respectively.

If the number of shuffle output partitions is greater than the maximum, `UnsafeShuffleWriter` throws a `IllegalArgumentException`.

```
UnsafeShuffleWriter can only be used for shuffles with at most 16777215 reduce partitions
```

NOTE: `UnsafeShuffleWriter` is created exclusively when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` is requested for a `ShuffleWriter`] (for a link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle]).

=== [[write]] `write` Method

[source, java]
----
void write(scala.collection.Iterator<Product2<K, V>> records)
throws IOException
----

NOTE: `write` is a part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract].

Internally, `write` traverses the input sequence of records (for a RDD partition) and <<insertRecordIntoSorter, insertRecordIntoSorter>> one by one. With all the records processed, `write` <<closeAndWriteOutput, closeAndWriteOutput>>.

In the end, `write` frees allocated resources.

CAUTION: FIXME

=== [[stop]] `stop` Method

[source, java]
----
Option<MapStatus> stop(boolean success)
----

NOTE: `stop` is a part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract].

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_file_transferTo]] `spark.file.transferTo`
| `true`
| Controls whether...FIXME

| [[spark_shuffle_sort_initialBufferSize]] `spark.shuffle.sort.initialBufferSize`
| `4096`
| Default initial sort buffer size

|===
