== [[UnsafeShuffleWriter]] UnsafeShuffleWriter

`UnsafeShuffleWriter` is a link:spark-ShuffleWriter.adoc[ShuffleWriter] that is used when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` is requested for a `ShuffleWriter`] (for a link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle]).

`UnsafeShuffleWriter` can use a specialized NIO-based merge procedure that avoids extra serialization/deserialization.

[TIP]
====
Enable `ERROR` or `DEBUG` logging levels for `org.apache.spark.shuffle.sort.UnsafeShuffleWriter` logger to see what happens in `UnsafeShuffleWriter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating `UnsafeShuffleWriter` Instance

CAUTION: FIXME

=== [[open]] `open` Method

CAUTION: FIXME

=== [[insertRecordIntoSorter]] `insertRecordIntoSorter` Method

CAUTION: FIXME

=== [[closeAndWriteOutput]] `closeAndWriteOutput` Method

CAUTION: FIXME

=== [[write]] `write` Method

[source, java]
----
void write(scala.collection.Iterator<Product2<K, V>> records)
throws IOException
----

NOTE: `write` is a part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract].

Internally, `write` traverses the input sequence of records (for a RDD partition) and <<insertRecordIntoSorter, insertRecordIntoSorter>> one by one. With all the records processed, `write` <<closeAndWriteOutput, closeAndWriteOutput>>.

In the end, `write` frees allocated resources.

CAUTION: FIXME

=== [[stop]] `stop` Method

[source, java]
----
Option<MapStatus> stop(boolean success)
----

NOTE: `stop` is a part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract].
