== [[ExternalSorter]] ExternalSorter

`ExternalSorter` is a `Spillable` of `WritablePartitionedPairCollection` of ``K``-key / ``C``-value pairs.

When <<creating-instance, created>> `ExternalSorter` expects three different types of data defined, i.e. `K`, `V`, `C`, for keys, values, and combiner (partial) values, respectively.

NOTE: `ExternalSorter` is exclusively used when link:spark-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-value pairs] (for reduce task when link:spark-rdd-ShuffleDependency.adoc#keyOrdering[`ShuffleDependency` has key ordering defined (to sort output)].

[TIP]
====
Enable `INFO` or `WARN` logging levels for `org.apache.spark.util.collection.ExternalSorter` logger to see what happens in `ExternalSorter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.collection.ExternalSorter=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating ExternalSorter Instance

`ExternalSorter` takes the following:

1. TaskContext
2. Optional link:spark-Aggregator.adoc[Aggregator]
3. Optional link:spark-rdd-Partitioner.adoc[Partitioner]
4. Optional Scala's http://www.scala-lang.org/api/current/scala/math/Ordering.html[Ordering]
5. Optional link:spark-Serializer.adoc[Serializer]

NOTE: `ExternalSorter` uses link:spark-sparkenv.adoc#serializer[`SparkEnv` to access the default `Serializer`].

NOTE: `ExternalSorter` is created when link:spark-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-value pairs] (for reduce task when link:spark-rdd-ShuffleDependency.adoc#keyOrdering[`ShuffleDependency` has key ordering defined (to sort output)].

=== [[spillMemoryIteratorToDisk]] `spillMemoryIteratorToDisk` Internal Method

[source, scala]
----
spillMemoryIteratorToDisk(inMemoryIterator: WritablePartitionedIterator): SpilledFile
----

CAUTION: FIXME

=== [[spill]] `spill` Method

[source, scala]
----
spill(collection: WritablePartitionedPairCollection[K, C]): Unit
----

NOTE: `spill` is a part of Spillable contract.

CAUTION: FIXME

=== [[maybeSpillCollection]] `maybeSpillCollection` Internal Method

[source, scala]
----
maybeSpillCollection(usingMap: Boolean): Unit
----

CAUTION: FIXME

=== [[insertAll]] `insertAll` Method

[source, scala]
----
insertAll(records: Iterator[Product2[K, V]]): Unit
----

CAUTION: FIXME

NOTE: `insertAll` is used when link:spark-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-value pairs] (for reduce task when link:spark-rdd-ShuffleDependency.adoc#keyOrdering[`ShuffleDependency` has key ordering defined (to sort output)].

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_shuffle_file_buffer]] `spark.shuffle.file.buffer`
| `32k` (bytes)
| Bytes unit assumed unless specified explicitly.

| [[spark_shuffle_spill_batchSize]] `spark.shuffle.spill.batchSize`
| `10000`
| Size of object batches when reading/writing from serializers.

|===
