== CoarseGrainedExecutorBackend

`CoarseGrainedExecutorBackend` manages a single link:spark-executor.adoc[executor] object. The internal `executor` object is created after a connection to a driver is established (i.e. after <<RegisteredExecutor, RegisteredExecutor>> has arrived).

.CoarseGrainedExecutorBackend and Others
image::images/CoarseGrainedExecutorBackend.png[align="center"]

`CoarseGrainedExecutorBackend` is an link:spark-executor-backends.adoc[executor backend] for link:spark-executor.adoc#coarse-grained-executor[coarse-grained executors] that live until the executor backend terminates.

`CoarseGrainedExecutorBackend` registers itself as a link:spark-rpc.adoc#rpcendpoint[RPC Endpoint] under the name *Executor*.

When started it connects to `driverUrl` (given as <<main, an option on command line>>), i.e.  link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend], for tasks to run.

CAUTION: What are `RegisterExecutor` and `RegisterExecutorResponse`? Why does `CoarseGrainedExecutorBackend` send it in `onStart`?

When it cannot connect to `driverUrl`, it terminates (with the exit code `1`).

CAUTION: What are `SPARK_LOG_URL_` env vars? Who sets them?

When the driver terminates, `CoarseGrainedExecutorBackend` exits (with exit code `1`).

```
ERROR Driver [remoteAddress] disassociated! Shutting down.
```

When *Executor* RPC Endpoint is started (`onStart`), it prints out INFO message to the logs:

```
INFO CoarseGrainedExecutorBackend: Connecting to driver:  [driverUrl]
```

All task status updates are sent along to `driverRef` as `StatusUpdate` messages.

[CAUTION]
====
FIXME Review the use of:

* Used in `SparkContext.createTaskScheduler`
====

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.executor.CoarseGrainedExecutorBackend` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.CoarseGrainedExecutorBackend=INFO
```
====

=== [[driverURL]] Driver's URL

The driver's URL is of the format `spark://[RpcEndpoint name]@[hostname]:[port]`, e.g. `spark://CoarseGrainedScheduler@192.168.1.6:64859`.

=== [[main]] main

CoarseGrainedExecutorBackend is a command-line application (it comes with `main` method).

It accepts the following options:

* `--driver-url` (required) - the driver's URL. See <<driverURL, driver's URL>>.

[[executor-id]]
* `--executor-id` (required) - the executor's id
* `--hostname` (required) - the name of the host
* `--cores` (required) - the number of cores (must be more than `0`)
* `--app-id` (required) - the id of the application
* `--worker-url` - the worker's URL, e.g. `spark://Worker@192.168.1.6:64557`
* `--user-class-path` - a URL/path to a resource to be added to CLASSPATH; can be specified multiple times.

Unrecognized options or required options missing cause displaying usage help and exit.

```
$ ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend

Usage: CoarseGrainedExecutorBackend [options]

 Options are:
   --driver-url <driverUrl>
   --executor-id <executorId>
   --hostname <hostname>
   --cores <cores>
   --app-id <appid>
   --worker-url <workerUrl>
   --user-class-path <url>
```

It first fetches Spark properties from link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend] (using the `driverPropsFetcher` RPC Environment and the endpoint reference given in <<driverURL, driver's URL>>).

For this, it creates `SparkConf`, reads `spark.executor.port` setting (defaults to `0`) and creates the `driverPropsFetcher` RPC Environment in link:spark-rpc.adoc#client-mode[client mode]. The RPC environment is used to resolve the driver's endpoint to post `RetrieveSparkProps` message.

It sends a (blocking) `RetrieveSparkProps` message to the driver (using the value for `driverUrl` command-line option). When the response (the driver's `SparkConf`) arrives it adds `spark.app.id` (using the value for `appid` command-line option) and creates a brand new `SparkConf`.

If `spark.yarn.credentials.file` is set, ...FIXME

A SparkEnv is created using link:spark-sparkenv.adoc#createExecutorEnv[SparkEnv.createExecutorEnv] (with `isLocal` being `false`).

CAUTION: FIXME

=== [[usage]] Usage

CAUTION: FIXME Where is `org.apache.spark.executor.CoarseGrainedExecutorBackend` used?

It is used in:

* `SparkDeploySchedulerBackend`
* `CoarseMesosSchedulerBackend`
* `SparkClassCommandBuilder` - ???
* `ExecutorRunnable`

=== [[start]] start

=== [[stop]] stop

=== [[requestTotalExecutors]] requestTotalExecutors

=== [[messages]] RPC Messages

==== [[RegisteredExecutor]] RegisteredExecutor

[source, scala]
----
RegisteredExecutor(hostname)
----

When `RegisteredExecutor` message arrives, you should see the following INFO in the logs:

```
INFO CoarseGrainedExecutorBackend: Successfully registered with driver
```

The internal `executor` is created using `executorId` constructor parameter, with `hostname` that has arrived and others.

==== [[RegisterExecutorFailed]] RegisterExecutorFailed

[source, scala]
----
RegisterExecutorFailed(message)
----

When a `RegisterExecutorFailed` message arrives, the following ERROR is printed out to the logs:

```
ERROR CoarseGrainedExecutorBackend: Slave registration failed: [message]
```

`CoarseGrainedExecutorBackend` then exits with the exit code `1`.

==== [[LaunchTask]] LaunchTask

[source, scala]
----
LaunchTask(data: SerializableBuffer)
----

The `LaunchTask` handler deserializes `TaskDescription` from `data` (using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]).

NOTE: `LaunchTask` message is sent by link:spark-scheduler-backends-coarse-grained.adoc#launchTasks[CoarseGrainedSchedulerBackend.launchTasks].

```
INFO CoarseGrainedExecutorBackend: Got assigned task [taskId]
```

It then launches the task on the executor (using link:spark-executor.adoc#launching-tasks[Executor.launchTask] method).

If however the internal `executor` field has not been created yet, it prints out the following ERROR to the logs:

```
ERROR CoarseGrainedExecutorBackend: Received LaunchTask command but executor was null
```

And it then exits.

==== KillTask(taskId, _, interruptThread)

`KillTask(taskId, _, interruptThread)` message kills a task (calls `Executor.killTask`).

If an executor has not been initialized yet (FIXME: why?), the following ERROR message is printed out to the logs and CoarseGrainedExecutorBackend exits:

```
ERROR Received KillTask command but executor was null
```

==== [[StopExecutor]] StopExecutor

`StopExecutor` message handler is receive-reply and blocking. When received, the handler prints the following INFO message to the logs:

```
INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
```

It then sends a `Shutdown` message to itself.

==== Shutdown

`Shutdown` stops the executor, itself and RPC Environment.
