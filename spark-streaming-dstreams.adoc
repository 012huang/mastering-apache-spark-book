== Discretized Streams (DStreams)

*Discretized Stream (DStream)* is the fundamental concept of Spark Streaming. It is basically a stream of link:spark-rdd.adoc[RDDs] with elements being the data received from input streams over link:spark-streaming-streamingcontext.adoc[batch duration].

It is modelled as `org.apache.spark.streaming.dstream.DStream` abstract class.

To create a `DStream` instance you use link:spark-streaming-streamingcontext.adoc#creating-receivers[StreamingContext] or other specialized add-ons for external input data sources that extend the built-in input stream constructors, e.g. link:spark-streaming-kafka.adoc[Kafka].

NOTE: New `DStreams` can only be created (and hence registered) before link:spark-streaming-streamingcontext.adoc#start[StreamingContext is started]. All other states lead to `IllegalStateException` being thrown.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.dstream.DStream` logger to see what happens inside a `DStream`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.dstream.DStream=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[contract]] DStream Contract

A `DStream` is defined by the following properties (with the names of the corresponding methods that subclasses have to implement):

* A collection of parent `DStreams` that this `DStream` depends on.
+
```
def dependencies: List[DStream[_]]
```

* A *slide duration* being a time interval after which the stream generates a RDD.
+
```
def slideDuration: Duration
```

* How to *compute* (_generate_) a RDD for the given batch.
+
```
def compute(validTime: Time): Option[RDD[T]]
```

=== [[register]] Registering Output Streams (register method)

`DStream` by design has no notion of being an input or output stream. It is link:spark-streaming-dstreamgraph.adoc[DStreamGraph] to know and be able to differentiate between input and output stream.

`DStream` comes with internal `register` method that registers a `DStream` as an output stream.

The internal private `foreachRDD` method uses `register` to register output streams to link:spark-streaming-dstreamgraph.adoc[DStreamGraph]. Whenever called, it creates link:spark-streaming-foreachdstreams.adoc[ForEachDStream] and calls `register` upon it. That is how streams become output streams.

=== [[generateJob]] Generating Streaming Jobs (generateJob method)

The internal `DStream.generateJob(time: Time)` method generates a streaming job for a batch `time`. It may or may not generate a streaming job for a batch time.

NOTE: It is called when link:spark-streaming-dstreamgraph.adoc#generateJobs[DStreamGraph generates jobs for a batch time].

It <<getOrCompute, computes an RDD for the batch>> and, if there is one, returns a link:spark-streaming.adoc#Job[streaming job] for the batch `time` and a job function that will link:spark-sparkcontext.adoc#running-jobs[run a Spark job] (with the generated RDD and the job function) when executed.

NOTE: The Spark job uses an empty function to calculate partitions of a RDD.

CAUTION: FIXME What happens when `SparkContext.runJob(rdd, emptyFunc)` is called with the empty function, i.e. `(iterator: Iterator[T]) => {}`?

=== [[getOrCompute]] Computing RDD for Batch (getOrCompute method)

The internal (`private final`) `getOrCompute(time: Time)` method returns an optional RDD for a batch (`time`).

It uses <<internal-registries, generatedRDDs>> to return the RDD if it has already been generated for the `time`. If not, it generates one by <<contract, computing the input stream>> (using `compute(validTime: Time)` method).

If there was anything to process in the input stream, i.e. <<contract, computing the input stream returned a RDD>>, the RDD is first link:spark-rdd-caching.adoc[persisted] (only if `storageLevel` for the input stream is different from `StorageLevel.NONE`).

You should see the following DEBUG message in the logs:

```
DEBUG Persisting RDD [id] for time [time] to [storageLevel]
```

The generated RDD is link:spark-rdd-checkpointing.adoc[checkpointed] if <<internal-registries, checkpointDuration>> is defined and the time interval between current and <<internal-registries, zero>> times is a multiple of <<internal-registries, checkpointDuration>>.

You should see the following DEBUG message in the logs:

```
DEBUG Marking RDD [id] for time [time] for checkpointing
```

The generated RDD is saved in the <<internal-registries, internal generatedRDDs registry>>.

=== [[clearMetadata]] Metadata Cleanup

NOTE: It is called when  link:spark-streaming-dstreamgraph.adoc#clearMetadata[DStreamGraph clears metadata for every output stream].

`clearMetadata(time: Time)` is called to remove old RDDs that have been generated so far (and collected in <<internal-registries, generatedRDDs>>). It is a sort of _garbage collector_.

When `clearMetadata(time: Time)` is called, it checks link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag (default enabled).

It collects generated RDDs (from <<internal-registries, generatedRDDs>>) that are older than <<internal-registries, rememberDuration>>.

You should see the following DEBUG message in the logs:

```
DEBUG Clearing references to old RDDs: [[time] -> [rddId], ...]
```

Regardless of link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag, all the collected RDDs are removed from <<internal-registries, generatedRDDs>>.

When link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag is set (it is by default), you should see the following DEBUG message in the logs:

```
DEBUG Unpersisting old RDDs: [id1, id2, ...]
```

For every RDD in the list, it link:spark-rdd-caching.adoc#unpersist[unpersists them (without blocking)] one by one and explicitly link:spark-rdd-blockrdd.adoc[removes blocks for BlockRDDs]. You should see the following INFO message in the logs:

```
INFO Removing blocks of RDD [blockRDD] of time [time]
```

After RDDs have been removed from <<internal-registries, generatedRDDs>> (and perhaps unpersisted), you should see the following DEBUG message in the logs:

```
DEBUG Cleared [size] RDDs that were older than [time]: [time1, time2, ...]
```

The stream passes the call to clear metadata to its <<contract, dependencies>>.

=== [[internal-registries]] Internal Registries

`DStream` implementations maintain the following internal registries:

* `generatedRDDs` is the mapping between batches (per time) and generated RDDs. See <<getOrCompute, Computing RDD for Batch (using getOrCompute Method)>> in this document.
* `zeroTime` as the zero time.
* `rememberDuration` as the duration for which the `DStream` will remember each RDD created.
* `storageLevel` (default: `NONE`) as the link:spark-rdd-caching.adoc#StorageLevel[StorageLevel] of the RDDs in the `DStream`.
* `checkpointDuration` as the duration for checkpoint (that is set using `def checkpoint(interval: Duration)` method)
* `restoredFromCheckpointData` is a flag to inform whether it was restored from checkpoint.
* `graph` being the reference to link:spark-streaming-dstreamgraph.adoc[DStreamGraph].
