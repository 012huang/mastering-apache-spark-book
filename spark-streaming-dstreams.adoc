== Discretized Streams (DStreams)

*Discretized Stream (DStream)* is the fundamental concept in Spark Streaming. It is modelled as `org.apache.spark.streaming.dstream.DStream` abstract class.

To create a `DStream` instance you use link:spark-streaming-streamingcontext.adoc#creating-receivers[StreamingContext].

NOTE: New `DStreams` can only be created (and hence registered) before link:spark-streaming-streamingcontext.adoc#start[StreamingContext is started]. All other states lead to `IllegalStateException` being thrown.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.dstream.DStream` logger to see what happens inside any `DStream`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.dstream.DStream=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[contract]] DStream Contract

A `DStream` is defined by the following properties (with the names of the corresponding methods that subclasses have to implement):

* A collection of parent `DStreams` on which this `DStream` depends on (as `def dependencies: List[DStream[_]]`)

* A *slide duration* being a time interval after which the stream generates a RDD (as `def slideDuration: Duration`).

* How to generate a RDD for the given batch (as `def compute(validTime: Time): Option[RDD[T]]`)

CAUTION: FIXME How does slide duration relate to batch interval? Is there any relation?

=== [[operators]] Stream Operators

You apply transformations (using *stream operators*) to create stream pipelines on datasets built out of the data received from input streams. You may use datasets as RDDs from other (non-streaming) data sources, but the datasets from input streams are the reason to use Spark Streaming in the first place, _ain't it_?

There are two types of operators:

* *output operators* that turn input streams into output ones, i.e. they register the input streams as output to link:spark-streaming-dstreamgraph.adoc[DStreamGraph] (using `DStreamGraph.addOutputStream`).
* ...

All `DStreams` offer the following operations:

* `print` to print 10 elements only or the more general version `print(num: Int)` to print up to `num` elements. See <<print, print operation>> in this document.
* `map`
* `flatMap`
* `filter`
* `glom`
* `repartition`
* `mapPartitions`
* `reduce`
* `count`
* `countByValue`
* a family of `foreachRDD`
* a family of `transform`
* a family of `transformWith`
* a family of `window`
* a family of `reduceByWindow`
* `countByWindow`
* `countByValueAndWindow`
* `union`
* a family of `slice`
* `saveAsObjectFiles`
* `saveAsTextFiles`

NOTE: `DStream` companion object offers a Scala implicit to convert `DStream[(K, V)]` to `PairDStreamFunctions` with methods on DStreams of key-value pairs.

==== [[print]] print Operator

`print(num: Int)` operator prints `num` first elements of each RDD in the input stream.

`print` uses `print(num: Int)` with `num` being `10`.

It is a *output operator* (that returns `Unit`).

For each batch, `print` operator prints the following header to the standard output (regardless of the number of elements to be printed out):

```
-------------------------------------------
Time: [time] ms
-------------------------------------------
```

Internally, it calls `RDD.take(num + 1)` (see link:spark-rdd-operations.adoc#actions[take action]) on each RDD in the stream to print `num` elements. It then prints `...` if there are more elements in the RDD (that would otherwise exceed `num` elements being requested to print).

It creates link:spark-streaming-foreachdstreams.adoc[ForEachDStream] stream and register it as an output stream.

=== [[generateJob]] Generating Streaming Jobs (using generateJob Method)

The internal `DStream.generateJob(time: Time)` method generates a streaming job for a batch (given `time`). It is acceptable to generate no streaming job for a batch.

NOTE: It is called when link:spark-streaming-dstreamgraph.adoc#DStreamGraph-generateJobs[DStreamGraph generates jobs for a batch].

It <<getOrCompute, computes an RDD for the batch>> and returns a streaming `Job` instance with the job function running a Spark job (using `SparkContext.runJob`) when executed.

NOTE: The Spark job uses an empty function to calculate partitions of a RDD.

CAUTION: FIXME What happens when `SparkContext.runJob(rdd, emptyFunc)` is called with the empty function, i.e. `(iterator: Iterator[T]) => {}`?

=== [[getOrCompute]] Computing RDD for Batch (using getOrCompute Method)

The internal (`private final`) `getOrCompute(time: Time)` method returns an optional RDD for a batch (`time`).

It uses <<internal-registries, generatedRDDs>> to return the RDD if it has already been generated for the `time`. If not, it generates one by <<contract, computing the input stream>> (using `compute(validTime: Time)` method).

If there was anything to process in the input stream, i.e. <<contract, computing the input stream returned a RDD>>, the RDD is first link:spark-rdd-caching.adoc[persisted] (only if `storageLevel` for the input stream is different from `StorageLevel.NONE`).

You should see the following DEBUG message in the logs:

```
DEBUG Persisting RDD [id] for time [time] to [storageLevel]
```

The generated RDD is link:spark-rdd-checkpointing.adoc[checkpointed] if <<internal-registries, checkpointDuration>> is defined and the time interval between current and <<internal-registries, zero>> times is a multiple of <<internal-registries, checkpointDuration>>.

You should see the following DEBUG message in the logs:

```
DEBUG Marking RDD [id] for time [time] for checkpointing
```

The generated RDD is saved in the <<internal-registries, internal generatedRDDs registry>>.

=== [[clearMetadata]] Metadata Cleanup

NOTE: It is called when  link:spark-streaming-dstreamgraph.adoc#clearMetadata[DStreamGraph clears metadata for every output stream].

`clearMetadata(time: Time)` is called to remove old RDDs that have been generated so far (and collected in <<internal-registries, generatedRDDs>>). It is a sort of _garbage collector_.

When `clearMetadata(time: Time)` is called, it checks link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag (default enabled).

It collects generated RDDs (from <<internal-registries, generatedRDDs>>) that are older than <<internal-registries, rememberDuration>>.

You should see the following DEBUG message in the logs:

```
DEBUG Clearing references to old RDDs: [[time] -> [rddId], ...]
```

Regardless of link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag, all the collected RDDs are removed from <<internal-registries, generatedRDDs>>.

When link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag is set (it is by default), you should see the following DEBUG message in the logs:

```
DEBUG Unpersisting old RDDs: [id1, id2, ...]
```

For every RDD in the list, it link:spark-rdd-caching.adoc#unpersist[unpersists them (without blocking)] one by one and explicitly link:spark-rdd-blockrdd.adoc[removes blocks for BlockRDDs]. You should see the following INFO message in the logs:

```
INFO Removing blocks of RDD [blockRDD] of time [time]
```

After RDDs have been removed from <<internal-registries, generatedRDDs>> (and perhaps unpersisted), you should see the following DEBUG message in the logs:

```
DEBUG Cleared [size] RDDs that were older than [time]: [time1, time2, ...]
```

The stream passes the call to clear metadata to its <<contract, dependencies>>.

=== [[internal-registries]] Internal Registries

`DStream` implementations maintain the following internal registries:

* `generatedRDDs` is the mapping between batches (per time) and generated RDDs. See <<getOrCompute, Computing RDD for Batch (using getOrCompute Method)>> in this document.
* `zeroTime` as the zero time.
* `rememberDuration` as the duration for which the `DStream` will remember each RDD created.
* `storageLevel` (default: `NONE`) as the link:spark-rdd-caching.adoc#StorageLevel[StorageLevel] of the RDDs in the `DStream`.
* `checkpointDuration` as the duration for checkpoint (that is set using `def checkpoint(interval: Duration)` method)
* `restoredFromCheckpointData` is a flag to inform whether it was restored from checkpoint.
* `graph` being the reference to link:spark-streaming-dstreamgraph.adoc[DStreamGraph].
