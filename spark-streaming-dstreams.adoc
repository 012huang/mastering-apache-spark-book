== DStreams

*Discretized Stream (DStream)* is the fundamental concept in Spark Streaming. It is modelled as `org.apache.spark.streaming.dstream.DStream` abstract class.

It requires link:spark-streaming-streamingcontext.adoc[StreamingContext] to be created.

NOTE: New `DStreams` can only be created when link:spark-streaming-streamingcontext.adoc[StreamingContext] is in `INITIALIZED` state (before it is started). All other states lead to `IllegalStateException` being thrown.

As a Scala abstract class it requires the following methods to be implemented by subclasses:

* `def slideDuration: Duration` that is the time interval after which the `DStream` generates a RDD.
* `def dependencies: List[DStream[_]]` that is a collection of parent `DStreams` on which this `DStream` depends on.
* `def compute(validTime: Time): Option[RDD[T]]` that knows how to generate a RDD for the given batch.

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.streaming.dstream.DStream` logger to see what happens inside any `DStream`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.dstream.DStream=INFO
```
====

=== [[operations]] DStream Operations

All `DStream` implementations offer the following operations:

* `map`
* `flatMap`
* `filter`
* `glom`
* `repartition`
* `mapPartitions`
* `reduce`
* `count`
* `countByValue`
* a family of `foreachRDD`
* a family of `transform`
* a family of `transformWith`
* `print` to print 10 elements using `print(num: Int)`
* a family of `window`
* a family of `reduceByWindow`
* `countByWindow`
* `countByValueAndWindow`
* `union`
* a family of `slice`
* `saveAsObjectFiles`
* `saveAsTextFiles`

NOTE: `DStream` companion object offers a Scala implicit to convert `DStream[(K, V)]` to `PairDStreamFunctions` with methods on DStreams of key-value pairs.

=== [[internal-registries]] Internal Registries

`DStream` implementations maintain the following internal registries:

* `generatedRDDs` as a mapping between times and generated RDDs.
* `zeroTime` as the zero time.
* `rememberDuration` as the duration for which the `DStream` will remember each RDD created.
* `storageLevel` (default: `NONE`) as the link:spark-rdd-caching.adoc#StorageLevel[StorageLevel] of the RDDs in the `DStream`.
* `checkpointDuration` as the duration for checkpoint (that is set using `def checkpoint(interval: Duration)` method)
* `restoredFromCheckpointData` about whether it was restored from checkpoint.
* `graph` as the reference to the whole link:spark-streaming.adoc#DStreamGraph[DStreamGraph].

=== [[DStreamGraph]] DStreamGraph

`DStreamGraph` (is a final helper class that) manages link:spark-streaming-dstreams.adoc[input and output streams].

It maintains the collections of `inputStreams`, i.e. link:spark-streaming-inputdstreams.adoc[InputDStream] instances and `outputStreams`, i.e. link:spark-streaming-dstreams.adoc[DStream] instances.

[NOTE]
====
`DStreamGraph` holds `batchDuration` (using `setBatchDuration(duration: Duration)`) for other parts of the Streaming application.

It appears that it is _the_ place for the value since it must be set before link:spark-streaming-jobgenerator.adoc[JobGenerator] can be instantiated.

It _is_ set while link:spark-streaming-streamingcontext.adoc[StreamingContext] is being instantiated and is validated (using `validate()` method of `StreamingContext` and `DStreamGraph`) before `StreamingContext` is started.
====

When `DStreamGraph` is started (using `start(time: Time)` method), it saves `time` as `startTime`, and calls `initialize()` and `remember()` methods on every output stream (one by one). It then starts the input streams (in parallel).

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.DStreamGraph` logger to see what happens in DStreamGraph.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.DStreamGraph=DEBUG
```
====

==== [[DStreamGraph-generateJobs]] DStreamGraph.generateJobs(time: Time)

`DStreamGraph.generateJobs(time: Time)` method is called to generate a Spark job for each registered output stream (using `DStream.generateJob`).

When it runs, you should see the following DEBUG message in the logs before the jobs are generated for a batch at a time:

```
DEBUG DStreamGraph: Generating jobs for time [time] ms
```

You should also see the following DEBUG message when the jobs have been generated:

```
DEBUG DStreamGraph: Generated [jobs.length] jobs for time [time] ms
```
