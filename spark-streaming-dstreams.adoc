== DStreams

*Discretized Stream (DStream)* is the fundamental concept in Spark Streaming. It is modelled as `org.apache.spark.streaming.dstream.DStream` abstract class.

It requires link:spark-streaming-streamingcontext.adoc[StreamingContext] to be created.

NOTE: New `DStreams` can only be created when link:spark-streaming-streamingcontext.adoc[StreamingContext] is in `INITIALIZED` state (before it is started). All other states lead to `IllegalStateException` being thrown.

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.streaming.dstream.DStream` logger to see what happens inside any `DStream`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.dstream.DStream=INFO
```
====

=== [[contract]] DStream Contract

A `DStream` is defined by the following properties (with the names of the corresponding methods that subclasses have to implement):

* A collection of parent `DStreams` on which this `DStream` depends on (as `def dependencies: List[DStream[_]]`)

* The time interval (aka *slide duration*) after which the `DStream` generates a RDD for a batch (as `def slideDuration: Duration`).

* How to generate a RDD for the given batch (as `def compute(validTime: Time): Option[RDD[T]]`)

=== [[operations]] DStream Operations

All `DStream` implementations offer the following operations:

* `map`
* `flatMap`
* `filter`
* `glom`
* `repartition`
* `mapPartitions`
* `reduce`
* `count`
* `countByValue`
* a family of `foreachRDD`
* a family of `transform`
* a family of `transformWith`
* `print` to print 10 elements using `print(num: Int)`
* a family of `window`
* a family of `reduceByWindow`
* `countByWindow`
* `countByValueAndWindow`
* `union`
* a family of `slice`
* `saveAsObjectFiles`
* `saveAsTextFiles`

NOTE: `DStream` companion object offers a Scala implicit to convert `DStream[(K, V)]` to `PairDStreamFunctions` with methods on DStreams of key-value pairs.

=== [[generateJob]] Generating Streaming Jobs (using generateJob Method)

The internal `DStream.generateJob(time: Time)` method generates a streaming job for a batch (given `time`). It is acceptable to generate no streaming job for a batch.

NOTE: It is called when <<DStreamGraph-generateJobs, DStreamGraph generates jobs for a batch>>.

It <<getOrCompute, computes an RDD for the batch>> and returns a streaming `Job` instance with the job function running a Spark job (using `SparkContext.runJob`) when executed.

NOTE: The Spark job uses an empty function to calculate partitions of a RDD.

CAUTION: FIXME What happens when `SparkContext.runJob(rdd, emptyFunc)` is called with the empty function, i.e. `(iterator: Iterator[T]) => {}`?

=== [[getOrCompute]] Computing RDD for Batch (using getOrCompute Method)

The internal (`private final`) `getOrCompute(time: Time)` method returns an optional RDD for a batch (`time`).

It uses <<internal-registries, generatedRDDs>> to return the RDD if it has already been generated for the `time`. If not, it generates one by <<contract, computing the input stream>> (using `compute(validTime: Time)` method).

If there was anything to process in the input stream, i.e. <<contract, computing the input stream returned a RDD>>, the RDD is first link:spark-rdd-caching.adoc[persisted] (only if `storageLevel` for the input stream is different from `StorageLevel.NONE`).

You should see the following DEBUG message in the logs:

```
DEBUG Persisting RDD [id] for time [time] to [storageLevel]
```

The generated RDD is link:spark-rdd-checkpointing.adoc[checkpointed] if <<internal-registries, checkpointDuration>> is defined and the time interval between current and <<internal-registries, zero>> times is a multiple of <<internal-registries, checkpointDuration>>.

You should see the following DEBUG message in the logs:

```
DEBUG Marking RDD [id] for time [time] for checkpointing
```

The generated RDD is saved in the <<internal-registries, internal generatedRDDs registry>>.

=== [[internal-registries]] Internal Registries

`DStream` implementations maintain the following internal registries:

* `generatedRDDs` is the mapping between batches (per time) and generated RDDs. See <<getOrCompute, Computing RDD for Batch (using getOrCompute Method)>> in this document.
* `zeroTime` as the zero time.
* `rememberDuration` as the duration for which the `DStream` will remember each RDD created.
* `storageLevel` (default: `NONE`) as the link:spark-rdd-caching.adoc#StorageLevel[StorageLevel] of the RDDs in the `DStream`.
* `checkpointDuration` as the duration for checkpoint (that is set using `def checkpoint(interval: Duration)` method)
* `restoredFromCheckpointData` is a flag to inform whether it was restored from checkpoint.
* `graph` as the reference to the <<DStreamGraph, DStreamGraph>>.

=== [[DStreamGraph]] DStreamGraph

`DStreamGraph` (is a final helper class that) manages link:spark-streaming-dstreams.adoc[input and output streams].

It maintains the collections of link:spark-streaming-inputdstreams.adoc[InputDStream] instances (as `inputStreams`) and output link:spark-streaming-dstreams.adoc[DStream] instances (as `outputStreams`), but, more importantly, <<DStreamGraph-generateJobs, it generates streaming jobs for output streams for a batch>>.

[NOTE]
====
`DStreamGraph` holds `batchDuration` (using `setBatchDuration(duration: Duration)`) for other parts of the Streaming application.

It appears that it is _the_ place for the value since it must be set before link:spark-streaming-jobgenerator.adoc[JobGenerator] can be instantiated.

It _is_ set while link:spark-streaming-streamingcontext.adoc[StreamingContext] is being instantiated and is validated (using `validate()` method of `StreamingContext` and `DStreamGraph`) before `StreamingContext` is started.
====

When `DStreamGraph` is started (using `start(time: Time)` method), it saves `time` as `startTime`, and calls `initialize()` and `remember()` methods on every output stream (one by one). It then starts the input streams (in parallel).

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.streaming.DStreamGraph` logger to see what happens in `DStreamGraph`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.DStreamGraph=DEBUG
```
====

==== [[DStreamGraph-generateJobs]] Generating Streaming Jobs for Output Streams for Batch

`DStreamGraph` is responsible for generating streaming jobs for output streams for a batch (given `Time`). When requested, it uses `DStreamGraph.generateJobs(time: Time)` to walk over each registered output stream and generates a streaming job (using `DStream.generateJob`).

NOTE: Who's calling it and when?

When the method runs, you should see the following DEBUG message in the logs before the jobs are generated for a batch:

```
DEBUG DStreamGraph: Generating jobs for time [time] ms
```

Right before the method finishes, you should see the following DEBUG message with the number of streaming jobs generated (as `jobs.length`):

```
DEBUG DStreamGraph: Generated [jobs.length] jobs for time [time] ms
```
