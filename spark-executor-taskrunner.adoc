== TaskRunner

*TaskRunner* is a thread of execution that manages a single individual link:spark-taskscheduler-tasks.adoc[task]. It can be <<run, run>> or <<kill, killed>> that boils down to link:spark-taskscheduler-tasks.adoc#run[running] or link:spark-taskscheduler-tasks.adoc#kill[killing the task] the `TaskRunner` object manages.

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.executor.Executor` logger to see what happens inside `TaskRunner` (since `TaskRunner` is an internal class of `Executor`).

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.executor.Executor=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== Lifecycle

CAUTION: FIXME Image with state changes

A `TaskRunner` object is created when link:spark-executor.adoc#launchTask[an executor is requested to launch a task].

It is created with an link:spark-executor-backends.adoc[ExecutorBackend] (to send the task's status updates to), task and attempt ids, task name, and serialized version of the task (as `ByteBuffer`).

=== [[run]] Running Task (run method)

NOTE: `run` is a part of https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[java.lang.Runnable] interface.

When `run` is executed, it creates a link:spark-taskscheduler.adoc#TaskMemoryManager[TaskMemoryManager] object (using the global link:spark-sparkenv.adoc#MemoryManager[MemoryManager] and the constructor's `taskId`).

It starts measuring the time to deserialize a task.

It sets the current context classloader.

CAUTION: FIXME What is part of the classloader?

It creates a new instance of the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer].

You should see the following INFO message in the logs:

```
INFO Executor: Running [taskName] (TID [taskId])
```

At this point, the task is considered running and the link:spark-executor-backends.adoc[ExecutorBackend.statusUpdate] is executed (with `taskId` and `TaskState.RUNNING` state).

`run` deserializes the task's environment (from `serializedTask` bytes using `Task.deserializeWithDependencies`) to have the task's files, jars and properties, and the proper task's bytes.

NOTE: The target task to run is not deserialized yet, but only its environment - the files, jars, and properties.

CAUTION: FIXME Describe `Task.deserializeWithDependencies`.

`updateDependencies(taskFiles, taskJars)` is called.

CAUTION: FIXME What does `updateDependencies` do?

This is the moment when the proper link:spark-taskscheduler-tasks.adoc[Task] object is deserialized (from `taskBytes`) using the earlier-created link:spark-sparkenv.adoc#closureSerializer[closure Serializer] object. The local properties (as `localProperties`) are initialized to be the task's properties (from the earlier call to `Task.deserializeWithDependencies`) and link:spark-taskscheduler.adoc#TaskMemoryManager[TaskMemoryManager].

NOTE: The task's properties were part of the serialized object passed on to the current `TaskRunner` object.

NOTE: Until `run` deserializes the task object, it is only available as the `serializedTask` byte buffer.

If <<kill, kill>> method has been called in the meantime, the execution stops by throwing a `TaskKilledException`. Otherwise, `TaskRunner` continues executing the task.

You should see the following DEBUG message in the logs:

```
DEBUG Executor: Task [taskId]'s epoch is [task.epoch]
```

TaskRunner sends update of the epoch of the task to link:spark-service-mapoutputtracker.adoc[MapOutputTracker].

CAUTION: FIXME Why is `MapOutputTracker.updateEpoch` needed?

link:spark-taskscheduler-tasks.adoc#run[Task runs] (with `taskId`, `attemptNumber`, and the globally-configured `MetricsSystem`).

CAUTION: FIXME Describe `finally` block.

If link:spark-taskscheduler-tasks.adoc#kill[the task was killed] a `TaskKilledException` is thrown (and the `TaskRunner` quits).

CAUTION: FIXME Finish me!

When a task finishes, it returns a value and `accumUpdates`, i.e. the result of link:spark-taskscheduler-tasks.adoc#collectAccumulatorUpdates[Task.collectAccumulatorUpdates] that collects the latest values of accumulators used.

The result value is serialized (using the other instance of `Serializer`, i.e. `serializer`).

NOTE: There are two `Serializer` instances in `SparkContext.env`.

It creates an instance of `DirectTaskResult` with the serialized result and the accumulators' values (as `accumUpdates`).

`run` serializes the `DirectTaskResult` (using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]) and calculates the result's size.

If `maxResultSize` is set and the size of the serialized result exceeds it, a `SparkException` is reported.

```
$ ./bin/spark-shell -c spark.driver.maxResultSize=1m

scala> sc.getConf.get("spark.driver.maxResultSize")
res0: String = 1m

scala> sc.range(0, 1024 * 1024 + 10, 1).collect
...
org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1 tasks (1031.4 KB) is bigger than spark.driver.maxResultSize (1024.0 KB)
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
```

CAUTION: FIXME Complete me.

A successful execution is "announced" as INFO to the logs:

```
INFO Executor: Finished [taskName] (TID [taskId]). [resultSize] bytes result sent to driver
```

The serialized result is sent to the driver using link:spark-executor-backends.adoc[ExecutorBackend] as `TaskState.FINISHED` and `serializedResult`.

When `TaskRunner` finishes, it is removed from the internal `runningTasks` map.

=== [[kill]] Killing Task (kill method)

[source, scala]
----
kill(interruptThread: Boolean): Unit
----

`kill` marks the current instance of `TaskRunner` as killed and passes the call to kill a task on to the task itself (if available).

When executed, you should see the following INFO message in the logs:

```
INFO TaskRunner: Executor is trying to kill [taskName] (TID [taskId])
```

Internally, `kill` enables the internal flag `killed` and executes its link:spark-taskscheduler-tasks.adoc#kill[Task.kill] method if a task is available.

NOTE: The internal flag `killed` is checked in <<run, run>> to stop executing the task. Calling link:spark-taskscheduler-tasks.adoc#kill[Task.kill] method allows for task interruptions later on.
