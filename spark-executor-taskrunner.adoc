== TaskRunner

*TaskRunner* is a thread of execution that runs a link:spark-taskscheduler-tasks.adoc[Task].

NOTE: `TaskRunner` can run a single task only. When TaskRunner finishes, it is removed from the internal `runningTasks` map.

A `TaskRunner` object is created when link:spark-executor.adoc#launchTask[an executor is requested to launch a task].

It requires an link:spark-executor-backends.adoc[ExecutorBackend] (to send the task's status updates to), task and attempt ids, task name, and serialized version of the task (as `ByteBuffer`).

Sending task status updates to link:spark-executor-backends.adoc[ExecutorBackend] is a mechanism to inform the executor backend about the task's state changes:

* the task being started, i.e. `TaskState.RUNNING`
* the task finish together with the serialized result, i.e. `TaskState.FINISHED`
* the task being killed, i.e. `TaskState.KILLED`
* the task failures, i.e. `TaskState.FAILED`

=== [[kill]] Killing Task (kill method)

`kill` marks `TaskRunner` as killed and passes the call to kill a task on to the task itself.

When executed, you should see the following INFO message in the logs:

```
INFO TaskRunner: Executor is trying to kill [taskName] (TID [taskId])
```

Internally, `kill` enables the internal flag `killed` and if the task is available executes its link:spark-taskscheduler-tasks.adoc#kill[kill] method.

NOTE: The internal flag `killed` is checked in <<run, run>> to stop executing the task. Calling link:spark-taskscheduler-tasks.adoc#kill[Task.kill] method allows for further task interruptions.

=== [[run]] Executing Task (run method)

When a `TaskRunner` starts running, it prints the following INFO message to the logs:

```
INFO Executor: Running [taskName] (TID [taskId])
```

`taskId` is the id of the task being executed in <<thread-pool, Executor task launch worker-[taskId]>>.

Information about it is sent to the driver using link:spark-executor-backends.adoc[ExecutorBackend] as `TaskState.RUNNING` and zero-byte `ByteBuffer`.

CAUTION: FIXME TaskMemoryManager and task serialization and deserialization

`run` first deserializes the task (using `Task.deserializeWithDependencies`), `updateDependencies(taskFiles, taskJars)`, and deserializes `serializedTask` task byte buffer into a link:spark-taskscheduler-tasks.adoc[Task] object (using the globally-configured `Serializer`). The task instance has the link:spark-taskscheduler.adoc#TaskMemoryManager[TaskMemoryManager] set.

NOTE: Until `run` deserializes the task object, it is only available as the `serializedTask` byte buffer.

This is the moment when a task can stop its execution if it was killed while being deserialized. If not killed, `TaskRunner` continues executing the task.

You should see the following DEBUG message in the logs:

```
DEBUG Task [taskId]'s epoch is [task.epoch]
```

TaskRunner sends update of the epoch of the task to link:spark-service-mapoutputtracker.adoc[MapOutputTracker].

CAUTION: FIXME Why is `MapOutputTracker.updateEpoch` needed?

Task runs (with `taskId`, `attemptNumber`, and the globally-configured `MetricsSystem`). See link:spark-taskscheduler-tasks.adoc#execution[Task Execution].

When a task finishes, it returns a value and `accumUpdates`, i.e. the result of link:spark-taskscheduler-tasks.adoc#collectAccumulatorUpdates[Task.collectAccumulatorUpdates] that collects the latest values of accumulators used.

The result value is serialized (using the other instance of `Serializer`, i.e. `serializer`).

NOTE: There are two `Serializer` instances in `SparkContext.env`.

It creates an instance of `DirectTaskResult` with the serialized result and the accumulators' values (as `accumUpdates`).

`run` serializes the `DirectTaskResult` (using `SparkEnv.closureSerializer`) and calculates the result's size.

If `maxResultSize` is set and the size of the serialized result exceeds it, a `SparkException` is reported.

```
$ ./bin/spark-shell -c spark.driver.maxResultSize=1m

scala> sc.getConf.get("spark.driver.maxResultSize")
res0: String = 1m

scala> sc.range(0, 1024 * 1024 + 10, 1).collect
...
org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1 tasks (1031.4 KB) is bigger than spark.driver.maxResultSize (1024.0 KB)
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
```

CAUTION: FIXME Complete me.

A successful execution is "announced" as INFO to the logs:

```
INFO Executor: Finished [taskName] (TID [taskId]). [resultSize] bytes result sent to driver
```

The serialized result is sent to the driver using link:spark-executor-backends.adoc[ExecutorBackend] as `TaskState.FINISHED` and `serializedResult`.
