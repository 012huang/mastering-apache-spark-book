== [[Optimizer]] Optimizer -- Logical Query Plan Optimizer

[[execute]]
`Optimizer` is a link:spark-sql-catalyst-RuleExecutor.adoc[RuleExecutor] that defines <<batches, collection of logical plan optimizations>>.

NOTE: The result of applying the <<batches, batches>> of `Optimizer` to a link:spark-sql-LogicalPlan.adoc[LogicalPlan] is called *optimized logical plan*.

[[batches]]
.Optimizer's Optimization Rules (in the order of execution)
[cols="2,1,3,3",options="header",width="100%"]
|===
^.^| Batch Name
^.^| Strategy
| Rules
| Description

.7+^.^| Finish Analysis
.7+^.^| `Once`
| EliminateSubqueryAliases
|

| EliminateView
|

| ReplaceExpressions
|

| link:spark-sql-Optimizer-GetCurrentDatabase.adoc#ComputeCurrentTime[ComputeCurrentTime]
|

| link:spark-sql-Optimizer-GetCurrentDatabase.adoc#GetCurrentDatabase[GetCurrentDatabase]
|

| RewriteDistinctAggregates
|

| ReplaceDeduplicateWithAggregate
|

^.^| Union
^.^| `Once`
| CombineUnions
|

^.^| Subquery
^.^| `Once`
| OptimizeSubqueries
|

.3+^.^| Replace Operators
.3+^.^| <<fixedPoint, FixedPoint>>
| ReplaceIntersectWithSemiJoin
|

| ReplaceExceptWithAntiJoin
|

| ReplaceDistinctWithAggregate
|

.2+^.^| Aggregate
.2+^.^| <<fixedPoint, FixedPoint>>
| RemoveLiteralFromGroupExpressions
|

| RemoveRepetitionFromGroupExpressions
|

.35+^.^| Operator Optimizations
.35+^.^| <<fixedPoint, FixedPoint>>
|PushProjectionThroughUnion
|

|ReorderJoin
|

|EliminateOuterJoin
|

|PushPredicateThroughJoin
|

|link:spark-sql-Optimizer-PushDownPredicate.adoc[PushDownPredicate]
|

|LimitPushDown
|

|link:spark-sql-Optimizer-ColumnPruning.adoc[ColumnPruning]
|

|InferFiltersFromConstraints
|

|CollapseRepartition
|

|CollapseProject
|

|CollapseWindow
|

|CombineFilters
|

|CombineLimits
|

|CombineUnions
|

|link:spark-sql-Optimizer-NullPropagation.adoc[NullPropagation]
|

|FoldablePropagation
|

|OptimizeIn
|

|link:spark-sql-Optimizer-ConstantFolding.adoc[ConstantFolding]
|

|ReorderAssociativeOperator
|

|LikeSimplification
|

|BooleanSimplification
|

|SimplifyConditionals
|

|RemoveDispensableExpressions
|

|SimplifyBinaryComparison
|

|PruneFilters
|

|EliminateSorts
|

|link:spark-sql-Optimizer-SimplifyCasts.adoc[SimplifyCasts]
|

|SimplifyCaseConversionExpressions
|

|RewriteCorrelatedScalarSubquery
|

|link:spark-sql-Optimizer-EliminateSerialization.adoc[EliminateSerialization]
|

|RemoveRedundantAliases
|

|RemoveRedundantProject
|

|SimplifyCreateStructOps
|

|SimplifyCreateArrayOps
|

|SimplifyCreateMapOps
|

^.^| Check Cartesian Products
^.^| `Once`
| CheckCartesianProducts
|

^.^| Join Reorder
^.^| `Once`
| CostBasedJoinReorder
|

^.^| Decimal Optimizations
^.^| <<fixedPoint, FixedPoint>>
| DecimalAggregates
|

^.^| Typed Filter Optimization
^.^| <<fixedPoint, FixedPoint>>
| link:spark-sql-Optimizer-CombineTypedFilters.adoc[CombineTypedFilters]
|

.2+^.^| LocalRelation
.2+^.^| <<fixedPoint, FixedPoint>>
| ConvertToLocalRelation
|

| link:spark-sql-Optimizer-PropagateEmptyRelation.adoc[PropagateEmptyRelation]
|

^.^| OptimizeCodegen
^.^| `Once`
| OptimizeCodegen
|

.2+^.^| RewriteSubquery
.2+^.^| `Once`
| RewritePredicateSubquery
|

| CollapseProject
|

|===

TIP: Consult the https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L48-L137[sources of `Optimizer`] for the up-to-date list of the optimization rules.

NOTE: *Catalyst* is a Spark SQL framework for manipulating trees. It can work with trees of relational operators and expressions in link:spark-sql-LogicalPlan.adoc[logical plans] before they end up as link:spark-sql-SparkPlan.adoc[physical execution plans].

[source, scala]
----
scala> sql("select 1 + 1 + 1").explain(true)
== Parsed Logical Plan ==
'Project [unresolvedalias(((1 + 1) + 1), None)]
+- OneRowRelation$

== Analyzed Logical Plan ==
((1 + 1) + 1): int
Project [((1 + 1) + 1) AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Optimized Logical Plan ==
Project [3 AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Physical Plan ==
*Project [3 AS ((1 + 1) + 1)#4]
+- Scan OneRowRelation[]
----

Spark 2.0 uses Catalyst's tree manipulation library to build an extensible *query plan optimizer* with a number of query optimizations.

Catalyst supports both rule-based and cost-based optimization.

[[internal-registries]]
.Optimizer's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[fixedPoint]] `fixedPoint`
| FIXME

Used when...FIXME
|===

=== [[SparkOptimizer]] `SparkOptimizer` -- The Default Logical Query Plan Optimizer

`SparkOptimizer` is the default <<Optimizer, logical query plan optimizer>> that is available as link:spark-sql-SessionState.adoc#optimizer[`optimizer` attribute of `SessionState`] with the <<batches, logical plan optimizations>>.

[source, scala]
----
sparkSession.sessionState.optimizer
----

NOTE: `SparkOptimizer` is _merely_ used to link:spark-sql-QueryExecution.adoc#optimizedPlan[compute the optimized `LogicalPlan` for a `QueryExecution`] (available as `optimizedPlan`).

`SparkOptimizer` requires a link:spark-sql-SessionCatalog.adoc[SessionCatalog], a link:spark-sql-SQLConf.adoc[SQLConf] and link:spark-sql-ExperimentalMethods.adoc[ExperimentalMethods] with user-defined experimental methods.

NOTE: ``SparkOptimizer``'s input `experimentalMethods` serves an extension point for custom link:spark-sql-ExperimentalMethods.adoc[ExperimentalMethods].

`SparkOptimizer` extends the <<batches, `Optimizer` batches>> with the following batches:

1. *Optimize Metadata Only Query* (as `OptimizeMetadataOnlyQuery`)
2. *Extract Python UDF from Aggregate* (as `ExtractPythonUDFFromAggregate`)
3. *Prune File Source Table Partitions* (as `PruneFileSourcePartitions`)
4. *User Provided Optimizers* for the input user-defined link:spark-sql-ExperimentalMethods.adoc[ExperimentalMethods]

You can see the result of executing `SparkOptimizer` on a query plan using link:spark-sql-QueryExecution.adoc#optimizedPlan[`optimizedPlan` attribute of `QueryExecution`].

[source, scala]
----
// Applying two filter in sequence on purpose
// We want to kick CombineTypedFilters optimizer in
val dataset = spark.range(10).filter(_ % 2 == 0).filter(_ == 0)

// optimizedPlan is a lazy value
// Only at the first time you call it you will trigger optimizations
// Next calls end up with the cached already-optimized result
// Use explain to trigger optimizations again
scala> dataset.queryExecution.optimizedPlan
res0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)
+- Range (0, 10, step=1, splits=Some(8))
----

[TIP]
====
Enable `DEBUG` or `TRACE` logging levels for `org.apache.spark.sql.execution.SparkOptimizer` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.SparkOptimizer=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[i-want-more]] Further reading or watching

1. https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html[Deep Dive into Spark SQLâ€™s Catalyst Optimizer]

2. (video) https://youtu.be/_1byVWTEK1s?t=19m7s[Modern Spark DataFrame and Dataset (Intermediate Tutorial)] by https://twitter.com/adbreind[Adam Breindel] from Databricks.
