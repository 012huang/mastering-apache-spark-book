== Operations - Transformations and Actions

=== [[transformations]] Transformations

A *transformation* is a lazy operation on a RDD that returns another RDD, like `map`, `flatMap`, `filter`, `reduceByKey`, `join`, `cogroup`, etc.

[source,scala]
----
scala> val words = lines.flatMap(_.split("\\s+"))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:26
----

You can chain transformations to create *pipelines* (lazy computations), but their execution is only performed after an action executes.

There are two kinds of transformations:

* <<narrow-transformations, narrow transformations>>
* <<wide-transformations, wide transformations>>

==== [[narrow-transformations]] Narrow transformations

*Narrow transformations* are the result of `map`, `filter` and such that is from the data from a single partition only, i.e. it is self-sustained.

An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.

Spark groups narrow transformations as a stage.

==== [[wide-transformations]] Wide transformations

*Wide transformations* are the result of `groupByKey` and `reduceByKey`. The data required to compute the records in a single partition may reside in many partitions of the parent RDD.

All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute link:spark-rdd-shuffle.adoc[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions.

=== [[actions]] Actions

An *action* triggers execution of <<transformations, RDD transformations>> and returns a value (to a Spark driver - the user program).

You can think of actions as a valve and until no action is fired, the data to be processed is not even in the pipes, i.e. transformations. They are only actions to materialize the entire processing pipeline with real data.

Action operations:

* `count`
* `reduce`
* `collect`
* `take`
* `first`
* link:spark-io.adoc#saving-rdds-to-files[saveAs* actions], e.g. `saveAsTextFile`, `saveAsHadoopFile`.
* `countByValue`

Actions launch link:spark-scheduler.adoc#jobs[jobs] (on all partitions of an RDD) using link:spark-sparkcontext.adoc#running-jobs[SparkContext.runJob].

[source,scala]
----
scala> words.count  // <1>
res0: Long = 502
----
<1> `words` is an RDD of `String`.

TIP: You should `cache` an RDD you work with when you want to execute two or more actions on it for better performance. Refer to link:spark-rdd-caching.adoc[RDD Caching / Persistence].

Before calling an action, Spark does closure/function cleaning (using `SparkContext.clean`) to make it ready to be serialized and send to executors.

NOTE: Spark uses `ClosureCleaner` to clean closures.
