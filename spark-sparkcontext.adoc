== SparkContext - the door to Spark

CAUTION: FIXME Review the code of `SparkContext`

From the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> *SparkContext* is the main entry point for Spark functionality.
> A SparkContext represents the connection to a Spark cluster,
> and can be used to create RDDs, accumulators and broadcast variables on that cluster.

You use `SparkContext` for the following group of functionalities:

* link:spark-cluster.adoc[Connect to a cluster]
* link:spark-configuration.adoc[Specify mandatory application name]
* link:spark-rdd.adoc[Create RDDs]
* link:spark-broadcast.adoc[Create broadcast variables]
* link:spark-accumulators.adoc[Use accumulators]
* Use services, e.g. link:spark-taskscheduler.adoc[Task Scheduler], link:spark-scheduler-listeners.adoc[Listener Bus], link:spark-blockmanager.adoc[Block Manager], <<coarse-grained-scheduler-backends, CoarseGrainedSchedulerBackends>>, link:spark-shuffle-service.adoc[Shuffle Service].
* <<running-jobs, Run jobs>>

=== [[sparkcontext-and-rdd]] SparkContext and RDDs

A SparkContext is a logical boundary for visibility of RDDs, and so RDDs can't be shared between SparkContexts.

.SparkContext is a living space for RDDs
image::diagrams/sparkcontext-rdds.png[align="center"]

=== [[master-urls]] Master URLs

Spark supports the following master URLs (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L2729-L2742[private object SparkMasterRegex]):

* *local*, *local[N]* and *local[{asterisk}]* for link:spark-local.adoc[Spark local]
* *local[N, maxRetries]* for link:spark-local.adoc[Spark local-with-retries]
* *local-cluster[N, cores, memory]* for simulating a Spark cluster of [N, cores, memory] locally
* *spark://host:port,host1:port1,...* for connecting to link:spark-standalone.adoc[Spark Standalone cluster(s)]
* *mesos://* or *zk://* for link:spark-mesos.adoc[Spark on Mesos cluster]
* *yarn-cluster* (deprecated: *yarn-standalone*) for link:spark-yarn.adoc[Spark on YARN (cluster mode)]
* *yarn-client* for link:spark-yarn.adoc[Spark on YARN cluster (client mode)]
* *simr://* for Simr cluster

You use a master URL with link:spark-submit.adoc[spark-submit for `--master` command-line option] or <<creating-sparkcontext, when creating SparkContext with `setMaster` method>>.

=== Instantiating Scheduler Backend and Task Scheduler

`SparkContext.createTaskScheduler` is executed as part of SparkContext's initialization to create a link:spark-taskscheduler.adoc[task scheduler] (based on a given master URL) and link:spark-execution-model.adoc#scheduler-backends[scheduler backend].

.SparkContext creates Task Scheduler and Scheduler Backend
image::diagrams/sparkcontext-createtaskscheduler.png[align="center"]

==== [[executor-allocation-client]] ExecutorAllocationClient and CoarseGrainedSchedulerBackends

SparkContext is an *ExecutorAllocationClient* for link:spark-execution-model.adoc#scheduler-backends[CoarseGrainedSchedulerBackends], i.e. a client that communicates with a (coarse-grain) cluster manager to request or kill executors.

=== [[creating-sparkcontext]] Creating SparkContext

You create a `SparkContext` instance using link:spark-configuration.adoc[Spark application's configuration] using a `SparkConfig` object.

```
scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> val conf = new SparkConf().setMaster("local[*]").setAppName("Hello world!")
conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7a8f69d6

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> val sc = new SparkContext(conf)
sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@50ee2523
```

=== SparkContext in Spark shell

In link:spark-shell.adoc[Spark shell], an instance of `SparkContext` is automatically created for you under the name `sc`.

You should see the name in the logs while `spark-shell` starts.

```
$ ./bin/spark-shell
Spark context available as sc.
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

`SparkContext` allows you to create many different RDDs from input sources like:

* Scala's collections, i.e. `sc.parallelize(0 to 100)`
* local or remote filesystems, i.e. `sc.textFile("README.md")`
* Any Hadoop `InputSource`, i.e.
+
```
sc.newAPIHadoopFile("filepath1, filepath2", classOf[NewTextInputFormat], classOf[LongWritable], classOf[Text])
```

Refer to link:spark-io.adoc[Using Input and Output (I/O)] to find out more on the IO API to create RDDs.

You can also use `SparkContext` to create counters and link:spark-accumulators.adoc[accumulators].

Using `SparkContext` allows you to configure Spark context, i.e. properties that govern things like link:spark-cluster.adoc[master URL], link:spark-hadoop.adoc[Hadoop API], etc.

=== SparkContext as the global configuration of services

SparkContext keeps track of:

* shuffle ids using `nextShuffleId` internal field for link:spark-scheduler.adoc#ShuffleMapStage[registering shuffle dependencies] to link:spark-shuffle-service.adoc[Shuffle Service].

=== [[running-jobs]] Running jobs

All link:spark-rdd.adoc#actions[RDD actions] in Spark launch link:spark-scheduler.adoc#jobs[jobs] (that are run on one or many partitions of the RDD) using `SparkContext.runJob(rdd: RDD[T], func: Iterator[T] => U): Array[U]`.

TIP: For some actions like `first()` and `lookup()`, there is no need to compute all the partitions of the RDD in a job. And Spark knows it.

[source,scala]
----
scala> import org.apache.spark.TaskContext
import org.apache.spark.TaskContext

scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1>
res0: Array[Int] = Array(1, 1)  // <2>
----
<1> Run a job using `runJob` on `lines` RDD with a function that returns 1 for every partition (of `lines` RDD).
<2> What can you say about the number of partitions of the `lines` RDD? Is your result `res0` different than mine? Why?

Running a job is essentially executing the `func` function on the `rdd` RDD and returning the result as an array (with elements being the results per partition).

`SparkContext.runJob` calls link:spark-scheduler.adoc#runJob[DAGScheduler.runJob()]. Before the method finishes, it does link:spark-rdd-checkpointing.adoc[checkpointing]. It triggers posting `JobSubmitted` event (see <<event-loop,Event loop>>).

When you stop SparkContext using `sc.stop`, you can no longer launch jobs using `runJob`. An attempt to do so will result in `java.lang.IllegalStateException`.

[source, scala]
----
scala> sc.stop
...
INFO SparkContext: Successfully stopped SparkContext

scala> sc.runJob(lines, (t: TaskContext, i: Iterator[Int]) => 1)
java.lang.IllegalStateException: SparkContext has been shutdown
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1812)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1833)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)
  ... 48 elided
----

By default, SparkContext uses (`private[spark]` class) `org.apache.spark.scheduler.DAGScheduler`, but you can develop your own custom DAGScheduler implementation, and use (`private[spark]`) `SparkContext.dagScheduler_=(ds: DAGScheduler)` method to assign yours.

=== Others

I'm unsure where to put them, but since I found them interesting, they should be _somewhere_.

==== spark.driver.allowMultipleContexts

Found in the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

The above quote is not necessarily correct when `spark.driver.allowMultipleContexts` is `true` (default: `false`). If `true`, Spark logs warnings instead of throwing exceptions when multiple SparkContexts are active, i.e. no other SparkContext is running in this JVM. Upon creating an instance of `SparkContext`, Spark marks the current thread as having it being created (very early in the instantiation process).

==== SparkContext.addJar / --jars

The jar you specify with `SparkContext.addJar` will be copied to all the worker nodes.

From http://deploymentzone.com/2015/01/30/spark-and-merged-csv-files/[SPARK AND MERGED CSV FILES]:

> Spark is like Hadoop - uses Hadoop, in fact - for performing actions like outputting data to HDFS. You'll know what I mean the first time you try to save "all-the-data.csv" and are surprised to find a directory named all-the-data.csv/ containing a 0 byte _SUCCESS file and then several part-0000n files for each partition that took part in the job.

==== Gotchas - things to watch for

Even you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors.

See https://issues.apache.org/jira/browse/SPARK-5063
