== SparkContext - the door to Spark

*SparkContext* (aka *Spark context*) represents the connection to a Spark execution environment, called link:spark-deployment-modes.adoc[deployment mode], and can only then be used to create RDDs, accumulators and broadcast variables, access Spark services and run jobs.

A Spark context is essentially a client of Spark's execution environment and acts as the master of your Spark application.

.Spark context acts as the master of your Spark application
image::diagrams/sparkcontext-services.png[align="center"]

As a developer, you can use `SparkContext` for the following functions:

* link:spark-cluster.adoc[Connect to a cluster]
* link:spark-configuration.adoc[Specify mandatory application name]
* <<creating-rdds, Creating RDDs>>
* link:spark-broadcast.adoc[Create broadcast variables]
* link:spark-accumulators.adoc[Use accumulators]
* Access services, e.g. link:spark-taskscheduler.adoc[Task Scheduler], link:spark-scheduler-listeners.adoc[Listener Bus], link:spark-blockmanager.adoc[Block Manager], <<coarse-grained-scheduler-backends, CoarseGrainedSchedulerBackends>>, link:spark-shuffle-service.adoc[Shuffle Service].
* <<running-jobs, Run jobs>>
* <<custom-schedulers, Setting up custom Scheduler Backend, Task Scheduler and DAGScheduler>>

Read the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext].

CAUTION: FIXME SparkStatusTracker

=== [[creating-sparkcontext]] Creating SparkContext

You create a `SparkContext` instance using a link:spark-configuration.adoc[SparkConf] object.

[source, scala]
----
scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> val conf = new SparkConf().setMaster("local[*]").setAppName("SparkMe App")
conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7a8f69d6

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> val sc = new SparkContext(conf)  // <1>
sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@50ee2523
----
<1> You can also use the other constructor of `SparkContext`, i.e. `new SparkContext(master="local[*]", appName="SparkMe App", new SparkConf)`, with master and application name specified explicitly

When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from services):

```
INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
```

Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores).

==== [[allowMultipleContexts]] spark.driver.allowMultipleContexts

Quoting the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

The above quote is not necessarily correct when `spark.driver.allowMultipleContexts` is `true` (default: `false`). If `true`, Spark logs warnings instead of throwing exceptions when multiple SparkContexts are active, i.e. multiple SparkContext are running in this JVM. When creating an instance of `SparkContext`, Spark marks the current thread as having it being created (very early in the instantiation process).

CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress.

=== [[sparkcontext-and-rdd]] SparkContext and RDDs

You use a Spark context to create RDDs (see <<creating-rdds, Creating RDD>>).

When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts.

.A Spark context creates a living space for RDDs.
image::diagrams/sparkcontext-rdds.png[align="center"]

=== SparkContext in Spark shell

In link:spark-shell.adoc[Spark shell], an instance of `SparkContext` is automatically created for you under the name `sc`.

Read link:spark-shell.adoc[Spark shell].

=== [[creating-rdds]] Creating RDD

`SparkContext` allows you to create many different RDDs from input sources like:

* Scala's collections, i.e. `sc.parallelize(0 to 100)`
* local or remote filesystems, i.e. `sc.textFile("README.md")`
* Any Hadoop `InputSource` using `sc.newAPIHadoopFile`

Read link:spark-rdd.adoc#creating-rdds[Creating RDDs] in link:spark-rdd.adoc[RDD - Resilient Distributed Dataset].

=== [[jars]] Distribute JARs to workers

The jar you specify with `SparkContext.addJar` will be copied to all the worker nodes.

The configuration setting `spark.jars` is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or `local:/path` for a file on every worker node.

```
scala> sc.addJar("build.sbt")
15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457
```

CAUTION: FIXME Why is HttpFileServer used for addJar?

=== SparkContext as the global configuration for services

SparkContext keeps track of:

* shuffle ids using `nextShuffleId` internal field for link:spark-dagscheduler.adoc#ShuffleMapStage[registering shuffle dependencies] to link:spark-shuffle-service.adoc[Shuffle Service].

=== [[running-jobs]] Running Jobs

All link:spark-rdd.adoc#actions[RDD actions] in Spark launch link:spark-dagscheduler.adoc#jobs[jobs] (that are run on one or many partitions of the RDD) using `SparkContext.runJob(rdd: RDD[T], func: Iterator[T] => U): Array[U]`.

TIP: For some actions like `first()` and `lookup()`, there is no need to compute all the partitions of the RDD in a job. And Spark knows it.

[source,scala]
----
scala> import org.apache.spark.TaskContext
import org.apache.spark.TaskContext

scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1>
res0: Array[Int] = Array(1, 1)  // <2>
----
<1> Run a job using `runJob` on `lines` RDD with a function that returns 1 for every partition (of `lines` RDD).
<2> What can you say about the number of partitions of the `lines` RDD? Is your result `res0` different than mine? Why?

Running a job is essentially executing the `func` function on the `rdd` RDD and returning the result as an array (with elements being the results per partition).

`SparkContext.runJob` calls link:spark-dagscheduler.adoc#runJob[DAGScheduler.runJob()]. Before the method finishes, it does link:spark-rdd-checkpointing.adoc[checkpointing]. It triggers posting `JobSubmitted` event (see <<event-loop,Event loop>>).

You can only run jobs when a Spark context is active, i.e. started. See <<stopping-spark-context, Stopping Spark context>>.

=== [[stopping-spark-context]] Stopping Spark Context

You can stop a Spark context using `SparkContext.stop` method. Stopping a Spark context stops a Spark application.

You have to create a new Spark context before using Spark features and services.

An attempt to use a Spark context after it was stopped will result in `java.lang.IllegalStateException: SparkContext has been shutdown`.

[source, scala]
----
scala> sc.stop
...
INFO SparkContext: Successfully stopped SparkContext

scala> sc.runJob(lines, (t: TaskContext, i: Iterator[Int]) => 1)
java.lang.IllegalStateException: SparkContext has been shutdown
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1812)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1833)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)
  ... 48 elided
----

=== [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler

By default, SparkContext uses (`private[spark]` class) `org.apache.spark.scheduler.DAGScheduler`, but you can develop your own custom DAGScheduler implementation, and use (`private[spark]`) `SparkContext.dagScheduler_=(ds: DAGScheduler)` method to assign yours.

It is also applicable to `SchedulerBackend` and `TaskScheduler` using `schedulerBackend_=(sb: SchedulerBackend)` and `taskScheduler_=(ts: TaskScheduler)` methods, respectively.

CAUTION: FIXME Make it an advanced exercise.

=== [[createTaskScheduler]] Creating Scheduler Backend and Task Scheduler

`SparkContext.createTaskScheduler` is executed as part of SparkContext's initialization to create a link:spark-taskscheduler.adoc[task scheduler] (based on a link:spark-deployment-modes.adoc#master-urls[ given master URL]) and link:spark-execution-model.adoc#scheduler-backends[scheduler backend].

.SparkContext creates Task Scheduler and Scheduler Backend
image::diagrams/sparkcontext-createtaskscheduler.png[align="center"]

=== [[events]] Events

When a Spark context starts, it triggers `SparkListenerEnvironmentUpdate` and `SparkListenerApplicationStart` events.

=== [[persistentRdds]] Persisted RDDs

FIXME When is the internal field `persistentRdds` used?

=== [[setting-default-log-level]] Setting Default Log Level Programatically

To adjust logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell], use `SparkContext.setLogLevel(logLevel: String)`.

[TIP]
====
`sc.setLogLevel("INFO")` becomes `org.apache.log4j.Level.toLevel(logLevel)` and `org.apache.log4j.Logger.getRootLogger().setLevel(l)` internally.

See https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L367-L378[org/apache/spark/SparkContext.scala].
====

=== [[initialization]] SparkContext's initialization

Let's walk through a typical initialization code of SparkContext in a Spark application and see what happens under the covers.

[source, scala]
----
import org.apache.spark.{SparkConf, SparkContext}

// 1. Create Spark configuration
val conf = new SparkConf()
  .setAppName("SparkMe Application")
  .setMaster("local[*]")

// 2. Create Spark context
val sc = new SparkContext(conf)
----

The very first information printed out is the following INFO message:

```
INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
```

Up to this point SparkContext has done nothing (beside checking <<allowMultipleContexts, whether SparkContexts can be shared or not using `spark.driver.allowMultipleContexts`>>).

An instance of link:spark-scheduler-listeners.adoc#SparkListenerBus[Listener Bus] is created.

It reads the current user name, i.e. a value of `SPARK_USER` environment variable or the currently logged-in user. It becomes `sparkUser`.

```
scala> sc.sparkUser
res8: String = jacek
```

CAUTION: FIXME Where is `sparkUser` useful?

It checks whether a master URL under `spark.master` and an application name under `spark.app.name` are defined and throws SparkException if not.

When `spark.logConf` is `true` (default: `false`) link:spark-configuration.adoc[SparkConf.toDebugString] is called.

CAUTION: `SparkConf.toDebugString` is called a bit too early as there are other settings configured afterwards and they are not in the printout.

It sets Spark driver host (`spark.driver.host` to localhost) and port (`spark.driver.port` to `0`) system properties unless already defined.

It sets `spark.executor.id` as `driver`.

It sets the jars and files based on `spark.jars` and `spark.files`, respectively.

If `spark.eventLog.enabled` was `true` (default: `false`), the internal field `_eventLogDir` is set to the value of `spark.eventLog.dir` property or simply `/tmp/spark-events`. Also, if `spark.eventLog.compress` is `true` (default: `false`), the short name of the CompressionCodec is assigned to _eventLogCodec. The config key is `spark.io.compression.codec` (default: `snappy`). The supported codecs are: `lz4`, `lzf`, and `snappy` or their short class names.

It sets `spark.externalBlockStore.folderName` to the value of `externalBlockStoreFolderName`.

CAUTION: FIXME: What's `externalBlockStoreFolderName`?

For `yarn-client` master URL, the system property `SPARK_YARN_MODE` is set to `true`.

An instance of `JobProgressListener` is created and becomes a listener in `listenerBus`.

`createSparkEnv` is called to create the Spark execution environment.

CAUTION: FIXME Step through the code

`MetadataCleaner` is created.

CAUTION: FIXME What's MetadataCleaner?

`SparkStatusTracker` is created.

CAUTION: FIXME What's SparkStatusTracker?

`_progressBar` is initialized to `ConsoleProgressBar` if the value of the boolean property `spark.ui.showConsoleProgress` (default: `true`) is `true` and SparkContext's INFO log level is disabled.

CAUTION: FIXME What's ConsoleProgressBar and non-INFO SparkContext. Where are the *visible* results? Where is `_progressBar` used?

`SparkUI.createLiveUI` gets called to set `_ui` if the property `spark.ui.enabled` (default: `true`) is `true`.

CAUTION: FIXME Step through SparkUI.createLiveUI. Where's `_ui` used?

`_hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf)` - FIXME What's that?

If there are jars given through the SparkContext constructor, they are added using `addJar`. Same for files using `addFile`.

`_executorMemory` is set to `spark.executor.memory` setting, or `SPARK_EXECUTOR_MEMORY` env var, or  (deprecated) `SPARK_MEM` env var, or eventually `1024` (FIXME: What's the unit of the value exactly?)

The value of `SPARK_PREPEND_CLASSES` environment variable is included in `executorEnvs`.

[CAUTION]
====
FIXME

* What's `_executorMemory`?
* What are "SPARK_TESTING", "spark.testing"? How do they contribute to `executorEnvs`?
* What's `executorEnvs`?
====

The Mesos scheduler backend's configuration is included in `executorEnvs`, i.e. `SPARK_EXECUTOR_MEMORY`, `_conf.getExecutorEnv`, and `SPARK_USER`.

*HeartbeatReceiver* RPC Endpoint is created using `HeartbeatReceiver`.

CAUTION: FIXME What's `_heartbeatReceiver`? Why does Spark need it?

<<createTaskScheduler, SparkContext.createTaskScheduler>> is executed (using the master URL).

CAUTION: FIXME Why is `_heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)` important?

Task Scheduler is started.

One SparkContext corresponds to one application.

The internal fields, `_applicationId` and `_applicationAttemptId`, are set. Application and attempt ids are specific to the implementation of link:spark-taskscheduler.adoc[Task Scheduler].

The setting *spark.app.id* is set to `_applicationId` and Web UI gets notified about the new value (using `setAppId(_applicationId)`). And also Block Manager (using `.initialize(_applicationId)`).

CAUTION: FIXME Why should UI and Block Manager know about the application id?

link:spark-metrics.adoc[Metric System] is started (after the application id is set using `spark.app.id`).

CAUTION: FIXME Why does Metric System need the application id?

The driver's metrics (servlet handler) are attached to the web ui after the metrics system is started.

`_eventLogger` is created and started if `isEventLogEnabled`. It uses `EventLoggingListener` that gets registered to `listenerBus`.

CAUTION: FIXME Why is `_eventLogger` required to be the internal field of SparkContext? Where is this used?

If link:spark-dynamic-allocation.adoc[dynamic allocation] is enabled, `_executorAllocationManager` is set to `ExecutorAllocationManager` and started.

`_cleaner` is set to link:spark-service-contextcleaner.adoc[ContextCleaner] if `spark.cleaner.referenceTracking` is `true` (default: `true`).

CAUTION: FIXME It'd be quite useful to have all the properties with their default values in `sc.getConf.toDebugString`, so when a configuration is not included but changes Spark runtime configuration, let's add it to `_conf`.

`setupAndStartListenerBus` link:spark-scheduler-listeners.adoc[registers user-defined listeners] and starts the listener bus that starts event delivery to the listeners.

The following methods are called (in the order given):

* postEnvironmentUpdate
* postApplicationStart
* _taskScheduler.postStartHook()
* _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))
* _env.metricsSystem.registerSource(e.executorAllocationManagerSource) if `_executorAllocationManager` set.
* _shutdownHookRef = ShutdownHookManager.addShutdownHook(

CAUTION: FIXME What are the above methods doing?

Any non-fatal Exception leads to termination of the Spark context instance.

CAUTION: FIXME What does `NonFatal` represent in Scala?
