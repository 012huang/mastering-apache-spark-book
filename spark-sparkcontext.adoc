== SparkContext - the door to Spark

*SparkContext* (aka *Spark context*) represents the connection to a link:spark-deployment-modes.adoc[Spark execution environment (deployment mode)].

You have to create a Spark context before using Spark features and services in your application. A Spark context can be used to <<creating-rdds, create RDDs>>, <<creating-accumulators, accumulators>> and <<creating-broadcasts, broadcast variables>>, access Spark services and <<running-jobs, run jobs>>.

A Spark context is essentially a client of Spark's execution environment and acts as the _master of your Spark application_ (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though).

.Spark context acts as the master of your Spark application
image::diagrams/sparkcontext-services.png[align="center"]

As a developer, you can use `SparkContext` for the following functions:

* link:spark-cluster.adoc[Connecting to a cluster]
* link:spark-configuration.adoc[Specifing mandatory application name]
* <<creating-rdds, Creating RDDs>>
* <<creating-accumulators, Creating accumulators>>
* <<creating-broadcasts, Creating broadcast variables>>
* Accessing services, e.g. link:spark-taskscheduler.adoc[Task Scheduler], link:spark-scheduler-listeners.adoc[Listener Bus], link:spark-blockmanager.adoc[Block Manager], <<coarse-grained-scheduler-backends, CoarseGrainedSchedulerBackends>>, link:spark-shuffle-service.adoc[Shuffle Service].
* <<running-jobs, Running jobs>>
* <<custom-schedulers, Setting up custom Scheduler Backend, Task Scheduler and DAGScheduler>>

Read the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext].

=== [[creating-sparkcontext]] Creating SparkContext

You create a `SparkContext` instance using a link:spark-configuration.adoc[SparkConf] object.

[source, scala]
----
scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> val conf = new SparkConf().setMaster("local[*]").setAppName("SparkMe App")
conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7a8f69d6

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> val sc = new SparkContext(conf)  // <1>
sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@50ee2523
----
<1> You can also use the other constructor of `SparkContext`, i.e. `new SparkContext(master="local[*]", appName="SparkMe App", new SparkConf)`, with master and application name specified explicitly

When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from services):

```
INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
```

Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores).

==== [[allowMultipleContexts]] spark.driver.allowMultipleContexts

Quoting the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

The above quote is not necessarily correct when `spark.driver.allowMultipleContexts` is `true` (default: `false`). If `true`, Spark logs warnings instead of throwing exceptions when multiple SparkContexts are active, i.e. multiple SparkContext are running in this JVM. When creating an instance of `SparkContext`, Spark marks the current thread as having it being created (very early in the instantiation process).

CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress.

=== [[sparkcontext-and-rdd]] SparkContext and RDDs

You use a Spark context to create RDDs (see <<creating-rdds, Creating RDD>>).

When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts.

.A Spark context creates a living space for RDDs.
image::diagrams/sparkcontext-rdds.png[align="center"]

=== SparkContext in Spark shell

In link:spark-shell.adoc[Spark shell], an instance of `SparkContext` is automatically created for you under the name `sc`.

Read link:spark-shell.adoc[Spark shell].

=== [[creating-rdds]] Creating RDD

`SparkContext` allows you to create many different RDDs from input sources like:

* Scala's collections, i.e. `sc.parallelize(0 to 100)`
* local or remote filesystems, i.e. `sc.textFile("README.md")`
* Any Hadoop `InputSource` using `sc.newAPIHadoopFile`

Read link:spark-rdd.adoc#creating-rdds[Creating RDDs] in link:spark-rdd.adoc[RDD - Resilient Distributed Dataset].

=== [[creating-accumulators]] Creating accumulators

CAUTION: FIXME

Read link:spark-accumulators.adoc[Use accumulators].

=== [[creating-broadcasts]] Creating broadcast variables

CAUTION: FIXME

Read link:spark-broadcast.adoc[Create broadcast variables].

=== [[jars]] Distribute JARs to workers

The jar you specify with `SparkContext.addJar` will be copied to all the worker nodes.

The configuration setting `spark.jars` is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or `local:/path` for a file on every worker node.

```
scala> sc.addJar("build.sbt")
15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457
```

CAUTION: FIXME Why is HttpFileServer used for addJar?

=== SparkContext as the global configuration for services

SparkContext keeps track of:

* shuffle ids using `nextShuffleId` internal field for link:spark-dagscheduler.adoc#ShuffleMapStage[registering shuffle dependencies] to link:spark-shuffle-service.adoc[Shuffle Service].

=== [[running-jobs]] Running Jobs

All link:spark-rdd.adoc#actions[RDD actions] in Spark launch link:spark-dagscheduler.adoc#jobs[jobs] (that are run on one or many partitions of the RDD) using `SparkContext.runJob(rdd: RDD[T], func: Iterator[T] => U): Array[U]`.

TIP: For some actions like `first()` and `lookup()`, there is no need to compute all the partitions of the RDD in a job. And Spark knows it.

[source,scala]
----
scala> import org.apache.spark.TaskContext
import org.apache.spark.TaskContext

scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1>
res0: Array[Int] = Array(1, 1)  // <2>
----
<1> Run a job using `runJob` on `lines` RDD with a function that returns 1 for every partition (of `lines` RDD).
<2> What can you say about the number of partitions of the `lines` RDD? Is your result `res0` different than mine? Why?

Running a job is essentially executing a `func` function on all or a subset of partitions in an `rdd` RDD and returning the result as an array (with elements being the results per partition).

`SparkContext.runJob` prints out the following INFO message:

```
INFO Starting job: ...
```

And it follows up on link:spark-rdd.adoc#spark.logLineage[spark.logLineage] and then hands over the execution to link:spark-dagscheduler.adoc#runJob[DAGScheduler.runJob()].

.Executing action
image::images/spark-runjob.png[align="center"]

Before the method finishes, it does link:spark-rdd-checkpointing.adoc[checkpointing] and posts `JobSubmitted` event (see <<event-loop,Event loop>>).

[CAUTION]
====
Spark can only run jobs when a Spark context is available and active, i.e. started. See <<stopping-spark-context, Stopping Spark context>>.

Since SparkContext runs inside a Spark driver, i.e. a Spark application, it must be alive to run jobs.
====

=== [[stopping-spark-context]] Stopping Spark Context

You can stop a Spark context using `SparkContext.stop` method. Stopping a Spark context stops a Spark application.

An attempt to use a Spark context after it was stopped will result in `java.lang.IllegalStateException: SparkContext has been shutdown`.

[source, scala]
----
scala> sc.stop

scala> sc.parallelize(0 to 5)
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
----

=== [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler

By default, SparkContext uses (`private[spark]` class) `org.apache.spark.scheduler.DAGScheduler`, but you can develop your own custom DAGScheduler implementation, and use (`private[spark]`) `SparkContext.dagScheduler_=(ds: DAGScheduler)` method to assign yours.

It is also applicable to `SchedulerBackend` and `TaskScheduler` using `schedulerBackend_=(sb: SchedulerBackend)` and `taskScheduler_=(ts: TaskScheduler)` methods, respectively.

CAUTION: FIXME Make it an advanced exercise.

=== [[createTaskScheduler]] Creating Scheduler Backend and Task Scheduler

`SparkContext.createTaskScheduler` is executed as part of <<initialization, SparkContext's initialization>> to create link:spark-taskscheduler.adoc[Task Scheduler] and link:spark-schedulerbackends.adoc[Scheduler Backend].

It uses the link:spark-deployment-modes.adoc#master-urls[given master URL] and information gathered so far.

.SparkContext creates Task Scheduler and Scheduler Backend
image::diagrams/sparkcontext-createtaskscheduler.png[align="center"]

=== [[events]] Events

When a Spark context starts, it triggers `SparkListenerEnvironmentUpdate` and `SparkListenerApplicationStart` events.

Refer to the section <<initialization, SparkContext's initialization>>.

=== [[persistentRdds]] Persisted RDDs

FIXME When is the internal field `persistentRdds` used?

=== [[setting-default-log-level]] Setting Default Log Level Programatically

You can use `SparkContext.setLogLevel(logLevel: String)` to adjust logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell].

[TIP]
====
`sc.setLogLevel("INFO")` becomes `org.apache.log4j.Level.toLevel(logLevel)` and `org.apache.log4j.Logger.getRootLogger().setLevel(l)` internally.

See https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L367-L378[org/apache/spark/SparkContext.scala].
====

=== [[SparkStatusTracker]] SparkStatusTracker

`SparkStatusTracker` requires a Spark context to work. It is created as part of <<initialization, SparkContext's initialization>>.

SparkStatusTracker is only used by <<ConsoleProgressBar, ConsoleProgressBar>>.

=== [[ConsoleProgressBar]] ConsoleProgressBar

`ConsoleProgressBar` shows the progress of active stages in console (to `stderr`). It polls the status of stages from <<SparkStatusTracker, SparkStatusTracker>> periodically and prints out active stages with more than one task. It keeps overwriting itself to hold in one line for at most 3 first concurrent stages at a time.

```
[Stage 0:====>          (316 + 4) / 1000][Stage 1:>                (0 + 0) / 1000][Stage 2:>                (0 + 0) / 1000]]]
```

The progress includes the stage's id, the number of completed, active, and total tasks.

It is useful when you `ssh` to workers and want to see the progress of active stages.

It is only instantiated if the value of the boolean property `spark.ui.showConsoleProgress` (default: `true`) is `true` and the log level of `org.apache.spark.SparkContext` logger is `WARN` or higher (refer to link:spark-logging.adoc[Logging]).

[source, scala]
----
import org.apache.log4j._
Logger.getLogger("org.apache.spark.SparkContext").setLevel(Level.WARN)
----

To print the progress nicely ConsoleProgressBar uses `COLUMNS` environment variable to know the width of the terminal. It assumes `80` columns.

The progress bar prints out the status after a stage has ran at least `500ms`, every `200ms` (the values are not configurable).

See the progress bar in Spark shell with the following:

[source]
----
$ ./bin/spark-shell --conf spark.ui.showConsoleProgress=true  # <1>

scala> sc.setLogLevel("OFF")  // <2>

scala> Logger.getLogger("org.apache.spark.SparkContext").setLevel(Level.WARN)  // <3>

scala> sc.parallelize(1 to 4, 4).map { n => Thread.sleep(500 + 200 * n); n }.count  // <4>
[Stage 2:>                                                          (0 + 4) / 4]
[Stage 2:==============>                                            (1 + 3) / 4]
[Stage 2:=============================>                             (2 + 2) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
----
<1> Make sure `spark.ui.showConsoleProgress` is `true`. It is by default.
<2> Disable (`OFF`) the root logger (that includes Spark's logger)
<3> Make sure `org.apache.spark.SparkContext` logger is at least `WARN`.
<4> Run a job with 4 tasks with 500ms initial sleep and 200ms sleep chunks to see the progress bar.

https://youtu.be/uEmcGo8rwek[Watch the short video] that show ConsoleProgressBar in action.

You may want to use the following example to see the progress bar in full glory - all 3 concurrent stages in console (borrowed from https://github.com/apache/spark/pull/3029#issuecomment-63244719[a comment to [SPARK-4017\] show progress bar in console #3029]):

```
> ./bin/spark-shell --conf spark.scheduler.mode=FAIR
scala> val a = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _)
scala> val b = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _)
scala> a.union(b).count()
```

=== [[initialization]] SparkContext's initialization

Let's walk through a typical initialization code of SparkContext in a Spark application and see what happens under the covers.

[source, scala]
----
import org.apache.spark.{SparkConf, SparkContext}

// 1. Create Spark configuration
val conf = new SparkConf()
  .setAppName("SparkMe Application")
  .setMaster("local[*]")

// 2. Create Spark context
val sc = new SparkContext(conf)
----

NOTE: The example uses Spark in link:spark-local.adoc[local mode], i.e. `setMaster("local[*]")`, but the initialization with link:spark-cluster.adoc[the other cluster modes] would follow similar steps.

It all starts with checking <<allowMultipleContexts, whether SparkContexts can be shared or not using `spark.driver.allowMultipleContexts`>>.

The very first information printed out is the version of Spark as an INFO message:

```
INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
```

An instance of link:spark-scheduler-listeners.adoc#SparkListenerBus[Listener Bus] is created (but not started yet).

The current user name is computed, i.e. read from a value of `SPARK_USER` environment variable or the currently logged-in user. It is available as later on as `sparkUser`.

```
scala> sc.sparkUser
res0: String = jacek
```

CAUTION: FIXME Where is `sparkUser` useful?

The initialization then checks whether a master URL as `spark.master` and an application name as `spark.app.name` are defined. SparkException is thrown if not.

When `spark.logConf` is `true` (default: `false`) link:spark-configuration.adoc[SparkConf.toDebugString] is called.

NOTE: `SparkConf.toDebugString` is called very early in the initialization process and other settings configured afterwards are not included. Use `sc.getConf.toDebugString` once SparkContext is initialized.

The Spark driver host (`spark.driver.host` to localhost) and port (`spark.driver.port` to `0`) system properties are set unless they are already defined.

`spark.executor.id` is set as `driver`.

TIP: Use `sc.getConf.get("spark.executor.id")` to know where the code is executed - link:spark-runtime-environment.adoc[driver or executors].

It sets the jars and files based on `spark.jars` and `spark.files`, respectively. These are files that are required for proper task execution on executors.

If `spark.eventLog.enabled` was `true` (default: `false`), the internal field `_eventLogDir` is set to the value of `spark.eventLog.dir` property or simply `/tmp/spark-events`. Also, if `spark.eventLog.compress` is `true` (default: `false`), the short name of the CompressionCodec is assigned to _eventLogCodec. The config key is `spark.io.compression.codec` (default: `snappy`). The supported codecs are: `lz4`, `lzf`, and `snappy` or their short class names.

It sets `spark.externalBlockStore.folderName` to the value of `externalBlockStoreFolderName`.

CAUTION: FIXME: What's `externalBlockStoreFolderName`?

For `yarn-client` master URL, the system property `SPARK_YARN_MODE` is set to `true`.

An instance of link:spark-webui.adoc#JobProgressListener[JobProgressListener] is created and registered to link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus].

A link:spark-runtime-environment.adoc#SparkEnv[Spark execution environment] (`SparkEnv`) is created (using `createSparkEnv` that in turn calls link:spark-runtime-environment.adoc#createDriverEnv[SparkEnv.createDriverEnv]).

`MetadataCleaner` is created.

CAUTION: FIXME What's MetadataCleaner?

Optional <<ConsoleProgressBar, ConsoleProgressBar>> with <<SparkStatusTracker, SparkStatusTracker>> are created.

`SparkUI.createLiveUI` gets called to set `_ui` if the property `spark.ui.enabled` (default: `true`) is `true`.

CAUTION: FIXME Step through `SparkUI.createLiveUI`. Where's `_ui` used?

`_hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf)` - FIXME What's that?

If there are jars given through the SparkContext constructor, they are added using `addJar`. Same for files using `addFile`.

The amount of memory to allocate to each executor in Mb (as `_executorMemory`) is calculated. It is the value of `spark.executor.memory` setting, or `SPARK_EXECUTOR_MEMORY` environment variable, or (now deprecated) `SPARK_MEM` environment variable, or eventually `1024`.

`_executorMemory` is later available as `sc.executorMemory` and used for LOCAL_CLUSTER_REGEX, link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set `executorEnvs("SPARK_EXECUTOR_MEMORY")`, MesosSchedulerBackend, CoarseMesosSchedulerBackend.

The value of `SPARK_PREPEND_CLASSES` environment variable is included in `executorEnvs`.

[CAUTION]
====
FIXME

* What's `_executorMemory`?
* What's the unit of the value of `_executorMemory` exactly?
* What are "SPARK_TESTING", "spark.testing"? How do they contribute to `executorEnvs`?
* What's `executorEnvs`?
====

The Mesos scheduler backend's configuration is included in `executorEnvs`, i.e. `SPARK_EXECUTOR_MEMORY`, `_conf.getExecutorEnv`, and `SPARK_USER`.

*HeartbeatReceiver* RPC Endpoint is created using `HeartbeatReceiver`.

CAUTION: FIXME What's `_heartbeatReceiver`? Why does Spark need it?

<<createTaskScheduler, SparkContext.createTaskScheduler>> is executed (using the master URL).

CAUTION: FIXME Why is `_heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)` important?

link:spark-taskscheduler.adoc[Task Scheduler] is started.

The internal fields, `_applicationId` and `_applicationAttemptId`, are set. Application and attempt ids are specific to the implementation of link:spark-taskscheduler.adoc[Task Scheduler].

The setting `spark.app.id` is set to `_applicationId` and Web UI gets notified about the new value (using `setAppId(_applicationId)`). And also Block Manager (using `initialize(_applicationId)`).

```
scala> sc.getConf.get("spark.app.id")
res1: String = local-1447834845413
```

CAUTION: FIXME Why should UI and Block Manager know about the application id?

link:spark-metrics.adoc[Metric System] is started (after the application id is set using `spark.app.id`).

CAUTION: FIXME Why does Metric System need the application id?

The driver's metrics (servlet handler) are attached to the web ui after the metrics system is started.

`_eventLogger` is created and started if `isEventLogEnabled`. It uses `EventLoggingListener` that gets registered to `listenerBus`.

CAUTION: FIXME Why is `_eventLogger` required to be the internal field of SparkContext? Where is this used?

If link:spark-dynamic-allocation.adoc[dynamic allocation] is enabled, `_executorAllocationManager` is set to `ExecutorAllocationManager` and started.

`_cleaner` is set to link:spark-service-contextcleaner.adoc[ContextCleaner] if `spark.cleaner.referenceTracking` is `true` (default: `true`).

CAUTION: FIXME It'd be quite useful to have all the properties with their default values in `sc.getConf.toDebugString`, so when a configuration is not included but does change Spark runtime configuration, it should be added to `_conf`.

`setupAndStartListenerBus` link:spark-scheduler-listeners.adoc[registers user-defined listeners] and starts link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus] that starts event delivery to the listeners.

`postEnvironmentUpdate` is called to post `SparkListenerEnvironmentUpdate` event over link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in link:spark-webui.adoc#environment-tab[Web UI's Environment tab].

`postApplicationStart` is called to post `SparkListenerApplicationStart` event over link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus].

`_taskScheduler.postStartHook()` is called to wait until link:spark-runtime-environment.adoc#scheduler-backends[Scheduler Backend] is ready.

CAUTION: FIXME When is a Scheduler Backend assigned to Task Scheduler and when is it ready?

Two new metrics sources are registered (via `_env.metricsSystem`):

* link:spark-blockmanager.adoc#metrics[BlockManagerSource]
* link:spark-dynamic-allocation.adoc#metrics[ExecutorAllocationManagerSource] (only when `_executorAllocationManager` is set)

`ShutdownHookManager.addShutdownHook()` is called to do SparkContext's cleanup.

CAUTION: FIXME What exactly does `ShutdownHookManager.addShutdownHook()` do?

Any non-fatal Exception leads to termination of the Spark context instance.

CAUTION: FIXME What does `NonFatal` represent in Scala?

`nextShuffleId` and `nextRddId` start with `0`.

CAUTION: FIXME Where are `nextShuffleId` and `nextRddId` used?

A new instance of Spark context is created and ready for operation.
