== SparkContext - the door to Spark

CAUTION: FIXME Review the code of `SparkContext`

From the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> *SparkContext* is the main entry point for Spark functionality.
> A SparkContext represents the connection to a Spark cluster,
> and can be used to create RDDs, accumulators and broadcast variables on that cluster.

And reading along...

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

The above quote is not necessarily correct when `spark.driver.allowMultipleContexts` is `true` (default: `false`). If `true`, Spark logs warnings instead of throwing exceptions when multiple SparkContexts are active, i.e. no other SparkContext is running in this JVM. Upon creating an instance of `SparkContext`, Spark marks the current thread as having it being created (very early in the instantiation process).

You create a SparkContext instance using link:spark-app-configuration.adoc[Spark application's configuration] using a `SparkConfig` object.

```
scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> val conf = new SparkConf().setMaster("local[*]").setAppName("Hello world!")
conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7a8f69d6

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> val sc = new SparkContext(conf)
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/10/03 14:18:39 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
15/10/03 14:18:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/10/03 14:18:40 INFO SecurityManager: Changing view acls to: jacek
15/10/03 14:18:40 INFO SecurityManager: Changing modify acls to: jacek
15/10/03 14:18:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacek); users with modify permissions: Set(jacek)
15/10/03 14:18:40 INFO Utils: Successfully started service 'NettyRpcEnv' on port 0.
15/10/03 14:18:40 INFO Slf4jLogger: Slf4jLogger started
15/10/03 14:18:40 INFO Remoting: Starting remoting
15/10/03 14:18:40 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.49:50807]
15/10/03 14:18:40 INFO Utils: Successfully started service 'sparkDriver' on port 50807.
15/10/03 14:18:40 INFO SparkEnv: Registering MapOutputTracker
15/10/03 14:18:40 INFO SparkEnv: Registering BlockManagerMaster
15/10/03 14:18:40 INFO DiskBlockManager: Created local directory at /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/blockmgr-599169dd-38c3-41ce-acfd-d8325872c230
15/10/03 14:18:40 INFO MemoryStore: MemoryStore started with capacity 1603.8 MB
15/10/03 14:18:40 INFO HttpFileServer: HTTP File server directory is /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-d980bce1-8a80-4ee6-a4ce-a2ace38a68e9/httpd-811f1b57-1fe3-48e7-8d6c-6240cc1633b3
15/10/03 14:18:40 INFO HttpServer: Starting HTTP Server
15/10/03 14:18:41 INFO Utils: Successfully started service 'HTTP file server' on port 50808.
15/10/03 14:18:41 INFO SparkEnv: Registering OutputCommitCoordinator
15/10/03 14:18:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/10/03 14:18:41 INFO SparkUI: Started SparkUI at http://192.168.0.49:4040
15/10/03 14:18:41 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
15/10/03 14:18:41 INFO Executor: Starting executor ID driver on host localhost
15/10/03 14:18:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50809.
15/10/03 14:18:41 INFO NettyBlockTransferService: Server created on 50809
15/10/03 14:18:41 INFO BlockManagerMaster: Trying to register BlockManager
15/10/03 14:18:41 INFO BlockManagerMasterEndpoint: Registering block manager localhost:50809 with 1603.8 MB RAM, BlockManagerId(driver, localhost, 50809)
15/10/03 14:18:41 INFO BlockManagerMaster: Registered BlockManager
sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@50ee2523
```

In link:spark-shell.adoc[Spark shell], an instance of `SparkContext` is created by default and is available as `sc`. The name is displayed in the logs upon `spark-shell` startup.

```
$ ./bin/spark-shell
Spark context available as sc.
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

As an easy way to get started you can use `sc.textFile` to read the local `README.md` file and then `map` it over to have an RDD of sequences of words:

```
scala> val wordsPerLine = sc.textFile("README.md").map(_.split(" ")).cache()
wordsPerLine: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[5] at map at <console>:24
```

You `cache()` it so the computation is not performed every time you work with `wordsPerLine`.

=== Others

The jar you specify with `SparkContext.addJar` will be copied to all the worker nodes.

From http://deploymentzone.com/2015/01/30/spark-and-merged-csv-files/[SPARK AND MERGED CSV FILES]:

> Spark is like Hadoop - uses Hadoop, in fact - for performing actions like outputting data to HDFS. You'll know what I mean the first time you try to save "all-the-data.csv" and are surprised to find a directory named all-the-data.csv/ containing a 0 byte _SUCCESS file and then several part-0000n files for each partition that took part in the job.

=== Gotchas - things to watch for

Even you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors.

See https://issues.apache.org/jira/browse/SPARK-5063
