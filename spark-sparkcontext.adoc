== SparkContext - the door to Spark

From the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> *SparkContext* is the main entry point for Spark functionality.
> A SparkContext represents the connection to a Spark cluster,
> and can be used to create RDDs, accumulators and broadcast variables on that cluster.

And reading along...

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

You create a SparkContext instance using link:spark-app-configuration.adoc[Spark application's configuration] using a SparkConfig object.

From http://deploymentzone.com/2015/01/30/spark-and-merged-csv-files/[SPARK AND MERGED CSV FILES]:

> Spark is like Hadoop - uses Hadoop, in fact - for performing actions like outputting data to HDFS. You'll know what I mean the first time you try to save "all-the-data.csv" and are surprised to find a directory named all-the-data.csv/ containing a 0 byte _SUCCESS file and then several part-0000n files for each partition that took part in the job.

=== Gotchas - things to watch for

Even you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors.

See https://issues.apache.org/jira/browse/SPARK-5063
