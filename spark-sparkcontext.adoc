== SparkContext - the door to Spark

CAUTION: FIXME Review the code of `SparkContext`

From the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> *SparkContext* is the main entry point for Spark functionality.
> A SparkContext represents the connection to a Spark cluster,
> and can be used to create RDDs, accumulators and broadcast variables on that cluster.

As you may have read in the above scaladoc, you use `SparkContext` for the following group of functionalities:

* link:spark-cluster.adoc[Connect to a cluster]
* link:spark-app-configuration.adoc[Specify mandatory application name]
* link:spark-rdd.adoc[Create RDDs]
* link:spark-broadcast.adoc[Create broadcast variables]
* link:spark-accumulators.adoc[Use accumulators]
* Access services, e.g. link:spark-taskscheduler.adoc[Task Scheduler], link:spark-listeners.adoc[Listener Bus], link:spark-block-manager.adoc[Block Manager].

=== How to create SparkContext

You create a `SparkContext` instance using link:spark-app-configuration.adoc[Spark application's configuration] using a `SparkConfig` object.

```
scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> val conf = new SparkConf().setMaster("local[*]").setAppName("Hello world!")
conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7a8f69d6

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> val sc = new SparkContext(conf)
sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@50ee2523
```

=== SparkContext in Spark shell

In link:spark-shell.adoc[Spark shell], an instance of `SparkContext` is automatically created for you under the name `sc`.

You should see the name in the logs while `spark-shell` starts.

```
$ ./bin/spark-shell
Spark context available as sc.
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

`SparkContext` allows you to create many different RDDs from input sources like:

* Scala's collections, i.e. `sc.parallelize(0 to 100)`
* local or remote filesystems, i.e. `sc.textFile("README.md")`
* Any Hadoop `InputSource`, i.e.
+
```
sc.newAPIHadoopFile("filepath1, filepath2", classOf[NewTextInputFormat], classOf[LongWritable], classOf[Text])
```

Refer to link:spark-files.adoc[Using Files] to find out more on the IO API to create RDDs.

You can also use `SparkContext` to create counters and link:spark-accumulators.adoc[accumulators].

Using `SparkContext` allows you to configure Spark context, i.e. properties that govern things like link:spark-cluster.adoc[master URL], link:spark-hadoop.adoc[Hadoop API], etc.

=== Others

I'm unsure where to put them, but since I found them interesting, they should be _somewhere_.

==== spark.driver.allowMultipleContexts

Found in the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

The above quote is not necessarily correct when `spark.driver.allowMultipleContexts` is `true` (default: `false`). If `true`, Spark logs warnings instead of throwing exceptions when multiple SparkContexts are active, i.e. no other SparkContext is running in this JVM. Upon creating an instance of `SparkContext`, Spark marks the current thread as having it being created (very early in the instantiation process).

==== SparkContext.addJar / --jars

The jar you specify with `SparkContext.addJar` will be copied to all the worker nodes.

From http://deploymentzone.com/2015/01/30/spark-and-merged-csv-files/[SPARK AND MERGED CSV FILES]:

> Spark is like Hadoop - uses Hadoop, in fact - for performing actions like outputting data to HDFS. You'll know what I mean the first time you try to save "all-the-data.csv" and are surprised to find a directory named all-the-data.csv/ containing a 0 byte _SUCCESS file and then several part-0000n files for each partition that took part in the job.

==== Gotchas - things to watch for

Even you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors.

See https://issues.apache.org/jira/browse/SPARK-5063
