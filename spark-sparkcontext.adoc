== [[SparkContext]] SparkContext - the door to Spark

*SparkContext* (aka *Spark context*) represents the connection to a link:spark-deployment-environments.adoc[Spark execution environment (deployment mode)].

You have to create a Spark context before using Spark features and services in your application. A Spark context can be used to <<creating-rdds, create RDDs>>, <<creating-accumulators, accumulators>> and <<creating-broadcast-variables, broadcast variables>>, access Spark services and <<running-jobs, run jobs>>.

A Spark context is essentially a client of Spark's execution environment and acts as the _master of your Spark application_ (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though).

.Spark context acts as the master of your Spark application
image::diagrams/sparkcontext-services.png[align="center"]

`SparkContext` offers the following functions:

* <<defaultParallelism, Default Level of Parallelism>>
* <<master-url, Specifying mandatory master URL>>
* <<application-name, Specifying mandatory application name>>
* <<creating-rdds, Creating RDDs>>
* <<creating-accumulators, Creating accumulators>>
* <<creating-broadcast-variables, Creating broadcast variables>>
* Accessing services, e.g. link:spark-taskscheduler.adoc[Task Scheduler], link:spark-scheduler-listeners.adoc[Listener Bus], link:spark-blockmanager.adoc[Block Manager], link:spark-scheduler-backends.adoc[Scheduler Backends], link:spark-shuffle-manager.adoc[Shuffle Manager].
* <<running-jobs, Running jobs>>
* <<custom-schedulers, Setting up custom Scheduler Backend, Task Scheduler and DAGScheduler>>
* <<closure-cleaning, Closure Cleaning>>
* <<submitJob, Submitting Jobs Asynchronously>>
* <<unpersist, Unpersisting RDDs, i.e. marking RDDs as non-persistent>>
* <<setting-local-properties, Setting local properties>>

Read the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext].

=== [[master]][[master-url]] Master URL

CAUTION: FIXME

link:spark-cluster.adoc[Connecting to a cluster]

=== [[appName]][[application-name]] Application Name

CAUTION: FIXME

link:spark-configuration.adoc[Specifying mandatory application name]

=== [[defaultParallelism]] Default Level of Parallelism

*Default level of parallelism* is the number of link:spark-rdd-partitions.adoc[partitions] when not specified explicitly by a user.

It is used for the methods like `SparkContext.parallelize`, `SparkContext.range` and `SparkContext.makeRDD` (as well as link:spark-streaming.adoc[Spark Streaming]'s `DStream.countByValue` and `DStream.countByValueAndWindow` and few other places). It is also used to instantiate link:spark-rdd-partitions.adoc#HashPartitioner[HashPartitioner] or for the minimum number of partitions in link:spark-rdd-hadooprdd.adoc[HadoopRDDs].

SparkContext queries link:spark-taskscheduler.adoc[TaskScheduler] for the default level of parallelism (refer to link:spark-taskscheduler.adoc#contract[TaskScheduler Contract]).

=== [[setting-local-properties]] Setting Local Properties

CAUTION: FIXME

CAUTION: FIXME It'd be nice to have an intro page about local properties. Dunno where it could belong to (?)

=== [[makeRDD]] SparkContext.makeRDD

CAUTION: FIXME

=== [[submitJob]] Submitting Jobs Asynchronously

`SparkContext.submitJob` submits a job in an asynchronous, non-blocking way (using link:spark-dagscheduler.adoc#submitJob[DAGScheduler.submitJob] method).

It cleans the `processPartition` input function argument and returns an instance of link:spark-rdd-actions.adoc#FutureAction[SimpleFutureAction] that holds the link:link:spark-dagscheduler.adoc#JobWaiter[JobWaiter] instance (it has received from `DAGScheduler.submitJob`).

CAUTION: FIXME What are `resultFunc`?

It is used in:

* link:spark-rdd-actions.adoc#AsyncRDDActions[AsyncRDDActions] methods
* link:spark-streaming.adoc[Spark Streaming] for link:spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver]

=== [[spark-configuration]] Spark Configuration

CAUTION: FIXME

=== [[creating-sparkcontext]] Creating SparkContext

You create a `SparkContext` instance using a link:spark-configuration.adoc[SparkConf] object.

[source, scala]
----
scala> import org.apache.spark.SparkConf
import org.apache.spark.SparkConf

scala> val conf = new SparkConf().setMaster("local[*]").setAppName("SparkMe App")
conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7a8f69d6

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> val sc = new SparkContext(conf)  // <1>
sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@50ee2523
----
<1> You can also use the other constructor of `SparkContext`, i.e. `new SparkContext(master="local[*]", appName="SparkMe App", new SparkConf)`, with master and application name specified explicitly

When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from services):

```
INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
```

Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores).

==== [[allowMultipleContexts]] spark.driver.allowMultipleContexts

Quoting the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

The above quote is not necessarily correct when `spark.driver.allowMultipleContexts` is `true` (default: `false`). If `true`, Spark logs warnings instead of throwing exceptions when multiple SparkContexts are active, i.e. multiple SparkContext are running in this JVM. When creating an instance of `SparkContext`, Spark marks the current thread as having it being created (very early in the instantiation process).

CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress.

=== [[sparkcontext-and-rdd]] SparkContext and RDDs

You use a Spark context to create RDDs (see <<creating-rdds, Creating RDD>>).

When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts.

.A Spark context creates a living space for RDDs.
image::diagrams/sparkcontext-rdds.png[align="center"]

=== SparkContext in Spark shell

In link:spark-shell.adoc[Spark shell], an instance of `SparkContext` is automatically created for you under the name `sc`.

Read link:spark-shell.adoc[Spark shell].

=== [[creating-rdds]] Creating RDD

`SparkContext` allows you to create many different RDDs from input sources like:

* Scala's collections, i.e. `sc.parallelize(0 to 100)`
* local or remote filesystems, i.e. `sc.textFile("README.md")`
* Any Hadoop `InputSource` using `sc.newAPIHadoopFile`

Read link:spark-rdd.adoc#creating-rdds[Creating RDDs] in link:spark-rdd.adoc[RDD - Resilient Distributed Dataset].

=== [[unpersist]] Unpersisting RDDs (Marking RDDs as non-persistent)

It removes an RDD from the master's link:spark-blockmanager.adoc[Block Manager] (calls `removeRdd(rddId: Int, blocking: Boolean)`) and the internal <<persistentRdds, persistentRdds>> mapping.

It finally posts link:spark-scheduler-listeners.adoc[an unpersist notification (as SparkListenerUnpersistRDD event)] to `listenerBus`.

=== [[setCheckpointDir]] Setting Checkpoint Directory (setCheckpointDir method)

[source, scala]
----
setCheckpointDir(directory: String)
----

`setCheckpointDir` method is used to set up the checkpoint directory...FIXME

CAUTION: FIXME

=== [[creating-accumulators]][[accumulator]] Creating accumulators

[source, scala]
----
accumulator[T](initialValue: T)(implicit param: AccumulatorParam[T]): Accumulator[T]
accumulator[T](initialValue: T, name: String)(implicit param: AccumulatorParam[T]): Accumulator[T]
----

`accumulator` methods create link:spark-accumulators.adoc[accumulators] of type `T` with the initial value `initialValue`.

[source, scala]
----
scala> val acc = sc.accumulator(0)
acc: org.apache.spark.Accumulator[Int] = 0

scala> val counter = sc.accumulator(0, "counter")
counter: org.apache.spark.Accumulator[Int] = 0

scala> counter.value
res2: Int = 0

scala> sc.parallelize(0 to 9).foreach(n => counter += n)

scala> counter.value
res4: Int = 45
----

`name` input parameter allows you to give a name to accumulators and have them displayed in the Spark UI (under Stages tab for a given stage).

.Accumulators in the Spark UI
image::images/spark-webui-accumulators.png[align="center"]

When working with unsupported type `T` you need to create a custom link:spark-accumulators.adoc#AccumulatorParam[AccumulatorParam[T\]].

TIP: Read link:spark-accumulators.adoc[Accumulators] to get more in-depth insight on the concept.

=== [[creating-broadcast-variables]] Creating Broadcast Variables

TIP: Read link:spark-broadcast.adoc[Broadcast Variables] to get more in-depth insight on the concept.

SparkContext comes with `broadcast` method to broadcast a value among Spark executors.

```
def broadcast[T: ClassTag](value: T): Broadcast[T]
```

It returns a `Broadcast[T]` instance that is a handle to a shared memory on executors.

```
scala> sc.broadcast("hello")
INFO MemoryStore: Ensuring 1048576 bytes of free space for block broadcast_0(free: 535953408, max: 535953408)
INFO MemoryStore: Ensuring 80 bytes of free space for block broadcast_0(free: 535953408, max: 535953408)
INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 80.0 B, free 80.0 B)
INFO MemoryStore: Ensuring 34 bytes of free space for block broadcast_0_piece0(free: 535953328, max: 535953408)
INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 B, free 114.0 B)
INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:61505 (size: 34.0 B, free: 511.1 MB)
INFO SparkContext: Created broadcast 0 from broadcast at <console>:25
res0: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0)
```

Spark transfers the value to Spark executors _once_, and tasks can share it without incurring repetitive network transmissions when requested multiple times.

.Broadcasting a value to executors
image::images/sparkcontext-broadcast-executors.png[align="center"]

When a broadcast value is created the following INFO message appears in the logs:

```
INFO SparkContext: Created broadcast [id] from broadcast at <console>:25
```

You should _not_ broadcast a RDD to use in tasks and Spark will warn you. It will not stop you, though. Consult https://issues.apache.org/jira/browse/SPARK-5063[SPARK-5063 Display more helpful error messages for several invalid operations].

```
scala> val rdd = sc.parallelize(0 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24

scala> sc.broadcast(rdd)
WARN SparkContext: Can not directly broadcast RDDs; instead, call collect() and broadcast the result (see SPARK-5063)
```

=== [[jars]] Distribute JARs to workers

The jar you specify with `SparkContext.addJar` will be copied to all the worker nodes.

The configuration setting `spark.jars` is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or `local:/path` for a file on every worker node.

```
scala> sc.addJar("build.sbt")
15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457
```

CAUTION: FIXME Why is HttpFileServer used for addJar?

=== SparkContext as the global configuration for services

SparkContext keeps track of:

* shuffle ids using `nextShuffleId` internal field for link:spark-dagscheduler.adoc#ShuffleMapStage[registering shuffle dependencies] to link:spark-shuffle-manager.adoc[Shuffle Service].

=== [[running-jobs]] Running Jobs

All link:spark-rdd.adoc#actions[RDD actions] in Spark launch link:spark-dagscheduler-jobs.adoc[jobs] (that are run on one or many partitions of the RDD) using `SparkContext.runJob(rdd: RDD[T], func: Iterator[T] => U): Array[U]`.

TIP: For some actions, e.g. `first()` and `lookup()`, there is no need to compute all the partitions of the RDD in a job. And Spark knows it.

[source,scala]
----
import org.apache.spark.TaskContext

scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1>
res0: Array[Int] = Array(1, 1)  // <2>
----
<1> Run a job using `runJob` on `lines` RDD with a function that returns 1 for every partition (of `lines` RDD).
<2> What can you say about the number of partitions of the `lines` RDD? Is your result `res0` different than mine? Why?

TIP: Read about `TaskContext` in link:spark-taskscheduler-taskcontext.adoc[TaskContext].

Running a job is essentially executing a `func` function on all or a subset of partitions in an `rdd` RDD and returning the result as an array (with elements being the results per partition).

`SparkContext.runJob` prints out the following INFO message:

```
INFO Starting job: ...
```

And it follows up on link:spark-rdd.adoc#spark.logLineage[spark.logLineage] and then hands over the execution to link:spark-dagscheduler.adoc#runJob[DAGScheduler.runJob()].

.Executing action
image::images/spark-runjob.png[align="center"]

Before the method finishes, it does link:spark-rdd-checkpointing.adoc[checkpointing] and posts `JobSubmitted` event (see <<event-loop,Event loop>>).

[CAUTION]
====
Spark can only run jobs when a Spark context is available and active, i.e. started. See <<stopping, Stopping Spark context>>.

Since SparkContext runs inside a Spark driver, i.e. a Spark application, it must be alive to run jobs.
====

=== [[stopping]] Stopping SparkContext

You can stop a Spark context using `SparkContext.stop()` method. Stopping a Spark context stops the link:spark-runtime-environment.adoc[Spark Runtime Environment] and effectively shuts down the entire Spark application (see link:spark-anatomy-spark-application.adoc[Anatomy of Spark Application]).

Calling `stop` many times leads to the following INFO message in the logs:

```
INFO SparkContext: SparkContext already stopped.
```

An attempt to use a stopped SparkContext's services will result in `java.lang.IllegalStateException: SparkContext has been shutdown`.

[source, scala]
----
scala> sc.stop

scala> sc.parallelize(0 to 5)
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
----

When a SparkContext is being stopped, it does the following:

* Posts a application end event `SparkListenerApplicationEnd` to link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus]
* Stops link:spark-webui.adoc[web UI]
* Requests link:spark-metrics.adoc[MetricSystem] to report metrics from all registered sinks (using `MetricsSystem.report()`)
* `metadataCleaner.cancel()`
* Stops link:spark-service-contextcleaner.adoc[ContextCleaner]
* Stops link:spark-service-executor-allocation-manager.adoc[ExecutorAllocationManager]
* Stops link:spark-dagscheduler.adoc[DAGScheduler]
* Stops link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus]
* Stops link:spark-scheduler-listeners-eventlogginglistener.adoc[EventLoggingListener]
* Stops <<HeartbeatReceiver, HeartbeatReceiver>>
* Stops <<ConsoleProgressBar, ConsoleProgressBar>>
* Stops link:spark-runtime-environment.adoc[SparkEnv]

If all went fine you should see the following INFO message in the logs:

```
INFO SparkContext: Successfully stopped SparkContext
```

=== [[HeartbeatReceiver]] HeartbeatReceiver

CAUTION: FIXME

`HeartbeatReceiver` is a link:spark-scheduler-listeners.adoc#SparkListener[SparkListener].

=== [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler

By default, SparkContext uses (`private[spark]` class) `org.apache.spark.scheduler.DAGScheduler`, but you can develop your own custom DAGScheduler implementation, and use (`private[spark]`) `SparkContext.dagScheduler_=(ds: DAGScheduler)` method to assign yours.

It is also applicable to `SchedulerBackend` and `TaskScheduler` using `schedulerBackend_=(sb: SchedulerBackend)` and `taskScheduler_=(ts: TaskScheduler)` methods, respectively.

CAUTION: FIXME Make it an advanced exercise.

=== [[createTaskScheduler]] Creating SchedulerBackend and TaskScheduler

`SparkContext.createTaskScheduler` is executed as part of <<initialization, SparkContext's initialization>> to create link:spark-taskscheduler.adoc[Task Scheduler] and link:spark-scheduler-backends.adoc[Scheduler Backend].

It uses the link:spark-deployment-environments.adoc#master-urls[master URL] to select right implementations.

.SparkContext creates Task Scheduler and Scheduler Backend
image::diagrams/sparkcontext-createtaskscheduler.png[align="center"]

=== [[events]] Events

When a Spark context starts, it triggers `SparkListenerEnvironmentUpdate` and `SparkListenerApplicationStart` events.

Refer to the section <<initialization, SparkContext's initialization>>.

=== [[persistentRdds]] Persisted RDDs

FIXME When is the internal field `persistentRdds` used?

=== [[setting-default-log-level]] Setting Default Log Level Programatically

You can use `SparkContext.setLogLevel(logLevel: String)` to adjust logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell].

[TIP]
====
`sc.setLogLevel("INFO")` becomes `org.apache.log4j.Level.toLevel(logLevel)` and `org.apache.log4j.Logger.getRootLogger().setLevel(l)` internally.

See https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L367-L378[org/apache/spark/SparkContext.scala].
====

=== [[SparkStatusTracker]] SparkStatusTracker

`SparkStatusTracker` requires a Spark context to work. It is created as part of <<initialization, SparkContext's initialization>>.

SparkStatusTracker is only used by <<ConsoleProgressBar, ConsoleProgressBar>>.

=== [[ConsoleProgressBar]] ConsoleProgressBar

`ConsoleProgressBar` shows the progress of active stages in console (to `stderr`). It polls the status of stages from <<SparkStatusTracker, SparkStatusTracker>> periodically and prints out active stages with more than one task. It keeps overwriting itself to hold in one line for at most 3 first concurrent stages at a time.

```
[Stage 0:====>          (316 + 4) / 1000][Stage 1:>                (0 + 0) / 1000][Stage 2:>                (0 + 0) / 1000]]]
```

The progress includes the stage's id, the number of completed, active, and total tasks.

It is useful when you `ssh` to workers and want to see the progress of active stages.

It is only instantiated if the value of the boolean property `spark.ui.showConsoleProgress` (default: `true`) is `true` and the log level of `org.apache.spark.SparkContext` logger is `WARN` or higher (refer to link:spark-logging.adoc[Logging]).

[source, scala]
----
import org.apache.log4j._
Logger.getLogger("org.apache.spark.SparkContext").setLevel(Level.WARN)
----

To print the progress nicely ConsoleProgressBar uses `COLUMNS` environment variable to know the width of the terminal. It assumes `80` columns.

The progress bar prints out the status after a stage has ran at least `500ms`, every `200ms` (the values are not configurable).

See the progress bar in Spark shell with the following:

[source]
----
$ ./bin/spark-shell --conf spark.ui.showConsoleProgress=true  # <1>

scala> sc.setLogLevel("OFF")  // <2>

scala> Logger.getLogger("org.apache.spark.SparkContext").setLevel(Level.WARN)  // <3>

scala> sc.parallelize(1 to 4, 4).map { n => Thread.sleep(500 + 200 * n); n }.count  // <4>
[Stage 2:>                                                          (0 + 4) / 4]
[Stage 2:==============>                                            (1 + 3) / 4]
[Stage 2:=============================>                             (2 + 2) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
----
<1> Make sure `spark.ui.showConsoleProgress` is `true`. It is by default.
<2> Disable (`OFF`) the root logger (that includes Spark's logger)
<3> Make sure `org.apache.spark.SparkContext` logger is at least `WARN`.
<4> Run a job with 4 tasks with 500ms initial sleep and 200ms sleep chunks to see the progress bar.

https://youtu.be/uEmcGo8rwek[Watch the short video] that show ConsoleProgressBar in action.

You may want to use the following example to see the progress bar in full glory - all 3 concurrent stages in console (borrowed from https://github.com/apache/spark/pull/3029#issuecomment-63244719[a comment to [SPARK-4017\] show progress bar in console #3029]):

```
> ./bin/spark-shell --conf spark.scheduler.mode=FAIR
scala> val a = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _)
scala> val b = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _)
scala> a.union(b).count()
```

=== [[closure-cleaning]] Closure Cleaning (clean method)

Every time an action is called, Spark cleans up the closure, i.e. the body of the action, before it is serialized and sent over the wire to executors.

SparkContext comes with `clean(f: F, checkSerializable: Boolean = true)` method that does this. It in turn calls `ClosureCleaner.clean` method.

Not only does `ClosureCleaner.clean` method clean the closure, but also does it transitively, i.e. referenced closures are cleaned transitively.

A closure is considered serializable as long as it does not explicitly reference unserializable objects. It does so by traversing the hierarchy of enclosing closures and null out any references that are not actually used by the starting closure.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.util.ClosureCleaner` logger to see what happens inside the class.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG
```
====

With `DEBUG` logging level you should see the following messages in the logs:

```
+++ Cleaning closure [func] ([func.getClass.getName]) +++
 + declared fields: [declaredFields.size]
     [field]
 ...
+++ closure [func] ([func.getClass.getName]) is now cleaned +++
```

Serialization is verified using a new instance of `Serializer` (as `SparkEnv.get.closureSerializer`). Refer to link:spark-serialization.adoc[Serialization].

CAUTION: FIXME an example, please.

=== [[initialization]] Creating a SparkContext

Let's walk through a typical initialization code of SparkContext in a Spark application and see what happens under the covers.

[source, scala]
----
import org.apache.spark.{SparkConf, SparkContext}

// 1. Create Spark configuration
val conf = new SparkConf()
  .setAppName("SparkMe Application")
  .setMaster("local[*]")

// 2. Create Spark context
val sc = new SparkContext(conf)
----

NOTE: The example uses Spark in link:spark-local.adoc[local mode], i.e. `setMaster("local[*]")`, but the initialization with link:spark-cluster.adoc[the other cluster modes] would follow similar steps.

It all starts with checking <<allowMultipleContexts, whether SparkContexts can be shared or not using `spark.driver.allowMultipleContexts`>>.

The very first information printed out is the version of Spark as an INFO message:

```
INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
```

An instance of link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus] is created (but not started yet).

The current user name is computed, i.e. read from a value of `SPARK_USER` environment variable or the currently logged-in user. It is available as later on as `sparkUser`.

```
scala> sc.sparkUser
res0: String = jacek
```

CAUTION: FIXME Where is `sparkUser` useful?

The initialization then checks whether a master URL as `spark.master` and an application name as `spark.app.name` are defined. SparkException is thrown if not.

When `spark.logConf` is `true` (default: `false`) link:spark-configuration.adoc[SparkConf.toDebugString] is called.

NOTE: `SparkConf.toDebugString` is called very early in the initialization process and other settings configured afterwards are not included. Use `sc.getConf.toDebugString` once SparkContext is initialized.

The Spark driver host (`spark.driver.host` to localhost) and port (`spark.driver.port` to `0`) system properties are set unless they are already defined.

`spark.executor.id` is set as `driver`.

TIP: Use `sc.getConf.get("spark.executor.id")` to know where the code is executed - link:spark-runtime-environment.adoc[driver or executors].

It sets the jars and files based on `spark.jars` and `spark.files`, respectively. These are files that are required for proper task execution on executors.

If `spark.eventLog.enabled` was `true` (default: `false`), the internal field `_eventLogDir` is set to the value of `spark.eventLog.dir` property or simply `/tmp/spark-events`. Also, if `spark.eventLog.compress` is `true` (default: `false`), the short name of the CompressionCodec is assigned to _eventLogCodec. The config key is `spark.io.compression.codec` (default: `snappy`). The supported codecs are: `lz4`, `lzf`, and `snappy` or their short class names.

It sets `spark.externalBlockStore.folderName` to the value of `externalBlockStoreFolderName`.

CAUTION: FIXME: What's `externalBlockStoreFolderName`?

For `yarn-client` master URL, the system property `SPARK_YARN_MODE` is set to `true`.

An instance of link:spark-webui.adoc#JobProgressListener[JobProgressListener] is created and registered to link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus].

A link:spark-runtime-environment.adoc#SparkEnv[Spark execution environment] (`SparkEnv`) is created (using `createSparkEnv` that in turn calls link:spark-runtime-environment.adoc#createDriverEnv[SparkEnv.createDriverEnv]).

`MetadataCleaner` is created.

CAUTION: FIXME What's MetadataCleaner?

Optional <<ConsoleProgressBar, ConsoleProgressBar>> with <<SparkStatusTracker, SparkStatusTracker>> are created.

`SparkUI.createLiveUI` gets called to set `_ui` if the property link:spark-webui.adoc#settings[spark.ui.enabled] is enabled (`true`).

CAUTION: FIXME Step through `SparkUI.createLiveUI`. Where's `_ui` used?

A Hadoop configuration is created. See <<hadoopConfiguration, Hadoop Configuration>>.

If there are jars given through the SparkContext constructor, they are added using `addJar`. Same for files using `addFile`.

At this point in time, the amount of memory to allocate to each executor (as `_executorMemory`) is calculated. It is the value of link:spark-executor.adoc#settings[spark.executor.memory] setting, or <<environment-variables, SPARK_EXECUTOR_MEMORY>> environment variable (or currently-deprecated `SPARK_MEM`), or defaults to `1024`.

`_executorMemory` is later available as `sc.executorMemory` and used for LOCAL_CLUSTER_REGEX, link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set `executorEnvs("SPARK_EXECUTOR_MEMORY")`, MesosSchedulerBackend, CoarseMesosSchedulerBackend.

The value of `SPARK_PREPEND_CLASSES` environment variable is included in `executorEnvs`.

[CAUTION]
====
FIXME

* What's `_executorMemory`?
* What's the unit of the value of `_executorMemory` exactly?
* What are "SPARK_TESTING", "spark.testing"? How do they contribute to `executorEnvs`?
* What's `executorEnvs`?
====

The Mesos scheduler backend's configuration is included in `executorEnvs`, i.e. <<environment-variables, SPARK_EXECUTOR_MEMORY>>, `_conf.getExecutorEnv`, and `SPARK_USER`.

*HeartbeatReceiver* RPC Endpoint is created using `HeartbeatReceiver` (as `_heartbeatReceiver`). Refer to <<HeartbeatReceiver, HeartbeatReceiver>>.

<<createTaskScheduler, SparkContext.createTaskScheduler>> is executed (using the master URL).

CAUTION: FIXME Why is `_heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)` important?

link:spark-taskscheduler.adoc[Task Scheduler] is started.

The internal fields, `_applicationId` and `_applicationAttemptId`, are set. Application and attempt ids are specific to the implementation of link:spark-taskscheduler.adoc[Task Scheduler].

The setting `spark.app.id` is set to `_applicationId` and Web UI gets notified about the new value (using `setAppId(_applicationId)`). And also Block Manager (using `initialize(_applicationId)`).

```
scala> sc.getConf.get("spark.app.id")
res1: String = local-1447834845413
```

CAUTION: FIXME Why should UI and Block Manager know about the application id?

link:spark-metrics.adoc[Metric System] is started (after the application id is set using `spark.app.id`).

CAUTION: FIXME Why does Metric System need the application id?

The driver's metrics (servlet handler) are attached to the web ui after the metrics system is started.

`_eventLogger` is created and started if `isEventLogEnabled`. It uses link:spark-scheduler-listeners-eventlogginglistener.adoc[EventLoggingListener] that gets registered to link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus].

CAUTION: FIXME Why is `_eventLogger` required to be the internal field of SparkContext? Where is this used?

If link:spark-dynamic-allocation.adoc[dynamic allocation] is enabled, `_executorAllocationManager` is set to `ExecutorAllocationManager` and started.

`_cleaner` is set to link:spark-service-contextcleaner.adoc[ContextCleaner] if `spark.cleaner.referenceTracking` is `true` (default: `true`).

CAUTION: FIXME It'd be quite useful to have all the properties with their default values in `sc.getConf.toDebugString`, so when a configuration is not included but does change Spark runtime configuration, it should be added to `_conf`.

`setupAndStartListenerBus` link:spark-scheduler-listeners.adoc[registers user-defined listeners] and starts link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus] that starts event delivery to the listeners.

`postEnvironmentUpdate` is called to post `SparkListenerEnvironmentUpdate` event over link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in link:spark-webui.adoc#environment-tab[Web UI's Environment tab].

`postApplicationStart` is called to post `SparkListenerApplicationStart` event over link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus].

`TaskScheduler.postStartHook()` is called (see link:spark-taskscheduler.adoc#contract[TaskScheduler Contract])

Two new metrics sources are registered (via `_env.metricsSystem`):

* link:spark-blockmanager.adoc#metrics[BlockManagerSource]
* link:spark-dynamic-allocation.adoc#metrics[ExecutorAllocationManagerSource] (only when `_executorAllocationManager` is set)

`ShutdownHookManager.addShutdownHook()` is called to do SparkContext's cleanup.

CAUTION: FIXME What exactly does `ShutdownHookManager.addShutdownHook()` do?

Any non-fatal Exception leads to termination of the Spark context instance.

CAUTION: FIXME What does `NonFatal` represent in Scala?

`nextShuffleId` and `nextRddId` start with `0`.

CAUTION: FIXME Where are `nextShuffleId` and `nextRddId` used?

A new instance of Spark context is created and ready for operation.

=== [[hadoopConfiguration]] Hadoop Configuration

While <<initialization, a SparkContext is created>>, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration] that is available as `_hadoopConfiguration`).

NOTE: link:spark-hadoop.adoc#SparkHadoopUtil[SparkHadoopUtil.get.newConfiguration] is used.

If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default `Configuration` object is returned.

If `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are both available, the following settings are set for the Hadoop configuration:

* `fs.s3.awsAccessKeyId`, `fs.s3n.awsAccessKeyId`, `fs.s3a.access.key` are set to the value of `AWS_ACCESS_KEY_ID`
* `fs.s3.awsSecretAccessKey`, `fs.s3n.awsSecretAccessKey`, and `fs.s3a.secret.key` are set to the value of `AWS_SECRET_ACCESS_KEY`

Every `spark.hadoop.` setting becomes a setting of the configuration with the prefix `spark.hadoop.` removed for the key.

The value of `spark.buffer.size` (default: `65536`) is used as the value of `io.file.buffer.size`.

=== [[environment-variables]] Environment Variables

* `SPARK_EXECUTOR_MEMORY` sets the amount of memory to allocate to each executor. See link:spark-executor.adoc#memory[Executor Memory].
