== [[SparkContext]] SparkContext - Entry Point to Spark

`SparkContext` (aka *Spark context*) is the entry point to Spark. It <<creating-instance-internals, sets up internal services>> and establishes a connection to a link:spark-deployment-environments.adoc[Spark execution environment (deployment mode)].

Once a <<creating-instance, `SparkContext` instance is created>> you can use it to <<creating-rdds, create RDDs>>, <<creating-accumulators, accumulators>> and <<creating-broadcast-variables, broadcast variables>>, access Spark services and <<running-jobs, run jobs>>.

A Spark context is essentially a client of Spark's execution environment and acts as the _master of your Spark application_ (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though).

.Spark context acts as the master of your Spark application
image::diagrams/sparkcontext-services.png[align="center"]

`SparkContext` offers the following functions:

* Getting current configuration
** <<getConf, SparkConf>>
** <<master, deployment environment>>
** <<appName, application name>>
** <<deployMode, deploy mode>>
** <<defaultParallelism, default level of parallelism>>
* Setting configuration
** <<master-url, mandatory master URL>>
** <<setting-local-properties, local properties>>
** <<setting-default-log-level, default log level>>
* Creating objects
** <<creating-rdds, RDDs>>
** <<creating-accumulators, accumulators>>
** <<creating-broadcast-variables, broadcast variables>>
* Accessing services, e.g. link:spark-taskscheduler.adoc[TaskScheduler], link:spark-scheduler-listeners.adoc[ListenerBus], link:spark-blockmanager.adoc[BlockManager], link:spark-scheduler-backends.adoc[SchedulerBackends], link:spark-shuffle-manager.adoc[ShuffleManager].
* <<running-jobs, Running jobs>>
* <<custom-schedulers, Setting up custom Scheduler Backend, TaskScheduler and DAGScheduler>>
* <<closure-cleaning, Closure Cleaning>>
* <<submitJob, Submitting Jobs Asynchronously>>
* <<unpersist, Unpersisting RDDs, i.e. marking RDDs as non-persistent>>

TIP: Read the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext].

=== [[listenerBus]] listenerBus

`listenerBus` is a <<LiveListenerBus, LiveListenerBus>> object that acts as a mechanism to announce events to other services in Spark.

NOTE: It is created while <<creating-instance-internals, SparkContext is being created>> and, since it is a single-JVM event bus, it is exclusively used on the driver.

NOTE: `listenerBus` is a `private[spark]` value in `SparkContext`.

==== [[LiveListenerBus]] LiveListenerBus

`LiveListenerBus` is a single-JVM <<SparkListenerBus, SparkListenerBus>> that uses <<LiveListenerBus-listenerThread, listenerThread to poll events>>. Emitters are supposed to use <<post, `post` method to post `SparkListenerEvent` events>>.

NOTE: The event queue is http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html[java.util.concurrent.LinkedBlockingQueue] with capacity of 10000 `SparkListenerEvent` events.

NOTE: An instance of `LiveListenerBus` is <<listenerBus, created in SparkContext>>.

===== [[LiveListenerBus-post]] Posting SparkListenerEvent Events

[source, scala]
----
post(event: SparkListenerEvent): Unit
----

`post` places the `SparkListenerEvent` event onto the internal `eventQueue` queue.

If the placement was not successful (it could not since it is tapped at 10000 events) <<LiveListenerBus-onDropEvent, onDropEvent>> method is called instead.

If the listener has already been stopped, the following ERROR appears in the logs:

```
ERROR [name] has already stopped! Dropping event [event]
```

===== [[LiveListenerBus-onDropEvent]] Event Dropped Callback (onDropEvent method)

[source, scala]
----
onDropEvent(event: SparkListenerEvent): Unit
----

`onDropEvent` is called when no further events can be added to the internal `eventQueue` queue (while <<LiveListenerBus-post, posting a SparkListenerEvent event>>).

It simply prints out the following ERROR message to the logs and ensures that it happens only once.

```
ERROR Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
```

NOTE: It uses the internal `logDroppedEvent` atomic variable to track the state.

===== [[LiveListenerBus-start]] Starting LiveListenerBus (start method)

[source, scala]
----
start(sc: SparkContext): Unit
----

`start` sets `SparkContext` to `LiveListenerBus` and starts <<LiveListenerBus-listenerThread, listenerThread to process events>>.

If `LiveListenerBus` has already been started, a `IllegalStateException` is thrown:

```
[name] already started!
```

===== [[LiveListenerBus-listenerThread]] listenerThread to Poll Events

`LiveListenerBus` uses `SparkListenerBus` single daemon thread that ensures that the polling events from the event queue is only after <<LiveListenerBus-start, the listener was started>> and only one event at a time.

CAUTION: FIXME There is some logic around no events in the queue.

==== [[SparkListenerBus]] SparkListenerBus

`SparkListenerBus` is a <<ListenerBus, ListenerBus>> that manages <<SparkListenerInterface, SparkListenerInterface>> listeners that process <<SparkListenerEvent, SparkListenerEvent>> events.

It comes with a custom `doPostEvent` method.

[source, scala]
----
doPostEvent(listener: SparkListenerInterface, event: SparkListenerEvent): Unit
----

`doPostEvent` method simply relays `SparkListenerEvent` events to `SparkListenerInterface` methods as follows:

* `SparkListenerStageSubmitted` => `onStageSubmitted`
* `SparkListenerStageCompleted` => `onStageCompleted`
* `SparkListenerJobStart` => `onJobStart`
* `SparkListenerJobEnd` => `onJobEnd`
* `SparkListenerTaskStart` => `onTaskStart`
* `SparkListenerTaskGettingResult` => `onTaskGettingResult`
* `SparkListenerTaskEnd` => `onTaskEnd`
* `SparkListenerEnvironmentUpdate` => `onEnvironmentUpdate`
* `SparkListenerBlockManagerAdded` => `onBlockManagerAdded`
* `SparkListenerBlockManagerRemoved` => `onBlockManagerRemoved`
* `SparkListenerUnpersistRDD` => `onUnpersistRDD`
* `SparkListenerApplicationStart` => `onApplicationStart`
* `SparkListenerApplicationEnd` => `onApplicationEnd`
* `SparkListenerExecutorMetricsUpdate` => `onExecutorMetricsUpdate`
* `SparkListenerExecutorAdded` => `onExecutorAdded`
* `SparkListenerExecutorRemoved` => `onExecutorRemoved`
* `SparkListenerBlockUpdated` => `onBlockUpdated`
* `SparkListenerLogStart` => event ignored
* other event types => `onOtherEvent`

NOTE: There are two custom `SparkListenerBus` listeners: <<LiveListenerBus, LiveListenerBus>> and <<ReplayListenerBus, ReplayListenerBus>>.

==== [[ReplayListenerBus]] ReplayListenerBus

==== [[SparkListenerInterface]] SparkListenerInterface

==== [[SparkListenerEvent]] SparkListenerEvent

==== [[ListenerBus]] ListenerBus

[source, scala]
----
ListenerBus[L <: AnyRef, E]
----

`ListenerBus` is an event bus that post events (of type `E`) to all registered listeners (of type `L`).

It manages `listeners` of type `L`, i.e. it can add to and remove listeners from an internal `listeners` collection.

It can post events of type `E` to all registered listeners (using `postToAll` method). It simply iterates over the internal `listeners` collection and executes the abstract `doPostEvent` method.

[source, scala]
----
doPostEvent(listener: L, event: E): Unit
----

NOTE: `doPostEvent` is provided by more specialized `ListenerBus` event buses.

In case of exception while posting an event to a listener you should see the following ERROR message in the logs and the exception.

```
ERROR Listener [listener] threw an exception
```

NOTE: There are three custom `ListenerBus` listeners: <<SparkListenerBus, SparkListenerBus>>, link:spark-sql-continuousquerymanager.adoc#ContinuousQueryListenerBus[ContinuousQueryListenerBus], and link:spark-streaming-jobscheduler.adoc#StreamingListenerBus[StreamingListenerBus].

[TIP]
====
Enable `ERROR` logging level for `org.apache.spark.util.ListenerBus` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.ListenerBus=ERROR
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating SparkContext

You can create a `SparkContext` instance with or without creating a link:spark-configuration.adoc[SparkConf] object first.

==== [[getOrCreate]] Getting Existing or Creating New SparkContext (getOrCreate methods)

[source, scala]
----
getOrCreate(): SparkContext
getOrCreate(conf: SparkConf): SparkContext
----

`SparkContext.getOrCreate` methods allow you to get the existing `SparkContext` or create a new one.

[source, scala]
----
import org.apache.spark.SparkContext
val sc = SparkContext.getOrCreate()

// Using an explicit SparkConf object
import org.apache.spark.SparkConf
val conf = new SparkConf()
  .setMaster("local[*]")
  .setAppName("SparkMe App")
val sc = SparkContext.getOrCreate(conf)
----

The no-param `getOrCreate` method requires that the two mandatory Spark settings - <<master, master>> and <<appName, application name>> - are specified using link:spark-submit.adoc[spark-submit].

==== [[constructors]] Constructors

[source, scala]
----
SparkContext()
SparkContext(conf: SparkConf)
SparkContext(master: String, appName: String, conf: SparkConf)
SparkContext(
  master: String,
  appName: String,
  sparkHome: String = null,
  jars: Seq[String] = Nil,
  environment: Map[String, String] = Map())
----

You can create a `SparkContext` instance using the four constructors.

[source, scala]
----
import org.apache.spark.SparkConf
val conf = new SparkConf()
  .setMaster("local[*]")
  .setAppName("SparkMe App")

import org.apache.spark.SparkContext
val sc = new SparkContext(conf)
----

When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services):

```
INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT
```

NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores).

=== [[getConf]] Getting Current SparkConf (getConf method)

[source, scala]
----
getConf: SparkConf
----

`getConf` returns the current link:spark-configuration.adoc[SparkConf].

NOTE: Changing the `SparkConf` object does not change the current configuration (as the method returns a copy).

=== [[master]][[master-url]] Getting Deployment Environment (master method)

[source, scala]
----
master: String
----

`master` method returns the current value of link:spark-configuration.adoc#spark.master[spark.master] which is the link:link:spark-deployment-environments.adoc[deployment environment] in use.

=== [[appName]] Getting Application Name (appName method)

[source, scala]
----
appName: String
----

`appName` returns the value of the mandatory link:spark-configuration.adoc#mandatory-settings[spark.app.name] setting.

NOTE: It is used in link:spark-standalone.adoc#SparkDeploySchedulerBackend[SparkDeploySchedulerBackend (to create a ApplicationDescription when it starts)], for link:spark-webui.adoc[SparkUI.createLiveUI] (when link:spark-webui.adoc#spark.ui.enabled[spark.ui.enabled] is enabled), when `postApplicationStart` is executed, and for Mesos and checkpointing in Spark Streaming.

=== [[deployMode]] Getting Deploy Mode (deployMode method)

[source,scala]
----
deployMode: String
----

`deployMode` returns the current value of link:spark-deploy-mode.adoc[spark.submit.deployMode] setting or `client` if not set.

=== [[getPoolForName]] Getting Pool by Name (getPoolForName method)

[source, scala]
----
getPoolForName(pool: String): Option[Schedulable]
----

`getPoolForName` returns a `Schedulable` by its name `pool`. The `Schedulable` may or may not exist.

NOTE: `getPoolForName` is part of the Developer's API.

CAUTION: FIXME Where is `getPoolForName` used?

=== [[getAllPools]] Getting All Pools (getAllPools method)

[source, scala]
----
getAllPools: Seq[Schedulable]
----

`getAllPools` collects the link:spark-taskscheduler-pool.adoc[Pools] in link:spark-taskscheduler.adoc#contract[TaskScheduler.rootPool].

NOTE: `TaskScheduler.rootPool` is part of the link:spark-taskscheduler.adoc#contract[TaskScheduler Contract].

NOTE: `getAllPools` is part of the Developer's API.

CAUTION: FIXME Where is the method used?

=== [[defaultParallelism]] Getting Default Level of Parallelism

*Default level of parallelism* is the number of link:spark-rdd-partitions.adoc[partitions] when not specified explicitly by a user.

It is used for the methods like `SparkContext.parallelize`, `SparkContext.range` and `SparkContext.makeRDD` (as well as link:spark-streaming.adoc[Spark Streaming]'s `DStream.countByValue` and `DStream.countByValueAndWindow` and few other places). It is also used to instantiate link:spark-rdd-partitions.adoc#HashPartitioner[HashPartitioner] or for the minimum number of partitions in link:spark-rdd-hadooprdd.adoc[HadoopRDDs].

SparkContext queries link:spark-taskscheduler.adoc[TaskScheduler] for the default level of parallelism (refer to link:spark-taskscheduler.adoc#contract[TaskScheduler Contract]).

=== [[setting-local-properties]][[setLocalProperty]] Setting Local Properties

[source, scala]
----
setLocalProperty(key: String, value: String): Unit
----

`setLocalProperty` sets a local thread-scoped `key` property to `value`.

[source, scala]
----
sc.setLocalProperty("spark.scheduler.pool", "myPool")
----

The goal of the local property concept is to differentiate between jobs submitted from different threads that may have different local properties set.

NOTE: It is used to link:spark-taskscheduler-FairSchedulableBuilder.adoc#spark.scheduler.pool[group jobs into pools in FAIR job scheduler by spark.scheduler.pool per-thread property].

If `value` is `null` the `key` property is removed the `key` from the local properties

[source, scala]
----
sc.setLocalProperty("spark.scheduler.pool", null)
----

A common use case for the local property concept is to set a local property in a thread, say link:spark-taskscheduler-FairSchedulableBuilder.adoc[spark.scheduler.pool], after which all jobs submitted within the thread will be grouped, say into a pool by FAIR job scheduler.

[source, scala]
----
val rdd = sc.parallelize(0 to 9)

sc.setLocalProperty("spark.scheduler.pool", "myPool")

// these two jobs (one per action) will run in the myPool pool
rdd.count
rdd.collect

sc.setLocalProperty("spark.scheduler.pool", null)

// this job will run in the default pool
rdd.count
----

=== [[makeRDD]] SparkContext.makeRDD

CAUTION: FIXME

=== [[submitJob]] Submitting Jobs Asynchronously

`SparkContext.submitJob` submits a job in an asynchronous, non-blocking way (using link:spark-dagscheduler.adoc#submitJob[DAGScheduler.submitJob] method).

It cleans the `processPartition` input function argument and returns an instance of link:spark-rdd-actions.adoc#FutureAction[SimpleFutureAction] that holds the link:link:spark-dagscheduler.adoc#JobWaiter[JobWaiter] instance (it has received from `DAGScheduler.submitJob`).

CAUTION: FIXME What are `resultFunc`?

It is used in:

* link:spark-rdd-actions.adoc#AsyncRDDActions[AsyncRDDActions] methods
* link:spark-streaming.adoc[Spark Streaming] for link:spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver]

=== [[spark-configuration]] Spark Configuration

CAUTION: FIXME

=== [[allowMultipleContexts]] spark.driver.allowMultipleContexts

Quoting the scaladoc of  http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext]:

> Only one SparkContext may be active per JVM. You must `stop()` the active SparkContext before creating a new one.

The above quote is not necessarily correct when `spark.driver.allowMultipleContexts` is `true` (default: `false`). If `true`, Spark logs warnings instead of throwing exceptions when multiple SparkContexts are active, i.e. multiple SparkContext are running in this JVM. When creating an instance of `SparkContext`, Spark marks the current thread as having it being created (very early in the instantiation process).

CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress.

=== [[sparkcontext-and-rdd]] SparkContext and RDDs

You use a Spark context to create RDDs (see <<creating-rdds, Creating RDD>>).

When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts.

.A Spark context creates a living space for RDDs.
image::diagrams/sparkcontext-rdds.png[align="center"]

=== [[creating-rdds]] Creating RDD

`SparkContext` allows you to create many different RDDs from input sources like:

* Scala's collections, i.e. `sc.parallelize(0 to 100)`
* local or remote filesystems, i.e. `sc.textFile("README.md")`
* Any Hadoop `InputSource` using `sc.newAPIHadoopFile`

Read link:spark-rdd.adoc#creating-rdds[Creating RDDs] in link:spark-rdd.adoc[RDD - Resilient Distributed Dataset].

=== [[unpersist]] Unpersisting RDDs (Marking RDDs as non-persistent)

It removes an RDD from the master's link:spark-blockmanager.adoc[Block Manager] (calls `removeRdd(rddId: Int, blocking: Boolean)`) and the internal <<persistentRdds, persistentRdds>> mapping.

It finally posts link:spark-scheduler-listeners.adoc[an unpersist notification (as SparkListenerUnpersistRDD event)] to `listenerBus`.

=== [[setCheckpointDir]] Setting Checkpoint Directory (setCheckpointDir method)

[source, scala]
----
setCheckpointDir(directory: String)
----

`setCheckpointDir` method is used to set up the checkpoint directory...FIXME

CAUTION: FIXME

=== [[creating-accumulators]][[accumulator]] Creating Accumulators

[source, scala]
----
accumulator[T](initialValue: T)(implicit param: AccumulatorParam[T]): Accumulator[T]
accumulator[T](initialValue: T, name: String)(implicit param: AccumulatorParam[T]): Accumulator[T]
----

`accumulator` methods create link:spark-accumulators.adoc[accumulators] of type `T` with the initial value `initialValue`.

[source, scala]
----
scala> val acc = sc.accumulator(0)
acc: org.apache.spark.Accumulator[Int] = 0

scala> val counter = sc.accumulator(0, "counter")
counter: org.apache.spark.Accumulator[Int] = 0

scala> counter.value
res2: Int = 0

scala> sc.parallelize(0 to 9).foreach(n => counter += n)

scala> counter.value
res4: Int = 45
----

`name` input parameter allows you to give a name to accumulators and have them displayed in the Spark UI (under Stages tab for a given stage).

.Accumulators in the Spark UI
image::images/spark-webui-accumulators.png[align="center"]

When working with unsupported type `T` you need to create a custom link:spark-accumulators.adoc#AccumulatorParam[AccumulatorParam[T\]].

TIP: Read link:spark-accumulators.adoc[Accumulators] to get more in-depth insight on the concept.

=== [[creating-broadcast-variables]] Creating Broadcast Variables

TIP: Read link:spark-broadcast.adoc[Broadcast Variables] to get more in-depth insight on the concept.

SparkContext comes with `broadcast` method to broadcast a value among Spark executors.

```
def broadcast(value: T): Broadcast[T]
```

It returns a `Broadcast[T]` instance that is a handle to a shared memory on executors.

```
scala> val hello = sc.broadcast("hello")
hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0)
```

Spark transfers the value to Spark executors _once_, and tasks can share it without incurring repetitive network transmissions when requested multiple times.

.Broadcasting a value to executors
image::images/sparkcontext-broadcast-executors.png[align="center"]

When a broadcast value is created the following INFO message appears in the logs:

```
INFO SparkContext: Created broadcast [id] from broadcast at <console>:25
```

[NOTE]
====
Spark does not support broadcasting RDDs.

```
scala> sc.broadcast(sc.range(0, 10))
java.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result.
  at scala.Predef$.require(Predef.scala:224)
  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1370)
  ... 48 elided
```
====

=== [[jars]] Distribute JARs to workers

The jar you specify with `SparkContext.addJar` will be copied to all the worker nodes.

The configuration setting `spark.jars` is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or `local:/path` for a file on every worker node.

```
scala> sc.addJar("build.sbt")
15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457
```

CAUTION: FIXME Why is HttpFileServer used for addJar?

=== SparkContext as the global configuration for services

SparkContext keeps track of:

* shuffle ids using `nextShuffleId` internal field for link:spark-dagscheduler.adoc#ShuffleMapStage[registering shuffle dependencies] to link:spark-shuffle-manager.adoc[Shuffle Service].

=== [[running-jobs]] Running Jobs

All link:spark-rdd.adoc#actions[RDD actions] in Spark launch link:spark-dagscheduler-jobs.adoc[jobs] (that are run on one or many partitions of the RDD) using `SparkContext.runJob(rdd: RDD[T], func: Iterator[T] => U): Array[U]`.

TIP: For some actions, e.g. `first()` and `lookup()`, there is no need to compute all the partitions of the RDD in a job. And Spark knows it.

[source,scala]
----
import org.apache.spark.TaskContext

scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1>
res0: Array[Int] = Array(1, 1)  // <2>
----
<1> Run a job using `runJob` on `lines` RDD with a function that returns 1 for every partition (of `lines` RDD).
<2> What can you say about the number of partitions of the `lines` RDD? Is your result `res0` different than mine? Why?

TIP: Read about `TaskContext` in link:spark-taskscheduler-taskcontext.adoc[TaskContext].

Running a job is essentially executing a `func` function on all or a subset of partitions in an `rdd` RDD and returning the result as an array (with elements being the results per partition).

`SparkContext.runJob` prints out the following INFO message:

```
INFO Starting job: ...
```

And it follows up on link:spark-rdd.adoc#spark.logLineage[spark.logLineage] and then hands over the execution to link:spark-dagscheduler.adoc#runJob[DAGScheduler.runJob()].

.Executing action
image::images/spark-runjob.png[align="center"]

Before the method finishes, it does link:spark-rdd-checkpointing.adoc[checkpointing] and posts `JobSubmitted` event (see <<event-loop,Event loop>>).

[CAUTION]
====
Spark can only run jobs when a Spark context is available and active, i.e. started. See <<stopping, Stopping Spark context>>.

Since SparkContext runs inside a Spark driver, i.e. a Spark application, it must be alive to run jobs.
====

=== [[stop]][[stopping]] Stopping SparkContext

You can stop a Spark context using `stop` method. Stopping a Spark context stops the link:spark-sparkenv.adoc[Spark Runtime Environment] and effectively shuts down the entire Spark application (see link:spark-anatomy-spark-application.adoc[Anatomy of Spark Application]).

Calling `stop` many times leads to the following INFO message in the logs:

```
INFO SparkContext: SparkContext already stopped.
```

An attempt to use a stopped SparkContext's services will result in `java.lang.IllegalStateException: SparkContext has been shutdown`.

[source, scala]
----
scala> sc.stop

scala> sc.parallelize(0 to 5)
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
----

When a SparkContext is being stopped, it does the following:

* Posts a application end event `SparkListenerApplicationEnd` to link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus]
* Stops link:spark-webui.adoc[web UI]
* Requests link:spark-metrics.adoc[MetricSystem] to report metrics from all registered sinks (using `MetricsSystem.report()`)
* `metadataCleaner.cancel()`
* Stops link:spark-service-contextcleaner.adoc[ContextCleaner]
* Stops link:spark-service-executor-allocation-manager.adoc[ExecutorAllocationManager]
* Stops link:spark-dagscheduler.adoc[DAGScheduler]
* Stops link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus]
* Stops link:spark-scheduler-listeners-eventlogginglistener.adoc[EventLoggingListener]
* Stops link:spark-sparkcontext-HeartbeatReceiver.adoc[HeartbeatReceiver]
* Stops <<ConsoleProgressBar, ConsoleProgressBar>>
* Stops link:spark-sparkenv.adoc[SparkEnv]

If all went fine you should see the following INFO message in the logs:

```
INFO SparkContext: Successfully stopped SparkContext
```

=== [[addSparkListener]] Registering SparkListener

CAUTION: FIXME

=== [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler

By default, SparkContext uses (`private[spark]` class) `org.apache.spark.scheduler.DAGScheduler`, but you can develop your own custom DAGScheduler implementation, and use (`private[spark]`) `SparkContext.dagScheduler_=(ds: DAGScheduler)` method to assign yours.

It is also applicable to `SchedulerBackend` and `TaskScheduler` using `schedulerBackend_=(sb: SchedulerBackend)` and `taskScheduler_=(ts: TaskScheduler)` methods, respectively.

CAUTION: FIXME Make it an advanced exercise.

=== [[createTaskScheduler]] Creating SchedulerBackend and TaskScheduler

[source, scala]
----
createTaskScheduler(
  sc: SparkContext,
  master: String,
  deployMode: String): (SchedulerBackend, TaskScheduler)
----

The private `createTaskScheduler` is executed as part of <<creating-instance, SparkContext's initialization>> to create link:spark-taskscheduler.adoc[Task Scheduler] and link:spark-scheduler-backends.adoc[Scheduler Backend] objects.

It uses the link:spark-deployment-environments.adoc#master-urls[master URL] to select right implementations.

.SparkContext creates Task Scheduler and Scheduler Backend
image::diagrams/sparkcontext-createtaskscheduler.png[align="center"]

=== [[events]] Events

When a Spark context starts, it triggers `SparkListenerEnvironmentUpdate` and `SparkListenerApplicationStart` events.

Refer to the section <<creating-instance, SparkContext's initialization>>.

=== [[persistentRdds]] Persisted RDDs

FIXME When is the internal field `persistentRdds` used?

=== [[setting-default-log-level]] Setting Default Log Level

[source, scala]
----
setLogLevel(logLevel: String)
----

`setLogLevel` allows you to set the root logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell].

Internally, `setLogLevel` calls `org.apache.log4j.Level.toLevel(logLevel)` and `org.apache.log4j.Logger.getRootLogger().setLevel(l)`.

=== [[SparkStatusTracker]] SparkStatusTracker

`SparkStatusTracker` requires a Spark context to work. It is created as part of <<creating-instance, SparkContext's initialization>>.

SparkStatusTracker is only used by <<ConsoleProgressBar, ConsoleProgressBar>>.

=== [[ConsoleProgressBar]] ConsoleProgressBar

`ConsoleProgressBar` shows the progress of active stages in console (to `stderr`). It polls the status of stages from <<SparkStatusTracker, SparkStatusTracker>> periodically and prints out active stages with more than one task. It keeps overwriting itself to hold in one line for at most 3 first concurrent stages at a time.

```
[Stage 0:====>          (316 + 4) / 1000][Stage 1:>                (0 + 0) / 1000][Stage 2:>                (0 + 0) / 1000]]]
```

The progress includes the stage's id, the number of completed, active, and total tasks.

It is useful when you `ssh` to workers and want to see the progress of active stages.

It is only instantiated if the value of the boolean property `spark.ui.showConsoleProgress` (default: `true`) is `true` and the log level of `org.apache.spark.SparkContext` logger is `WARN` or higher (refer to link:spark-logging.adoc[Logging]).

[source, scala]
----
import org.apache.log4j._
Logger.getLogger("org.apache.spark.SparkContext").setLevel(Level.WARN)
----

To print the progress nicely ConsoleProgressBar uses `COLUMNS` environment variable to know the width of the terminal. It assumes `80` columns.

The progress bar prints out the status after a stage has ran at least `500ms`, every `200ms` (the values are not configurable).

See the progress bar in Spark shell with the following:

[source]
----
$ ./bin/spark-shell --conf spark.ui.showConsoleProgress=true  # <1>

scala> sc.setLogLevel("OFF")  // <2>

scala> Logger.getLogger("org.apache.spark.SparkContext").setLevel(Level.WARN)  // <3>

scala> sc.parallelize(1 to 4, 4).map { n => Thread.sleep(500 + 200 * n); n }.count  // <4>
[Stage 2:>                                                          (0 + 4) / 4]
[Stage 2:==============>                                            (1 + 3) / 4]
[Stage 2:=============================>                             (2 + 2) / 4]
[Stage 2:============================================>              (3 + 1) / 4]
----
<1> Make sure `spark.ui.showConsoleProgress` is `true`. It is by default.
<2> Disable (`OFF`) the root logger (that includes Spark's logger)
<3> Make sure `org.apache.spark.SparkContext` logger is at least `WARN`.
<4> Run a job with 4 tasks with 500ms initial sleep and 200ms sleep chunks to see the progress bar.

https://youtu.be/uEmcGo8rwek[Watch the short video] that show ConsoleProgressBar in action.

You may want to use the following example to see the progress bar in full glory - all 3 concurrent stages in console (borrowed from https://github.com/apache/spark/pull/3029#issuecomment-63244719[a comment to [SPARK-4017\] show progress bar in console #3029]):

```
> ./bin/spark-shell --conf spark.scheduler.mode=FAIR
scala> val a = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _)
scala> val b = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _)
scala> a.union(b).count()
```

=== [[closure-cleaning]] Closure Cleaning (clean method)

Every time an action is called, Spark cleans up the closure, i.e. the body of the action, before it is serialized and sent over the wire to executors.

SparkContext comes with `clean(f: F, checkSerializable: Boolean = true)` method that does this. It in turn calls `ClosureCleaner.clean` method.

Not only does `ClosureCleaner.clean` method clean the closure, but also does it transitively, i.e. referenced closures are cleaned transitively.

A closure is considered serializable as long as it does not explicitly reference unserializable objects. It does so by traversing the hierarchy of enclosing closures and null out any references that are not actually used by the starting closure.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.util.ClosureCleaner` logger to see what happens inside the class.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG
```
====

With `DEBUG` logging level you should see the following messages in the logs:

```
+++ Cleaning closure [func] ([func.getClass.getName]) +++
 + declared fields: [declaredFields.size]
     [field]
 ...
+++ closure [func] ([func.getClass.getName]) is now cleaned +++
```

Serialization is verified using a new instance of `Serializer` (as link:spark-sparkenv.adoc#closureSerializer[closure Serializer]). Refer to link:spark-serialization.adoc[Serialization].

CAUTION: FIXME an example, please.

=== [[hadoopConfiguration]] Hadoop Configuration

While a <<creating-instance, `SparkContext` is being created>>, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration] that is available as `_hadoopConfiguration`).

NOTE: link:spark-hadoop.adoc#SparkHadoopUtil[SparkHadoopUtil.get.newConfiguration] is used.

If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default `Configuration` object is returned.

If `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are both available, the following settings are set for the Hadoop configuration:

* `fs.s3.awsAccessKeyId`, `fs.s3n.awsAccessKeyId`, `fs.s3a.access.key` are set to the value of `AWS_ACCESS_KEY_ID`
* `fs.s3.awsSecretAccessKey`, `fs.s3n.awsSecretAccessKey`, and `fs.s3a.secret.key` are set to the value of `AWS_SECRET_ACCESS_KEY`

Every `spark.hadoop.` setting becomes a setting of the configuration with the prefix `spark.hadoop.` removed for the key.

The value of `spark.buffer.size` (default: `65536`) is used as the value of `io.file.buffer.size`.

=== [[environment-variables]] Environment Variables

* `SPARK_EXECUTOR_MEMORY` sets the amount of memory to allocate to each executor. See link:spark-executor.adoc#memory[Executor Memory].

=== [[creating-instance-internals]] Internals of Creating SparkContext

Let's walk through the initialization sequence of `SparkContext` in a Spark application and learn about what happens under the covers.

[source, scala]
----
import org.apache.spark.{SparkConf, SparkContext}

// 1. Create Spark configuration
val conf = new SparkConf()
  .setAppName("SparkMe Application")
  .setMaster("local[*]")

// 2. Create Spark context
val sc = new SparkContext(conf)
----

NOTE: The example uses Spark in link:spark-local.adoc[local mode], i.e. `setMaster("local[*]")`, but the initialization with link:spark-cluster.adoc[the other cluster modes] would follow similar steps.

It all starts with checking <<allowMultipleContexts, whether SparkContexts can be shared or not using `spark.driver.allowMultipleContexts`>>.

The very first information printed out is the version of Spark as an INFO message:

```
INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT
```

<<listenerBus, listenerBus>> is created.

The current user name is computed, i.e. read from a value of `SPARK_USER` environment variable or the currently logged-in user. It is available as later on as `sparkUser`.

```
scala> sc.sparkUser
res0: String = jacek
```

CAUTION: FIXME Where is `sparkUser` useful?

It saves the input `SparkConf` (as `_conf`).

CAUTION: FIXME Review `_conf.validateSettings()`

It ensures that the first mandatory setting - `spark.master` is defined. `SparkException` is thrown if not.

```
A master URL must be set in your configuration
```

It ensures that the other mandatory setting - `spark.app.name` is defined. `SparkException` is thrown if not.

```
An application name must be set in your configuration
```

For link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc[Spark on YARN in cluster deploy mode], it checks existence of `spark.yarn.app.id`. `SparkException` is thrown if it does not exist.

```
Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.
```

CAUTION: FIXME How to "trigger" the exception? What are the steps?

When `spark.logConf` is enabled link:spark-configuration.adoc[SparkConf.toDebugString] is called.

NOTE: `SparkConf.toDebugString` is called very early in the initialization process and other settings configured afterwards are not included. Use `sc.getConf.toDebugString` once SparkContext is initialized.

The driver's host and port are set if missing. `spark.driver.host` becomes the value of <<localHostName, Utils.localHostName>> (or an exception is thrown) while `spark.driver.port` is set to `0`.

NOTE: `spark.driver.host` and `spark.driver.port` are expected to be set on the driver. It is later asserted by link:spark-sparkenv.adoc#createDriverEnv[SparkEnv.createDriverEnv].

`spark.executor.id` setting is set to `driver`.

TIP: Use `sc.getConf.get("spark.executor.id")` to know where the code is executed - link:spark-sparkenv.adoc[driver or executors].

It sets the jars and files based on `spark.jars` and `spark.files`, respectively. These are files that are required for proper task execution on executors.

If link:spark-scheduler-listeners-eventlogginglistener.adoc[event logging] is enabled, i.e. link:spark-scheduler-listeners-eventlogginglistener.adoc#spark.eventLog.enabled[spark.eventLog.enabled] is `true`, the internal field `_eventLogDir` is set to the value of link:spark-scheduler-listeners-eventlogginglistener.adoc#spark.eventLog.dir[spark.eventLog.dir] setting or the default value `/tmp/spark-events`. Also, if `spark.eventLog.compress` is `true` (default: `false`), the short name of the CompressionCodec is assigned to _eventLogCodec. The config key is `spark.io.compression.codec` (default: `snappy`). The supported codecs are: `lz4`, `lzf`, and `snappy` or their short class names.

It sets `spark.externalBlockStore.folderName` to the value of `externalBlockStoreFolderName`.

CAUTION: FIXME: What's `externalBlockStoreFolderName`?

For link:spark-yarn-client-yarnclientschedulerbackend.adoc[Spark on YARN in client deploy mode], the system property `SPARK_YARN_MODE` is set to `true`.

CAUTION: FIXME Why is `SPARK_YARN_MODE` system env required?

An instance of link:spark-webui.adoc#JobProgressListener[JobProgressListener] is created and registered to link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus].

A <<createSparkEnv, `SparkEnv` is created>>.

`MetadataCleaner` is created.

CAUTION: FIXME What's MetadataCleaner?

Optional <<ConsoleProgressBar, ConsoleProgressBar>> with <<SparkStatusTracker, SparkStatusTracker>> are created.

`SparkUI.createLiveUI` gets called to set `_ui` if the property link:spark-webui.adoc#settings[spark.ui.enabled] is enabled (`true`).

CAUTION: FIXME Step through `SparkUI.createLiveUI`. Where's `_ui` used?

A Hadoop configuration is created. See <<hadoopConfiguration, Hadoop Configuration>>.

If there are jars given through the SparkContext constructor, they are added using `addJar`. Same for files using `addFile`.

At this point in time, the amount of memory to allocate to each executor (as `_executorMemory`) is calculated. It is the value of link:spark-executor.adoc#settings[spark.executor.memory] setting, or <<environment-variables, SPARK_EXECUTOR_MEMORY>> environment variable (or currently-deprecated `SPARK_MEM`), or defaults to `1024`.

`_executorMemory` is later available as `sc.executorMemory` and used for LOCAL_CLUSTER_REGEX, link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set `executorEnvs("SPARK_EXECUTOR_MEMORY")`, MesosSchedulerBackend, CoarseMesosSchedulerBackend.

The value of `SPARK_PREPEND_CLASSES` environment variable is included in `executorEnvs`.

[CAUTION]
====
FIXME

* What's `_executorMemory`?
* What's the unit of the value of `_executorMemory` exactly?
* What are "SPARK_TESTING", "spark.testing"? How do they contribute to `executorEnvs`?
* What's `executorEnvs`?
====

The Mesos scheduler backend's configuration is included in `executorEnvs`, i.e. <<environment-variables, SPARK_EXECUTOR_MEMORY>>, `_conf.getExecutorEnv`, and `SPARK_USER`.

link:spark-sparkcontext-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint] is registered (as `_heartbeatReceiver`).

<<createTaskScheduler, SparkContext.createTaskScheduler>> is executed (using the master URL) and the result becomes the internal `_schedulerBackend` and `_taskScheduler`.

NOTE: The internal `_schedulerBackend` and `_taskScheduler` are used by `schedulerBackend` and `taskScheduler` methods, respectively.

link:spark-dagscheduler.adoc#creating-instance[DAGScheduler is created] (as `_dagScheduler`).

[[TaskSchedulerIsSet]]
`SparkContext` sends a blocking link:spark-sparkcontext-HeartbeatReceiver.adoc#TaskSchedulerIsSet[`TaskSchedulerIsSet` message to HeartbeatReceiver RPC endpoint] (to inform that the `TaskScheduler` is now available).

link:spark-taskscheduler.adoc#start[TaskScheduler is started].

The internal fields, `_applicationId` and `_applicationAttemptId`, are set (using `applicationId` and `applicationAttemptId` from the link:spark-taskscheduler.adoc#contract[TaskScheduler Contract]).

The setting `spark.app.id` is set to the current application id and Web UI gets notified about it if used (using `setAppId(_applicationId)`). The application id is also passed on to the link:spark-blockmanager.adoc#initialize[BlockManager to initialize it].

CAUTION: FIXME Why should UI know about the application id?

link:spark-metrics.adoc[Metric System] is started (after the application id is set using `spark.app.id`).

CAUTION: FIXME Why does Metric System need the application id?

The driver's metrics (servlet handler) are attached to the web ui after the metrics system is started.

`_eventLogger` is created and started if `isEventLogEnabled`. It uses link:spark-scheduler-listeners-eventlogginglistener.adoc[EventLoggingListener] that gets registered to link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus].

CAUTION: FIXME Why is `_eventLogger` required to be the internal field of SparkContext? Where is this used?

If link:spark-dynamic-allocation.adoc[dynamic allocation] is enabled, `_executorAllocationManager` is set to `ExecutorAllocationManager` and started.

`_cleaner` is set to link:spark-service-contextcleaner.adoc[ContextCleaner] if `spark.cleaner.referenceTracking` is `true` (default: `true`).

CAUTION: FIXME It'd be quite useful to have all the properties with their default values in `sc.getConf.toDebugString`, so when a configuration is not included but does change Spark runtime configuration, it should be added to `_conf`.

`setupAndStartListenerBus` link:spark-scheduler-listeners.adoc[registers user-defined listeners] and starts <<listenerBus, listenerBus>> that starts event delivery to the listeners.

`postEnvironmentUpdate` is called to post `SparkListenerEnvironmentUpdate` event over link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in link:spark-webui.adoc#environment-tab[Web UI's Environment tab].

`SparkListenerApplicationStart` event is posted to link:spark-scheduler-listeners.adoc#listener-bus[Listener Bus] (using internal `postApplicationStart` method).

CAUTION: FIXME Why is this needed?

link:spark-taskscheduler.adoc#contract[TaskScheduler.postStartHook] is called.

NOTE: `TaskScheduler.postStartHook` does nothing by default, but the link:spark-taskschedulerimpl.adoc#postStartHook[only implementation TaskSchedulerImpl comes with its own postStartHook] and blocks the current thread until a SchedulerBackend is ready.

Two new metrics sources are registered (via `_env.metricsSystem`):

* link:spark-blockmanager.adoc#metrics[BlockManagerSource]
* link:spark-dynamic-allocation.adoc#metrics[ExecutorAllocationManagerSource] (only when `_executorAllocationManager` is set)

`ShutdownHookManager.addShutdownHook()` is called to do SparkContext's cleanup.

CAUTION: FIXME What exactly does `ShutdownHookManager.addShutdownHook()` do?

Any non-fatal Exception leads to termination of the Spark context instance.

CAUTION: FIXME What does `NonFatal` represent in Scala?

`nextShuffleId` and `nextRddId` start with `0`.

CAUTION: FIXME Where are `nextShuffleId` and `nextRddId` used?

A new instance of Spark context is created and ready for operation.

==== [[createSparkEnv]] Creating SparkEnv for Driver (createSparkEnv method)

[source, scala]
----
createSparkEnv(
  conf: SparkConf,
  isLocal: Boolean,
  listenerBus: LiveListenerBus): SparkEnv
----

`createSparkEnv` simply delegates the call to link:spark-sparkenv.adoc#createDriverEnv[SparkEnv to create a `SparkEnv` for the driver].

It calculates the number of cores to `1` for `local` master URL, the number of processors available for JVM for `*` or the exact number in the master URL, or `0` for the cluster master URLs.

==== [[localHostName]] Utils.localHostName

`localHostName` computes the local host name.

It starts by checking `SPARK_LOCAL_HOSTNAME` environment variable for the value. If it is not defined, it uses `SPARK_LOCAL_IP` to find the name (using `InetAddress.getByName`). If it is not defined either, it calls `InetAddress.getLocalHost` for the name.

NOTE: `Utils.localHostName` is executed when <<creating-instance, SparkContext is being created>>.

CAUTION: FIXME Review the rest.
