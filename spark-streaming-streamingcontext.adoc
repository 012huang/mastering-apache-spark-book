== StreamingContext

`StreamingContext` is the main entry point for all Spark Streaming functionality. Whatever you do in Spark Streaming has to start from <<creating-instance, creating an instance of StreamingContext>>.

NOTE: `StreamingContext` belongs to `org.apache.spark.streaming` package.

Having an instance of `StreamingContext`, you can <<creating-receivers, create ReceiverInputDStreams>> that you apply transformations to to build a streaming pipeline.

Once satisfied, you <<start, start StreamingContext>> that sets the stream transformations in motion.

=== [[creating-instance]] Creating Instance

You can create a new instance of `StreamingContext` (and you will eventually) using the following constructors:

* `StreamingContext(path: String)`
* `StreamingContext(path: String, hadoopConf: Configuration)`
* `StreamingContext(path: String, sparkContext: SparkContext)`
* `StreamingContext(conf: SparkConf, batchDuration: Duration)`
* `StreamingContext(master: String, appName: String, batchDuration: Duration, sparkHome: String, jars: Seq[String], environment: Map[String,String])`
* `StreamingContext(sparkContext: SparkContext, batchDuration: Duration)`

where:

* `path` is the checkpoint directory.

NOTE: `StreamingContext(conf: SparkConf, batchDuration: Duration)` constructor seems the most often used in Spark Streaming applications.

Internally, when you create a new instance of `StreamingContext`  it first checks whether a link:spark-sparkcontext.adoc[SparkContext] or the checkpoint directory are given.

[TIP]
====
`StreamingContext` will warn you when you use `local` or `local[1]` link:spark-deployment-environments.adoc#master-urls[master URLs]:

[options="wrap"]
----
WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
----
====

.StreamingContext and Dependencies
image::images/streaming-streamingcontext.png[align="center"]

A link:spark-streaming-dstreamgraph.adoc[DStreamGraph] is created.

A link:spark-streaming-jobscheduler.adoc[JobScheduler] is created.

A link:spark-streaming-streaminglisteners.adoc#StreamingJobProgressListener[StreamingJobProgressListener] is created.

link:spark-streaming-webui.adoc[Streaming tab] in web UI is created (when link:spark-webui.adoc#settings[spark.ui.enabled] is set).

A link:spark-streaming.adoc#StreamingSource[StreamingSource] is instantiated.

At this point, `StreamingContext` enters `INITIALIZED` state.

=== [[creating-receivers]] Creating ReceiverInputDStreams

`StreamingContext` offers the following methods to create link:spark-streaming-receiverinputdstreams.adoc[ReceiverInputDStreams]:

* `receiverStream[T](receiver: Receiver[T]): ReceiverInputDStream[T]`
* `actorStream[T](props: Props, name: String, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2, supervisorStrategy: SupervisorStrategy = ActorSupervisorStrategy.defaultStrategy): ReceiverInputDStream[T]`
* `socketTextStream(hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2): ReceiverInputDStream[String]`
* `socketStream[T](hostname: String, port: Int, converter: (InputStream) => Iterator[T], storageLevel: StorageLevel): ReceiverInputDStream[T]`
* `rawSocketStream[T](hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2): ReceiverInputDStream[T]`

`StreamingContext` offers the following methods to create link:spark-streaming-inputdstreams.adoc[InputDStreams]:

* `queueStream[T](queue: Queue[RDD[T]], oneAtATime: Boolean = true): InputDStream[T]`
* `queueStream[T](queue: Queue[RDD[T]], oneAtATime: Boolean, defaultRDD: RDD[T]): InputDStream[T]`

You can also use two additional methods in `StreamingContext` to build (or better called _compose_) custom link:spark-streaming-dstreams.adoc[DStream]:

* `union[T](streams: Seq[DStream[T]]): DStream[T]`
* `transform[T](dstreams: Seq[DStream[_]], transformFunc: (Seq[RDD[_]], Time) => RDD[T]): DStream[T]`

=== [[isCheckpointPresent]] isCheckpointPresent

CAUTION: FIXME

=== [[checkpoint-interval]][[checkpointDuration]] Checkpoint Interval

The *checkpoint interval* is an internal property of `StreamingContext` and corresponds to link:spark-streaming-dstreamgraph.adoc#batch-interval[batch interval] or checkpoint interval of the checkpoint (when <<isCheckpointPresent, checkpoint was present>>).

NOTE: The checkpoint interval property is also called *graph checkpointing interval*.

CAUTION: FIXME Describe _checkpoint interval of the checkpoint_ above. Could it be the batch interval anyway?

CAUTION: FIXME It can also be restored from a checkpoint. Can it be different than the batch interval..._ever_?

<<validate, checkpoint interval is mandatory>> when <<checkpointDir, checkpoint directory>> is defined (i.e. not `null`).

=== [[checkpointDir]][[checkpoint-directory]] Checkpoint Directory

A *checkpoint directory* is a HDFS-compatible directory where link:spark-streaming-checkpointing.adoc[checkpoints] can be stored.

You can set the checkpoint directory when a <<creating-instance, StreamingContext is created>> or later using <<checkpoint, checkpoint>> method.

Internally, a checkpoint directory is tracked as `checkpointDir`.

=== [[initialCheckpoint]] Initial Checkpoint

*Initial checkpoint* is the checkpoint file the current `StreamingContext` has been recreated from.

=== [[checkpoint]] Setting Checkpoint Directory (checkpoint method)

[source, scala]
----
checkpoint(directory: String): Unit
----

You use `checkpoint` method to set up a HDFS-compatible `directory` where the checkpoint data will be persisted (refer to link:spark-streaming-checkpointing.adoc[Checkpointing] for more detailed coverage).

NOTE: Spark creates the directory unless it exists already.

It uses link:spark-sparkcontext.adoc#hadoopConfiguration[SparkContext.hadoopConfiguration] to get the file system to `mkdir` the directory on. Once created, the full directory path is passed on to link:spark-sparkcontext.adoc#setCheckpointDir[SparkContext.setCheckpointDir] method.

NOTE: Calling `checkpoint` with `null` as `directory` clears the checkpoint directory that effectively disables checkpointing.

=== [[start]] Starting StreamingContext (using start method)

You start stream processing using `StreamingContext.start()` method.

You should see the following INFO message in the logs upon successful start:

```
INFO StreamingContext: StreamingContext started
```

When you call `start()`, it checks out the state of `StreamingContext` that can only be `INITIALIZED` to do anything meaningful.

When in `INITIALIZED` state, it first checks whether another `StreamingContext` instance has already been started and throws `IllegalStateException` exception if it was.

Otherwise, when no other `StreamingContext` instance is started, it performs setup validation, i.e.:

* It does link:spark-streaming-dstreamgraph.adoc#dstreamgraph-validation[validation of DStreamGraph].

* It checks proper configuration of link:spark-streaming-checkpointing.adoc[checkpointing] (directory and duration are set, and that the `DStream` checkpoint is serializable).

* When link:spark-dynamic-allocation.adoc[dynamic allocation] is enabled, it prints the following WARN message to the logs:
+
[options="wrap"]
----
WARN StreamingContext: Dynamic Allocation is enabled for this application. Enabling Dynamic allocation for Spark Streaming applications can cause data loss if Write Ahead Log is not enabled for non-replayable sources like Flume. See the programming guide for details on how to enable the Write Ahead Log
----

=== [[stopping]] Stopping StreamingContext (using stop methods)

You stop `StreamingContext` using one of the three variants of `stop` method:

* `stop(stopSparkContext: Boolean = true)`
* `stop(stopSparkContext: Boolean, stopGracefully: Boolean)`

NOTE: The first `stop` method uses link:spark-streaming-settings.adoc[spark.streaming.stopSparkContextByDefault] configuration setting that controls `stopSparkContext` input parameter.

`stop` methods stop the execution of the streams immediately (`stopGracefully` is `false`) or wait for the processing of all received data to be completed (`stopGracefully` is `true`).

`stop` reacts appropriately depending on the state of `StreamingContext`. The end state is always `STOPPED`.

When in `INITIALIZED` state, it prints the WARN message to the logs:

```
WARN StreamingContext: StreamingContext has not been started yet
```

When in `STOPPED` state, it prints the WARN message to the logs:

```
WARN StreamingContext: StreamingContext has already been stopped
```

It is only in `ACTIVE` state when `stop` does more than printing out the WARN messages to the logs, i.e. it does the following (in order):

* link:spark-streaming-jobscheduler.adoc#stopping[JobScheduler is stopped].

* link:spark-streaming.adoc#StreamingSource[StreamingSource] is removed from link:spark-metrics.adoc[MetricsSystem] (using `MetricsSystem.removeSource`)

* link:spark-streaming-webui.adoc[Streaming tab] is detached (using `StreamingTab.detach`).

* `ContextWaiter` is `notifyStop()`

* `shutdownHookRef` is cleared.

CAUTION: FIXME When is `shutdownHookRef` executed? It doesn't seem to be so at `stop`?

At that point, the following INFO message is printed out to the logs and `StreamingContext` enters `STOPPED` state.

```
INFO StreamingContext: StreamingContext stopped successfully
```

If a user requested to stop the underlying SparkContext (when `stopSparkContext` is `true`), link:spark-sparkcontext.adoc#stopping[it is now attempted to be stopped].

=== [[states]] States

CAUTION: FIXME There are different states StreamingContext can be in. Describe it.

=== [[validate]] Setup Validation

CAUTION: FIXME Describe `validate` method
