== [[Sink]] Sink

=== [[ForeachSink]] ForeachSink

`ForeachSink` is a typed <<Sink, Sink>> that passes records (of the same type `T`) to <<ForeachWriter, ForeachWriter>> (one record at a time per partition). It is used exclusively for link:spark-sql-streaming-DataStreamWriter.adoc#foreach[foreach] operator.

[source, scala]
----
val records = spark.readStream
  .format("text")
  .load("server-logs/*.out")
  .as[String]

import org.apache.spark.sql.ForeachWriter
val writer = new ForeachWriter[String] {
  override def open(partitionId: Long, version: Long) = true
  override def process(value: String) = println(value)
  override def close(errorOrNull: Throwable) = {}
}

records.writeStream
  .queryName("server-logs processor")
  .foreach(writer)
  .start
----

Internally, `addBatch` (the only method from the <<contract, Sink Contract>>) takes records from the input link:spark-sql-dataframe.adoc[DataFrame] (as `data`), transforms them to expected type `T` (of this `ForeachSink`) and (now as a link:spark-sql-dataset.adoc[Dataset]) link:spark-sql-dataset.adoc#foreachPartition[processes each partition].

[source, scala]
----
addBatch(batchId: Long, data: DataFrame): Unit
----

It then opens the constructor's <<ForeachWriter, ForeachWriter>> (for the link:spark-taskscheduler-taskcontext.adoc#getPartitionId[current partition] and the input batch) and passes the records to process (one at a time per partition).

CAUTION: FIXME Why does Spark track whether the writer failed or not? Why couldn't it `finally` and do `close`?

CAUTION: FIXME Can we have a constant for `"foreach"` for `source` in `DataStreamWriter`?

=== [[ForeachWriter]] ForeachWriter

CAUTION: FIXME

=== [[contract]] Sink Contract

CAUTION: FIXME

=== [[FileStreamSink]] FileStreamSink

CAUTION: FIXME

It uses link:spark-sql-SQLConf.adoc#spark.sql.streaming.fileSink.log.deletion[spark.sql.streaming.fileSink.log.deletion] (as `isDeletingExpiredLog`)

=== [[ConsoleSink]] ConsoleSink

CAUTION: FIXME

[source, scala]
----
spark.readStream
  .format("text")
  .load("server-logs/*.out")
  .as[String]
  .writeStream
  .queryName("server-logs processor")
  .format("console")
  .start

scala> spark.streams.active.foreach(println)
Streaming Query - server-logs processor [state = ACTIVE]

// in another terminal
$ echo hello > server-logs/hello.out

// in the terminal with Spark
-------------------------------------------
Batch: 0
-------------------------------------------
+-----+
|value|
+-----+
|hello|
+-----+
----

=== [[MemorySink]] MemorySink

`MemorySink` is an memory-based `Sink` particularly useful for testing. It stores the results in memory.

It is available as `memory` format that requires a query name (by `queryName` method or `queryName` option).

[source, scala]
----
...FIXME
----

NOTE: It was introduced in the https://github.com/apache/spark/pull/12119[pull request for [SPARK-14288\][SQL\] Memory Sink for streaming].

Use `toDebugString` to see the batches.

Its aim is to allow users to test streaming applications in the Spark shell or other local tests.

You can set `checkpointLocation` using `option` method or it will be set to link:spark-sql-settings.adoc#spark.sql.streaming.checkpointLocation[spark.sql.streaming.checkpointLocation] setting.

If `spark.sql.streaming.checkpointLocation` is set, the code uses `$location/$queryName` directory.

Finally, when no `spark.sql.streaming.checkpointLocation` is set, a temporary directory `memory.stream` under `java.io.tmpdir` is used with `offsets` subdirectory inside.

NOTE: The directory is cleaned up at shutdown using `ShutdownHookManager.registerShutdownDeleteDir`.

[source, scala]
----
val nums = (0 to 10).toDF("num")

scala> val outStream = nums.write
  .format("memory")
  .queryName("memStream")
  .startStream()
16/04/11 19:37:05 INFO HiveSqlParser: Parsing command: memStream
outStream: org.apache.spark.sql.StreamingQuery = Continuous Query - memStream [state = ACTIVE]
----

It creates `MemorySink` instance based on the schema of the DataFrame it operates on.

It creates a new DataFrame using `MemoryPlan` with `MemorySink` instance created earlier and registers it as a temporary table (using link:spark-sql-dataframe.adoc#registerTempTable[DataFrame.registerTempTable] method).

NOTE: At this point you can query the table as if it were a regular non-streaming table using link:spark-sql-sqlcontext.adoc#sql[sql] method.

A new link:spark-sql-StreamingQuery.adoc[StreamingQuery] is started (using link:spark-sql-StreamingQueryManager.adoc#startQuery[StreamingQueryManager.startQuery]) and returned.

CAUTION: FIXME Describe `else` part.
