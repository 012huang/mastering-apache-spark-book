== Spark Standalone cluster

=== Introduction

In *Standalone cluster mode* Spark allocates resources based on cores. By default, an application will grab all the cores in the cluster unless `spark.cores.max` is specified. It means an application could get executors of different sizes (in terms of cores).

Standalone cluster mode is subject to the constraint that only one executor can be allocated on each worker per application.

A node is a machine, and there's not a good reason to run more than one worker per machine. So two worker nodes typically means two machines, each a Spark worker.

Workers hold many executors for many applications. One application has executors on many workers.

=== SPARK_WORKER_INSTANCES (and SPARK_WORKER_CORES)

There is really no need to run multiple workers per machine in Spark 1.5 (perhaps in 1.4, too). You can run multiple executors on the same machine with one worker.

Use `SPARK_WORKER_INSTANCES` in `spark-env.sh` to define the number of worker instances (default: 1).

If you use `SPARK_WORKER_INSTANCES`, make sure to set `SPARK_WORKER_CORES` explicitly to limit the cores per worker, or else each worker will try to use all the cores.

You can set up the number of cores as an command line argument when you start a worker daemon using `--cores`.

There are two reasons why you want to have multiple instances:

* garbage pauses collector can hurt throughput for large JVMs
* Heap size of >32 GB can’t use CompressedOoops

=== Multiple executors per worker in Standalone mode

CAUTION: It can be a duplicate of the above section.

Since the change https://issues.apache.org/jira/browse/SPARK-1706[SPARK-1706 Allow multiple executors per worker in Standalone mode] in Spark 1.4 it's currently possible to start multiple executors in a single JVM process of a worker.

To launch multiple executors on a machine you start multiple standalone workers, each with its own JVM. It introduces unnecessary overhead due to these JVM processes, provided that there are enough cores on that worker.

If you are running Spark in standalone mode on memory rich nodes it can be beneficial to have multiple worker instances on the same node as a very large heap size has two disadvantages:

* Garbage collector pauses can hurt throughput of Spark jobs.
* Heap size of >32 GB can’t use CompressedOoops. So https://blog.codecentric.de/en/2014/02/35gb-heap-less-32gb-java-jvm-memory-oddities/[35 GB is actually less than 32 GB].

Mesos and YARN can, out of the box, support packing multiple, smaller executors onto the same physical host, so requesting smaller executors doesn’t mean your application will have fewer overall resources.

The following command will launch 3 worker instances on each node. Each worker instance will use two cores.

```
SPARK_WORKER_INSTANCES=3 SPARK_WORKER_CORES=2 ./sbin/start-slaves.sh
```

=== Example 2-workers-on-1-node cluster (one executor per worker)

The following steps are a recipe for a Spark Standalone cluster with 2 workers on a single machine that could easily be your laptop.

[IMPORTANT]
====
You can use the Spark Standalone cluster in the following ways:

* Use `spark-shell` with `--master MASTER_URL`
* Use http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf[SparkConf.setMaster(MASTER_URL)] in your Spark application

For our learning purposes, `MASTER_URL` is `spark://localhost:7077`.
====

1. Start a standalone master server.

  ./sbin/start-master.sh
+
Notes:

* Use `SPARK_LOCAL_HOSTNAME` to set up the hostname in general or `SPARK_MASTER_HOST` specifically for `start-master.sh`.
* Use `--ip`, `-i`, `--host`, or `-h` to override `SPARK_MASTER_HOST`.
* Use `SPARK_MASTER_PORT` (number) for the master's port (defaults to `7077`).
* Use `--port` or `-p` to override `SPARK_MASTER_PORT`.
* Use `SPARK_MASTER_WEBUI_PORT` (number) for the port of the master's WebUI (defaults to `8080`).
* Use `--webui-port` (number) to override `SPARK_MASTER_WEBUI_PORT`.
* Use `--properties-file` for a custom Spark properties file (defaults to `$SPARK_HOME/conf/spark-defaults.conf`).
** Use `spark.master.ui.port` to override `--webui-port`.
* Use `SPARK_CONF_DIR` for the configuration directory (defaults to `$SPARK_HOME/conf`).
* Use `spark.worker.timeout` (default: `60`)
* Use `spark.deploy.retainedApplications` (default: `200`)
* Use `spark.deploy.retainedDrivers` (default: `200`)
* Use `spark.dead.worker.persistence` (default: `15`)
* Use `spark.deploy.recoveryMode` (default: `NONE`)
* Use `SPARK_PUBLIC_DNS` env var to override hostname of the master
* Use `spark.deploy.spreadOut` (default: `true`) to allow users to set a flag that will perform round-robin scheduling across the nodes (spreading out each app among all the nodes) instead of trying to consolidate each app onto a small # of nodes.
* Use `spark.deploy.defaultCores` (default: `Int.MaxValue`)
* `spark.master.rest.enabled` (default: `true`)
+
NOTE: The command above in turn executes `org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080`
+
[TIP]
====
Debug the master using the following command. It suspends the process until it gets connected (using JPDA client, e.g. your IDE).
```
/Library/Java/JavaVirtualMachines/Current/Contents/Home/bin/java -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 -cp /Users/jacek/dev/oss/spark/sbin/../conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-1.6.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar -Xms1g -Xmx1g org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080
```
====

1. Open master’s web UI at http://localhost:8080 to know the current setup - no workers and applications.
+
.Master's web UI with no workers and applications
image::images/spark-standalone-console-master-only.png[]

1. Start the first worker.

  ./sbin/start-slave.sh spark://japila.local:7077
+
NOTE: The command above in turn executes `org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://japila.local:7077`

1. Check out master’s web UI at http://localhost:8080 to know the current setup - one worker.
+
.Master's web UI with one worker ALIVE
image::images/spark-standalone-console-one-worker.png[]
+
Note the number of CPUs and memory, 8 and 15 GBs, respectively (one gigabyte left for the OS -- _oh, how generous, my dear Spark!_).

1. Let's stop the worker to start over with custom configuration. You use `./sbin/stop-slave.sh` to stop the worker.

  ./sbin/stop-slave.sh

1. Check out master’s web UI at http://localhost:8080 to know the current setup - one worker in *DEAD* state.
+
.Master's web UI with one worker DEAD
image::images/spark-standalone-console-worker-dead.png[]

1. Start a worker using `--cores 2` and `--memory 4g` for two CPU cores and 4 GB of RAM.

  ./sbin/start-slave.sh spark://japila.local:7077 --cores 2 --memory 4g
+
NOTE: The command translates to `org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://japila.local:7077 --cores 2 --memory 4g`

1. Check out master’s web UI at http://localhost:8080 to know the current setup - one worker *ALIVE* and another *DEAD*.
+
.Master's web UI with one worker ALIVE and one DEAD
image::images/spark-standalone-console-workers-alive-and-dead.png[]

1. Configuring cluster using `conf/spark-env.sh`
+
There's the `conf/spark-env.sh.template` template to start from.
+
We're going to use the following `conf/spark-env.sh`:
+
[source,shell]
.conf/spark-env.sh
----
SPARK_WORKER_CORES=2 # <1>
SPARK_WORKER_INSTANCES=2 # <2>
SPARK_WORKER_MEMORY=2g
----
<1> the number of cores per worker
<2> the number of workers per node (a machine)


1. Start the workers.

  ./sbin/start-slave.sh spark://japila.local:7077
+
As the command progresses, it prints out _starting org.apache.spark.deploy.worker.Worker, logging to_ for each worker. You defined two workers in `conf/spark-env.sh` using `SPARK_WORKER_INSTANCES`, so you should see two lines.
+
  $ ./sbin/start-slave.sh spark://japila.local:7077
  starting org.apache.spark.deploy.worker.Worker, logging to ../logs/spark-jacek-org.apache.spark.deploy.worker.Worker-1-japila.local.out
  starting org.apache.spark.deploy.worker.Worker, logging to ../logs/spark-jacek-org.apache.spark.deploy.worker.Worker-2-japila.local.out

1. Check out master’s web UI at http://localhost:8080 to know the current setup - at least two workers should be *ALIVE*.
+
.Master's web UI with two workers ALIVE
image::images/spark-standalone-console-two-workers-alive.png[]
+
[NOTE]
====
Use `jps` on master to see the instances given they all run on the same machine, e.g. `localhost`).

....
$ jps
6580 Worker
4872 Master
6874 Jps
6539 Worker
....
====

1. Stop all instances - the driver and the workers.

  ./sbin/stop-all.sh

=== SparkContext initialization in Standalone cluster

When you create a `SparkContext` using `spark://` master URL, `TaskSchedulerImpl` is the sole implementation of `TaskScheduler` interface.

Each TaskScheduler schedules tasks for a single SparkContext. The schedulers get sets of tasks submitted to them from the `DAGScheduler` for each stage, and are responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers. They return events to the DAGScheduler.

The default implementation can schedule tasks for multiple types of clusters by acting through a SchedulerBackend. It handles common logic, like determining a scheduling order across jobs, waking up to launch speculative tasks, etc.

* `spark.task.maxFailures` (default: `4`) - Number of individual task failures before giving up on the job. The only place where it is used is `org.apache.spark.scheduler.TaskSchedulerImpl` to initialize `org.apache.spark.scheduler.TaskSetManager`

FIXME `spark.speculation.interval` (default: `100ms`) - how often to check for speculative tasks. Where else?

FIXME `spark.starvation.timeout` (default: `15s`) - Threshold above which we warn user initial TaskSet may be starved

FIXME `spark.task.cpus` (default: `1`) - CPUs to request per task

Keeps track of task ids and executor ids, executors per host, hosts per rack

FIXME `DAGScheduler` & `SchedulerBackend` - what is it and what does it do?

`spark.scheduler.mode` (default: `FIFO`) can be of any of `FAIR`, `FIFO`, `NONE` values. `FAIR` and `FIFO` determine which policy is used to order tasks amongst a Schedulable's sub-queues. `NONE` is used when a Schedulable has no sub-queues.

FIXME Where is `spark.scheduler.mode` used?

FIXME `TaskResultGetter` what is that and what for?

You can give one or many comma-separated masters URLs in `spark://` URL.

FIXME What does a many-master URL do?

`SparkDeploySchedulerBackend` is created and later passed to initialize `TaskSchedulerImpl`.

A pair of backend and scheduler is returned.

The result is two have a pair of a backend and a scheduler.
