== Spark Standalone cluster

=== Introduction

*Spark Standalone cluster* (aka _Spark deploy cluster_ or _standalone cluster_) is the Spark built-in cluster. It comes with the default distribution of Apache Spark (available from the project's http://spark.apache.org/downloads.html[Download Spark]).

*Standalone Master* (often written _standalone Master_) is the cluster manager for Spark Standalone cluster. It can be started and stopped using link:spark-standalone-master-scripts.adoc[custom management scripts for standalone Master].

*Standalone workers* (aka _standalone slaves_) are the workers in Spark Standalone cluster. They can be started and stopped using link:spark-standalone-worker-scripts.adoc[custom management scripts for standalone Workers].

Spark Standalone cluster is one of the three available clustering options in Spark (refer to link:spark-cluster.adoc[Running Spark on cluster]).

In Standalone cluster mode Spark allocates resources based on cores. By default, an application will grab all the cores in the cluster (refer to <<settings, Settings>>).

Standalone cluster mode is subject to the constraint that only one executor can be allocated on each worker per application.

Once a Spark Standalone cluster has been started, you can access it using `spark://` master URL (refer to link:spark-deployment-modes.adoc#master-urls[Master URLs]).

=== [[round-robin-scheduling]] Round-robin Scheduling Across Nodes

<<settings, spark.deploy.spreadOut>> setting controls whether or not to perform round-robin scheduling across the nodes (spreading out each app among all the nodes). It defaults to `true`.

FIXME

=== Master WebUI

FIXME MasterWebUI

```
INFO Utils: Successfully started service 'MasterUI' on port 8080.
INFO MasterWebUI: Started MasterWebUI at http://192.168.1.4:8080
```

=== [[rest-server]] REST Server

The standalone Master starts the REST Server service for alternative application submission that is supposed to work across Spark versions. It is enabled by default (see <<settings, spark.master.rest.enabled>>).

`RestSubmissionClient` is the client.

The server includes a JSON representation of `SubmitRestProtocolResponse` in the HTTP body.

The following INFOs show up when the Master Endpoint starts up (`Master#onStart` is called) with REST Server enabled:

```
INFO Utils: Successfully started service on port 6066.
INFO StandaloneRestServer: Started REST server for submitting applications on port 6066
```

=== [[recovery-mode]] Recovery Mode

A standalone Master can run with *recovery mode* enabled and be able to recover state among the available swarm of masters. By default, there is no recovery, i.e. no persistence and no election.

It uses `spark.deploy.recoveryMode` to define the recovery mode for master (see <<settings, spark.deploy.recoveryMode>>).

The Recovery Mode enables election of the leader master among the masters.

FIXME Why would I want to have many masters? What are the use cases?

=== [[leader-election]] Leader Election

Master endpoint is `LeaderElectable`, i.e. FIXME

CAUTION: FIXME

=== RPC Messages

Master communicates with drivers, executors and configures itself using *RPC messages*.

The following message types are accepted by master (see `Master#receive` or `Master#receiveAndReply` methods):

* `ElectedLeader` for <<leader-election, Leader Election>>
* `CompleteRecovery`
* `RevokedLeadership`
* <<RegisterApplication, RegisterApplication>>
* `ExecutorStateChanged`
* `DriverStateChanged`
* `Heartbeat`
* `MasterChangeAcknowledged`
* `WorkerSchedulerStateResponse`
* `UnregisterApplication`
* `CheckForWorkerTimeOut`
* `RegisterWorker`
* `RequestSubmitDriver`
* `RequestKillDriver`
* `RequestDriverStatus`
* `RequestMasterState`
* `BoundPortsRequest`
* `RequestExecutors`
* `KillExecutors`

==== [[RegisterApplication]] RegisterApplication event

A *RegisterApplication* event is sent by link:spark-standalone.adoc#AppClient[AppClient] to standalone Master. The event holds information about the application being deployed (`ApplicationDescription`) and the driver's endpoint reference.

`ApplicationDescription` describes an application by its name, maximum number of cores, executor's memory, command, appUiUrl, and user with optional eventLogDir and eventLogCodec for Event Logs, and the number of cores per executor.

FIXME

=== SPARK_WORKER_INSTANCES (and SPARK_WORKER_CORES)

There is really no need to run multiple workers per machine in Spark 1.5 (perhaps in 1.4, too). You can run multiple executors on the same machine with one worker.

Use `SPARK_WORKER_INSTANCES` (default: `1`) in `spark-env.sh` to define the number of worker instances.

If you use `SPARK_WORKER_INSTANCES`, make sure to set `SPARK_WORKER_CORES` explicitly to limit the cores per worker, or else each worker will try to use all the cores.

You can set up the number of cores as an command line argument when you start a worker daemon using `--cores`.

=== Multiple executors per worker in Standalone mode

CAUTION: It can be a duplicate of the above section.

Since the change https://issues.apache.org/jira/browse/SPARK-1706[SPARK-1706 Allow multiple executors per worker in Standalone mode] in Spark 1.4 it's currently possible to start multiple executors in a single JVM process of a worker.

To launch multiple executors on a machine you start multiple standalone workers, each with its own JVM. It introduces unnecessary overhead due to these JVM processes, provided that there are enough cores on that worker.

If you are running Spark in standalone mode on memory-rich nodes it can be beneficial to have multiple worker instances on the same node as a very large heap size has two disadvantages:

* Garbage collector pauses can hurt throughput of Spark jobs.
* Heap size of >32 GB can’t use CompressedOoops. So https://blog.codecentric.de/en/2014/02/35gb-heap-less-32gb-java-jvm-memory-oddities/[35 GB is actually less than 32 GB].

Mesos and YARN can, out of the box, support packing multiple, smaller executors onto the same physical host, so requesting smaller executors doesn’t mean your application will have fewer overall resources.

=== [[initialization]] SparkContext initialization in Standalone cluster

When you create a `SparkContext` using `spark://` master URL...FIXME

Keeps track of task ids and executor ids, executors per host, hosts per rack

You can give one or many comma-separated masters URLs in `spark://` URL.

A pair of backend and scheduler is returned.

The result is two have a pair of a backend and a scheduler.

=== [[spark-deploy-scheduler-backend]] SparkDeploySchedulerBackend

`SparkDeploySchedulerBackend` is the link:spark-execution-model.adoc#scheduler-backends[Scheduler Backend] for Spark Standalone, i.e. it is used when you link:spark-sparkcontext.adoc#creating-sparkcontext[create a SparkContext] using `spark://` link:spark-deployment-modes.adoc#master-urls[master URL].

It requires a link:spark-taskscheduler.adoc[Task Scheduler], a link:spark-sparkcontext.adoc[Spark context], and a collection of link:spark-deployment-modes.adoc#master-urls[master URLs].

It is a specialized link:spark-runtime-environment.adoc#CoarseGrainedSchedulerBackend[CoarseGrainedSchedulerBackend] that uses <<AppClient, AppClient>> and is a `AppClientListener`.

.SparkDeploySchedulerBackend.start() (while SparkContext starts)
image::images/SparkDeploySchedulerBackend-AppClient-start.png[align="center"]

CAUTION: FIXME `AppClientListener` & `LauncherBackend` & `ApplicationDescription`

=== [[AppClient]] AppClient

`AppClient` is an interface to allow Spark applications to talk to a Spark deploy cluster (using a RPC Environment). It takes an RPC Environment, a collection of master URLs, a `ApplicationDescription`, and a `AppClientListener`.

`AppClient` registers *AppClient* RPC endpoint (using `ClientEndpoint`) to a given RPC Environment.

CAUTION: FIXME What RPC Env does it belong to?

It uses `appclient-receive-and-reply-threadpool`. FIXME

==== [[appclient-initialization]] Initialization - AppClient.start() method

When AppClient starts, `AppClient.start()` method is called that merely registers <<appclient-rpc-endpoint, AppClient RPC Endpoint>>.

==== [[appclient-rpc-endpoint]] AppClient RPC Endpoint

...FIXME

==== Others

* killExecutors
* start
* stop

==== [[RegisterApplication]] RegisterApplication message

An AppClient registers the application to a single master (regardless of link:spark-deployment-modes.adoc#master-urls[the number of the standalone masters given in the master URL]).

.AppClient registers application to standalone Master
image::images/appclient-registerapplication.png[align="center"]

It uses the separate thread pool *appclient-register-master-threadpool* to asynchronously sent `RegisterApplication` messages, one per standalone master.

```
INFO AppClient$ClientEndpoint: Connecting to master spark://localhost:7077...
```

An AppClient tries connecting to a standalone master 3 times every 20 seconds per master before giving up. They are not configurable parameters.

=== [[startup-internals]] Internals of org.apache.spark.deploy.master.Master

[TIP]
====
You can debug a Standalone master using the following command:

[source]
----
java -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 -cp /Users/jacek/dev/oss/spark/conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-1.6.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar -Xms1g -Xmx1g org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080
----

The above command suspends (`suspend=y`) the process until a JPDA debugging client, e.g. your IDE, is connected, and that Spark is available under `/Users/jacek/dev/oss/spark`. Change it to meet your environment.
====

When `Master` starts, it first creates the <<spark-configuration.adoc#default-configuration, default SparkConf configuration>> whose values it then overrides using  <<environment-variables, environment variables>> and <<command-line-options, command-line options>>.

A fully-configured master instance requires `host`, `port` (default: `7077`), `webUiPort` (default: `8080`) settings defined.

TIP: When in troubles, consult link:spark-tips-and-tricks.adoc[Spark Tips and Tricks] document.

It starts <<rpcenv, RPC Environment>> with necessary endpoints and lives until the RPC environment terminates.

=== [[worker-management]] Worker Management

Master uses `master-forward-message-thread` to schedule a thread every `spark.worker.timeout` to check workers' availability and remove timed-out workers.

It is that Master sends `CheckForWorkerTimeOut` message to itself to trigger verification.

When a worker hasn't responded for `spark.worker.timeout`, it is assumed dead and the following WARN message appears in the logs:

```
WARN Removing [worker.id] because we got no heartbeat in [spark.worker.timeout] seconds
```

=== [[environment-variables]] System Environment Variables

Master uses the following system environment variables (directly or indirectly):

* `SPARK_LOCAL_HOSTNAME` - the custom host name
* `SPARK_LOCAL_IP` - the custom IP to use when `SPARK_LOCAL_HOSTNAME` is not set
* `SPARK_MASTER_HOST` (not `SPARK_MASTER_IP` as used in `start-master.sh` script above!) - the master custom host
* `SPARK_MASTER_PORT` (default: `7077`) - the master custom port
* `SPARK_MASTER_IP` (default: `hostname` command's output)
* `SPARK_MASTER_WEBUI_PORT` (default: `8080`) - the port of the master's WebUI. Overriden by `spark.master.ui.port` if set in the properties file.
* `SPARK_PUBLIC_DNS` (default: hostname) - the custom master hostname for WebUI's http URL and master's address.
* `SPARK_CONF_DIR` (default: `$SPARK_HOME/conf`) - the directory of the default properties file `spark-defaults.conf` from which all properties that start with `spark.` prefix are loaded.

=== States

Master can be in the following states:

* `STANDBY` - the initial state while Master is initializing
* `ALIVE`
* `RECOVERING`
* `COMPLETING_RECOVERY`

CAUTION: FIXME

=== [[rpcenv]] RPC Environment

The `org.apache.spark.deploy.master.Master` class starts link:spark-rpc.adoc[sparkMaster RPC environment].

```
INFO Utils: Successfully started service 'sparkMaster' on port 7077.
```

It then registers `Master` endpoint.

.sparkMaster - the RPC Environment for Spark Standalone's master
image::images/sparkMaster-rpcenv.png[align="center"]

Master endpoint is a `ThreadSafeRpcEndpoint` and `LeaderElectable` (see <<leader-election, Leader Election>>).

The Master endpoint starts the daemon single-thread scheduler pool `master-forward-message-thread`. It is used for worker management, i.e. removing any timed-out workers.

```
"master-forward-message-thread" #46 daemon prio=5 os_prio=31 tid=0x00007ff322abb000 nid=0x7f03 waiting on condition [0x000000011cad9000]
```

Application ids follows the pattern `app-yyyyMMddHHmmss`.

Master keeps track of the following:

* workers (`workers`)
* mapping between ids and applications (`idToApp`)
* waiting applications (`waitingApps`)
* applications (`apps`)
* mapping between ids and workers (`idToWorker`)
* mapping between RPC address and workers (`addressToWorker`)
* `endpointToApp`
* `addressToApp`
* `completedApps`
* `nextAppNumber`
* mapping between application ids and their Web UIs (`appIdToUI`)
* drivers (`drivers`)
* `completedDrivers`
* drivers currently spooled for scheduling (`waitingDrivers`)
* `nextDriverNumber`

The Master is in fact the Master RPC Endpoint that you can access using RPC port (low-level operation communication) or link:spark-webui.adoc[Web UI].

`MasterWebUI` is the Web UI server for the standalone master. Master starts Web UI to listen to `http://[master's hostname]:webUIPort`, e.g. `http://localhost:8080`.

The following INFO shows up when the Master endpoint starts up (`Master#onStart` is called):

```
INFO Master: Starting Spark master at spark://japila.local:7077
INFO Master: Running Spark version 1.6.0-SNAPSHOT
```

=== [[metrics]] Metrics

Master uses link:spark-metrics.adoc[Spark Metrics System] (via `MasterSource`) to report metrics about internal status.

The name of the source is *master*.

It emits the following metrics:

* `workers` - the number of all workers (any state)
* `aliveWorkers` - the number of alive workers
* `apps` - the number of applications
* `waitingApps` - the number of waiting applications

The name of the other source is *applications*

[CAUTION]
====
FIXME

* Review `org.apache.spark.metrics.MetricsConfig`
* How to access the metrics for master? See `Master#onStart`
* Review `masterMetricsSystem` and `applicationMetricsSystem`
====

=== [[settings]] Settings

[CAUTION]
====
FIXME

* Where are `RETAINED_`'s properties used?
====

Master uses the following properties:

* `spark.cores.max` (default: `0`) - total expected number of cores (FIXME `totalExpectedCores`). When set, an application could get executors of different sizes (in terms of cores).
* `spark.worker.timeout` (default: `60`) - time (in seconds) when no heartbeat from a worker means it is lost. See <<worker-management, Worker Management>>.
* `spark.deploy.retainedApplications` (default: `200`)
* `spark.deploy.retainedDrivers` (default: `200`)
* `spark.dead.worker.persistence` (default: `15`)
* `spark.deploy.recoveryMode` (default: `NONE`) - possible modes: `ZOOKEEPER`, `FILESYSTEM`, or `CUSTOM`. Refer to <<recovery-mode, Recovery Mode>>.
* `spark.deploy.recoveryMode.factory` - the class name of the custom `StandaloneRecoveryModeFactory`.
* `spark.deploy.recoveryDirectory` (default: empty) - the directory to persist recovery state
* `spark.deploy.spreadOut` (default: `true`) - perform round-robin scheduling across the nodes (spreading out each app among all the nodes). Refer to <<round-robin-scheduling, Round-robin Scheduling Across Nodes>>
* `spark.deploy.defaultCores` (default: `Int.MaxValue`, i.e. unbounded)- the number of maxCores for applications that don't specify it.
* `spark.master.rest.enabled` (default: `true`) - <<rest-server, master's REST Server>> for alternative application submission that is supposed to work across Spark versions.
* `spark.master.rest.port` (default: `6066`) - the port of <<rest-server, master's REST Server>>
* `spark.ui.killEnabled` (default: `true`)
