== Spark Standalone cluster

=== Introduction

In standalone mode Spark allocates resources based on cores. By default, an application will grab all the cores in the cluster unless `spark.cores.max` is specified. It means an application could get executors of different sizes (in terms of cores).

Standalone mode is subject to the constraint that only one executor can be allocated on each worker per application.

=== Example 2-workers-on-1-node cluster (one executor per worker)

The following steps are a recipe for a Spark Standalone cluster with 2 workers on a single machine that could easily be your laptop.

[IMPORTANT]
====
You can use the Spark Standalone cluster in the following ways:

* Use `spark-shell` with `--master MASTER_URL`
* Use http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf[SparkConf.setMaster(MASTER_URL)] in your Spark application

For our learning purposes, `MASTER_URL` is `spark://localhost:7077`.
====

1. Start a standalone master server.

  ./sbin/start-master.sh
+
Notes:

* Use `SPARK_LOCAL_HOSTNAME` to set up the hostname in general or `SPARK_MASTER_HOST` specifically for `start-master.sh`.
* Use `--ip`, `-i`, `--host`, or `-h` to override `SPARK_MASTER_HOST`.
* Use `SPARK_MASTER_PORT` (number) for the master's port (defaults to `7077`).
* Use `--port` or `-p` to override `SPARK_MASTER_PORT`.
* Use `SPARK_MASTER_WEBUI_PORT` (number) for the port of the master's WebUI (defaults to `8080`).
* Use `--webui-port` (number) to override `SPARK_MASTER_WEBUI_PORT`.
* Use `--properties-file` for a custom Spark properties file (defaults to `$SPARK_HOME/conf/spark-defaults.conf`).
** Use `spark.master.ui.port` to override `--webui-port`.
* Use `SPARK_CONF_DIR` for the configuration directory (defaults to `$SPARK_HOME/conf`).
* Use `spark.worker.timeout` (default: `60`)
* Use `spark.deploy.retainedApplications` (default: `200`)
* Use `spark.deploy.retainedDrivers` (default: `200`)
* Use `spark.dead.worker.persistence` (default: `15`)
* Use `spark.deploy.recoveryMode` (default: `NONE`)
* Use `SPARK_PUBLIC_DNS` env var to override hostname of the master
* Use `spark.deploy.spreadOut` (default: `true`) to allow users to set a flag that will perform round-robin scheduling across the nodes (spreading out each app among all the nodes) instead of trying to consolidate each app onto a small # of nodes.
* Use `spark.deploy.defaultCores` (default: `Int.MaxValue`)
* `spark.master.rest.enabled` (default: `true`)
+
NOTE: The command above in turn executes `org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080`
+
[TIP]
====
Debug the master using the following command. It suspends the process until it gets connected (using JPDA client, e.g. your IDE).
```
/Library/Java/JavaVirtualMachines/Current/Contents/Home/bin/java -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 -cp /Users/jacek/dev/oss/spark/sbin/../conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-1.6.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar -Xms1g -Xmx1g org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080
```
====

1. Open master’s web UI at http://localhost:8080 to know the current setup - no workers and applications.
+
.Master's web UI with no workers and applications
image::images/spark-standalone-console-master-only.png[]

1. Start the first worker.

  ./sbin/start-slave.sh spark://japila.local:7077
+
NOTE: The command above in turn executes `org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://japila.local:7077`

1. Check out master’s web UI at http://localhost:8080 to know the current setup - one worker.
+
.Master's web UI with one worker ALIVE
image::images/spark-standalone-console-one-worker.png[]
+
Note the number of CPUs and memory, 8 and 15 GBs, respectively (one gigabyte left for the OS -- _oh, how generous, my dear Spark!_).

1. Let's stop the worker to start over with custom configuration. You use `./sbin/stop-slave.sh` to stop the worker.

  ./sbin/stop-slave.sh

1. Check out master’s web UI at http://localhost:8080 to know the current setup - one worker in *DEAD* state.
+
.Master's web UI with one worker DEAD
image::images/spark-standalone-console-worker-dead.png[]

1. Start a worker using `--cores 2` and `--memory 4g` for two CPU cores and 4 GB of RAM.

  ./sbin/start-slave.sh spark://japila.local:7077 --cores 2 --memory 4g
+
NOTE: The command translates to `org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://japila.local:7077 --cores 2 --memory 4g`

1. Check out master’s web UI at http://localhost:8080 to know the current setup - one worker *ALIVE* and another *DEAD*.
+
.Master's web UI with one worker ALIVE and one DEAD
image::images/spark-standalone-console-workers-alive-and-dead.png[]

1. Configuring cluster using `conf/spark-env.sh`
+
There's the `conf/spark-env.sh.template` template to start from.
+
We're going to use the following `conf/spark-env.sh`:
+
[source,shell]
.conf/spark-env.sh
----
SPARK_WORKER_CORES=2 # <1>
SPARK_WORKER_INSTANCES=2 # <2>
SPARK_WORKER_MEMORY=2g
----
<1> the number of cores per worker
<2> the number of workers per node (a machine)


1. Start the workers.

  ./sbin/start-slave.sh spark://japila.local:7077
+
As the command progresses, it prints out _starting org.apache.spark.deploy.worker.Worker, logging to_ for each worker. You defined two workers in `conf/spark-env.sh` using `SPARK_WORKER_INSTANCES`, so you should see two lines.
+
  $ ./sbin/start-slave.sh spark://japila.local:7077
  starting org.apache.spark.deploy.worker.Worker, logging to ../logs/spark-jacek-org.apache.spark.deploy.worker.Worker-1-japila.local.out
  starting org.apache.spark.deploy.worker.Worker, logging to ../logs/spark-jacek-org.apache.spark.deploy.worker.Worker-2-japila.local.out

1. Check out master’s web UI at http://localhost:8080 to know the current setup - at least two workers should be *ALIVE*.
+
.Master's web UI with two workers ALIVE
image::images/spark-standalone-console-two-workers-alive.png[]
+
[NOTE]
====
Use `jps` on master to see the instances given they all run on the same machine, e.g. `localhost`).

....
$ jps
6580 Worker
4872 Master
6874 Jps
6539 Worker
....
====

1. Stop all instances - the driver and the workers.

  ./sbin/stop-all.sh
