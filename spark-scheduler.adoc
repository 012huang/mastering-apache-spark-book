== DAGScheduler

[NOTE]
====
The introduction that follows is an almost exact copy of scaladoc of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[org.apache.spark.scheduler.DAGScheduler]. As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[the sources] and read the page afterwards.

_"Reading the sources"_?! Yes, I _am_ kidding!
====

=== Introduction

*DAGScheduler* is the present and only scheduling layer in Apache Spark that implements stage-oriented scheduling.

It does three things in Spark (detailed explanations follow):

* It computes a DAG of stages for a job.
* It determines the preferred locations to run each task on.
* It handles failures due to shuffle output files being lost.

It computes https://en.wikipedia.org/wiki/Directed_acyclic_graph[a directed acyclic graph (DAG)] of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run the job. It then submits stages to link:spark-taskscheduler.adoc[TaskScheduler] that runs them on the cluster.

In addition to coming up with the DAG of stages, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes these to link:spark-taskscheduler.adoc[TaskScheduler].

Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures *within* a stage that are not caused by shuffle file loss are handled by the TaskScheduler, which will retry each task a small number of times before cancelling the whole stage.

*DAGScheduler* uses an event queue architecture in which a thread can post an `DAGSchedulerEvent` event, e.g. a new job being submitted (`JobSubmitted`), that the scheduler reads and executes, sequentially.

[TIP]
====
Add the following line to `conf/log4j.properties` to see what happens under the covers of `DAGScheduler`:

```
log4j.logger.org.apache.spark.scheduler.DAGScheduler=DEBUG
```
====

DAGScheduler has a direct dependency on link:spark-taskscheduler.adoc[Task Scheduler], link:spark-listeners.adoc[Listener Bus], spark-service-mapoutputtracker.adoc#MapOutputTrackerMaster[MapOutputTrackerMaster] and link:spark-blockmanager.adoc[Block Manager]. It also  reports metrics about its execution (refer to the section <<metrics, Metrics>>).

=== [[jobs]] Jobs

A *job* (aka _action job_ or _active job_) is a top-level work item (computation) submitted to DAGScheduler to <<stages, compute a stage>> as either <<ResultStage, ResultStage>> that is <<spark-rdd.adoc#actions,a result of executing an action on an RDD>> in a Spark user program or (_still work in progress_) <<ShuffleMapStage, ShuffleMapStage>> for <<adaptive-query-planning, adaptive query planning>>.

.Job "wraps" a stage that can depends on a series of stages
image::diagrams/job-stage.png[align="center"]

The stage of a job can depend on other stages and so execution of a single job can trigger execution of a series of stages.

Computing a job is equivalent to computing the partitions of the RDD the action has been executed upon. Note that not all partitions have always to be computed for <<ResultStage, ResultStages>> for actions like `first()` and `lookup()`.

.Computing a job is computing the partitions of an RDD
image::diagrams/rdd-job-partitions.png[align="center"]

Internally, a running job is represented by an instance of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala[private[spark\] class org.apache.spark.scheduler.ActiveJob].

[CAUTION]
====
FIXME

* Where are instances of ActiveJob used?
====

All actions in Spark use `SparkContext.runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U]` to schedule jobs (that are run on all partitions of the RDD and return the results in an array).

[source,scala]
----
scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1)
res0: Array[Int] = Array(1, 1)
----

Eventually, `SparkContext.runJob` uses `DAGScheduler.runJob()` and `submitJob()` (and also does link:spark-rdd-checkpointing.adoc[checkpointing]). It triggers posting `JobSubmitted` event (see <<event-loop,Event loop>>).

A job can be one of two logical types (that are only distinguished by an internal `finalStage` field of `ActiveJob`):

* *Map-stage job* that computes the map output files for a <<ShuffleMapStage, ShuffleMapStage>> (for `submitMapStage`) before any downstream stages are submitted. It is used for <<adaptive-query-planning, adaptive query planning>>, to look at map output statistics before submitting later stages.
* *Result job* that computes a <<ResultStage, ResultStage>> to execute an action

Jobs track how many partitions have already been computed (using `finished` array of `Boolean` field).

==== Job Listener and Events

You can listen for job completion or failure events after submitting a job to the DAGScheduler using `JobListener`.

The job listener is notified each time a task succeeds, as well as if the whole job fails (and no further `taskSucceeded` events will happen).

The following are the job listeners used:

* `JobWaiter` waits until DAGScheduler completes the job and passes the results of tasks to a `resultHandler` function.
* `ApproximateActionListener` FIXME

=== [[stages]] Stages (and a bit on job submission)

When a job is submitted, a new <<ResultStage, ResultStage>> stage is created (with the parent stages linked in `newResultStage`).

The stage is then transformed into a set of (parallel) tasks that all compute results of a function on partitions of a RDD that need to run as part of a Spark job.

Stages are created by breaking the RDD graph at shuffle boundaries. DAGScheduler splits up a job into a collection of one or many stages. Stages are split up at the boundaries where shuffle occurs, called *shuffle boundaries*.

*Shuffle boundaries* introduce a barrier where stages/tasks must wait for the previous stage to finish to fetch outputs.

.DAGScheduler splits a job into stages
image::diagrams/scheduler-job-splits-into-stages.png[align="center"]

RDD operations with link:spark-rdd.adoc[narrow dependencies], like `map()` and `filter()`, are pipelined together into one set of tasks in each stage, but operations with shuffle dependencies require multiple stages, i.e. one to write a set of map output files, and another to read those files after a barrier.

In the end, every stage will have only shuffle dependencies on other stages, and may compute multiple operations inside it. The actual pipelining of these operations happens in the `RDD.compute()` functions of various RDDs, e.g. `MappedRDD`, `FilteredRDD`, etc.

Each stage contains a sequence of link:spark-rdd.adoc[narrow transformations] that can be completed without link:spark-rdd-shuffle.adoc[shuffling] the entire data set, separated at shuffle boundaries.

DAGScheduler runs stages in topological order.

There are two types of stages:

* <<ShuffleMapStage, ShuffleMapStage>> is an intermediate stage (in the execution of a DAG) that produces data for other stage(s). It writes *map output files* for a shuffle.
* <<ResultStage, ResultStage>> is the final stage that executes a Spark action, e.g. `count()`, `save()`, by running a function on an RDD.

.DAGScheduler and Stages for a job
image::diagrams/scheduler-job-shuffles-result-stages.png[align="center"]

The following INFO messages show in the logs:

```
15/10/13 08:30:16 INFO DAGScheduler: Got job 0 (count at <console>:25) with 2 output partitions
15/10/13 08:30:16 INFO DAGScheduler: Final stage: ResultStage 0 (count at <console>:25)
15/10/13 08:30:16 INFO DAGScheduler: Parents of final stage: List()
INFO DAGScheduler: Missing parents: List()
```

When DAGScheduler executes a job it first submits the final stage that may in turn trigger submission of the other stages of the missing parents of the stage (recursively).

[CAUTION]
====
FIXME Code review

* `DAGScheduler.submitStage` and DEBUG logs of `DAGScheduler`
====

When the current stage has no parent stages to submit, it is submitted.

```
INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[9] at map at <console>:25), which has no missing parents
```

[CAUTION]
====
FIXME What's `ShuffleMapStage.outputLocs`?
====

`DAGScheduler` keeps track of stages in different states:

* waiting
* running
* failed

At some point of time in a stage's life, every partition of the stage gets transformed into a task - `ShuffleMapTask` or `ResultTask` for `ShuffleMapStage` and `ResultStage`, respectively.

Partitions are computed in jobs, and result stages may not always need to compute all partitions in their target RDD, e.g. for actions like `first()` and `lookup()`.

`DAGScheduler` prints the following INFO message when there are tasks to submit:

```
INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[86] at reduceByKey at <console>:24)
```

There is also the following DEBUG message with pending partitions:

```
DEBUG DAGScheduler: New pending partitions: Set(0)
```

Tasks are later submitted to link:spark-taskscheduler.adoc[Task Scheduler] (via `taskScheduler.submitTasks`).

When no tasks in a stage can be submitted, the following DEBUG message shows in the logs:

```
FIXME
```

Each stage has also a `firstJobId`, identifying the job that first submitted the stage.

A stage has an *id*.

==== Stage sharing

Stages can be shared across multiple jobs, if these jobs reuse the same RDDs.

FIXME: Where in the code is this used?

==== [[ShuffleMapStage]] ShuffleMapStage

A *ShuffleMapStage* (represented by  https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala[org.apache.spark.scheduler.ShuffleMapStage]) is an intermediate stage (in the execution of a DAG) that produces data for link:spark-rdd-shuffle.adoc[a shuffle] and is an input for the other stages in the DAG of stages.

In other words, ShuffleMapStage is a stage with additional link:spark-rdd-dependencies.adoc#shuffle-dependency[ShuffleDependency] - the shuffle that it is part of.

NOTE: ShuffleMapStages can also be submitted independently as jobs with `DAGScheduler.submitMapStage` for <<adaptive-query-planning, Adaptive Query Planning>>.

The number of the partitions of an RDD is exactly the number of the tasks in a ShuffleMapStage.

The output locations (`outputLocs`) of a ShuffleMapStage are the same as used by its link:spark-rdd-dependencies.adoc#shuffle-dependency[ShuffleDependency]. Output locations can be missing, i.e. partitions have not been cached or are lost.

[CAUTION]
====
FIXME Where is `ShuffleMapStage` used?

* Review `ShuffleMapStage`'s scaladoc
* `newShuffleMapStage`
* `getShuffleMapStage`
* `newOrUsedShuffleStage`
* `handleMapStageSubmitted`
* `shuffleToMapStage` - `private[scheduler]` HashMap
====

==== [[ResultStage]] ResultStage

A *ResultStage* is the final stage that applies a function on some partitions of an RDD to compute the result of an action.

.Job creates ResultStage
image::diagrams/dagscheduler-job-resultstage.png[align="center"]

* `JobSubmitted` message => `DAGScheduler.handleJobSubmitted` creates `ResultStage`

[CAUTION]
====
FIXME

* How is the stage used?
** DAGScheduler.handleJobSubmitted
** DAGScheduler.newResultStage
** DAGScheduler.submitMissingTasks
====

=== [[adaptive-query-planning]] Adaptive Query Planning

See https://issues.apache.org/jira/browse/SPARK-9850[SPARK-9850 Adaptive execution in Spark] for the design document. The work is currently in progress.

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L661[DAGScheduler.submitMapStage] method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages.

=== RDD, job execution, stages, and partitions

When link:spark-scheduler.adoc[DAGScheduler schedules a job to execute an action on a RDD], it spawns parallel tasks to compute (partial) results per partition.

The number of partition in a job depends on the type of a stage - be it `ResultStage` or `ShuffleMapStage` (refer to link:spark-scheduler.adoc[Stages (aka job submission)]).

For some actions like `first()` and `lookup()`, there is no need to compute all the partitions of a job of a target RDD.

=== Fault recovery - stage attempts

A single stage can be re-executed in multiple *attempts* due to fault recovery. The number of attempts is configured (FIXME).

If `TaskScheduler` reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits that lost stage. This is detected through a `CompletionEvent` with `FetchFailed`, or an `ExecutorLost` event. `DAGScheduler` will wait a small amount of time to see whether other nodes or tasks fail, then resubmit `TaskSets` for any lost stage(s) that compute the missing tasks.

Please note that tasks from the old attempts of a stage could still be running.

A stage object tracks multiple `StageInfo` objects to pass to Spark listeners or the web UI.

The latest `StageInfo` for the most recent attempt for a stage is accessible through `latestInfo`.

=== [[execution-events]] Execution Events

A `SparkListenerJobStart` event is posted to link:spark-listeners.adoc[listenerBus].

CAUTION: FIXME What events are posted and when?

=== [[event-loop]] Event loop - dag-scheduler-event-loop

`DAGScheduler.eventProcessLoop` (of type `DAGSchedulerEventProcessLoop`) - is the event process loop to which Spark posts jobs to schedule their execution.

Internally, it uses https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/LinkedBlockingDeque.html[java.util.concurrent.LinkedBlockingDeque] blocking deque that grows indefinitely (i.e. up to https://docs.oracle.com/javase/7/docs/api/java/lang/Integer.html#MAX_VALUE[Integer.MAX_VALUE] events).

The name of the single "logic" thread that reads events and takes decisions is *dag-scheduler-event-loop*.

```
"dag-scheduler-event-loop" #89 daemon prio=5 os_prio=31 tid=0x00007f809bc0a000 nid=0xc903 waiting on condition [0x0000000125826000]
```

The following are the current types of `DAGSchedulerEvent` events that are handled by `DAGScheduler`:

* `JobSubmitted` - posted when an action job is submitted to DAGScheduler (via `submitJob` or `runApproximateJob`). It then calls `DAGScheduler.handleJobSubmitted`.
* `MapStageSubmitted` - posted when a shuffle map stage is submitted (via `submitMapStage`). It then calls `DAGScheduler.handleMapStageSubmitted`.
* `StageCancelled`
* `JobCancelled`
* `JobGroupCancelled`
* `AllJobsCancelled`
* `BeginEvent` - posted when `TaskSetManager` reports that a task is starting.
+
`dagScheduler.handleBeginEvent` is executed in turn.
* `GettingResultEvent` - posted when `TaskSetManager` reports that a task has completed and results are being fetched remotely.
+
`dagScheduler.handleGetTaskResult` executes in turn.
* `CompletionEvent` - posted when link:spark-taskscheduler.adoc#tasksetmanager[TaskSetManager] reports that a task has completed successfully or failed. See <<completionevent, CompletionEvent>>.
* `ExecutorAdded`
* `ExecutorLost`
* `TaskSetFailed`
* `ResubmitFailedStages`

[CAUTION]
====
FIXME

* What is an approximate job (as in `DAGScheduler.runApproximateJob`)?
* statistics? `MapOutputStatistics`?
====

==== [[completionevent]] CompletionEvent

CAUTION: FIXME Unfinished

DAGScheduler is told about a task end through `DAGScheduler.handleTaskCompletion` event handler.

FIXME Who's calling the handle and when?

It causes `updateAccumulators` call.

When a task has finished, it triggers  link:spark-taskscheduler.adoc#tasksetmanager[TaskSetManager] to send a `CompletionEvent` message to DAGScheduler.

FIXME Communication Flow Diagram

Internally, link:spark-taskscheduler.adoc#tasksetmanager[TaskSetManager] calls `DAGScheduler.taskEnded` to post the `CompletionEvent` event on `eventProcessLoop`.

* `DAGScheduler.onReceive` calls `dagScheduler.handleTaskCompletion(completion)`

=== [[cache-tracking]] Cache Tracking

DAGScheduler tracks which RDDs are cached to avoid recomputing them and likewise remembers which shuffle map stages have already produced output files to avoid redoing the map side of a shuffle.

The internal `cacheLocs` is a map with keys being RDD ids and the values being arrays indexed by partition numbers. Each array value is the set of locations where that RDD partition is cached.

[CAUTION]
====
FIXME:

* A diagram would be awesome.
* Review the use of `cacheLocs`
====

If link:spark-rdd-caching.adoc[a storage level of an RDD is NONE], there is no need to get locations from link:spark-blockmanager.adoc[block manager]. Otherwise, `RDDBlockId` is created and Block Manager gets asked for locations.

CAUTION: FIXME Review `TaskLocation`

=== Preferred Locations

DAGScheduler computes where to run each task in a stage based on link:spark-rdd.adoc#preferred-locations[the preferred locations of its underlying RDDs], or <<cache-tracking, the location of cached or shuffle data>>.

=== [[metrics]] Metrics

Spark's DAGScheduler uses link:spark-metrics.adoc[Spark Metrics System] (via `DAGSchedulerSource`) to report metrics about its execution.

The name of the source is *DAGScheduler*.

It emits the following numbers:

* stage / failedStages - the number of failed stages
* stage / runningStages - the number of running stages
* stage / waitingStages - the number of waiting stages
* job / allJobs - the number of all jobs
* job / activeJobs - the number of active jobs

=== Other goodies

* List of `ScheduledExecutorService` in DAGScheduler (perhaps should be described for Spark in general):
** `dag-scheduler-message` - a daemon thread pool using `j.u.c.ScheduledThreadPoolExecutor` with core pool size `1`. It is used to post `ResubmitFailedStages` when `FetchFailed` is reported.
