== Checkpointing

*Checkpointing* is a process that allows you to create *fault-tolerant stream processing pipelines* by <<CheckpointWriter-write, writing received records>> to a <<streamingcontext-checkpoint, highly-available HDFS-compatible storage>>.

link:spark-streaming-dstreams.adoc[DStreams] can checkpoint <<checkpoint-data, input data>> at specified <<checkpoing-interval, time intervals>>.

=== [[streamingcontext-checkpoint]] Marking StreamingContext as  Checkpointed

You use link:spark-streaming-streamingcontext.adoc#checkpoint[StreamingContext.checkpoint] method to set up a HDFS-compatible *checkpoint directory* where <<checkpoint-data, checkpoint data>> will be persisted, as follows:

[source, scala]
----
ssc.checkpoint("_checkpoint")
----

=== [[checkpoing-interval]] Checkpoint Interval and Checkpointing DStreams

You can set up periodic checkpointing of a dstream every *checkpoint interval* using link:spark-streaming-dstreams.adoc#checkpoint[DStream.checkpoint] method.

[source, scala]
----
val ssc: StreamingContext = ...
// set the checkpoint directory
ssc.checkpoint("_checkpoint")
val ds: DStream[Int] = ...
val cds: DStream[Int] = ds.checkpoint(Seconds(5))
ssc.start
----

=== [[checkpoint-data]] Checkpoint Data

CAUTION: What is checkpoint data? Code review `DStreamCheckpointData`.

`DStreamCheckpointData` works with a single dstream. An instance of `DStreamCheckpointData` is created when a dstream is.

It is used in `updateCheckpointData`, `clearCheckpointData` or `restoreCheckpointData`.

It tracks checkpoint data in the internal `data` registry that records batch time and the checkpoint data at that time. The internal checkpoint data can be anything that a dstream wants to checkpoint. `DStreamCheckpointData` returns the registry when `currentCheckpointFiles` method is called.

NOTE: By default, `DStreamCheckpointData` records the checkpoint files to which the generated RDDs of the DStream has been saved.

=== [[Checkpoint]] Checkpoint

`Checkpoint` class requires a link:spark-streaming-streamingcontext.adoc[StreamingContext] and `checkpointTime` time to be instantiated.

NOTE: `Checkpoint` class is written to a persistent storage (aka _serialized_) using <<CheckpointWriter-write, CheckpointWriter.write>> method and read back (aka _deserialize_) using <<Checkpoint-deserialize, Checkpoint.deserialize>>.

NOTE: link:spark-streaming-streamingcontext.adoc#initial-checkpoint[Initial checkpoint] is the checkpoint a StreamingContext was started with.

It is merely a collection of the settings of the current streaming runtime environment that is supposed to recreate the environment after it goes down due to a failure or when the link:spark-streaming-streamingcontext.adoc#stop[streaming context is stopped immediately].

It collects the settings from the input `StreamingContext` (and indirectly from the corresponding link:spark-streaming-jobscheduler.adoc[JobScheduler] and link:spark-sparkcontext.adoc[SparkContext]):

* The link:spark-sparkcontext.adoc#master-url[master URL from SparkContext] as `master`.
* The link:spark-sparkcontext.adoc#application-name[mandatory application name from SparkContext] as `framework`.
* The link:spark-sparkcontext.adoc#jars[jars to distribute to workers from SparkContext] as `jars`.
* The link:spark-streaming-dstreamgraph.adoc[DStreamGraph] as `graph`
* The link:spark-streaming-streamingcontext.adoc#checkpoint-directory[checkpoint directory] as `checkpointDir`
* The link:spark-streaming-streamingcontext.adoc#checkpoint-interval[checkpoint interval] as `checkpointDuration`
* The link:spark-streaming-jobscheduler.adoc#getPendingTimes[collection of pending batches to process] as `pendingTimes`
* The link:spark-sparkcontext.adoc#spark-configuration[Spark configuration (aka SparkConf)] as `sparkConfPairs`

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.streaming.Checkpoint` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.Checkpoint=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

==== [[Checkpoint-serialize]] Serializing Checkpoint (serialize method)

[source, scala]
----
serialize(checkpoint: Checkpoint, conf: SparkConf): Array[Byte]
----

`serialize` serializes the `checkpoint` object. It does so by creating a compression codec to write the input `checkpoint` object with and returns the result as a collection of bytes.

CAUTION: FIXME Describe compression codecs in Spark.

==== [[Checkpoint-deserialize]] Deserializing Checkpoint (deserialize method)

[source, scala]
----
deserialize(inputStream: InputStream, conf: SparkConf): Checkpoint
----

`deserialize` reconstructs a <<Checkpoint, Checkpoint>> object from the input `inputStream`. It uses a compression codec and once read <<Checkpoint-validate, the just-built Checkpoint object is validated>> and returned back.

==== [[Checkpoint-validate]] Validating Checkpoint (validate method)

[source, scala]
----
validate()
----

`validate` validates the <<Checkpoint, Checkpoint>>. It ensures that `master`, `framework`, `graph`, and `checkpointTime` are defined, i.e. not `null`.

You should see the following INFO message in the logs when the object passes the validation:

```
INFO Checkpoint for time [checkpointTime] validated
```

=== [[CheckpointWriter]] CheckpointWriter

An instance of `CheckpointWriter` is created (lazily) when `JobGenerator` is, but only when link:spark-streaming-jobgenerator.adoc#shouldCheckpoint[JobGenerator is configured for checkpointing].

It uses the internal <<CheckpointWriter-executor, single-thread thread pool executor>> to <<CheckpointWriteHandler, process checkpoint writes asynchronously>>.

==== [[CheckpointWriter-executor]] Single-Thread Thread Pool Executor

`executor` is the internal single-thread thread pool executor for executing ...FIXME

==== [[CheckpointWriter-write]] Writing Checkpoint for Batch Time (write method)

[source, scala]
----
write(checkpoint: Checkpoint, clearCheckpointDataLater: Boolean): Unit
----

`write` method <<Checkpoint-serialize, serializes the checkpoint object>> and passes the serialized form to <<CheckpointWriteHandler, CheckpointWriteHandler>> to write asynchronously (i.e. on a separate thread) using <<CheckpointWriter-executor, single-thread thread pool executor>>.

NOTE: It is called when  link:spark-streaming-jobgenerator.adoc#DoCheckpoint[JobGenerator receives DoCheckpoint event and the batch time is eligible for checkpointing].

You should see the following INFO message in the logs:

```
INFO Submitted checkpoint of time [checkpoint.checkpointTime] writer queue
```

If the asynchronous checkpoint write fails, you should see the following ERROR in the logs:

```
ERROR Could not submit checkpoint task to the thread pool executor
```

==== [[CheckpointWriter-stop]] Stopping CheckpointWriter (using stop method)

CAUTION: FIXME

=== [[CheckpointWriteHandler]] CheckpointWriteHandler

`CheckpointWriteHandler` is an (internal) thread of execution that does checkpoint writes. It is executed with `checkpointTime`, the serialized form of the checkpoint, and whether to clean checkpoint data later flag (as `clearCheckpointDataLater`).

NOTE: It is used by CheckpointWriter to queue a <<CheckpointWriter-write, checkpoint write for a batch time>>.

It records the current checkpoint time (in `latestCheckpointTime`) and calculates the name of the checkpoint file.

NOTE: The name of the checkpoint file is `checkpoint-[checkpointTime.milliseconds]`.

CAUTION: FIXME Is `checkpointTime.milliseconds` a batch time?

It uses a backup file to do atomic write, i.e. it writes to the checkpoint backup file first and renames the result file to the final checkpoint file name.

NOTE: The name of the checkpoint backup file is `checkpoint-[checkpointTime.milliseconds].bk`.

NOTE: `CheckpointWriteHandler` does 3 write attempts at the maximum. The value is not configurable.

When attempting to write, you should see the following INFO message in the logs:

```
INFO CheckpointWriter: Saving checkpoint for time [checkpointTime] ms to file '[checkpointFile]'
```

NOTE: It deletes any checkpoint backup files that may exist from the previous attempts.

It then deletes checkpoint files when there are more than 10.

NOTE: The number of checkpoint files when the deletion happens, i.e. *10*, is fixed and not configurable.

You should see the following INFO message in the logs:

```
INFO CheckpointWriter: Deleting [file]
```

If all went fine, you should see the following INFO message in the logs:

```
INFO CheckpointWriter: Checkpoint for time [checkpointTime] ms saved to file '[checkpointFile]', took [bytes] bytes and [time] ms
```

link:spark-streaming-jobgenerator.adoc#onCheckpointCompletion[JobGenerator is informed that the checkpoint write completed] (with `checkpointTime` and `clearCheckpointDataLater` flag).

In case of write failures, you can see the following WARN message in the logs:

```
WARN CheckpointWriter: Error in attempt [attempts] of writing checkpoint to [checkpointFile]
```

If the number of write attempts exceeded (the fixed) 10 or <<CheckpointWriter-stop, CheckpointWriter was stopped>> before any successful checkpoint write, you should see the following WARN message in the logs:

```
WARN CheckpointWriter: Could not write checkpoint for time [checkpointTime] to file [checkpointFile]'
```

=== [[CheckpointReader]] CheckpointReader

CAUTION: FIXME Describe me!
