== Checkpointing

*Checkpointing* is a process of <<CheckpointWriter-write, writing received records (by means of input dstreams)>> to a <<streamingcontext-checkpoint, highly-available HDFS-compatible storage>>. It allows creating *fault-tolerant stream processing pipelines* so when a failure occurs input dstreams can restore the before-failure streaming state and continue stream processing (as if nothing had happened).

link:spark-streaming-dstreams.adoc[DStreams] can checkpoint <<checkpoint-data, input data>> at specified <<checkpoing-interval, time intervals>>.

=== [[streamingcontext-checkpoint]] Marking StreamingContext as  Checkpointed

You use link:spark-streaming-streamingcontext.adoc#checkpoint[StreamingContext.checkpoint] method to set up a HDFS-compatible *checkpoint directory* where <<checkpoint-data, checkpoint data>> will be persisted, as follows:

[source, scala]
----
ssc.checkpoint("_checkpoint")
----

=== [[checkpoing-interval]] Checkpoint Interval and Checkpointing DStreams

You can set up periodic checkpointing of a dstream every *checkpoint interval* using link:spark-streaming-dstreams.adoc#checkpoint[DStream.checkpoint] method.

[source, scala]
----
val ssc: StreamingContext = ...
// set the checkpoint directory
ssc.checkpoint("_checkpoint")
val ds: DStream[Int] = ...
val cds: DStream[Int] = ds.checkpoint(Seconds(5))
// do something with the input dstream
cds.print
----

=== [[recreating-streamingcontext]] Recreating StreamingContext from Checkpoint

You can create a StreamingContext from a link:spark-streaming-streamingcontext.adoc#checkpoint-directory[checkpoint directory], i.e. recreate a fully-working StreamingContext as recorded in the last valid checkpoint file that was written to the checkpoint directory.

NOTE: You can also link:spark-streaming-streamingcontext.adoc#creating-instance[create a brand new StreamingContext] (and putting checkpoints aside).

When you use `StreamingContext(path: String)` constructor (or link:spark-streaming-streamingcontext.adoc#creating-instance[the variants thereof]), it uses link:spark-sparkcontext.adoc#hadoopConfiguration[Hadoop configuration] to access `path` directory on a Hadoop-supported file system.

CAUTION: FIXME Review `this(path: String)` regarding `SparkHadoopUtil`.

Effectively, the two variants use `StreamingContext(path: String, hadoopConf: Configuration)` constructor that <<CheckpointReader-read, reads the latest valid checkpoint file>>.

NOTE: `SparkContext` and batch interval are set to their corresponding values using the checkpoint file.

==== [[recreating-streamingcontext-example]] Example: Recreating StreamingContext from Checkpoint

The following Scala code demonstrates how to use the checkpoint directory `_checkpoint` to (re)create the StreamingContext or create one from scratch.

[source, scala]
----
val appName = "Recreating StreamingContext from Checkpoint"
val sc = new SparkContext("local[*]", appName, new SparkConf())

val checkpointDir = "_checkpoint"

def createSC(): StreamingContext = {
  val ssc = new StreamingContext(sc, batchDuration = Seconds(5))
  ssc.checkpoint(checkpointDir)
  ssc
}
val ssc = StreamingContext.getOrCreate(checkpointDir, createSC)

// Create constant input dstream with the RDD
val rdd = sc.parallelize(0 to 9)
import org.apache.spark.streaming.dstream.ConstantInputDStream
val cis = new ConstantInputDStream(ssc, rdd)

// Sample stream computation
cis.print

// Start streaming processing
ssc.start
----

=== [[checkpoint-data]] Checkpoint Data

CAUTION: What is checkpoint data? Code review `DStreamCheckpointData`.

`DStreamCheckpointData` works with a single dstream. An instance of `DStreamCheckpointData` is created when a dstream is.

It is used in `updateCheckpointData`, `clearCheckpointData` or `restoreCheckpointData`.

It tracks checkpoint data in the internal `data` registry that records batch time and the checkpoint data at that time. The internal checkpoint data can be anything that a dstream wants to checkpoint. `DStreamCheckpointData` returns the registry when `currentCheckpointFiles` method is called.

NOTE: By default, `DStreamCheckpointData` records the checkpoint files to which the generated RDDs of the DStream has been saved.

=== [[Checkpoint]] Checkpoint

`Checkpoint` class requires a link:spark-streaming-streamingcontext.adoc[StreamingContext] and `checkpointTime` time to be instantiated. The internal property `checkpointTime` corresponds to the batch time it represents.

NOTE: `Checkpoint` class is written to a persistent storage (aka _serialized_) using <<CheckpointWriter-write, CheckpointWriter.write>> method and read back (aka _deserialize_) using <<Checkpoint-deserialize, Checkpoint.deserialize>>.

NOTE: link:spark-streaming-streamingcontext.adoc#initial-checkpoint[Initial checkpoint] is the checkpoint a StreamingContext was started with.

It is merely a collection of the settings of the current streaming runtime environment that is supposed to recreate the environment after it goes down due to a failure or when the link:spark-streaming-streamingcontext.adoc#stop[streaming context is stopped immediately].

It collects the settings from the input `StreamingContext` (and indirectly from the corresponding link:spark-streaming-jobscheduler.adoc[JobScheduler] and link:spark-sparkcontext.adoc[SparkContext]):

* The link:spark-sparkcontext.adoc#master-url[master URL from SparkContext] as `master`.
* The link:spark-sparkcontext.adoc#application-name[mandatory application name from SparkContext] as `framework`.
* The link:spark-sparkcontext.adoc#jars[jars to distribute to workers from SparkContext] as `jars`.
* The link:spark-streaming-dstreamgraph.adoc[DStreamGraph] as `graph`
* The link:spark-streaming-streamingcontext.adoc#checkpoint-directory[checkpoint directory] as `checkpointDir`
* The link:spark-streaming-streamingcontext.adoc#checkpoint-interval[checkpoint interval] as `checkpointDuration`
* The link:spark-streaming-jobscheduler.adoc#getPendingTimes[collection of pending batches to process] as `pendingTimes`
* The link:spark-sparkcontext.adoc#spark-configuration[Spark configuration (aka SparkConf)] as `sparkConfPairs`

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.streaming.Checkpoint` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.Checkpoint=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

==== [[Checkpoint-serialize]] Serializing Checkpoint (serialize method)

[source, scala]
----
serialize(checkpoint: Checkpoint, conf: SparkConf): Array[Byte]
----

`serialize` serializes the `checkpoint` object. It does so by creating a compression codec to write the input `checkpoint` object with and returns the result as a collection of bytes.

CAUTION: FIXME Describe compression codecs in Spark.

==== [[Checkpoint-deserialize]] Deserializing Checkpoint (deserialize method)

[source, scala]
----
deserialize(inputStream: InputStream, conf: SparkConf): Checkpoint
----

`deserialize` reconstructs a <<Checkpoint, Checkpoint>> object from the input `inputStream`. It uses a compression codec and once read <<Checkpoint-validate, the just-built Checkpoint object is validated>> and returned back.

NOTE: `deserialize` is called when <<CheckpointReader-read, reading the latest valid checkpoint file>>.

==== [[Checkpoint-validate]] Validating Checkpoint (validate method)

[source, scala]
----
validate(): Unit
----

`validate` validates the <<Checkpoint, Checkpoint>>. It ensures that `master`, `framework`, `graph`, and `checkpointTime` are defined, i.e. not `null`.

NOTE: `validate` is called when a <<Checkpoint-deserialize, checkpoint is deserialized from an input stream>>.

You should see the following INFO message in the logs when the object passes the validation:

```
INFO Checkpoint: Checkpoint for time [checkpointTime] ms validated
```

==== [[Checkpoint-getCheckpointFiles]] Get Collection of Checkpoint Files from Directory (getCheckpointFiles method)

[source, scala]
----
getCheckpointFiles(checkpointDir: String, fsOption: Option[FileSystem] = None): Seq[Path]
----

`getCheckpointFiles` method returns a collection of checkpoint files from the given checkpoint directory `checkpointDir`.

The method sorts the checkpoint files by time with a temporary `.bk` checkpoint file first (given a pair of a checkpoint file and its backup file).

=== [[CheckpointWriter]] CheckpointWriter

An instance of `CheckpointWriter` is created (lazily) when `JobGenerator` is, but only when link:spark-streaming-jobgenerator.adoc#shouldCheckpoint[JobGenerator is configured for checkpointing].

It uses the internal <<CheckpointWriter-executor, single-thread thread pool executor>> to <<CheckpointWriteHandler, execute checkpoint writes asynchronously>> and does so until it is <<CheckpointWriter-stop, stopped>>.

==== [[CheckpointWriter-write]] Writing Checkpoint for Batch Time (write method)

[source, scala]
----
write(checkpoint: Checkpoint, clearCheckpointDataLater: Boolean): Unit
----

`write` method <<Checkpoint-serialize, serializes the checkpoint object>> and passes the serialized form to <<CheckpointWriteHandler, CheckpointWriteHandler>> to write asynchronously (i.e. on a separate thread) using <<CheckpointWriter-executor, single-thread thread pool executor>>.

NOTE: It is called when  link:spark-streaming-jobgenerator.adoc#DoCheckpoint[JobGenerator receives DoCheckpoint event and the batch time is eligible for checkpointing].

You should see the following INFO message in the logs:

```
INFO CheckpointWriter: Submitted checkpoint of time [checkpoint.checkpointTime] ms writer queue
```

If the asynchronous checkpoint write fails, you should see the following ERROR in the logs:

```
ERROR Could not submit checkpoint task to the thread pool executor
```

==== [[CheckpointWriter-stop]] Stopping CheckpointWriter (using stop method)

[source, scala]
----
stop(): Unit
----

`CheckpointWriter` uses the internal `stopped` flag to mark whether it is stopped or not.

NOTE: `stopped` flag is disabled, i.e. `false`, when `CheckpointWriter` is created.

`stop` method checks the internal `stopped` flag and returns if it says it is stopped already.

If not, it orderly shuts down the <<CheckpointWriter-executor, internal single-thread thread pool executor>> and awaits termination for 10 seconds. During that time, any asynchronous checkpoint writes can be safely finished, but no new tasks will be accepted.

NOTE: The wait time before `executor` stops is fixed, i.e. not configurable, and is set to 10 seconds.

After 10 seconds, when the thread pool did not terminate, `stop` stops it forcefully.

You should see the following INFO message in the logs:

```
INFO CheckpointWriter: CheckpointWriter executor terminated? [terminated], waited for [time] ms.
```

`CheckpointWriter` is marked as stopped, i.e. `stopped` flag is set to `true`.

==== [[CheckpointWriter-executor]] Single-Thread Thread Pool Executor

`executor` is an internal single-thread thread pool executor for executing <<CheckpointWriteHandler, asynchronous checkpoint writes using CheckpointWriteHandler>>.

It shuts down when <<CheckpointWriter-stop, CheckpointWriter is stopped>> (with a 10-second graceful period before it terminated forcefully).

=== [[CheckpointWriteHandler]] CheckpointWriteHandler -- Asynchronous Checkpoint Writes

`CheckpointWriteHandler` is an (internal) thread of execution that does checkpoint writes. It is instantiated with `checkpointTime`, the serialized form of the checkpoint, and whether or not to clean checkpoint data later flag (as `clearCheckpointDataLater`).

NOTE: It is only used by <<CheckpointWriter, CheckpointWriter>> to queue a <<CheckpointWriter-write, checkpoint write for a batch time>>.

It records the current checkpoint time (in `latestCheckpointTime`) and calculates the name of the checkpoint file.

NOTE: The name of the checkpoint file is `checkpoint-[checkpointTime.milliseconds]`.

It uses a backup file to do atomic write, i.e. it writes to the checkpoint backup file first and renames the result file to the final checkpoint file name.

NOTE: The name of the checkpoint backup file is `checkpoint-[checkpointTime.milliseconds].bk`.

NOTE: `CheckpointWriteHandler` does 3 write attempts at the maximum. The value is not configurable.

When attempting to write, you should see the following INFO message in the logs:

```
INFO CheckpointWriter: Saving checkpoint for time [checkpointTime] ms to file '[checkpointFile]'
```

NOTE: It deletes any checkpoint backup files that may exist from the previous attempts.

It then deletes checkpoint files when there are more than 10.

NOTE: The number of checkpoint files when the deletion happens, i.e. *10*, is fixed and not configurable.

You should see the following INFO message in the logs:

```
INFO CheckpointWriter: Deleting [file]
```

If all went fine, you should see the following INFO message in the logs:

```
INFO CheckpointWriter: Checkpoint for time [checkpointTime] ms saved to file '[checkpointFile]', took [bytes] bytes and [time] ms
```

link:spark-streaming-jobgenerator.adoc#onCheckpointCompletion[JobGenerator is informed that the checkpoint write completed] (with `checkpointTime` and `clearCheckpointDataLater` flag).

In case of write failures, you can see the following WARN message in the logs:

```
WARN CheckpointWriter: Error in attempt [attempts] of writing checkpoint to [checkpointFile]
```

If the number of write attempts exceeded (the fixed) 10 or <<CheckpointWriter-stop, CheckpointWriter was stopped>> before any successful checkpoint write, you should see the following WARN message in the logs:

```
WARN CheckpointWriter: Could not write checkpoint for time [checkpointTime] to file [checkpointFile]'
```

=== [[CheckpointReader]] CheckpointReader

`CheckpointReader` is a `private[streaming]` helper class to <<CheckpointReader-read, read the latest valid checkpoint file to recreate StreamingContext from (given the checkpoint directory)>>.

==== [[CheckpointReader-read]] Reading Latest Valid Checkpoint File

[source, scala]
----
read(checkpointDir: String): Option[Checkpoint]
read(checkpointDir: String, conf: SparkConf,
     hadoopConf: Configuration, ignoreReadError: Boolean = false): Option[Checkpoint]
----

`read` methods read the latest valid checkpoint file from a checkpoint directory `checkpointDir`. They differ in whether Spark configuration `conf` and Hadoop configuration `hadoopConf` are given or created in place.

NOTE: The 4-parameter `read` method is used by <<recreating-streamingcontext, StreamingContext to recreate itself from a checkpoint file>>.

The first `read` throws no `SparkException` when no checkpoint file could be read.

NOTE: It appears that no part of Spark Streaming uses the simplified version of `read`.

`read` uses Apache Hadoop's https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Path.java[Path] and https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java[Configuration] to get the checkpoint files (using <<Checkpoint-getCheckpointFiles, Checkpoint.getCheckpointFiles>>) in reverse order.

If there is no checkpoint file in the checkpoint directory, it returns None.

You should see the following INFO message in the logs:

```
INFO CheckpointReader: Checkpoint files found: [checkpointFiles]
```

The method reads all the checkpoints (from the youngest to the oldest) until one is successfully loaded, i.e. <<Checkpoint-deserialize, deserialized>>.

You should see the following INFO message in the logs just before deserializing a checkpoint `file`:

```
INFO CheckpointReader: Attempting to load checkpoint from file [file]
```

If the checkpoint file was loaded, you should see the following INFO messages in the logs:

```
INFO CheckpointReader: Checkpoint successfully loaded from file [file]
INFO CheckpointReader: Checkpoint was generated at time [checkpointTime]
```

In case of any issues while loading a checkpoint file, you should see the following WARN in the logs and the corresponding exception:

```
WARN CheckpointReader: Error reading checkpoint from file [file]
```

Unless `ignoreReadError` flag is disabled, when no checkpoint file could be read, `SparkException` is thrown with the following message:

```
Failed to read checkpoint from directory [checkpointPath]
```

`None` is returned at this point and the method finishes.
