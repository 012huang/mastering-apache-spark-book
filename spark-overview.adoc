== Overview of Spark

When you hear *Apache Spark* it can be two things - the Spark engine aka *Spark Core* or the Spark project with Spark Core and link:spark-frameworks.adoc[Spark Application Frameworks], i.e. link:spark-sql.adoc[Spark SQL], Spark Streaming, link:spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core.

.The Spark Platform
image::diagrams/spark-platform.png[]

It is pretty much as Hadoop where it can mean different things for different people, and Spark has heavily been and still is influenced by Hadoop.

=== Why Spark

Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand.

==== Leverages the Best in distributed batch data processing

When you think about *distributed batch data processing*, link:spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution.

Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine.

For many, Spark is Hadoop++, i.e. MapReduce done in a (slightly?) better way.

And it should *not* come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all.

==== RDD API - Distributed Scala Collections API

As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API].

It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense).

So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender.

==== Unified development and deployment environment for all

Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many link:spark-frameworks.adoc[Spark Application Frameworks] leveraging the concept of link:spark-rdd.adoc[RDD], i.e. link:spark-sql.adoc[Spark SQL], Spark Streaming, link:spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib.adoc[Spark MLlib]), a structured data queries (link:spark-sql.adoc[Spark SQL]) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation.

It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project.

==== Interactive exploration

It is also called _ad hoc queries_.

Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data (_The Big Data_). It's all interactive and very useful to explore the data before final production release.

Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using `--master`) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, Spark Streaming, and Spark GraphX.

Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX).

==== Single environment

Regardless of which programming language you are good at, be it Scala, Java, Python or R, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform.

You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (DataFrames), Spark MLlib (Pipelines), Spark GraphX (???), or Spark Streaming (DStreams).

Or use them all in a single application.

==== Rich set of supported data sources

Spark can read from many types of data sources - relational, NoSQL, file systems, etc.

Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications.

==== Tools unavailable then, at your fingertips now

As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice).

Spark embraces many concepts in a single unified development and runtime environment.

* Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling `pipe()`).
* DataFrames from R are available in Scala, Java, Python, R APIs.
* Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib.

This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehousesthe with Thrift JDBC/ODBC server in Spark SQL).

Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too.

==== Low-level Optimizations

Apache Spark uses a *directed acyclic graph (DAG) of computation stages*. It postpones any processing until really required for actions. Spark's *lazy evaluation* gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more).

==== Excels at iterative workloads

Many Machine Learning algorithms require plenty of iterations before the result models get optimal.

Spark can cache intermediate data in memory for faster model building and training. Once the data is loaded to memory (as an initial step), reusing it multiple times doesn't incur performance slowdowns.

Also, graph algorithms can traverse graphs one connection per iteration.

==== ETL done easier

Spark gives *Extract, Transform and Load (ETL)* a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem.

Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java).

==== Unified API (for different computation models)

Spark offers one *unified API* for batch analytics, SQL queries, real-time analysis, machine learning and graph processing. Developers no longer have to learn many different processing engines per use case.

=== Overview

http://spark.apache.org/[Apache Spark] is an *open-source parallel distributed general-purpose cluster computing framework* with *in-memory big data processing* with APIs in Scala, Java, Python, and R.

In contrast to Hadoop’s two-stage disk-based MapReduce processing engine, Spark’s multi-stage in-memory computing engine allows for running most computations in memory, and hence very often provides better performance (there are reports about being up to 100 times faster!) for certain applications, e.g. iterative algorithms or interactive data mining. Spark aims at speed, ease of use, and interactive analytics.

Spark is often called *cluster computing engine* or simply *execution engine*.

Spark comes with two modes of operation: *batch* and *streaming*.

Spark is a *distributed platform for executing complex multi-stage applications*, like *machine learning algorithms*, and *interactive ad hoc queries*. Spark provides an efficient abstraction for in-memory cluster computing called link:spark-rdd.adoc[Resilient Distributed Dataset].

Using link:spark-frameworks.adoc[Spark Application Frameworks], Spark simplifies access to machine learning and predictive analytics at scale.

Spark is written in http://scala-lang.org/[Scala], but supports other languages, i.e. Java, Python, and R.

If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is an alternative.

* Access any data type across any data source.
* Huge demand for storage and data processing.

The Apache Spark project is an umbrella for http://spark.apache.org/sql/[SQL] (with DataFrames), http://spark.apache.org/streaming/[streaming], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph] processing engines built atop Spark Core. You can run them all in a single application using a consistent API.

Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix).

Spark can access data from many link:spark-data-sources.adoc[data sources].

Apache Spark's Streaming and SQL programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics.

At a high level, any Spark application creates *RDDs* out of some input, run link:spark-rdd.adoc[(lazy) transformations] of these RDDs to some other form (shape), and finally perform link:spark-rdd.adoc[actions] to collect or store data. Not much, huh?

You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to.

In https://youtu.be/yEvzXQbqUCg?t=4m55s[Going from Hadoop to Spark: A Case Study, Sujee Maniyam 20150223]:

> Spark is like emacs - once you join emacs, you can't leave emacs.
