== Overview of Spark

The goal is to be able to describe what Spark is and more importantly where and how to use it.

NOTE: To be covered: scheduling, deploying, configuration, tunning and debugging of Spark applications

=== Why Spark

Quite contrary to the common approach of first giving an overview of a topic or product and only then going through "Whys", I'll do the latter first.

==== Unified runtime environment

Regardless of the tools you use - the Spark API for Scala, Java, Python, R, or link:spark_shell.adoc[the Spark shell], or the many components built on top of link:spark-rdd.adoc[RDD] like link:spark-sql.adoc[Spark SQL], Spark MLlib, Spark Streaming, or link:graphx.adoc[Spark GraphX] you can still use the same environment to leverage the large amount of data sets to yield a result, be it a prediction (as in Spark MLlib) or just a large distributed computation.

==== Interactive exploration

It's also called _ad hoc queries_.

Using link:spark_shell.adoc[the Spark shell] you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using `--master`) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, Spark Streaming, and Spark GraphX.

Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX).

==== Single environment

Regardless of which programming language you are good at, be it Scala, Java, Python or R, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform.

You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (DataFrames), Spark MLlib (Pipelines), Spark GraphX (???), or Spark Streaming (DStreams).

Or use them all in a single application.

==== Rich set of supported data sources

Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (as with the Spark shell) or in applications.

==== Tools unavailable then, at your fingertips now

As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice).

Spark embraces many concepts in a single unified development and runtime environment.

* Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling `pipe()`).
* DataFrames from R are available in Scala, Java, Python, R APIs.
* Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib.

This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehousesthe with Thrift JDBC/ODBC server in Spark SQL).

Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too.

=== Overview

http://spark.apache.org/[Apache Spark] is an open-source cluster computing framework with in-memory processing. Because it runs computations in memory (as often as possible), it's faster than existing Map Reduce processing engines for iterative algorithms or interactive data mining. Spark aims at speed, ease of use, and analytics.

Spark is the platform for executing complex multi-stage applications, like machine learning algorithms, and interactive ad-hoc queries. Spark provides an efficient abstraction for in-memory cluster computing called link:spark-rdd.adoc[Resilient Distributed Datasets], and can run 100x faster than Hadoop for these workloads.

It supports http://scala-lang.org/[Scala], Java, and Python APIs for development.

If you have large amounts of data that requires low latency processing that a typical Map/Reduce program cannot provide, Spark is the alternative.

Apache Spark delivers 100x the performance of Apache Hadoop for certain Map/Reduce workloads with its advanced in-memory computing engine.

* Access any data type across any data source.
* Huge demand for storage and data processing.

The Apache Spark project is an umbrella for http://spark.apache.org/sql/[SQL] (with DataFrames), http://spark.apache.org/streaming/[streaming], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph] processing engines built atop Spark Core. You can run them all in a single application using a consistent API.

Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix).

Spark can access data from http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html[Hadoop Distributed File System (HDFS)], http://cassandra.apache.org/[Cassandra], http://hbase.apache.org/[HBase], or https://aws.amazon.com/s3/[S3].

Apache Spark's Streaming and SQL programming models with MLlib and GraphX make it easier for developers and data scientists to build apps that exploit machine learning and graph analytics.

At a high level, any Spark application creates *RDDs* out of some input, run *transformations* of the RDDs to some other form (shape), and perform *actions* to collect or save data. Not much, huh?

You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three people will spend quite a lot of their time with Spark to finally reach the point where it suits the needs. Programmers use Spark or Java APIs (and work mostly with RDDs), data engineers use higher-level abstractions like DataFrames or Pipelines API using Python or external tools (that connects to Spark), and finally it all can only be possible because the administrators set up Spark clusters and deployed Spark applications properly.
