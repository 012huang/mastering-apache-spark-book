== DataStreamWriter

CAUTION: FIXME

[source, scala]
----
val df: DataFrame = ...

import org.apache.spark.sql.streaming.ProcessingTime
import scala.concurrent.duration._
df.writeStream
  .queryName("textStream")
  .trigger(ProcessingTime(10.seconds))
  .format("console")
  .start
----

=== [[outputMode]] Specifying Output Mode (outputMode method)

[source, scala]
----
outputMode(outputMode: OutputMode): DataStreamWriter[T]
----

`outputMode` specifies *output mode* of a streaming link:spark-sql-dataset.adoc[Dataset] which is what gets written to a link:spark-sql-sink.adoc[streaming sink] when there is a new data available.

Currently, the following output modes are supported:

* `OutputMode.Append` -- only the new rows in the streaming dataset will be written to a sink.

* `OutputMode.Complete` -- entire streaming dataset (with all the rows) will be written to a sink every time there are updates. It is supported only for streaming queries with aggregations.

=== [[queryName]] queryName

[source, scala]
----
queryName(queryName: String): DataStreamWriter[T]
----

`queryName` sets the name of a link:spark-sql-StreamingQuery.adoc[streaming query].

Internally, it is just an additional <<option, option>> with the key `queryName`.

=== [[trigger]] trigger

[source, scala]
----
trigger(trigger: Trigger): DataStreamWriter[T]
----

`trigger` sets the interval of trigger (batch) for the streaming query.

NOTE: `Trigger` specifies how often results should be produced by a link:spark-sql-StreamingQuery.adoc[StreamingQuery]. See link:spark-sql-trigger.adoc[Trigger].

The default trigger is `ProcessingTime(0L)` which means as fast as possible.

=== [[start]] start methods

[source, scala]
----
start(path: String): StreamingQuery
start(): StreamingQuery
----

=== [[foreach]] foreach
