== [[YarnSchedulerBackend]] YarnSchedulerBackend - Coarse-Grained Scheduler Backend for YARN

`YarnSchedulerBackend` is an abstract link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend] for YARN that contains common logic for the client and cluster YARN scheduler backends, i.e. link:spark-yarn-client-yarnclientschedulerbackend.adoc[YarnClientSchedulerBackend] and link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc[YarnClusterSchedulerBackend] respectively.

`YarnSchedulerBackend` expects link:spark-taskschedulerimpl.adoc[TaskSchedulerImpl] and link:spark-sparkcontext.adoc[SparkContext] to initialize itself.

It works for a single Spark application (as `appId` of type `ApplicationId`)

CAUTION: FIXME It may be a note for scheduler backends in general.

=== [[reset]] Resetting YarnSchedulerBackend

NOTE: It is a part of link:spark-scheduler-backends-coarse-grained.adoc#contract[CoarseGrainedSchedulerBackend Contract].

`reset` link:spark-scheduler-backends-coarse-grained.adoc#reset[resets] the parent `CoarseGrainedSchedulerBackend` scheduler backend and link:spark-service-executor-allocation-manager.adoc[ExecutorAllocationManager] (accessible by `SparkContext.executorAllocationManager`).

=== [[totalExpectedExecutors]] totalExpectedExecutors

`totalExpectedExecutors` is a value that is `0` <<creating-instance, initially when a `YarnSchedulerBackend` instance is created>> but later changes when Spark on YARN starts in  link:spark-yarn-client-yarnclientschedulerbackend.adoc#totalExpectedExecutors[client mode] or link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc#totalExpectedExecutors[cluster mode].

NOTE: After Spark on YARN is started, `totalExpectedExecutors` is initialized to a proper value.

It is used in <<sufficientResourcesRegistered, sufficientResourcesRegistered>>.

CAUTION: FIXME Where is this used?

=== [[initialization]][[creating-instance]] Creating YarnSchedulerBackend Instance

When created, `YarnSchedulerBackend` sets the internal <<minRegisteredRatio, minRegisteredRatio>> which is `0.8` when <<spark.scheduler.minRegisteredResourcesRatio, spark.scheduler.minRegisteredResourcesRatio>> is _not_ set or the link:spark-scheduler-backends-coarse-grained.adoc#minRegisteredRatio[parent's minRegisteredRatio].

<<totalExpectedExecutors, totalExpectedExecutors>> is set to `0`.

It creates a link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc[YarnSchedulerEndpoint] (as `yarnSchedulerEndpoint`) and registers it as *YarnScheduler* with the link:spark-rpc.adoc[RPC Environment].

It sets the internal `askTimeout` link:spark-rpc.adoc#ask-timeout[Spark timeout for RPC ask operations] using the `SparkContext` constructor parameter.

It sets optional `appId` (of type `ApplicationId`), `attemptId` (for cluster mode only and of type `ApplicationAttemptId`).

It also creates `SchedulerExtensionServices` object (as `services`).

CAUTION: FIXME What is `SchedulerExtensionServices`?

The internal <<shouldResetOnAmRegister, shouldResetOnAmRegister>> flag is turned off.

=== [[sufficientResourcesRegistered]] sufficientResourcesRegistered

`sufficientResourcesRegistered` checks whether `totalRegisteredExecutors` is greater than or equals to `totalExpectedExecutors` multiplied by <<minRegisteredRatio, minRegisteredRatio>>.

NOTE: It overrides the parent's link:spark-scheduler-backends-coarse-grained.adoc#sufficientResourcesRegistered[CoarseGrainedSchedulerBackend.sufficientResourcesRegistered].

CAUTION: FIXME Where's this used?

=== [[minRegisteredRatio]] minRegisteredRatio

`minRegisteredRatio` is set when <<creating-instance, `YarnSchedulerBackend` is created>>.

It is used in <<sufficientResourcesRegistered, sufficientResourcesRegistered>>.

=== [[start]] Starting the Backend (start method)

`start` creates a `SchedulerExtensionServiceBinding` object (using `SparkContext`, `appId`, and `attemptId`) and starts it (using `SchedulerExtensionServices.start(binding)`).

NOTE: A `SchedulerExtensionServices` object is created when <<initialization, YarnSchedulerBackend is initialized>> and available as `services`.

Ultimately, it calls the parent's link:spark-executor-backends-coarse-grained.adoc#start[CoarseGrainedSchedulerBackend.start].

[NOTE]
====
`start` throws `IllegalArgumentException` when the internal `appId` has not been set yet.

```
java.lang.IllegalArgumentException: requirement failed: application ID unset
```
====

=== [[stop]] Stopping the Backend (stop method)

`stop` calls the parent's link:spark-executor-backends-coarse-grained.adoc#requestTotalExecutors[CoarseGrainedSchedulerBackend.requestTotalExecutors] (using `(0, 0, Map.empty)` parameters).

CAUTION: FIXME Explain what `0, 0, Map.empty` means after the method's described for the parent.

It calls the parent's link:spark-executor-backends-coarse-grained.adoc#stop[CoarseGrainedSchedulerBackend.stop].

Ultimately, it stops the internal `SchedulerExtensionServiceBinding` object (using `services.stop()`).

CAUTION: FIXME Link the description of `services.stop()` here.

=== [[bindToYarn]] Recording Application and Attempt Ids (bindToYarn method)

[source, scala]
----
bindToYarn(appId: ApplicationId, attemptId: Option[ApplicationAttemptId]): Unit
----

`bindToYarn` sets the internal `appId` and `attemptId` to the value of the input parameters, `appId` and `attemptId`, respectively.

NOTE: <<start, start>> requires `appId`.

=== [[internal-registries]] Internal Registries

==== [[shouldResetOnAmRegister]] shouldResetOnAmRegister flag

When <<creating-instance, YarnSchedulerBackend is created>>, `shouldResetOnAmRegister` is disabled (i.e. `false`).

`shouldResetOnAmRegister` controls link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc#RegisterClusterManager[whether to reset `YarnSchedulerBackend` when another `RegisterClusterManager` RPC message arrives].

It allows resetting internal state after the initial  ApplicationManager failed and a new one was registered.

NOTE: It can only happen in link:spark-deploy-mode.adoc#client[client deploy mode].

=== [[settings]] Settings

==== [[spark.scheduler.minRegisteredResourcesRatio]] spark.scheduler.minRegisteredResourcesRatio

`spark.scheduler.minRegisteredResourcesRatio` (default: `0.8`)
