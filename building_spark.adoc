== Building Spark

You can download Apache Spark from http://spark.apache.org/downloads.html[the project's web site], but the packages are only for *Scala 2.10*.

If you want a *Scala 2.11* version of Apache Spark _"users should download the Spark source package and build with Scala 2.11 support"_ (quoted from the Note at http://spark.apache.org/downloads.html[Download Spark]).

The build process for Scala 2.11 takes around 15 mins (on a decent machine) and is so simple that it's hard to refuse the urge to do it yourself. It requires two steps:

1. Executing `./dev/change-scala-version.sh 2.11` in the source directory (so `pom.xml` use the version of Scala).
1. Executing the following build command:

    ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean install

The most important part is `-Dscala-2.11` for the Maven profile `scala-2.11` that triggers the build to use the latest and greatest Scala **2.11.7**.

After a couple of minutes your freshly baked distro is ready to fly!

I'm using Oracle Java 8 to build Spark.

```
➜  spark git:(master) ./dev/change-scala-version.sh 2.11
➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean install
Using `mvn` from path: /usr/local/bin/mvn
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Spark Project Parent POM
[INFO] Spark Project Launcher
[INFO] Spark Project Networking
[INFO] Spark Project Shuffle Streaming Service
[INFO] Spark Project Unsafe
[INFO] Spark Project Core
[INFO] Spark Project Bagel
[INFO] Spark Project GraphX
[INFO] Spark Project Streaming
[INFO] Spark Project Catalyst
[INFO] Spark Project SQL
[INFO] Spark Project ML Library
[INFO] Spark Project Tools
[INFO] Spark Project Hive
[INFO] Spark Project REPL
[INFO] Spark Project YARN Shuffle Service
[INFO] Spark Project YARN
[INFO] Spark Project Assembly
[INFO] Spark Project External Twitter
[INFO] Spark Project External Flume Sink
[INFO] Spark Project External Flume
[INFO] Spark Project External Flume Assembly
[INFO] Spark Project External MQTT
[INFO] Spark Project External MQTT Assembly
[INFO] Spark Project External ZeroMQ
[INFO] Spark Project External Kafka
[INFO] Spark Project Examples
[INFO] Spark Project External Kafka Assembly
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Parent POM 1.5.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] --- maven-clean-plugin:2.6.1:clean (default-clean) @ spark-parent_2.11 ---
...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [  2.938 s]
[INFO] Spark Project Launcher ............................. SUCCESS [ 10.974 s]
[INFO] Spark Project Networking ........................... SUCCESS [  9.889 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  6.377 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 10.682 s]
[INFO] Spark Project Core ................................. SUCCESS [02:12 min]
[INFO] Spark Project Bagel ................................ SUCCESS [  7.279 s]
[INFO] Spark Project GraphX ............................... SUCCESS [ 16.959 s]
[INFO] Spark Project Streaming ............................ SUCCESS [ 33.305 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [ 55.814 s]
[INFO] Spark Project SQL .................................. SUCCESS [ 58.444 s]
[INFO] Spark Project ML Library ........................... SUCCESS [01:12 min]
[INFO] Spark Project Tools ................................ SUCCESS [  4.781 s]
[INFO] Spark Project Hive ................................. SUCCESS [ 44.211 s]
[INFO] Spark Project REPL ................................. SUCCESS [  6.767 s]
[INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  7.852 s]
[INFO] Spark Project YARN ................................. SUCCESS [ 12.603 s]
[INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 13.249 s]
[INFO] Spark Project Assembly ............................. SUCCESS [01:33 min]
[INFO] Spark Project External Twitter ..................... SUCCESS [  8.075 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [  7.210 s]
[INFO] Spark Project External Flume ....................... SUCCESS [ 10.089 s]
[INFO] Spark Project External Flume Assembly .............. SUCCESS [  2.410 s]
[INFO] Spark Project External MQTT ........................ SUCCESS [ 16.326 s]
[INFO] Spark Project External MQTT Assembly ............... SUCCESS [  5.674 s]
[INFO] Spark Project External ZeroMQ ...................... SUCCESS [  7.946 s]
[INFO] Spark Project External Kafka ....................... SUCCESS [ 12.450 s]
[INFO] Spark Project Examples ............................. SUCCESS [01:27 min]
[INFO] Spark Project External Kafka Assembly .............. SUCCESS [  5.620 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 12:44 min
[INFO] Finished at: 2015-09-05T15:19:48+02:00
[INFO] Final Memory: 107M/1622M
[INFO] ------------------------------------------------------------------------
```

Please note the messages that say the version of Spark (_Building Spark Project Parent POM 1.5.0-SNAPSHOT_), Scala version (_maven-clean-plugin:2.6.1:clean (default-clean) @ spark-parent_2.11_) and the Spark modules built.

The above command gives you the latest version of *Apache Spark 1.5-SNAPSHOT* with *Scala 2.11.7* (see https://github.com/apache/spark/blob/master/pom.xml#L2436-L2445[the configuration of scala-2.11 profile]).

Check out the version of Spark using `./bin/spark-shell --version`:

```
➜  spark git:(master) ✗ ./bin/spark-shell --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.5.0-SNAPSHOT
      /_/

Type --help for more information.
```

Run Spark's shell using `./bin/spark-shell` script.

```
➜  spark git:(master) ✗ ./bin/spark-shell
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/08/31 14:11:44 INFO SecurityManager: Changing view acls to: jacek
15/08/31 14:11:44 INFO SecurityManager: Changing modify acls to: jacek
15/08/31 14:11:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacek); users with modify permissions: Set(jacek)
15/08/31 14:11:44 INFO HttpServer: Starting HTTP Server
15/08/31 14:11:44 INFO Utils: Successfully started service 'HTTP server' on port 55442.
15/08/31 14:11:48 INFO Main: Spark class server started at http://192.168.99.1:55442
15/08/31 14:11:48 INFO SparkContext: Running Spark version 1.5.0-SNAPSHOT
15/08/31 14:11:48 INFO SecurityManager: Changing view acls to: jacek
15/08/31 14:11:48 INFO SecurityManager: Changing modify acls to: jacek
15/08/31 14:11:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacek); users with modify permissions: Set(jacek)
15/08/31 14:11:48 INFO Slf4jLogger: Slf4jLogger started
15/08/31 14:11:48 INFO Remoting: Starting remoting
15/08/31 14:11:48 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.99.1:55443]
15/08/31 14:11:48 INFO Utils: Successfully started service 'sparkDriver' on port 55443.
15/08/31 14:11:48 INFO SparkEnv: Registering MapOutputTracker
15/08/31 14:11:48 INFO SparkEnv: Registering BlockManagerMaster
15/08/31 14:11:48 INFO DiskBlockManager: Created local directory at /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/blockmgr-6dc0c9cb-8d7a-488e-b088-66e0567f7b67
15/08/31 14:11:48 INFO MemoryStore: MemoryStore started with capacity 530.0 MB
15/08/31 14:11:48 INFO HttpFileServer: HTTP File server directory is /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-5fbb99d3-44d4-4e8d-8811-96fc173fb341/httpd-7e991721-8615-49fc-b1af-a014567790e6
15/08/31 14:11:48 INFO HttpServer: Starting HTTP Server
15/08/31 14:11:48 INFO Utils: Successfully started service 'HTTP file server' on port 55444.
15/08/31 14:11:48 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/31 14:11:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/31 14:11:48 INFO SparkUI: Started SparkUI at http://192.168.99.1:4040
15/08/31 14:11:48 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
15/08/31 14:11:48 INFO Executor: Starting executor ID driver on host localhost
15/08/31 14:11:48 INFO Executor: Using REPL class URI: http://192.168.99.1:55442
15/08/31 14:11:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55445.
15/08/31 14:11:49 INFO NettyBlockTransferService: Server created on 55445
15/08/31 14:11:49 INFO BlockManagerMaster: Trying to register BlockManager
15/08/31 14:11:49 INFO BlockManagerMasterEndpoint: Registering block manager localhost:55445 with 530.0 MB RAM, BlockManagerId(driver, localhost, 55445)
15/08/31 14:11:49 INFO BlockManagerMaster: Registered BlockManager
15/08/31 14:11:49 INFO Main: Created spark context..
Spark context available as sc.
15/08/31 14:11:49 INFO Main: Created sql context..
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.5.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

Spark is right under `sc` which is the http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[SparkContext] for the session.

Follow http://spark.apache.org/docs/latest/quick-start.html[Quick Start] to get started with Spark.

Close the Spark session using `Ctrl+D` or type `:quit`.

```
scala> :quit
15/08/31 14:15:38 INFO SparkUI: Stopped Spark web UI at http://192.168.99.1:4040
15/08/31 14:15:38 INFO DAGScheduler: Stopping DAGScheduler
15/08/31 14:15:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/31 14:15:38 INFO MemoryStore: MemoryStore cleared
15/08/31 14:15:38 INFO BlockManager: BlockManager stopped
15/08/31 14:15:38 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/31 14:15:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/31 14:15:38 INFO SparkContext: Successfully stopped SparkContext
15/08/31 14:15:38 INFO ShutdownHookManager: Shutdown hook called
15/08/31 14:15:38 INFO ShutdownHookManager: Deleting directory /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-4cfd9622-f495-4cd3-a07d-19591e640a61
15/08/31 14:15:38 INFO ShutdownHookManager: Deleting directory /private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/spark-5df5b0fd-a175-406c-985a-1686c9e0e95b
15/08/31 14:15:38 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/31 14:15:38 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
```
