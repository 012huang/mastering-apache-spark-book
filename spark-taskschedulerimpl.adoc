== TaskSchedulerImpl - Default TaskScheduler

`TaskSchedulerImpl` is the default implementation of link:spark-taskscheduler.adoc#contract[TaskScheduler Contract]. It can schedule tasks for multiple types of cluster managers by using a link:spark-scheduler-backends.adoc[Scheduler Backend].

When your Spark application starts, i.e. at the time when an instance of SparkContext is created, TaskSchedulerImpl with a SchedulerBackend and DAGScheduler are created and started, too.

.TaskSchedulerImpl and Other Services
image::images/taskschedulerimpl-sparkcontext-schedulerbackend-dagscheduler.png[align="center"]

CAUTION: FIXME Why `hasReceivedTask` and `hasLaunchedTask`?

NOTE: The source code is in https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala[org.apache.spark.scheduler.TaskSchedulerImpl].

=== [[starting]] Starting

When TaskSchedulerImpl is started (using `start()` method), it starts the link:spark-scheduler-backends.adoc[scheduler backend] it manages.

Below is a figure of the method calls in Spark Standalone mode.

.Starting TaskSchedulerImpl in Spark Standalone mode
image::images/taskschedulerimpl-start-standalone.png[align="center"]

TIP: SparkContext starts a TaskScheduler.

=== [[postStartHook]] Post-Start Initialization

TaskSchedulerImpl comes with its own `postStartHook()` (see <<contract, TaskScheduler Contract>>) to wait until a scheduler backend is ready (see link:spark-scheduler-backends.adoc#contract[SchedulerBackend Contract]).

Internally, it uses `waitBackendReady()` to do the waiting and looping.

=== [[stopping]] Stopping

When TaskSchedulerImpl is stopped (using `stop()` method), it does the following:

* Shuts down the internal `task-scheduler-speculation` thread pool executor (used for <<speculative-execution, Speculative execution of tasks>>).
* Stops link:spark-scheduler-backends.adoc[SchedulerBackend].
* Stops link:spark-taskscheduler.adoc#TaskResultGetter[TaskResultGetter].
* Cancels `starvationTimer` timer.

=== [[defaultParallelism]] Default Level of Parallelism

*Default level of parallelism* is a hint for sizing jobs.

`TaskSchedulerImpl` uses link:spark-scheduler-backends.adoc#defaultParallelism[SchedulerBackend.defaultParallelism()] to calculate the value, i.e. it just passes it along to a scheduler backend.

=== [[submitTasks]] Task Submission

link:spark-taskscheduler-tasks.adoc[Tasks] are submitted for execution as a link:spark-taskscheduler-tasksets.adoc[TaskSet] using `submitTasks(taskSet: TaskSet)` method.

.TaskScheduler.submitTasks
image::images/taskscheduler-submitTasks.png[align="center"]

If there are tasks to launch for missing partitions in a stage, DAGScheduler executes `submitTasks` (see link:spark-dagscheduler.adoc#submitMissingTasks[submitMissingTasks for Stage and Job]).

When this method is called, you should see the following INFO message in the logs:

```
INFO TaskSchedulerImpl: Adding task set [taskSet.id] with [tasks.length] tasks
```

It creates a new link:spark-tasksetmanager.adoc[TaskSetManager] for the given TaskSet and the acceptable number of task failures.

CAUTION: FIXME There are other steps not included here.

It then calls `backend.reviveOffers()`.

TIP: Use `dag-scheduler-event-loop` thread to step through the code in a debugger.

=== [[resourceOffers]] Resource Offers

`resourceOffers(offers: Seq[WorkerOffer])` method is called by a cluster manager or link:spark-local.adoc#LocalBackend[LocalBackend] (for local mode) to offer free resources available on the executors to run tasks on.

.TaskSchedulerImpl.resourceOffers under the hood
image::images/taskscheduler-resourceOffers.png[align="center"]

A `WorkerOffer` is a 3-tuple with executor id, host, and free cores available.

For each offer, the `resourceOffers` method tracks hosts per executor id (using `executorIdToHost`) and sets `0` as the number of tasks running on the executor if there is no tasks running already (using `executorIdToTaskCount`). It also tracks executor id per host.

WARNING: FIXME BUG? Why is the executor id *not* added to `executorsByHost`?

`DAGScheduler.executorAdded(executorId, host)` is called for a new host.

WARNING: FIXME BUG? Why is `executorAdded` called for a new host added? Can't we have more executors on a host? The name of the method is misleading then.

CAUTION: FIXME a picture with `executorAdded` call from TaskSchedulerImpl to DAGScheduler.

CAUTION: FIXME Why is `getRackForHost` important?

It builds a list of tasks (using `TaskDescription`) to assign to each worker.

CAUTION: FIXME What does the line `val sortedTaskSets = rootPool.getSortedTaskSetQueue` mean? It uses no method's local variables. There can be many TaskSetManagers in the result.

`rootPool.getSortedTaskSetQueue` is called to build a collection of tasksets (as TaskSetManagers in `sortedTaskSets`).

For each taskset (represented by a TaskSetManager), the following DEBUG message is printed out to the logs:

```
DEBUG parentName: [taskSet.parent.name], name: [taskSet.name], runningTasks: [taskSet.runningTasks]
```

And if a new host was added to the pool (using `newExecAvail` - FIXME when exactly?), each TaskSetManager gets notified about new executor added (using `TaskSetManager.executorAdded()`). FIXME So what?

WARNING: FIXME BUG? Why is the name `newExecAvail` since it's called for a new host added? Can't we have more executors on a host? The name of the method could be misleading.

For each taskset in `sortedTaskSets`, different locality preferences are checked...FIXME

Check whether the number of cores in an offer is more than the number of cores needed for a task (using <<settings, spark.task.cpus>>).

When `resourceOffers` managed to launch a task, the internal field `hasLaunchedTask` is set.

CAUTION: FIXME Why is there a need for `hasLaunchedTask`? Can TaskSchedulerImpl launch more tasks later?

=== [[internal-data-structures]] Internal Data Structures

CAUTION: FIXME How/where are these mappings used?

TaskSchedulerImpl tracks the following information in its internal data structures:

* the number of link:spark-taskscheduler-tasks.adoc[tasks] already scheduled for execution (`nextTaskId`).
* link:spark-taskscheduler-tasksets.adoc[TaskSets] by stage and attempt ids (`taskSetsByStageIdAndAttempt`)
* link:spark-taskscheduler-tasks.adoc[tasks] to their link:spark-tasksetmanager.adoc[TaskSetManagers] (`taskIdToTaskSetManager`)
* link:spark-taskscheduler-tasks.adoc[tasks] to link:spark-executor.adoc[executors] (`taskIdToExecutorId`)
* the number of link:spark-taskscheduler-tasks.adoc[tasks] running per link:spark-executor.adoc[executor] (`executorIdToTaskCount`)
* the set of link:spark-executor.adoc[executors] on each host (`executorsByHost`)
* the set of hosts per rack (`hostsByRack`)
* executor ids to corresponding host (`executorIdToHost`).

=== [[speculative-execution]] Speculative execution of tasks

*Speculative tasks* (also *speculatable tasks* or *task strugglers*) are tasks that run slower than most of the all tasks in a job.

*Speculative execution of tasks* is a health-check procedure that checks for tasks to be *speculated*, i.e. running slower in a stage than the median of all successfully completed tasks in a taskset. Such slow tasks will be re-launched in another worker. It will not stop the slow tasks, but run a new copy in parallel.

It is executed periodically by the TaskScheduler for link:spark-cluster.adoc[clustered deployment modes], when link:spark-tasksetmanager.adoc#tasksetmanager-settings[spark.speculation] is enabled (`true`).

With `spark.speculation` enabled, the following INFO message appears in the logs:

```
INFO Starting speculative execution thread
```

It is scheduled using *task-scheduler-speculation* daemon thread pool using `j.u.c.ScheduledThreadPoolExecutor` with core pool size `1`.

It is executed for link:spark-tasksetmanager.adoc#zombie-state[non-zombie TaskSetManagers] with more than one task to execute.

The process computes link:spark-tasksetmanager.adoc#tasksetmanager-settings[spark.speculation.quantile] of all the tasks and checks whether the number is greater than the number of tasks completed successfully.

You can find the DEBUG message in the logs:

```
DEBUG Checking for speculative tasks: minFinished =
```

It then computes the median duration of all the completed task length threshold for speculation to have it multiplied by link:spark-tasksetmanager.adoc#tasksetmanager-settings[spark.speculation.multiplier]. It has to be at least `100`.

In the logs at DEBUG level:

```
DEBUG Task length threshold for speculation:
```

For each active task for which there is only one copy running and the task takes more time than the threshold, it gets marked as *speculatable*.

In the logs at INFO level:

```
INFO Marking task %d in stage %s (on %s) as speculatable because it ran more than %.0f ms
```

The job with speculatable tasks should finish while speculative tasks are running, and it will leave these tasks running - no KILL command yet.

The check procedure is in link:spark-tasksetmanager.adoc[TaskSetManager.checkSpeculatableTasks] method.

1. How does Spark handle repeated results of speculative tasks since there are copies launched?

=== [[TaskResultGetter]] TaskResultGetter

`TaskResultGetter` is a helper class for <<statusUpdate, TaskSchedulerImpl.statusUpdate>>. It _asynchronously_ fetches the task results of tasks that have finished successfully (using <<enqueueSuccessfulTask, enqueueSuccessfulTask>>) or fetches the reasons of failures for failed tasks (using <<enqueueFailedTask, enqueueFailedTask>>). It then sends the "results" back to `TaskSchedulerImpl`.

CAUTION: FIXME Image with the dependencies

TIP: Consult link:spark-taskscheduler-tasks.adoc#states[Task States] in Tasks to learn about the different task states.

The only instance of `TaskResultGetter` is created as part of a `TaskSchedulerImpl` creation (as `taskResultGetter`) . It requires a `SparkEnv` and `TaskSchedulerImpl`. It is stopped when `TaskSchedulerImpl` stops.

`TaskResultGetter` offers the following methods:

* <<enqueueSuccessfulTask, enqueueSuccessfulTask>>
* <<enqueueFailedTask, enqueueFailedTask>>

The methods use the internal (daemon thread) thread pool *task-result-getter* (as `getTaskResultExecutor`) with <<settings, spark.resultGetter.threads>> so they can be executed asynchronously.

==== [[enqueueSuccessfulTask]] enqueueSuccessfulTask

`enqueueSuccessfulTask(taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer)` first deserializes `TaskResult` (from `serializedData` using `SparkEnv.closureSerializer`).

If the result is `DirectTaskResult`, the method checks `taskSetManager.canFetchMoreResults(serializedData.limit())` and possibly quits. If not, it deserializes the result (using `SparkEnv.serializer`).

CAUTION: FIXME Review `taskSetManager.canFetchMoreResults(serializedData.limit())`.

If the result is `IndirectTaskResult`, the method checks `taskSetManager.canFetchMoreResults(size)` and possibly removes the block id (using `SparkEnv.blockManager.master.removeBlock(blockId)`) and quits. If not, you should see the following DEBUG message in the logs:

```
DEBUG Fetching indirect task result for TID [tid]
```

`scheduler.handleTaskGettingResult(taskSetManager, tid)` gets called. And `sparkEnv.blockManager.getRemoteBytes(blockId)`.

Failure in getting task result from BlockManager results in calling `scheduler.handleFailedTask(taskSetManager, tid, TaskState.FINISHED, TaskResultLost)` and quit.

The task result is deserialized to `DirectTaskResult` (using `SparkEnv.closureSerializer`) and `sparkEnv.blockManager.master.removeBlock(blockId)` is called afterwards.

`scheduler.handleSuccessfulTask(taskSetManager, tid, result)` is called.

Any `ClassNotFoundException` or non fatal exceptions lead to `TaskSetManager.abort`.

==== [[enqueueFailedTask]] enqueueFailedTask

CAUTION: FIXME

==== [[settings]] Settings

* `spark.resultGetter.threads` (default: `4`) - the number of threads for <<TaskResultGetter, TaskResultGetter>>.

=== [[statusUpdate]] statusUpdate

`statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer)` is called by link:spark-scheduler-backends.adoc[scheduler backends] to inform about task state changes (see link:spark-taskscheduler-tasks.adoc#states[Task States] in Tasks).

CAUTION: FIXME image with scheduler backends calling `TaskSchedulerImpl.statusUpdate`.

It is called by:

* link:spark-scheduler-backends-coarse-grained.adoc[CoarseGrainedSchedulerBackend] when `StatusUpdate(executorId, taskId, state, data)` comes.
* link:spark-mesos.adoc#MesosSchedulerBackend[MesosSchedulerBackend] when `org.apache.mesos.Scheduler.statusUpdate` is called.
* link:spark-local.adoc#LocalEndpoint[LocalEndpoint] when `StatusUpdate(taskId, state, serializedData)` comes.

When `statusUpdate` starts, it checks the current state of the task and act accordingly.

If a task became `TaskState.LOST` and there is still an executor assigned for the task (it seems it may not given the check), the executor is marked as lost (or sometimes called failed). The executor is later announced as such using `DAGScheduler.executorLost` with link:spark-scheduler-backends.adoc#reviveOffers[SchedulerBackend.reviveOffers()] being called afterwards.

CAUTION: FIXME Why is link:spark-scheduler-backends.adoc#reviveOffers[SchedulerBackend.reviveOffers()] called only for lost executors?

CAUTION: FIXME Review `TaskSchedulerImpl.removeExecutor`

The method looks up the link:spark-tasksetmanager.adoc[TaskSetManager] for the task (using `taskIdToTaskSetManager`).

When the TaskSetManager is found and the task is in finished state, the task is removed from the internal data structures, i.e. `taskIdToTaskSetManager` and `taskIdToExecutorId`, and the number of currently running tasks for the executor(s) is decremented (using `executorIdToTaskCount`).

For a `FINISHED` task, link:spark-taskscheduler-tasksets.adoc[TaskSet.removeRunningTask] is called and then <<TaskResultGetter, TaskResultGetter.enqueueSuccessfulTask>>.

For a task in `FAILED`, `KILLED`, or `LOST` state, link:spark-taskscheduler-tasksets.adoc[TaskSet.removeRunningTask] is called (as for the `FINISHED` state) and then <<TaskResultGetter,TaskResultGetter.enqueueFailedTask>>.

If the TaskSetManager could not be found, the following ERROR shows in the logs:

```
ERROR Ignoring update with state [state] for TID [tid] because its task set is gone (this is likely the result of receiving duplicate task finished status updates)
```

=== [[scheduling-mode]] Scheduling Modes

Task Scheduler uses a scheduling mode that determines policy to order tasks across a Schedulable's sub-queues.

It is configured by <<settings, spark.scheduler.mode>> setting that can accept the following values:

* *FIFO* - no pools; one root pool with link:spark-tasksetmanager.adoc[TaskSetManager]; lower priority gets Schedulable sooner or earlier stage wins.
* *FAIR* - more advanced FIXME
* *NONE* means no sub-queues

=== [[settings]] Settings

* `spark.task.maxFailures` (default: `4` for link:spark-cluster.adoc[cluster mode] and `1` for link:spark-local.adoc[local] except link:spark-local.adoc[local-with-retries]) - The number of individual task failures before giving up on the entire TaskSet and the job afterwards.
+
It is used in TaskSchedulerImpl to initialize link:spark-tasksetmanager.adoc[TaskSetManager].
* `spark.task.cpus` (default: `1`) - how many CPUs to request per task in a SparkContext. You cannot have different number of CPUs per task in a single SparkContext.
* `spark.scheduler.mode` (default: `FIFO`) can be of any of `FAIR`, `FIFO`, or `NONE`. Refer to <<scheduling-mode, scheduling mode>>.
* `spark.speculation.interval` (default: `100ms`) - how often to check for speculative tasks.
* `spark.starvation.timeout` (default: `15s`) - Threshold above which Spark warns a user that an initial TaskSet may be starved.
