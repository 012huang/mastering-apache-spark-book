== [[WindowExec]] WindowExec Unary Physical Operator

`WindowExec` is a link:spark-sql-SparkPlan.adoc#UnaryExecNode[unary physical operator] that represents link:spark-sql-LogicalPlan-Window.adoc[Window] unary logical operator at execution.

[source, scala]
----
val dataset = spark.range(start = 0, end = 13, step = 1, numPartitions = 4)

import org.apache.spark.sql.expressions.Window
val groupsOrderedById = Window.partitionBy('group).rangeBetween(-2, Window.currentRow).orderBy('id)
val query = dataset.
  withColumn("group", 'id % 4).
  select('*, sum('id) over groupsOrderedById as "sum")

scala> query.explain
== Physical Plan ==
Window [sum(id#25L) windowspecdefinition(group#244L, id#25L ASC NULLS FIRST, RANGE BETWEEN 2 PRECEDING AND CURRENT ROW) AS sum#249L], [group#244L], [id#25L ASC NULLS FIRST]
+- *Sort [group#244L ASC NULLS FIRST, id#25L ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(group#244L, 200)
      +- *Project [id#25L, (id#25L % 4) AS group#244L]
         +- *Range (0, 13, step=1, splits=4)

val plan = query.queryExecution.executedPlan
import org.apache.spark.sql.execution.window.WindowExec
val we = plan.asInstanceOf[WindowExec]
----

.WindowExec in web UI (Details for Query)
image::images/spark-sql-WindowExec-webui-query-details.png[align="center"]

`WindowExec` is <<creating-instance, created>> exclusively when link:spark-sql-SparkStrategy-BasicOperators.adoc#Window[BasicOperators] execution planning strategy converts link:spark-sql-LogicalPlan-Window.adoc[Window] unary logical operator.

[[output]]
The link:spark-sql-catalyst-QueryPlan.adoc#output[output schema] of `WindowExec` are the link:spark-sql-Expression-Attribute.adoc[attributes] of <<child, child>> physical operator and <<windowExpression, window expressions>>.

[source, scala]
----
val schema = query.queryExecution.executedPlan.output.toStructType
scala> println(schema.treeString)
root
 |-- id: long (nullable = false)
 |-- group: long (nullable = true)
 |-- sum: long (nullable = true)

// we is WindowExec created earlier
// child's output
scala> println(we.child.output.toStructType.treeString)
root
 |-- id: long (nullable = false)
 |-- group: long (nullable = true)

// window expressions' output
scala> println(we.windowExpression.map(_.toAttribute).toStructType.treeString)
root
 |-- sum: long (nullable = true)
----

[[requiredChildDistribution]]
.WindowExec's Required Child Output Distribution
[cols="1",options="header",width="100%"]
|===
| Single Child

| `ClusteredDistribution` (per <<partitionSpec, window partition specifications expressions>>)
|===

If no window partition specification is specified, `WindowExec` prints out the following WARN message to the logs (and the child's distribution requirement is `AllTuples`):

```
WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
```

[TIP]
====
Enable `WARN` logging level for `org.apache.spark.sql.execution.WindowExec` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.WindowExec=WARN
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[doExecute]] Generating Internal Binary Rows -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

Internally, `doExecute` link:spark-sql-SparkPlan.adoc#execute[executes the child physical operator] and maps over partitions (using `RDD.mapPartitions` operator).

```
scala> :type we
org.apache.spark.sql.execution.window.WindowExec

scala> val rdd = we.execute
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.catalyst.InternalRow] = MapPartitionsRDD[44] at execute at <console>:38

scala> println(rdd.toDebugString)
(200) MapPartitionsRDD[42] at execute at <console>:39 []
  |   MapPartitionsRDD[41] at execute at <console>:39 []
  |   ShuffledRowRDD[34] at execute at <console>:39 []
  +-(4) MapPartitionsRDD[33] at execute at <console>:39 []
     |  MapPartitionsRDD[32] at execute at <console>:39 []
     |  MapPartitionsRDD[31] at execute at <console>:39 []
     |  ParallelCollectionRDD[30] at execute at <console>:39 []
```

`doExecute` creates a `Iterator[InternalRow]` that...FIXME

CAUTION: FIXME

NOTE: `doExecute` uses link:spark-sql-SQLConf.adoc#spark.sql.windowExec.buffer.spill.threshold[spark.sql.windowExec.buffer.spill.threshold] property (default: `4096`) as the threshold for the number of rows buffered in `ExternalAppendOnlyUnsafeRowArray` buffer.

NOTE: `ExternalAppendOnlyUnsafeRowArray` is used to collect `UnsafeRow` objects from the child's partitions (one partition per buffer and up to `spark.sql.windowExec.buffer.spill.threshold`).

NOTE: `doExecute` is a part of link:spark-sql-SparkPlan.adoc#doExecute[SparkPlan Contract] to produce the result of a structured query as an `RDD` of link:spark-sql-InternalRow.adoc[internal binary rows].

=== [[creating-instance]] Creating WindowExec Instance

`WindowExec` takes the following when created:

* [[windowExpression]] Window link:spark-sql-Expression.adoc#NamedExpression[named expressions]
* [[partitionSpec]] Window partition specifications link:spark-sql-Expression.adoc[expressions]
* [[orderSpec]] Collection of `SortOrder` objects for window order specifications
* [[child]] Child link:spark-sql-SparkPlan.adoc[physical operator]

=== [[windowFrameExpressionFactoryPairs]] Lookup Table for WindowExpressions and Factory Functions for WindowFunctionFrame -- `windowFrameExpressionFactoryPairs` Lazy Value

[source, scala]
----
windowFrameExpressionFactoryPairs:
  Seq[(mutable.Buffer[WindowExpression], InternalRow => WindowFunctionFrame)]
----

`windowFrameExpressionFactoryPairs` is a lookup table with <<windowFrameExpressionFactoryPairs-two-element-expression-list-value, window expressions>> and <<windowFrameExpressionFactoryPairs-factory-functions, factory functions>> for `WindowFunctionFrame` (per key-value pair in `framedFunctions` lookup table).

A factory function is a function that takes an link:spark-sql-InternalRow.adoc[InternalRow] and produces a `WindowFunctionFrame` (described in the table below)

Internally, `windowFrameExpressionFactoryPairs` first builds `framedFunctions` lookup table with <<windowFrameExpressionFactoryPairs-four-element-tuple-key, 4-element tuple keys>> and <<windowFrameExpressionFactoryPairs-two-element-expression-list-value, 2-element expression list values>> (described in the table below).

`windowFrameExpressionFactoryPairs` finds link:spark-sql-Expression-WindowExpression.adoc[WindowExpression] expressions in the input <<windowExpression, windowExpression>> and for every `WindowExpression` takes the link:spark-sql-Expression-WindowSpecDefinition.adoc#frameSpecification[window frame specification] (of type `SpecifiedWindowFrame` that is used to find frame type and start and end frame positions).

[[windowFrameExpressionFactoryPairs-four-element-tuple-key]]
.framedFunctions's FrameKey -- 4-element Tuple for Frame Keys (in positional order)
[cols="1,2",options="header",width="100%"]
|===
| Element
| Description

| Name of the kind of function
a|

* *AGGREGATE* for link:spark-sql-Expression-AggregateFunction.adoc[AggregateFunction] (in link:spark-sql-Expression-AggregateExpression.adoc[AggregateExpression]s) or `AggregateWindowFunction`

* *OFFSET* for `OffsetWindowFunction`

| `FrameType`
| `RangeFrame` or `RowFrame`

| Window frame's start position
a|

* Positive number for `CurrentRow` (0) and `ValueFollowing`
* Negative number for `ValuePreceding`
* Empty when unspecified

| Window frame's end position
a|

* Positive number for `CurrentRow` (0) and `ValueFollowing`
* Negative number for `ValuePreceding`
* Empty when unspecified
|===

[[windowFrameExpressionFactoryPairs-two-element-expression-list-value]]
.framedFunctions's 2-element Tuple Values (in positional order)
[cols="1,2",options="header",width="100%"]
|===
| Element
| Description

| Collection of window expressions
| link:spark-sql-Expression-WindowExpression.adoc[WindowExpression]

| Collection of window functions
a|

* link:spark-sql-Expression-AggregateFunction.adoc[AggregateFunction] (in link:spark-sql-Expression-AggregateExpression.adoc[AggregateExpression]s) or `AggregateWindowFunction`

* `OffsetWindowFunction`
|===

`windowFrameExpressionFactoryPairs` creates a `AggregateProcessor` for `AGGREGATE` frame keys in `framedFunctions` lookup table.

[[windowFrameExpressionFactoryPairs-factory-functions]]
.windowFrameExpressionFactoryPairs' Factory Functions (in creation order)
[cols="1,2,2",options="header",width="100%"]
|===
| Frame Name
| FrameKey
| WindowFunctionFrame

| Offset Frame
| `("OFFSET", RowFrame, Some(offset), Some(h))`
| `OffsetWindowFunctionFrame`

| Growing Frame
| `("AGGREGATE", frameType, None, Some(high))`
| `UnboundedPrecedingWindowFunctionFrame`

| Shrinking Frame
| `("AGGREGATE", frameType, Some(low), None)`
| `UnboundedFollowingWindowFunctionFrame`

| Moving Frame
| `("AGGREGATE", frameType, Some(low), Some(high))`
| `SlidingWindowFunctionFrame`

| Entire Partition Frame
| `("AGGREGATE", frameType, None, None)`
| `UnboundedWindowFunctionFrame`
|===

NOTE: `lazy val` in Scala is computed when first accessed and once only (for the entire lifetime of the owning object instance).

NOTE: `windowFrameExpressionFactoryPairs` is used exclusively when `WindowExec` is <<doExecute, executed>>.
