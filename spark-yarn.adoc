== Spark on YARN

=== Introduction to YARN

http://www.ibm.com/developerworks/library/bd-yarn-intro/index.html[Introduction to YARN] by *Adam Kawa* is an excellent introduction to YARN. Here are the most important facts to get you going.

* Hadoop 2.0 comes with *Yet Another Resource Negotiator (YARN)* that is _a generic cluster resource management framework that can run applications on a Hadoop cluster._ (see http://twill.incubator.apache.org/[Apache Twill])

* YARN model of computation (aka YARN components):
** *ResourceManager* runs as a master daemon and manages ApplicationMasters and NodeManagers.
** *ApplicationMaster* is a lightweight process that coordinates the execution of tasks of an application and asks the ResourceManager for resource containers for tasks. It monitors tasks, restarts failed ones, etc. It can run any type of tasks, be them MapReduce tasks or Giraph tasks, or Spark tasks.
** *NodeManager* offers resources (memory and CPU) as resource containers.
** *NameNode*
** *Container* can run tasks, including ApplicationMasters.
* YARN manages distributed applications.
* YARN offers (macro-level) container allocation.
* Hadoop for storing and processing large amount of data on a cluster of commodity hardware.
* The Pre-YARN MapReduce engine - *MRv1* - was rewritten for YARN. It became yet another YARN distributed application called *MRv2*.

There's another article that covers the fundamentals of YARN - http://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/[Untangling Apache Hadoop YARN, Part 1]. Notes follow:

* A *host* is the Hadoop term for a computer (also called a *node*, in YARN terminology).
* A *cluster* is two or more hosts connected by a high-speed local network.
** It can technically also be a single host used for debugging and simple testing.
** Master hosts are a small number of hosts reserved to control the rest of the cluster. Worker hosts are the non-master hosts in the cluster.
** A *master* host is the communication point for a client program. A master host sends the work to the rest of the cluster, which consists of *worker* hosts.
* In a YARN cluster, there are two types of hosts:
** The *ResourceManager* is the master daemon that communicates with the client, tracks resources on the cluster, and orchestrates work by assigning tasks to NodeManagers.
*** In a Hadoop cluster with YARN running, the master process is called the ResourceManager and the worker processes are called NodeManagers.
** A *NodeManager* is a worker daemon that launches and tracks processes spawned on worker hosts.
** The NodeManager on each host keeps track of the local host’s resources, and the ResourceManager keeps track of the cluster’s total.
* The YARN configuration file is an XML file that contains properties. This file is placed in a well-known location on each host in the cluster and is used to configure the ResourceManager and NodeManager. By default, this file is named `yarn-site.xml`.
* YARN currently defines two resources, vcores and memory.
** vcore = usage share of a CPU core.
* Each NodeManager tracks its own local resources and communicates its resource configuration to the ResourceManager, which keeps a running total of the cluster’s available resources.
** By keeping track of the total, the ResourceManager knows how to allocate resources as they are requested.
* A *container* in YARN holds resources on the YARN cluster.
** A container hold request consists of vcore and memory.
* Once a hold has been granted on a host, the NodeManager launches a process called a *task*.
* An application is a YARN client program that is made up of one or more tasks.
* For each running application, a special piece of code called an ApplicationMaster helps coordinate tasks on the YARN cluster. The ApplicationMaster is the first process run after the application starts.
* An application running tasks on a YARN cluster consists of the following steps:
** The application starts and talks to the ResourceManager (running on the master) for the cluster.
** The ResourceManager makes a single container request on behalf of the application.
** The ApplicationMaster starts running within that container.
** The ApplicationMaster requests subsequent containers from the ResourceManager that are allocated to run tasks for the application. Those tasks do most of the status communication with the ApplicationMaster.
** Once all tasks are finished, the ApplicationMaster exits. The last container is de-allocated from the cluster.
** The application client exits. (The ApplicationMaster launched in a container is more specifically called a managed AM).
* The ResourceManager, NodeManager, and ApplicationMaster work together to manage the cluster’s resources and ensure that the tasks, as well as the corresponding application, finish cleanly.

[CAUTION]
====
FIXME: Where is `ApplicationMaster.registerAM` used?

* Registering the ApplicationMaster with the RM.
* Contains a map with hints about where to allocate containers.
====

=== How it works

The Spark driver in Spark on YARN launches a number of executors. Each executor processes a partition of HDFS-based data.

=== YarnAllocator

`YarnAllocator` requests containers from the YARN ResourceManager and decides what to do with containers when YARN fulfills these requests. It uses YARN's AMRMClient APIs.

=== ExecutorAllocationClient

`ExecutorAllocationClient` is a client class that communicates with the cluster manager to request or kill executors. This is currently supported only in YARN mode.

CAUTION: FIXME See the code and deduce its use.

=== Misc

* `SPARK_YARN_MODE` property and environment variable
** `true` when `yarn-client` used for master URL
** It's set by Spark internally for YARN mode
* `yarn-cluster` and `yarn-client` modes
* `spark-submit --deploy-mode cluster`
* `org.apache.spark.deploy.yarn.YarnSparkHadoopUtil`
* YARN integration has some advantages, like link:spark-dynamic-allocation.adoc[dynamic allocation]. If you enable dynamic allocation, after the stage including InputSplits gets submitted, Spark will try to request an appropriate number of executors.
* On YARN, a Spark executor maps to a single YARN container.
* The memory in the YARN resource requests is `--executor-memory` + what's set for `spark.yarn.executor.memoryOverhead`, which defaults to 10% of `--executor-memory`.
* if YARN has enough resources it will deploy the executors distributed across the cluster, then each of them will try to process the data locally (`NODE_LOCAL` in Spark Web UI), with as many splits in parallel as you defined in `spark.executor.cores`.
* _"YarnClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources"_
* Mandatory settings (`spark-defaults.conf`) for dynamic allocation:
+
```
spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
```
* Optional settings for dynamic allocation (to tune it):
+
```
spark.dynamicAllocation.minExecutors     0
spark.dynamicAllocation.maxExecutors     N
spark.dynamicAllocation.initialExecutors 0
```
* `spark.dynamicAllocation.minExecutors` requires `spark.dynamicAllocation.initialExecutors`
* Review `spark.dynamicAllocation.*` settings
* YARN UI under scheduler - pools where Spark operates
* `spark.scheduler.mode=FIFO` on the Spark UI
