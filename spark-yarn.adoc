== Spark on YARN

Spark on YARN supports <<multiple-application-attempts, multiple application attempts>>.

The <<masterURL, master URL>> can be `yarn` or (deprecated since Spark 2.0) one of `yarn-client` or `yarn-cluster`.

NOTE: Since Spark *2.0.0*, `yarn` master URL is the only proper master URL.

You need to have <<yarn-support, Spark that has been compiled with YARN support>>.

There are few settings that are specific to YARN and not so much (see <<settings, Settings>>).

TIP: You can start link:spark-submit.adoc[spark-submit] with `--verbose` command-line option to have some settings displayed, including YARN-specific. See <<spark-submit, spark-submit and YARN options>>.

=== [[spark-submit]] spark-submit and YARN options

When you submit your Spark applications using link:spark-submit.adoc[spark-submit] you can use the following YARN-specific command-line options:

* `--archives`
* `--executor-cores`
* `--keytab`
* `--num-executors`
* `--principal`
* `--queue`

TIP: Read about the corresponding settings in <<settings, Settings>> in this document.

=== [[yarn-support]] Spark with YARN support

You need to have Spark that link:spark-building-from-sources.adoc[has been compiled with YARN support], i.e. the class link:spark-yarn-client.adoc[org.apache.spark.deploy.yarn.Client] must be on the CLASSPATH.

Otherwise, you will see the following error in the logs and Spark will exit.

```
Error: Could not load YARN classes. This copy of Spark may not have been compiled with YARN support.
```

=== [[masterURL]] Master URL

Since Spark *2.0.0*, the only proper master URL is `yarn`.

```
./bin/spark-submit --master yarn ...
```

Before Spark 2.0.0, you could have used `yarn-client` or `yarn-cluster`, but it is now deprecated. When you use the master URL other than `yarn` you will see the following warning in the logs:

```
Warning: Master yarn-client is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
```

=== [[keytab]] Keytab

CAUTION: FIXME

When a principal is specified a keytab must be specified, too.

The settings `spark.yarn.principal` and `spark.yarn.principal` will be set to respective values and `UserGroupInformation.loginUserFromKeytab` will be called with their values as input arguments.

=== [[settings]] Settings

CAUTION: FIXME Where and how are they used?

The following settings are specific to Spark on YARN:

* `spark.yarn.queue` -- See the corresponding  <<spark-submit, --queue command-line option for spark-submit>>.
* `spark.yarn.dist.jars`
* `spark.yarn.dist.files`
* `spark.yarn.dist.archives` -- See the corresponding <<spark-submit, --archives command-line option for spark-submit>>.
* `spark.yarn.principal` -- See the corresponding <<spark-submit, --principal command-line option for spark-submit>>.
* `spark.yarn.keytab` -- See the corresponding <<spark-submit, --keytab command-line option for spark-submit>>.

There are others which are supported:

* `spark.executor.cores`
* `spark.executor.memory`
* `spark.driver.memory`
* `spark.driver.cores`

=== How it works

The Spark driver in Spark on YARN launches a number of executors. Each executor processes a partition of HDFS-based data.

=== YarnAllocator

`YarnAllocator` requests containers from the YARN ResourceManager and decides what to do with containers when YARN fulfills these requests. It uses YARN's AMRMClient APIs.

=== [[executor-allocation-client]] ExecutorAllocationClient

*ExecutorAllocationClient* is a client class that communicates with the cluster manager to request or kill executors.

This is currently supported only in YARN mode.

CAUTION: FIXME See the code and deduce its use.

=== Misc

* `SPARK_YARN_MODE` property and environment variable
** `true` when `yarn-client` used for master URL
** It's set by Spark internally for YARN mode
* `yarn-cluster` and `yarn-client` modes
* `spark-submit --deploy-mode cluster`
* `org.apache.spark.deploy.yarn.YarnSparkHadoopUtil`
* YARN integration has some advantages, like link:spark-dynamic-allocation.adoc[dynamic allocation]. If you enable dynamic allocation, after the stage including InputSplits gets submitted, Spark will try to request an appropriate number of executors.
* On YARN, a Spark executor maps to a single YARN container.
* The memory in the YARN resource requests is `--executor-memory` + what's set for `spark.yarn.executor.memoryOverhead`, which defaults to 10% of `--executor-memory`.
* if YARN has enough resources it will deploy the executors distributed across the cluster, then each of them will try to process the data locally (`NODE_LOCAL` in Spark Web UI), with as many splits in parallel as you defined in `spark.executor.cores`.
* _"YarnClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources"_
* Mandatory settings (`spark-defaults.conf`) for dynamic allocation:
+
```
spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
```
* Optional settings for dynamic allocation (to tune it):
+
```
spark.dynamicAllocation.minExecutors     0
spark.dynamicAllocation.maxExecutors     N
spark.dynamicAllocation.initialExecutors 0
```
* `spark.dynamicAllocation.minExecutors` requires `spark.dynamicAllocation.initialExecutors`
* Review `spark.dynamicAllocation.*` settings
* YARN UI under scheduler - pools where Spark operates

=== [[cluster-mode]] Cluster Mode

Spark on YARN supports submitting Spark applications in link:spark-submit.adoc#deploy-modes[cluster deploy mode].

In cluster deploy mode Spark on YARN uses <<YarnClusterSchedulerBackend, YarnClusterSchedulerBackend>>.

=== [[YarnClusterSchedulerBackend]] YarnClusterSchedulerBackend

`YarnClusterSchedulerBackend` is a link:spark-scheduler-backends.adoc[scheduler backend] for Spark on YARN in <<cluster-mode, cluster deploy mode>>.

This is the only scheduler backend that supports <<multiple-application-attempts, multiple application attempts>> and link:spark-scheduler-backends.adoc#getDriverLogUrls[URLs for driver's logs] to display as links in the web UI in the Executors tab for the driver.

It uses `spark.yarn.app.attemptId` under the covers (that the YARN resource manager sets?).

==== [[multiple-application-attempts]] Multiple Application Attempts

Spark on YARN supports *multiple application attempts* in <<cluster-mode, Cluster Mode>>.

CAUTION: FIXME

=== [[YarnScheduler]] YarnScheduler

CAUTION: FIXME Review

It appears that this is a custom implementation to keep track of racks per host that is used in link:spark-tasksetmanager.adoc#resourceOffer[TaskSetManager.resourceOffer] to find a task with `RACK_LOCAL` locality preferences.
