== Spark on YARN

You can link:spark-submit.adoc[submit your Spark applications] to a Hadoop YARN cluster using `yarn` <<masterURL, master URL>> in link:spark-yarn-client-yarnclientschedulerbackend.adoc[client] (default) or link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc[cluster] deploy modes.

[source, scala]
----
spark-submit --master yarn --deploy-mode cluster mySparkApp.jar
----

NOTE: Since Spark *2.0.0*, `yarn` master URL is the only proper master URL and you can use `--deploy-mode` to choose between `client` (default) or `cluster` modes.

In that sense, a Spark application is a YARN-compatible execution framework that can be deployed to a YARN cluster. On YARN, a Spark executor maps to a single YARN container.

NOTE: In order to deploy applications to YARN clusters, you need to <<yarn-support, use Spark with YARN support>>.

Spark on YARN supports <<multiple-application-attempts, multiple application attempts>> and supports link:spark-data-locality.adoc[data locality with the data in HDFS]. You can also run Spark in a link:spark-yarn-kerberos.adoc[secured Hadoop environment using Kerberos authentication].

There are few settings that are specific to YARN (see <<settings, Settings>>).

TIP: You can start link:spark-submit.adoc[spark-submit] with `--verbose` command-line option to have some settings displayed, including YARN-specific. See <<spark-submit, spark-submit and YARN options>>.

The memory in the YARN resource requests is `--executor-memory` + what's set for `spark.yarn.executor.memoryOverhead`, which defaults to 10% of `--executor-memory`.

If YARN has enough resources it will deploy the executors distributed across the cluster, then each of them will try to process the data locally (`NODE_LOCAL` in Spark Web UI), with as many splits in parallel as you defined in link:spark-executor.adoc#spark.executor.cores[spark.executor.cores].

=== [[spark-submit]] spark-submit and YARN options

When you submit your Spark applications using link:spark-submit.adoc[spark-submit] you can use the following YARN-specific command-line options:

* `--archives`
* `--executor-cores`
* `--keytab`
* `--num-executors`
* `--principal`
* link:spark-submit.adoc#queue[--queue]

TIP: Read about the corresponding settings in <<settings, Settings>> in this document.

=== [[yarn-support]] Spark with YARN support

You need to have Spark that link:spark-building-from-sources.adoc[has been compiled with YARN support], i.e. the class link:spark-yarn-client.adoc[org.apache.spark.deploy.yarn.Client] must be on the CLASSPATH.

Otherwise, you will see the following error in the logs and Spark will exit.

```
Error: Could not load YARN classes. This copy of Spark may not have been compiled with YARN support.
```

=== [[masterURL]] Master URL

Since Spark *2.0.0*, the only proper master URL is `yarn`.

```
./bin/spark-submit --master yarn ...
```

Before Spark 2.0.0, you could have used `yarn-client` or `yarn-cluster`, but it is now deprecated. When you use the deprecated master URLs, you should see the following warning in the logs:

```
Warning: Master yarn-client is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
```

=== [[keytab]] Keytab

CAUTION: FIXME

When a principal is specified a keytab must be specified, too.

The settings `spark.yarn.principal` and `spark.yarn.principal` will be set to respective values and `UserGroupInformation.loginUserFromKeytab` will be called with their values as input arguments.

=== [[environment-variables]] Environment Variables

==== [[SPARK_DIST_CLASSPATH]] SPARK_DIST_CLASSPATH

`SPARK_DIST_CLASSPATH` is a distribution-defined CLASSPATH to add to processes.

It is used to link:spark-yarn-client.adoc#populateClasspath[populate CLASSPATH for ApplicationMaster and executors].

=== [[settings]] Settings

CAUTION: FIXME Where and how are they used?

The following settings (aka system properties) are specific to Spark on YARN.

==== [[spark.yarn.submit.file.replication]] spark.yarn.submit.file.replication

`spark.yarn.submit.file.replication` is the replication factor (number) for files uploaded by Spark to HDFS.

==== [[spark.yarn.config.gatewayPath]] spark.yarn.config.gatewayPath

`spark.yarn.config.gatewayPath` (default: `null`) is the root of configuration paths that is present on gateway nodes, and will be replaced with the corresponding path in cluster machines.

It is used when link:spark-yarn-client.adoc#getClusterPath[`Client` resolves a path to be YARN NodeManager-aware].

==== [[spark.yarn.config.replacementPath]] spark.yarn.config.replacementPath

`spark.yarn.config.replacementPath` (default: `null`) is the path to use as a replacement for <<spark.yarn.config.gatewayPath, spark.yarn.config.gatewayPath>> when launching processes in the YARN cluster.

It is used when link:spark-yarn-client.adoc#getClusterPath[`Client` resolves a path to be YARN NodeManager-aware].

==== [[spark.yarn.user.classpath.first]] spark.yarn.user.classpath.first

CAUTION: FIXME

==== [[spark.yarn.archive]] spark.yarn.archive

`spark.yarn.archive` is the location of the archive containing jars files with Spark classes. It cannot be a `local:` URI.

It is used to link:spark-yarn-client.adoc#populateClasspath[populate CLASSPATH for `ApplicationMaster` and executors].

==== [[spark.yarn.queue]] spark.yarn.queue

CAUTION: FIXME Where is this used?

==== [[spark.yarn.jars]] spark.yarn.jars

`spark.yarn.jars` is the location of the Spark jars.

```
--conf spark.yarn.jar=hdfs://master:8020/spark/spark-assembly-2.0.0-hadoop2.7.2.jar
```

It is used to link:spark-yarn-client.adoc#populateClasspath[populate the CLASSPATH for `ApplicationMaster` and `ExecutorRunnables`] (when <<spark.yarn.archive, spark.yarn.archive>> is not defined).

NOTE: `spark.yarn.jar` setting is deprecated as of Spark 2.0.

==== [[spark.yarn.report.interval]] spark.yarn.report.interval

`spark.yarn.report.interval` (default: `1s`) is the interval (in milliseconds) between reports of the current application status.

It is used in link:spark-yarn-client.adoc#monitorApplication[Client.monitorApplication].

==== [[spark.yarn.dist.jars]] spark.yarn.dist.jars

`spark.yarn.dist.jars` (default: empty) is a collection of additional jars to distribute.

It is used when link:spark-yarn-client.adoc#[Client distributes additional resources] as specified using <<spark-submit, `--jars` command-line option for spark-submit>>.

==== [[spark.yarn.dist.files]] spark.yarn.dist.files

`spark.yarn.dist.files` (default: empty) is a collection of additional files to distribute.

It is used when link:spark-yarn-client.adoc#[Client distributes additional resources] as specified using <<spark-submit, `--files` command-line option for spark-submit>>.

==== [[spark.yarn.dist.archives]] spark.yarn.dist.archives

`spark.yarn.dist.archives` (default: empty) is a collection of additional archives to distribute.

It is used when link:spark-yarn-client.adoc#[Client distributes additional resources] as specified using <<spark-submit, `--archives` command-line option for spark-submit>>.

==== [[spark.yarn.principal]] spark.yarn.principal

`spark.yarn.principal` -- See the corresponding <<spark-submit, --principal command-line option for spark-submit>>.

==== [[spark.yarn.keytab]] spark.yarn.keytab

`spark.yarn.keytab` -- See the corresponding <<spark-submit, --keytab command-line option for spark-submit>>.

=== [[multiple-application-attempts]] Multiple Application Attempts

Spark on YARN supports *multiple application attempts* in link:spark-yarn-cluster-yarnclusterschedulerbackend.adoc[cluster mode].

CAUTION: FIXME
