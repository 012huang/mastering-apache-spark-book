== Shuffle Manager

=== Introduction

Spark comes with a pluggable mechanism for *shuffle systems*.

*Shuffle Manager* (aka *Shuffle Service*) is a Spark service that tracks link:spark-dagscheduler.adoc#ShuffleMapStage[shuffle dependencies for ShuffleMapStage]. The driver and executors all have their own Shuffle Service.

The setting <<settings, spark.shuffle.manager>> sets up the default shuffle manager.

The driver registers shuffles with a shuffle manager, and executors (or tasks running locally in the driver) can ask to read and write data.

It is network-addressable, i.e. it is available on a host and port.

There can be many shuffle services running simultaneously and a driver registers with all of them when link:spark-scheduler-backends.adoc[CoarseGrainedSchedulerBackend] is used.

The service is available under `SparkEnv.get.shuffleManager`.

When link:spark-rdd-shuffledrdd.adoc[ShuffledRDD] is computed it reads partitions from it.

The name appears https://github.com/apache/spark/commit/2da3a9e98e5d129d4507b5db01bba5ee9558d28e[here], twice in link:spark-building-from-sources.adoc[the build's output] and others.

Review the code in `network/shuffle` module.

* When is data eligible for shuffling?
* Get the gist of _"The shuffle files are not currently cleaned up when using Spark on Mesos with the external shuffle service"_

=== [[contract]] ShuffleManager Contract

NOTE: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala[org.apache.spark.shuffle.ShuffleManager] is a `private[spark]` Scala trait.

Every ShuffleManager implementation has to offer the following services:

* Is identified by a short name (as `shortName`)
* Registers shuffles so they are addressable by a `ShuffleHandle`  (using `registerShuffle`)
* Returns a `ShuffleWriter` for a partition (using `getWriter`)
* Returns a `ShuffleReader` for a range of partitions (using `getReader`)
* Removes shuffles (using `unregisterShuffle`)
* Returns a `ShuffleBlockResolver` (using `shuffleBlockResolver`)
* Can be stopped (using `stop`)

=== Available Implementations

Spark comes with two implementations of <<contract, ShuffleManager contract>>:

* <<SortShuffleManager, org.apache.spark.shuffle.sort.SortShuffleManager>> (short name: `sort` or `tungsten-sort`)
* `org.apache.spark.shuffle.hash.HashShuffleManager` (short name: `hash`)

CAUTION: FIXME Exercise for a custom implementation of Shuffle Manager using `private[spark] ShuffleManager` trait.

==== [[SortShuffleManager]] SortShuffleManager

`SortShuffleManager` is a shuffle manager with the short name being `sort`.

It uses `IndexShuffleBlockResolver` as the `shuffleBlockResolver`.

=== [[external-shuffle-service]] External Shuffle Service

`org.apache.spark.deploy.ExternalShuffleService` is the external shuffle service.

When <<settings, spark.shuffle.service.enabled>> is enabled, the service provides a mechanism to manage shuffle output files so they are available for executors. It offers an uninterrupted access to the shuffle output files regardless of executors being killed or down.

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.deploy.ExternalShuffleService` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.ExternalShuffleService=INFO
```
====

You can start an external shuffle service instance using the following command:

```
./bin/spark-class org.apache.spark.deploy.ExternalShuffleService
```

When the server starts, you should see the following INFO message in the logs:

```
INFO ExternalShuffleService: Starting shuffle service on port [port] with useSasl = [useSasl]
```

[TIP]
====
Enable `TRACE` logging level for `org.apache.spark.network.shuffle.ExternalShuffleBlockHandler` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.network.shuffle.ExternalShuffleBlockHandler=TRACE
```
====

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.network.shuffle.ExternalShuffleBlockResolver` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.network.shuffle.ExternalShuffleBlockResolver=DEBUG
```
====

You should see the following INFO message in the logs:

```
INFO ExternalShuffleBlockResolver: Registered executor [AppExecId] with [executorInfo]
```

You should also see the following messages when a SparkContext is closed:

```
INFO ExternalShuffleBlockResolver: Application [appId] removed, cleanupLocalDirs = [cleanupLocalDirs]
INFO ExternalShuffleBlockResolver: Cleaning up executor [AppExecId]'s [executor.localDirs.length] local dirs
DEBUG ExternalShuffleBlockResolver: Successfully cleaned up directory: [localDir]
```

=== [[settings]] Settings

* `spark.shuffle.manager` (default: `sort`) sets up the default shuffle manager by a short name or the fully-qualified class name of a custom implementation.

* `spark.shuffle.service.enabled` (default: `false`) controls whether an external shuffle service should be used. When enabled, the Spark driver will register with the shuffle service. See <<external-shuffle-service, External Shuffle Service>>.
+
It is used in link:spark-mesos.adoc#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend] to instantiate `MesosExternalShuffleClient`.

* `spark.shuffle.service.port` (default: `7337`)

* `spark.shuffle.spill` (default: `true`) - no longer in used, and when `false` the following WARNING shows in the logs:
+
```
WARN SortShuffleManager: spark.shuffle.spill was set to false, but this configuration is ignored as of Spark 1.6+. Shuffle will continue to spill to disk when necessary.
```
