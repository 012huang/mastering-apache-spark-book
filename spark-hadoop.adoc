== Spark and Hadoop

* *Hadoop* is an umbrella term:
** *HDFS* (Hadoop Distributed File System) - the storage
** *MapReduce* - the compute engine
** *YARN* (Yet Another Resource Negotiator) - the resource manager
* YARN could be considered a cornerstone of Hadoop OS (operating system) for big distributed data with HDFS as the storage along with YARN to schedule processes.
* YARN is essentially a container system and scheduler designed primarily for use with a Hadoop-based cluster.
* The containers in YARN are capable of running various types of tasks.
* Resource manager, node manager, container, application master, jobs
* focused on data storage and offline batch analysis
* Hadoop is storage and compute platform:
** MapReduce is the computing part.
** HDFS is the storage.
* Hadoop is a resource and cluster manager (YARN)
* Spark runs on YARN clusters, and can read from and save data to HDFS.
** leverages link:spark-data-locality.adoc[data locality]
* Spark needs distributed file system and HDFS (or Amazon S3, but slower) is a great choice.
* HDFS allows for link:spark-data-locality.adoc[data locality].
* Excellent throughput when Spark and Hadoop are both distributed and co-located on the same (YARN or Mesos) cluster nodes.
* HDFS offers (important for initial loading of data):
** high data locality
** high throughput when co-located with Spark
** low latency because of data locality
** very reliable because of replication
* When reading data from HDFS, each `InputSplit` maps to exactly a Spark partition.
