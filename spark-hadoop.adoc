== Spark and Hadoop

* Hadoop is often meant as a combination of Hadoop File System (HDFS), MapReduce, and Hadoop YARN.
* Hadoop is storage and compute platform:
** MapReduce is the computing part.
** HDFS is the storage.
* Hadoop is a resource and cluster manager (YARN)
* Spark runs on YARN clusters, and can read from and save data to HDFS.
** leverages link:spark-data-locality.adoc[data locality]
* Spark needs distributed file system and HDFS (or Amazon S3, but slower) is a great choice.
* HDFS allows for link:spark-data-locality.adoc[data locality].
* Excellent throughput when Spark and Hadoop are both distributed and co-located on the same (YARN or Mesos) cluster nodes.
* HDFS offers (important for initial loading of data):
** high data locality
** high throughput when co-located with Spark
** low latency because of data locality
** very reliable because of replication
* When reading data from HDFS, each `InputSplit` maps to exactly a Spark partition.
