== Scheduler Backends

Spark comes with a pluggable backend mechanism called *scheduler backend* (aka _backend scheduler_) for multiple backend schedulers, each with their own custom task scheduling modes.

`SchedulerBackend` is the Spark interface to different task scheduling systems, i.e. link:spark-local.adoc#LocalBackend[Spark local], link:spark-standalone.adoc[Spark Standalone], link:spark-mesos.adoc[Mesos] or link:spark-yarn.adoc[YARN]. It is the approach to utilize other task schedulers.

CAUTION: FIXME Have an exercise to create a SchedulerBackend.

Being a scheduler backend in Spark assumes a http://mesos.apache.org/[Apache Mesos]-like model in which "an application" gets *resource offers* as machines become available and can launch tasks on them.

TIP: Understanding how Apache Mesos works can greatly improve understanding Spark.

Scheduler backends are started and stopped as part of Spark's initialization and stop.

A scheduler backend can `reviveOffers`, calculate `defaultParallelism`, kill tasks, return application attempt ids (supported only by `YarnClusterSchedulerBackend`) and URLs for the driver logs.

Spark comes with the following scheduler backends:

* *LocalBackend* that is used in link:spark-local.adoc#LocalBackend[local mode].
* <<CoarseGrainedSchedulerBackend, CoarseGrainedSchedulerBackend>>
** *SparkDeploySchedulerBackend* used in link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone] (and local-cluster - FIXME)
** YarnSchedulerBackend
*** YarnClientSchedulerBackend
*** *YarnClusterSchedulerBackend* used in link:spark-yarn.adoc#YarnClusterSchedulerBackend[Spark on YARN in cluster mode]
** link:spark-mesos.adoc#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend]
** SimrSchedulerBackend
* link:spark-mesos.adoc#MesosSchedulerBackend[MesosSchedulerBackend]

==== [[CoarseGrainedSchedulerBackend]] CoarseGrainedSchedulerBackend

*CoarseGrainedSchedulerBackend* is a scheduler backend that waits for coarse-grained executors to connect to it through Akka (FIXME Akka? Still? You sure?). It talks to a cluster manager for resources for executors (register, remove).

*Coarse-grained executors* are executors that this backend holds for the duration of the Spark job rather than relinquishing executors whenever a task is done and asking the scheduler to launch a new executor for each new task.

It requires a link:spark-taskscheduler.adoc[Task Scheduler], and a link:spark-rpc.adoc[RPC Environment].

It uses link:spark-scheduler-listeners.adoc[Listener Bus].

It registers the `CoarseGrainedScheduler` RPC endpoint (`driverEndpoint` internal field).

It tracks:

* the total number of cores in the cluster
* the total number of executors that are currently registered
* executors (`ExecutorData`)
* executors to be removed (`executorsPendingToRemove`)
* hosts and the number of possible tasks possibly running on them
* lost executors with no real exit reason
* tasks per slaves (`taskIdsOnSlave`)

CAUTION: FIXME Where are these counters used?

* `spark.scheduler.minRegisteredResourcesRatio` (default: `0`) - double value between 0 and 1. FIXME
* `spark.scheduler.maxRegisteredResourcesWaitingTime` (default: `30s`) FIXME

===== [[CoarseGrainedScheduler]] CoarseGrainedScheduler RPC Endpoint

When link:spark-schedulerbackends.adoc#CoarseGrainedSchedulerBackend[CoarseGrainedSchedulerBackend] starts, it registers *CoarseGrainedScheduler* driver endpoint.

It tracks:

* RPC addresses for executors (`addressToExecutorId`)

It uses `driver-revive-thread` daemon single-thread thread pool for ...FIXME

CAUTION: FIXME A potential issue with `driverEndpoint.asInstanceOf[NettyRpcEndpointRef].toURI` - doubles `spark://` prefix.

* `spark.scheduler.revive.interval` (default: `1s`) - time between reviving offers.
