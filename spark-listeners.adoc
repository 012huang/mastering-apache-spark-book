== Listeners (for events from Spark's Scheduler)

A Spark *listener* is a class that extends https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.SparkListener[org.apache.spark.scheduler.SparkListener] that receives events from link:spark-scheduler.adoc[DAGScheduler].

It can receive events about:

* when a stage completes successfully or fails
* when a stage is submitted
* when a task starts
* when a task begins remotely fetching its result
* when a task ends
* when a job starts
* when a job ends
* when environment properties have been updated
* when a new block manager has joined
* when an existing block manager has been removed
* when an RDD is manually unpersisted by the application
* when the application starts
* when the application ends
* when the driver receives task metrics from an executor in a heartbeat.
* when the driver registers a new executor.
* when the driver removes an executor.
* when the driver receives a block update info.

The `spark.extraListeners` (default: empty) setting is a comma-separated list of listener class names that are registered with Spark's listener bus when link:spark-sparkcontext.adoc[SparkContext is initialized].

```
$ ./bin/spark-shell --conf spark.extraListeners=i.do.not.exist.CustomSparkListener
...
org.apache.spark.SparkException: Exception when registering SparkListener
  at org.apache.spark.SparkContext.setupAndStartListenerBus(SparkContext.scala:2145)
  at org.apache.spark.SparkContext.<init>(SparkContext.scala:562)
  at org.apache.spark.repl.Main$.createSparkContext(Main.scala:79)
  ... 47 elided
Caused by: java.lang.ClassNotFoundException: i.do.not.exist.CustomSparkListener
  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at org.apache.spark.util.Utils$.classForName(Utils.scala:173)
  at org.apache.spark.SparkContext$$anonfun$setupAndStartListenerBus$1.apply(SparkContext.scala:2113)
  at org.apache.spark.SparkContext$$anonfun$setupAndStartListenerBus$1.apply(SparkContext.scala:2110)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
  at org.apache.spark.SparkContext.setupAndStartListenerBus(SparkContext.scala:2110)
  ... 49 more
```

It is assumed that the classes come with one of the following (in this order):

* a single-argument constructor that accepts `SparkConf`
* a zero-argument constructor

[TIP]
====
Set `INFO` on `org.apache.spark.SparkContext` logger to see the extra listeners being registered.
====

Internal listeners:

* web UI and <<event-logging, event logging>> listeners

=== [[event-logging]] Event Logging

* `spark.eventLog.enabled` (default: `false`)

=== [[exercise]] Exercise: Using Listeners to monitor Spark's Scheduler

FIXME...
