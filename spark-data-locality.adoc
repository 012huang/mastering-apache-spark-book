== Data locality

Spark relies on _data locality_, aka _proximity to data source_, i.e. Spark jobs are sensitive to where the data is located and should be running on Hadoop YARN cluster if the data comes from HDFS.

Concept of *locality-aware scheduling*.

Spark tries to execute tasks as close to the data as possible to minimize data transfer (over the wire).

.Locality Level in the Spark UI
image::images/sparkui-stages-locality-level.png[]

There are the following task localities (consult https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskLocality$[org.apache.spark.scheduler.TaskLocality] object):

* `PROCESS_LOCAL`
* `NODE_LOCAL`
* `NO_PREF`
* `RACK_LOCAL`
* `ANY`

Task location can either be a host or a pair of a host and an executor.
