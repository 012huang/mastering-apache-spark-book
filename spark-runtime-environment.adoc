== Spark Runtime Environment

*Spark Runtime Environment* is the runtime environment with Spark services that interact with each other to build Spark computing platform.

<<SparkEnv, SparkEnv>> holds all runtime environment services for a running Spark instance.

=== [[SparkEnv]] SparkEnv

*SparkEnv* holds all runtime environment objects for a running Spark instance, either link:spark-master.adoc[master] or link:spark-workers.adoc[worker].

[TIP]
====
Turn `DEBUG` logging level for `org.apache.spark.SparkEnv` logger on to learn the low-level details of `SparkEnv`.

Add the following line to `conf/log4j.properties` with requested `DEBUG` log level:

```
log4j.logger.org.apache.spark.SparkEnv=DEBUG
```
====

You can access the Spark environment using `SparkEnv.get`.

```
scala> import org.apache.spark._
import org.apache.spark._

scala> SparkEnv.get
res0: org.apache.spark.SparkEnv = org.apache.spark.SparkEnv@2220c5f7
```

=== [[createDriverEnv]] SparkEnv.createDriverEnv

`SparkEnv.createDriverEnv` creates the Spark execution environment for a driver, aka *driver's (execution) environment*.

The method accepts an instance of link:spark-configuration.adoc[SparkConf], link:spark-deployment-modes.adoc[whether it runs in local mode or not], link:spark-scheduler-listeners.adoc#listener-bus[an instance of listener bus], the number of driver's cores to use for execution in local mode or `0` otherwise, and a link:spark-service-outputcommitcoordinator.adoc[OutputCommitCoordinator] (default: none).

The following two driver's properties are expected to be available in the Spark configuration:

* `spark.driver.host` - the hostname of the driver
* `spark.driver.port` - the port of the driver.

For Akka-based RPC Environment (obsolete since Spark 1.6.0-SNAPSHOT), the name of the actor system for the driver is *sparkDriver*. See link:spark-rpc.adoc#client-mode[clientMode] how it is created in detail.

=== [[createExecutorEnv]] SparkEnv.createExecutorEnv

`SparkEnv.createExecutorEnv` creates an Spark environment for an executor.

It uses SparkConf, the executor's identifier, hostname, port, the number of cores, and whether or not it runs in local mode.

For Akka-based RPC Environment (obsolete since Spark 1.6.0-SNAPSHOT), the name of the actor system for an executor is *sparkExecutor*.
