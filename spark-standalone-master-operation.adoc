== Operating Spark Standalone master

You can start a link:spark-standalone.adoc[Spark Standalone] *master* using <<start-script, sbin/start-master.sh>> and stop it using <<stop-script, sbin/stop-master.sh>>.

The initialization is described in <<startup-internals, Internals of org.apache.spark.deploy.master.Master>>.

=== [[start-script]] sbin/start-master.sh

`sbin/start-master.sh` script starts a Spark master on the machine the script is executed on.

```
./sbin/start-master.sh
```

The script prepares the command line to start the class `org.apache.spark.deploy.master.Master` and by default runs as follows:

```
org.apache.spark.deploy.master.Master \
  --ip japila.local --port 7077 --webui-port 8080
```

NOTE: The command uses `SPARK_PRINT_LAUNCH_COMMAND` (default: `1`) that when set prints out the launch command to standard error output (`System.err`).

It has support for starting Tachyon using `--with-tachyon` command line option. It assumes `tachyon/bin/tachyon` command be available in Spark's home directory.

The script uses the following helper scripts:

* `sbin/spark-config.sh`
* `bin/load-spark-env.sh`

Ultimately, the script calls `sbin/spark-daemon.sh start` to kick off `org.apache.spark.deploy.master.Master` with parameter `1` and `--ip`, `--port`, and `--webui-port` <<options, command-line options>>.

==== [[command-line-options]] Command-line Options

You can use the following command-line options:

* `--host` or `-h` the hostname to listen on; overrides <<environment-variables, SPARK_MASTER_HOST>>.
* `--ip` or `-i` (deprecated) the IP to listen on
* `--port` or `-p` - command-line version of <<environment-variables, SPARK_MASTER_PORT>> that overrides it.
* `--webui-port` - command-line version of <<environment-variables, SPARK_MASTER_WEBUI_PORT>> that overrides it.
* `--properties-file` (default: `$SPARK_HOME/conf/spark-defaults.conf`) - the path to a custom Spark properties file
* `--help`

=== [[stop-script]] sbin/stop-master.sh

You can stop a Spark Standalone master using `sbin/stop-master.sh` script.

```
./sbin/stop-master.sh
```

CAUTION: FIXME Review the script

It effectively sends SIGTERM to the master's process.

You should see the ERROR in master's logs:

```
ERROR Master: RECEIVED SIGNAL 15: SIGTERM
```

=== [[environment-variables]] System environment variables

Master uses the following system environment variables (directly or indirectly):

* `SPARK_LOCAL_HOSTNAME` - the custom host name
* `SPARK_LOCAL_IP` - the custom IP to use when `SPARK_LOCAL_HOSTNAME` is not set
* `SPARK_MASTER_HOST` (not `SPARK_MASTER_IP` as used in `start-master.sh` script above!) - the master custom host
* `SPARK_MASTER_PORT` (default: `7077`) - the master custom port
* `SPARK_MASTER_IP` (default: `hostname` command's output)
* `SPARK_MASTER_WEBUI_PORT` (default: `8080`) - the port of the master's WebUI. Overriden by `spark.master.ui.port` if set in the properties file.
* `SPARK_PUBLIC_DNS` (default: hostname) - the custom master hostname for WebUI's http URL and master's address.
* `SPARK_CONF_DIR` (default: `$SPARK_HOME/conf`) - the directory of the default properties file `spark-defaults.conf` from which all properties that start with `spark.` prefix are loaded.

=== States

Master can be in the following states:

* `STANDBY` - the initial state while Master is initializing
* `ALIVE`
* `RECOVERING`
* `COMPLETING_RECOVERY`

CAUTION: FIXME

=== [[rpcenv]] RPC Environment

The `org.apache.spark.deploy.master.Master` class starts link:spark-rpc.adoc[sparkMaster RPC environment].

```
INFO Utils: Successfully started service 'sparkMaster' on port 7077.
```

It then registers `Master` endpoint.

.sparkMaster - the RPC Environment for Spark Standalone's master
image::images/sparkMaster-rpcenv.png[align="center"]

Master endpoint is a `ThreadSafeRpcEndpoint` and `LeaderElectable` (see <<leader-election, Leader Election>>).

The Master endpoint starts the daemon single-thread scheduler pool `master-forward-message-thread`. It is used for worker management, i.e. removing any timed-out workers.

```
"master-forward-message-thread" #46 daemon prio=5 os_prio=31 tid=0x00007ff322abb000 nid=0x7f03 waiting on condition [0x000000011cad9000]
```

Application ids follows the pattern `app-yyyyMMddHHmmss`.

Master keeps track of the following:

* workers (`workers`)
* mapping between ids and applications (`idToApp`)
* waiting applications (`waitingApps`)
* applications (`apps`)
* mapping between ids and workers (`idToWorker`)
* mapping between RPC address and workers (`addressToWorker`)
* `endpointToApp`
* `addressToApp`
* `completedApps`
* `nextAppNumber`
* mapping between application ids and their Web UIs (`appIdToUI`)
* drivers (`drivers`)
* `completedDrivers`
* drivers currently spooled for scheduling (`waitingDrivers`)
* `nextDriverNumber`

The Master is in fact the Master RPC Endpoint that you can access using RPC port (low-level operation communication) or link:spark-webui.adoc[Web UI].

`MasterWebUI` is the Web UI server for the standalone master. Master starts Web UI to listen to `http://[master's hostname]:webUIPort`, e.g. `http://localhost:8080`.

The following INFO shows up when the Master endpoint starts up (`Master#onStart` is called):

```
INFO Master: Starting Spark master at spark://japila.local:7077
INFO Master: Running Spark version 1.6.0-SNAPSHOT
```

=== [[metrics]] Metrics

Master uses link:spark-metrics.adoc[Spark Metrics System] (via `MasterSource`) to report metrics about internal status.

The name of the source is *master*.

It emits the following metrics:

* `workers` - the number of all workers (any state)
* `aliveWorkers` - the number of alive workers
* `apps` - the number of applications
* `waitingApps` - the number of waiting applications

The name of the other source is *applications*

[CAUTION]
====
FIXME

* Review `org.apache.spark.metrics.MetricsConfig`
* How to access the metrics for master? See `Master#onStart`
* Review `masterMetricsSystem` and `applicationMetricsSystem`
====

=== [[worker-management]] Worker Management

Master uses `master-forward-message-thread` to schedule a thread every `spark.worker.timeout` to check workers' availability and remove timed-out workers.

It is that Master sends `CheckForWorkerTimeOut` message to itself to trigger verification.

When a worker hasn't responded for `spark.worker.timeout`, it is assumed dead and the following WARN message appears in the logs:

```
WARN Removing [worker.id] because we got no heartbeat in [spark.worker.timeout] seconds
```

=== [[properties]] Properties

[CAUTION]
====
FIXME

* Where are `RETAINED_`'s properties used?
====

Master uses the following properties:

* `spark.worker.timeout` (default: `60`) - time (in seconds) when no heartbeat from a worker means it is lost. See <<worker-management, Worker Management>>.
* `spark.deploy.retainedApplications` (default: `200`)
* `spark.deploy.retainedDrivers` (default: `200`)
* `spark.dead.worker.persistence` (default: `15`)
* `spark.deploy.recoveryMode` (default: `NONE`) - possible modes: `ZOOKEEPER`, `FILESYSTEM`, or `CUSTOM`. Refer to <<recovery-mode, Recovery Mode>>.
* `spark.deploy.recoveryMode.factory` - the class name of the custom `StandaloneRecoveryModeFactory`.
* `spark.deploy.recoveryDirectory` (default: empty) - the directory to persist recovery state
* `spark.deploy.spreadOut` (default: `true`) - perform round-robin scheduling across the nodes (spreading out each app among all the nodes). Refer to <<round-robin-scheduling, Round-robin Scheduling Across Nodes>>
* `spark.deploy.defaultCores` (default: `Int.MaxValue`, i.e. unbounded)- the number of maxCores for applications that don't specify it.
* `spark.master.rest.enabled` (default: `true`) - <<rest-server, master's REST Server>> for alternative application submission that is supposed to work across Spark versions.
* `spark.master.rest.port` (default: `6066`) - the port of <<rest-server, master's REST Server>>
* `spark.ui.killEnabled` (default: `true`)

=== [[recovery-mode]] Recovery Mode

Master can run with *recovery mode* enabled and be able to recover state among the available swarm of masters. By default, there is no recovery, i.e. no persistence and no election.

It uses `spark.deploy.recoveryMode` to define the recovery mode for master (see <<properties, spark.deploy.recoveryMode>>).

The Recovery Mode enables election of the leader master among the masters.

FIXME Why would I want to have many masters? What are the use cases?

=== [[rest-server]] REST Server

Master starts REST Server for alternative application submission that is supposed to work across Spark versions. It enabled by default (see <<properties, spark.master.rest.enabled>>).

FIXME StandaloneRestServer

The following INFOs show up when the Master Endpoint starts up (`Master#onStart` is called) with REST Server enabled:

```
INFO Utils: Successfully started service on port 6066.
INFO StandaloneRestServer: Started REST server for submitting applications on port 6066
```

=== [[round-robin-scheduling]] Round-robin Scheduling Across Nodes

<<properties, spark.deploy.spreadOut>> property controls whether or not to perform round-robin scheduling across the nodes (spreading out each app among all the nodes). It defaults to `true`.

FIXME

=== Master WebUI

FIXME MasterWebUI

```
INFO Utils: Successfully started service 'MasterUI' on port 8080.
INFO MasterWebUI: Started MasterWebUI at http://192.168.1.4:8080
```

=== [[leader-election]] Leader Election

Master endpoint is `LeaderElectable`, i.e. FIXME

CAUTION: FIXME

=== Messages

Master communicates with drivers, executors and sets itself up using *messages*.

The following message types are accepted by master (see `Master#receive` or `Master#receiveAndReply` methods):

* `ElectedLeader` for <<leader-election, Leader Election>>)
* `CompleteRecovery`
* `RevokedLeadership`
* `RegisterApplication`
* `ExecutorStateChanged`
* `DriverStateChanged`
* `Heartbeat`
* `MasterChangeAcknowledged`
* `WorkerSchedulerStateResponse`
* `UnregisterApplication`
* `CheckForWorkerTimeOut`
* `RegisterWorker`
* `RequestSubmitDriver`
* `RequestKillDriver`
* `RequestDriverStatus`
* `RequestMasterState`
* `BoundPortsRequest`
* `RequestExecutors`
* `KillExecutors`

=== [[startup-internals]] Internals of org.apache.spark.deploy.master.Master

[TIP]
====
You can debug a Standalone master using the following command:

[source]
----
java -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 -cp /Users/jacek/dev/oss/spark/conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-1.6.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar -Xms1g -Xmx1g org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080
----

The above command suspends (`suspend=y`) the process until a JPDA debugging client, e.g. your IDE, is connected, and that Spark is available under `/Users/jacek/dev/oss/spark`. Change it to meet your environment.
====

When `Master` starts, it first creates the <<spark-configuration.adoc#default-configuration, default SparkConf configuration>> whose values it then overrides using  <<environment-variables, environment variables>> and <<command-line-options, command-line options>>.

A fully-configured master instance requires `host`, `port` (default: `7077`), `webUiPort` (default: `8080`) settings defined.

TIP: When in troubles, consult link:spark-tips-and-tricks.adoc[Spark Tips and Tricks] document.

It starts <<rpcenv, RPC Environment>> with necessary endpoints and lives until the RPC environment terminates.
