== [[SQLConf]] SQLConf

`SQLConf` is an internal key-value configuration store for <<parameters, parameters and hints>> used in Spark SQL.

[NOTE]
====
`SQLConf` is publicly available through the user-facing interface `RuntimeConfig` that you can access using link:spark-sql-SparkSession.adoc#conf[SparkSession].

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

scala> spark.conf
res0: org.apache.spark.sql.RuntimeConfig = org.apache.spark.sql.RuntimeConfig@6b58a0f9
----
====

`SQLConf` offers methods to <<get, get>>, <<set, set>>, <<unset, unset>> or <<clear, clear>> their values, but has also the <<accessor-methods, accessor methods>> to read the current value of a parameter or hint.

You can access a session-specific `SQLConf` using link:spark-sql-SparkSession.adoc#sessionState[SessionState]:

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

import org.apache.spark.sql.internal.SQLConf
scala> val conf: SQLConf = spark.sessionState.conf
conf: org.apache.spark.sql.internal.SQLConf = org.apache.spark.sql.internal.SQLConf@5e388a77
----

[[accessor-methods]]
.SQLConf's Accessor Methods (in alphabetical order)
[cols="1,1,1",options="header",width="100%"]
|===
| Name
| Parameter / Hint
| Description

| [[adaptiveExecutionEnabled]] `adaptiveExecutionEnabled`
| <<spark.sql.adaptive.enabled, spark.sql.adaptive.enabled>>
| Used exclusively for `EnsureRequirements` link:spark-sql-EnsureRequirements.adoc#withExchangeCoordinator[to add ExchangeCoordinator] (when adaptive query execution is enabled)

| [[broadcastTimeout]] `broadcastTimeout`
| <<spark.sql.broadcastTimeout, spark.sql.broadcastTimeout>>
| Used exclusively in link:spark-sql-SparkPlan-BroadcastExchangeExec.adoc[BroadcastExchangeExec] (for broadcasting a table to executors).

| [[dataFramePivotMaxValues]] `dataFramePivotMaxValues`
| <<spark.sql.pivotMaxValues, spark.sql.pivotMaxValues>>
| Used exclusively in link:spark-sql-RelationalGroupedDataset.adoc#pivot[pivot] operator.

| [[dataFrameRetainGroupColumns]] `dataFrameRetainGroupColumns`
| <<spark.sql.retainGroupColumns, spark.sql.retainGroupColumns>>
| Used exclusively in link:spark-sql-RelationalGroupedDataset.adoc[RelationalGroupedDataset] when creating the result `Dataset` (after `agg`, `count`, `mean`, `max`, `avg`, `min`, and `sum` operators).

| [[numShufflePartitions]] `numShufflePartitions`
| <<spark.sql.shuffle.partitions, spark.sql.shuffle.partitions>>
a|

Used in:

* Dataset's link:spark-sql-dataset-operators.adoc#repartition[repartition] operator (for a link:spark-sql-LogicalPlan-Repartition-RepartitionByExpression.adoc#RepartitionByExpression[RepartitionByExpression] logical operator)
* link:spark-sql-SparkSqlAstBuilder.adoc#withRepartitionByExpression[SparkSqlAstBuilder] (for a link:spark-sql-LogicalPlan-Repartition-RepartitionByExpression.adoc#RepartitionByExpression[RepartitionByExpression] logical operator)
* link:spark-sql-SparkStrategy-JoinSelection.adoc#canBuildLocalHashMap[JoinSelection] execution planning strategy
* link:spark-sql-LogicalPlan-RunnableCommand.adoc#SetCommand[SetCommand] logical command
* link:spark-sql-EnsureRequirements.adoc#defaultNumPreShufflePartitions[EnsureRequirements] physical plan optimization

| [[joinReorderEnabled]] `joinReorderEnabled`
| <<spark.sql.cbo.joinReorder.enabled, spark.sql.cbo.joinReorder.enabled>>
| Used exclusively in link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical plan optimization

| [[starSchemaDetection]] `starSchemaDetection`
| <<spark.sql.cbo.starSchemaDetection, spark.sql.cbo.starSchemaDetection>>
| Used exclusively in link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection`)

| [[wholeStageEnabled]] `wholeStageEnabled`
| <<spark.sql.codegen.wholeStage, spark.sql.codegen.wholeStage>>
a| Used in:

* link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages] to control codegen
* `ParquetFileFormat` to control row batch reading

| [[wholeStageFallback]] `wholeStageFallback`
| <<spark.sql.codegen.fallback, spark.sql.codegen.fallback>>
| Used exclusively when `WholeStageCodegenExec` is link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc#doExecute[executed].

| [[wholeStageMaxNumFields]] `wholeStageMaxNumFields`
| <<spark.sql.codegen.maxFields, spark.sql.codegen.maxFields>>
a|

Used in:

* link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages] to control codegen
* `ParquetFileFormat` to control row batch reading

| [[useObjectHashAggregation]] `useObjectHashAggregation`
| <<spark.sql.execution.useObjectHashAggregateExec, spark.sql.execution.useObjectHashAggregateExec>>
| Used exclusively in `Aggregation` execution planning strategy when link:spark-sql-SparkStrategy-Aggregation.adoc#AggUtils-createAggregate[selecting a physical plan].
|===

[[parameters]]
.Parameters and Hints (in alphabetical order)
[cols=",1,2",options="header",width="100%"]
|===
| Name
| Default Value
| Description

| [[spark.sql.adaptive.enabled]] `spark.sql.adaptive.enabled`
| `false`
| Enables adaptive query execution

| [[spark.sql.broadcastTimeout]] `spark.sql.broadcastTimeout`
| `5 * 60`
| Timeout in seconds for the broadcast wait time in broadcast joins.

When negative, it is assumed infinite (i.e. `Duration.Inf`)

Used through <<broadcastTimeout, SQLConf.broadcastTimeout>>.

| [[spark.sql.cbo.enabled]] `spark.sql.cbo.enabled`
| `false`
a| Enables cost-based optimization (CBO) for estimation of plan statistics when enabled (i.e. `true`).

Used (through `SQLConf.cboEnabled` method) in:

* link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection` for `reorderStarJoins`)
* link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical plan optimization
* For link:spark-sql-LogicalPlan.adoc#computeStats[statistics estimates] in `Project`, `Filter`, link:spark-sql-LogicalPlan-Join.adoc[Join], and link:spark-sql-LogicalPlan-Aggregate.adoc[Aggregate] logical plans

| [[spark.sql.cbo.joinReorder.enabled]] `spark.sql.cbo.joinReorder.enabled`
| `false`
a| Enables join reorder for cost-based optimization (CBO).

Use <<joinReorderEnabled, joinReorderEnabled>> method to access the current value.

| [[spark.sql.cbo.starSchemaDetection]] `spark.sql.cbo.starSchemaDetection`
| `false`
a| Enables *join reordering* based on star schema detection for cost-based optimization (CBO) in link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization.

Use <<starSchemaDetection, starSchemaDetection>> method to access the current value.

| [[spark.sql.codegen.fallback]] `spark.sql.codegen.fallback`
| `true`
| *(internal)* Whether the whole stage codegen could be temporary disabled for the part of a query that has failed to compile generated code (`true`) or not (`false`).

Use <<wholeStageFallback, wholeStageFallback>> method to access the current value.

| [[spark.sql.codegen.maxFields]] `spark.sql.codegen.maxFields`
| `100`
| *(internal)* Maximum number of output fields (including nested fields) that whole-stage codegen supports. Going above the number deactivates whole-stage codegen.

Use <<wholeStageMaxNumFields, wholeStageMaxNumFields>> method to access the current value.

| [[spark.sql.codegen.wholeStage]] `spark.sql.codegen.wholeStage`
| `true`
| *(internal)* Whether the whole stage (of multiple physical operators) will be compiled into a single Java method (`true`) or not (`false`).

Use <<wholeStageEnabled, wholeStageEnabled>> method to access the current value.

| [[spark.sql.execution.useObjectHashAggregateExec]] `spark.sql.execution.useObjectHashAggregateExec`
| `true`
| Decides if we use `ObjectHashAggregateExec` (in link:spark-sql-SparkStrategy-Aggregation.adoc#AggUtils-createAggregate[Aggregation] execution planning strategy).

Use <<useObjectHashAggregation, useObjectHashAggregation>> method to access the current value.

| [[spark.sql.optimizer.maxIterations]] `spark.sql.optimizer.maxIterations`
| `100`
| Maximum number of iterations for link:spark-sql-Analyzer.adoc#fixedPoint[Analyzer] and  link:spark-sql-Optimizer.adoc#fixedPoint[Optimizer].

| [[spark.sql.pivotMaxValues]] `spark.sql.pivotMaxValues`
| `10000`
| Maximum number of (distinct) values that will be collected without error (when doing a link:spark-sql-RelationalGroupedDataset.adoc#pivot[pivot] without specifying the values for the pivot column)

Use <<dataFramePivotMaxValues, dataFramePivotMaxValues>> method to access the current value.

| [[spark.sql.retainGroupColumns]] `spark.sql.retainGroupColumns`
| `true`
| Controls whether to retain columns used for aggregation or not (in link:spark-sql-RelationalGroupedDataset.adoc[RelationalGroupedDataset] operators).

Use <<dataFrameRetainGroupColumns, dataFrameRetainGroupColumns>> method to access the current value.

| [[spark.sql.selfJoinAutoResolveAmbiguity]] `spark.sql.selfJoinAutoResolveAmbiguity`
| `true`
| Control whether to resolve ambiguity in join conditions for link:spark-sql-joins.adoc#join[self-joins] automatically.

| [[spark.sql.shuffle.partitions]] `spark.sql.shuffle.partitions`
| `200`
| Default number of partitions to use when shuffling data for joins or aggregations.

Corresponds to Apache Hive's https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-mapred.reduce.tasks[mapred.reduce.tasks] property that Spark considers deprecated.

| [[spark.sql.streaming.fileSink.log.deletion]] `spark.sql.streaming.fileSink.log.deletion`
| `true`
| Controls whether to delete the expired log files in link:spark-sql-streaming-sink.adoc#FileStreamSink[file stream sink].

| [[spark.sql.streaming.fileSink.log.cleanupDelay]] `spark.sql.streaming.fileSink.log.cleanupDelay`
| FIXME
| FIXME

| [[spark.sql.streaming.schemaInference]] `spark.sql.streaming.schemaInference`
| FIXME
| FIXME

| [[spark.sql.streaming.fileSink.log.compactInterval]] `spark.sql.streaming.fileSink.log.compactInterval`
| FIXME
| FIXME
|===

NOTE: `SQLConf` is a `private[sql]` serializable class in `org.apache.spark.sql.internal` package.

=== [[get]] Getting Parameters and Hints

You can get the current parameters and hints using the following family of `get` methods.

[source, scala]
----
getConfString(key: String): String
getConf[T](entry: ConfigEntry[T], defaultValue: T): T
getConf[T](entry: ConfigEntry[T]): T
getConf[T](entry: OptionalConfigEntry[T]): Option[T]
getConfString(key: String, defaultValue: String): String
getAllConfs: immutable.Map[String, String]
getAllDefinedConfs: Seq[(String, String, String)]
----

=== [[set]] Setting Parameters and Hints

You can set parameters and hints using the following family of `set` methods.

[source, scala]
----
setConf(props: Properties): Unit
setConfString(key: String, value: String): Unit
setConf[T](entry: ConfigEntry[T], value: T): Unit
----

=== [[unset]] Unsetting Parameters and Hints

You can unset parameters and hints using the following family of `unset` methods.

[source, scala]
----
unsetConf(key: String): Unit
unsetConf(entry: ConfigEntry[_]): Unit
----

=== [[clear]] Clearing All Parameters and Hints

[source, scala]
----
clear(): Unit
----

You can use `clear` to remove all the parameters and hints in `SQLConf`.
