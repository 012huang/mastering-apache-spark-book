== [[SQLConf]] SQLConf

`SQLConf` is a key-value configuration store for <<parameters, parameters and hints used in Spark SQL>>. It offers methods to <<get, get>>, <<set, set>>, <<unset, unset>> or <<clear, clear>> their values.

You can access the current `SQLConf` using link:spark-sql-SparkSession.adoc#conf[SparkSession]:

[source, scala]
----
val spark: SparkSession = ???
val conf = spark.conf
----

[[parameters]]
.Parameters and Hints (in alphabetical order)
[cols=",1,2",options="header",width="100%"]
|===
| Name
| Default Value
| Description

| [[spark.sql.cbo.enabled]] `spark.sql.cbo.enabled`
| `false`
a| Enables cost-based optimization (CBO) for estimation of plan statistics when enabled (i.e. `true`).

Used (through `SQLConf.cboEnabled` method) in:

* link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection` for `reorderStarJoins`)
* link:spark-sql-Optimizer.adoc#CostBasedJoinReorder[CostBasedJoinReorder] logical plan optimization
* For link:spark-sql-LogicalPlan.adoc#computeStats[statistics estimates] in `Project`, `Filter`, link:spark-sql-LogicalPlan-Join.adoc[Join], and link:spark-sql-LogicalPlan-Aggregate.adoc[Aggregate] logical plans

| [[spark.sql.cbo.joinReorder.enabled]] `spark.sql.cbo.joinReorder.enabled`
| `false`
a| Enables join reorder for cost-based optimization (CBO).

Used (through `SQLConf.joinReorderEnabled` method) in:

* &hellip;

| [[spark.sql.cbo.starSchemaDetection]] `spark.sql.cbo.starSchemaDetection`
| `false`
a| Enables join reordering based on star schema detection for cost-based optimization (CBO).

Used (through `SQLConf.starSchemaDetection` method) in:

* &hellip;

| [[spark.sql.optimizer.maxIterations]] `spark.sql.optimizer.maxIterations`
| `100`
| Maximum number of iterations for link:spark-sql-Analyzer.adoc#fixedPoint[Analyzer] and  link:spark-sql-Optimizer.adoc#fixedPoint[Optimizer].

| [[spark.sql.selfJoinAutoResolveAmbiguity]] `spark.sql.selfJoinAutoResolveAmbiguity`
| `true`
| Control whether to resolve ambiguity in join conditions for link:spark-sql-joins.adoc#join[self-joins] automatically.

| [[spark.sql.streaming.fileSink.log.deletion]] `spark.sql.streaming.fileSink.log.deletion`
| `true`
| Controls whether to delete the expired log files in link:spark-sql-streaming-sink.adoc#FileStreamSink[file stream sink].

| [[spark.sql.streaming.fileSink.log.cleanupDelay]] `spark.sql.streaming.fileSink.log.cleanupDelay`
| FIXME
| FIXME

| [[spark.sql.streaming.schemaInference]] `spark.sql.streaming.schemaInference`
| FIXME
| FIXME

| [[spark.sql.streaming.fileSink.log.compactInterval]] `spark.sql.streaming.fileSink.log.compactInterval`
| FIXME
| FIXME
|===

NOTE: `SQLConf` is a `private[sql]` serializable class in `org.apache.spark.sql.internal` package.

=== [[get]] Getting Parameters and Hints

You can get the current parameters and hints using the following family of `get` methods.

[source, scala]
----
getConfString(key: String): String
getConf[T](entry: ConfigEntry[T], defaultValue: T): T
getConf[T](entry: ConfigEntry[T]): T
getConf[T](entry: OptionalConfigEntry[T]): Option[T]
getConfString(key: String, defaultValue: String): String
getAllConfs: immutable.Map[String, String]
getAllDefinedConfs: Seq[(String, String, String)]
----

=== [[set]] Setting Parameters and Hints

You can set parameters and hints using the following family of `set` methods.

[source, scala]
----
setConf(props: Properties): Unit
setConfString(key: String, value: String): Unit
setConf[T](entry: ConfigEntry[T], value: T): Unit
----

=== [[unset]] Unsetting Parameters and Hints

You can unset parameters and hints using the following family of `unset` methods.

[source, scala]
----
unsetConf(key: String): Unit
unsetConf(entry: ConfigEntry[_]): Unit
----

=== [[clear]] Clearing All Parameters and Hints

[source, scala]
----
clear(): Unit
----

You can use `clear` to remove all the parameters and hints in `SQLConf`.
