== [[SQLConf]] SQLConf

`SQLConf` is a key-value configuration store for <<parameters, parameters and hints used in Spark SQL>>.

`SQLConf` offers methods to <<get, get>>, <<set, set>>, <<unset, unset>> or <<clear, clear>> their values, but has also the <<accessor-methods, accessor methods>> to read the current value of a parameter or hint.

You can access the current `SQLConf` (indirectly as `RuntimeConfig`) using link:spark-sql-SparkSession.adoc#conf[SparkSession]:

[source, scala]
----
val spark: SparkSession = ...
import org.apache.spark.sql.RuntimeConfig
val conf: RuntimeConfig = spark.conf
----

[[accessor-methods]]
.SQLConf's Accessor Methods (in alphabetical order)
[cols="1,1,1",options="header",width="100%"]
|===
| Name
| Parameter / Hint
| Description

| [[adaptiveExecutionEnabled]] `adaptiveExecutionEnabled`
| <<spark.sql.adaptive.enabled, spark.sql.adaptive.enabled>>
| Used exclusively for `EnsureRequirements` link:spark-sql-EnsureRequirements.adoc#withExchangeCoordinator[to add ExchangeCoordinator] (when adaptive query execution is enabled)

| [[broadcastTimeout]] `broadcastTimeout`
| <<spark.sql.broadcastTimeout, spark.sql.broadcastTimeout>>
| Used exclusively in link:spark-sql-SparkPlan-BroadcastExchangeExec.adoc[BroadcastExchangeExec] (for broadcasting a table to executors).

| [[dataFramePivotMaxValues]] `dataFramePivotMaxValues`
| <<spark.sql.pivotMaxValues, spark.sql.pivotMaxValues>>
| Used exclusively in link:spark-sql-RelationalGroupedDataset.adoc#pivot[pivot] operator.

| [[dataFrameRetainGroupColumns]] `dataFrameRetainGroupColumns`
| <<spark.sql.retainGroupColumns, spark.sql.retainGroupColumns>>
| Used exclusively in link:spark-sql-RelationalGroupedDataset.adoc[RelationalGroupedDataset] when creating the result `Dataset` (after `agg`, `count`, `mean`, `max`, `avg`, `min`, and `sum` operators).

| [[numShufflePartitions]] `numShufflePartitions`
| <<spark.sql.shuffle.partitions, spark.sql.shuffle.partitions>>
a|

Used in:

* link:spark-sql-dataset-operators.adoc#repartition[Dataset.repartition] operator
* link:spark-sql-SparkPlanner.adoc[SparkPlanner]
* link:spark-sql-SparkSqlAstBuilder.adoc#withRepartitionByExpression[SparkSqlAstBuilder]
* link:spark-sql-JoinSelection.adoc[JoinSelection] strategy
* link:spark-sql-EnsureRequirements.adoc[EnsureRequirements] physical plan optimization

| [[joinReorderEnabled]] `joinReorderEnabled`
| <<spark.sql.cbo.joinReorder.enabled, spark.sql.cbo.joinReorder.enabled>>
| Used exclusively in link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical plan optimization

| [[starSchemaDetection]] `starSchemaDetection`
| <<spark.sql.cbo.starSchemaDetection, spark.sql.cbo.starSchemaDetection>>
| Used exclusively in link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection`)
|===

[[parameters]]
.Parameters and Hints (in alphabetical order)
[cols=",1,2",options="header",width="100%"]
|===
| Name
| Default Value
| Description

| [[spark.sql.adaptive.enabled]] `spark.sql.adaptive.enabled`
| `false`
| Enables adaptive query execution

| [[spark.sql.broadcastTimeout]] `spark.sql.broadcastTimeout`
| `5 * 60`
| Timeout in seconds for the broadcast wait time in broadcast joins.

When negative, it is assumed infinite (i.e. `Duration.Inf`)

Used through <<broadcastTimeout, SQLConf.broadcastTimeout>>.

| [[spark.sql.cbo.enabled]] `spark.sql.cbo.enabled`
| `false`
a| Enables cost-based optimization (CBO) for estimation of plan statistics when enabled (i.e. `true`).

Used (through `SQLConf.cboEnabled` method) in:

* link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization (and indirectly in `StarSchemaDetection` for `reorderStarJoins`)
* link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical plan optimization
* For link:spark-sql-LogicalPlan.adoc#computeStats[statistics estimates] in `Project`, `Filter`, link:spark-sql-LogicalPlan-Join.adoc[Join], and link:spark-sql-LogicalPlan-Aggregate.adoc[Aggregate] logical plans

| [[spark.sql.cbo.joinReorder.enabled]] `spark.sql.cbo.joinReorder.enabled`
| `false`
a| Enables join reorder for cost-based optimization (CBO).

Use <<joinReorderEnabled, joinReorderEnabled>> method to access the current value.

| [[spark.sql.cbo.starSchemaDetection]] `spark.sql.cbo.starSchemaDetection`
| `false`
a| Enables *join reordering* based on star schema detection for cost-based optimization (CBO) in link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin] logical plan optimization.

Use <<starSchemaDetection, starSchemaDetection>> method to access the current value.

| [[spark.sql.optimizer.maxIterations]] `spark.sql.optimizer.maxIterations`
| `100`
| Maximum number of iterations for link:spark-sql-Analyzer.adoc#fixedPoint[Analyzer] and  link:spark-sql-Optimizer.adoc#fixedPoint[Optimizer].

| [[spark.sql.pivotMaxValues]] `spark.sql.pivotMaxValues`
| `10000`
| Maximum number of (distinct) values that will be collected without error (when doing a link:spark-sql-RelationalGroupedDataset.adoc#pivot[pivot] without specifying the values for the pivot column)

Use <<dataFramePivotMaxValues, dataFramePivotMaxValues>> method to access the current value.

| [[spark.sql.retainGroupColumns]] `spark.sql.retainGroupColumns`
| `true`
| Controls whether to retain columns used for aggregation or not (in link:spark-sql-RelationalGroupedDataset.adoc[RelationalGroupedDataset] operators).

Use <<dataFrameRetainGroupColumns, dataFrameRetainGroupColumns>> method to access the current value.

| [[spark.sql.selfJoinAutoResolveAmbiguity]] `spark.sql.selfJoinAutoResolveAmbiguity`
| `true`
| Control whether to resolve ambiguity in join conditions for link:spark-sql-joins.adoc#join[self-joins] automatically.

| [[spark.sql.shuffle.partitions]] `spark.sql.shuffle.partitions`
| `200`
| Default number of partitions to use when shuffling data for joins or aggregations.

Corresponds to the now-deprecated `mapred.reduce.tasks` property.

| [[spark.sql.streaming.fileSink.log.deletion]] `spark.sql.streaming.fileSink.log.deletion`
| `true`
| Controls whether to delete the expired log files in link:spark-sql-streaming-sink.adoc#FileStreamSink[file stream sink].

| [[spark.sql.streaming.fileSink.log.cleanupDelay]] `spark.sql.streaming.fileSink.log.cleanupDelay`
| FIXME
| FIXME

| [[spark.sql.streaming.schemaInference]] `spark.sql.streaming.schemaInference`
| FIXME
| FIXME

| [[spark.sql.streaming.fileSink.log.compactInterval]] `spark.sql.streaming.fileSink.log.compactInterval`
| FIXME
| FIXME
|===

NOTE: `SQLConf` is a `private[sql]` serializable class in `org.apache.spark.sql.internal` package.

=== [[get]] Getting Parameters and Hints

You can get the current parameters and hints using the following family of `get` methods.

[source, scala]
----
getConfString(key: String): String
getConf[T](entry: ConfigEntry[T], defaultValue: T): T
getConf[T](entry: ConfigEntry[T]): T
getConf[T](entry: OptionalConfigEntry[T]): Option[T]
getConfString(key: String, defaultValue: String): String
getAllConfs: immutable.Map[String, String]
getAllDefinedConfs: Seq[(String, String, String)]
----

=== [[set]] Setting Parameters and Hints

You can set parameters and hints using the following family of `set` methods.

[source, scala]
----
setConf(props: Properties): Unit
setConfString(key: String, value: String): Unit
setConf[T](entry: ConfigEntry[T], value: T): Unit
----

=== [[unset]] Unsetting Parameters and Hints

You can unset parameters and hints using the following family of `unset` methods.

[source, scala]
----
unsetConf(key: String): Unit
unsetConf(entry: ConfigEntry[_]): Unit
----

=== [[clear]] Clearing All Parameters and Hints

[source, scala]
----
clear(): Unit
----

You can use `clear` to remove all the parameters and hints in `SQLConf`.
