== [[RpcEnv]] RpcEnv -- RPC Environment

[CAUTION]
====
FIXME

* How to know the available endpoints in the environment? See the exercise link:exercises/spark-exercise-custom-rpc-environment.adoc[Developing RPC Environment].
====

*RPC Environment* (aka *RpcEnv*) is an environment for link:spark-rpc-RpcEndpoint.adoc[RpcEndpoints] to process messages. A RPC Environment manages the entire lifecycle of RpcEndpoints:

* registers (sets up) endpoints (by name or uri)
* routes incoming messages to them
* stops them

A RPC Environment is defined by the *name*, *host*, and *port*. It can also be controlled by a *security manager*.

You can create a RPC Environment using <<create, RpcEnv.create>> factory methods.

The only implementation of RPC Environment is link:spark-rpc-netty.adoc[Netty-based implementation]. Read the section <<RpcEnvFactory, RpcEnvFactory>>.

A link:spark-rpc-RpcEndpoint.adoc[RpcEndpoint] defines how to handle *messages* (what *functions* to execute given a message). RpcEndpoints register (with a name or uri) to `RpcEnv` to receive messages from link:spark-RpcEndpointRef.adoc[RpcEndpointRefs].

.RpcEnvironment with RpcEndpoints and RpcEndpointRefs
image::diagrams/rpcenv-endpoints.png[align="center"]

RpcEndpointRefs can be looked up by *name* or *uri* (because different RpcEnvs may have different naming schemes).

`org.apache.spark.rpc` package contains the machinery for RPC communication in Spark.

=== [[setupEndpointRefByURI]] Obtaining RpcEndpointRef by URI -- `setupEndpointRefByURI` Method

CAUTION: FIXME

=== [[shutdown]] `shutdown` Method

CAUTION: FIXME

=== [[setupEndpoint]] `setupEndpoint` Method

CAUTION: FIXME

=== [[awaitTermination]] `awaitTermination` Method

CAUTION: FIXME

=== [[RpcEnvFactory]] RpcEnvFactory

Spark comes with (`private[spark] trait`) `RpcEnvFactory` which is the factory contract to create a RPC Environment.

An RpcEnvFactory implementation has a single method `create(config: RpcEnvConfig): RpcEnv` that returns a `RpcEnv` for a given <<RpcEnvConfig, RpcEnvConfig>>.

There are two `RpcEnvFactory` implementations in Spark:

* `netty` using `org.apache.spark.rpc.netty.NettyRpcEnvFactory`. This is the default factory for RpcEnv as of Spark 1.6.0-SNAPSHOT.
* `akka` using `org.apache.spark.rpc.akka.AkkaRpcEnvFactory`

You can choose an RPC implementation to use by <<settings, spark.rpc>> (default: `netty`). The setting can be one of the two short names for the known RpcEnvFactories - `netty` or `akka` - or a fully-qualified class name of your custom factory (including Netty-based and Akka-based implementations).

```
$ ./bin/spark-shell --conf spark.rpc=netty

$ ./bin/spark-shell --conf spark.rpc=org.apache.spark.rpc.akka.AkkaRpcEnvFactory
```

=== [[ThreadSafeRpcEndpoint]] ThreadSafeRpcEndpoint

`ThreadSafeRpcEndpoint` is a marker link:spark-rpc-RpcEndpoint.adoc[RpcEndpoint] that does nothing by itself but tells...

CAUTION: FIXME What is marker?

NOTE: `ThreadSafeRpcEndpoint` is a `private[spark] trait`.

=== [[RpcAddress]] RpcAddress

*RpcAddress* is the logical address for an RPC Environment, with hostname and port.

RpcAddress is encoded as a *Spark URL*, i.e. `spark://host:port`.

=== [[RpcEndpointAddress]] RpcEndpointAddress

*RpcEndpointAddress* is the logical address for an endpoint registered to an RPC Environment, with <<RpcAddress, RpcAddress>> and *name*.

It is in the format of *spark://[name]@[rpcAddress.host]:[rpcAddress.port]*.

=== [[stop]] Stopping RpcEndpointRef -- `stop` Method

[source, scala]
----
stop(endpoint: RpcEndpointRef): Unit
----

CAUTION: FIXME

=== [[endpoint-lookup-timeout]] Endpoint Lookup Timeout

When a remote endpoint is resolved, a local RPC environment connects to the remote one. It is called *endpoint lookup*. To configure the time needed for the endpoint lookup you can use the following settings.

It is a prioritized list of *lookup timeout* properties (the higher on the list, the more important):

* `spark.rpc.lookupTimeout`
* <<spark.network.timeout, spark.network.timeout>>

Their value can be a number alone (seconds) or any number with time suffix, e.g. `50s`, `100ms`, or `250us`. See <<settings, Settings>>.

=== [[ask-timeout]] Ask Operation Timeout

*Ask operation* is when a RPC client expects a response to a message. It is a blocking operation.

You can control the time to wait for a response using the following settings (in that order):

* <<spark.rpc.askTimeout, spark.rpc.askTimeout>>
* <<spark.network.timeout, spark.network.timeout>>

Their value can be a number alone (seconds) or any number with time suffix, e.g. `50s`, `100ms`, or `250us`. See <<settings, Settings>>.

=== Exceptions

When RpcEnv catches uncaught exceptions, it uses `RpcCallContext.sendFailure` to send exceptions back to the sender, or logging them if no such sender or `NotSerializableException`.

If any error is thrown from one of link:spark-rpc-RpcEndpoint.adoc[RpcEndpoint] methods except `onError`, `onError` will be invoked with the cause. If `onError` throws an error, RpcEnv will ignore it.

=== [[client-mode]] Client Mode = is this an executor or the driver?

When an RPC Environment is initialized link:spark-sparkenv.adoc#createDriverEnv[as part of the initialization of the driver] or link:spark-sparkenv.adoc#createExecutorEnv[executors] (using `RpcEnv.create`), `clientMode` is `false` for the driver and `true` for executors.

```
RpcEnv.create(actorSystemName, hostname, port, conf, securityManager, clientMode = !isDriver)
```

Refer to <<client-mode,Client Mode>> in Netty-based RpcEnv for the implementation-specific details.

=== [[RpcEnvConfig]] RpcEnvConfig

`RpcEnvConfig` is a placeholder for an instance of link:spark-configuration.adoc[SparkConf], the name of the RPC Environment, host and port, a security manager, and <<client-mode, clientMode>>.

=== [[create]][[rpcenv-create]] Creating RpcEnv -- `create` Factory Methods

[source, scala]
----
create(
  name: String,
  host: String,
  port: Int,
  conf: SparkConf,
  securityManager: SecurityManager,
  clientMode: Boolean = false): RpcEnv

create(
  name: String,
  bindAddress: String,
  advertiseAddress: String,
  port: Int,
  conf: SparkConf,
  securityManager: SecurityManager,
  clientMode: Boolean): RpcEnv
----

`create` assumes that you have a <<RpcEnvFactory, RpcEnvFactory>> with an empty constructor so that it can be created via Java Reflection API that is available under `spark.rpc` setting.

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark.rpc]] `spark.rpc`
| `netty`
| RPC implementation to use. See <<RpcEnvFactory, RpcEnvFactory>>.

| [[spark.rpc.lookupTimeout]] `spark.rpc.lookupTimeout`
| `120s`
| Timeout to use for RPC remote endpoint lookup. Refer to <<endpoint-lookup-timeout, Endpoint Lookup Timeout>>

| [[spark.rpc.numRetries]] `spark.rpc.numRetries`
| `3`
| Number of attempts to send a message to and receive a response from a remote endpoint.

| [[spark.rpc.numRetries]] `spark.rpc.retry.wait`
| `3s`
| Time to wait between retries.

| [[spark.rpc.askTimeout]] `spark.rpc.askTimeout`
| `120s`
| Timeout for RPC ask calls. Refer to <<ask-timeout, Ask Operation Timeout>>.

| [[spark.network.timeout]] `spark.network.timeout`
| `120s`
| Network timeout to use for RPC remote endpoint lookup. Fallback for <<spark.rpc.askTimeout, spark.rpc.askTimeout>>.
|===
