== SQLContext

*SQLContext* is the entry point for Spark SQL. Whatever you do in Spark SQL it has to start from <<creating-instance, creating an instance of SQLContext>>.

A `SQLContext` object requires a `SparkContext`, a `CacheManager`, and a `SQLListener`. They are all `transient` and do not participate in serializing a SQLContext.

=== [[creating-instance]] Creating SQLContext Instance

You can create a `SQLContext` using the following constructors:

* `SQLContext(sc: SparkContext)`
* `SQLContext.getOrCreate(sc: SparkContext)`
* `SQLContext.newSession()` allows for creating a new instance of `SQLContext` with a separate SQL configuration (through a shared `SparkContext`).

=== Configuration Properties

You can set Spark SQL configuration properties using:

* `setConf(props: Properties): Unit`
* `setConf(key: String, value: String): Unit`

You can get the current value of a configuration property by key using:

* `getConf(key: String): String`
* `getConf(key: String, defaultValue: String): String`
* `getAllConfs: immutable.Map[String, String]`

NOTE: Properties that start with *spark.sql* are reserved for Spark SQL.

=== Creating DataFrames

==== emptyDataFrame

[source, scala]
----
emptyDataFrame: DataFrame
----

`emptyDataFrame` creates an empty DataFrame. It calls `createDataFrame` with an empty `RDD[Row]` and an empty schema `StructType(Nil)`.

==== createDataFrame for RDD and Seq

[source, scala]
----
createDataFrame[A <: Product](rdd: RDD[A]): DataFrame
createDataFrame[A <: Product](data: Seq[A]): DataFrame
----

`createDataFrame` family of methods can create a `DataFrame` from an `RDD` of Scala's Product types like case classes or tuples or `Seq` thereof.

==== createDataFrame for RDD of Row with Explicit Schema

[source, scala]
----
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
----

This variant of `createDataFrame` creates a `DataFrame` from `RDD` of `Row` and explicit schema.

=== Registering User-Defined Functions (UDF)

[source, scala]
----
udf: UDFRegistration
----

`udf` method gives access to `UDFRegistration` to manipulate user-defined functions.

=== Caching DataFrames in In-Memory Cache

[source, scala]
----
isCached(tableName: String): Boolean
----

`isCached` method asks `CacheManager` whether `tableName` table is cached in memory or not. It simply requests `CacheManager` for `CachedData` and when exists, it assumes the table is cached.

[source, scala]
----
cacheTable(tableName: String): Unit
----

You can cache a table in memory using `cacheTable`.

CAUTION: Why would I want to cache a table?

[source, scala]
----
uncacheTable(tableName: String)
clearCache(): Unit
----

`uncacheTable` and `clearCache` remove one or all in-memory cached tables.

=== [[implicits]] Implicits - SQLContext.implicits

The `implicits` object is a helper class with methods to convert Scala objects into Datasets, DataFrames, and ...FIXME

[source, scala]
----
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._
----

It holds link:spark-sql-dataset.adoc#Encoder[Encoders] for Scala "primitive" types like `Int`, `Double`, `String`, and their collections.

It offers support for creating `Dataset` from `RDD` of any types (for which an Encoder exists in scope), or case classes or tuples, and `Seq`.

It also offers conversions from Scala's `Symbol` or `$` to `Column`.

It also offers conversions from `RDD` or `Seq` of `Product` types like case classes or tuples to `DataFrame`. It has direct conversions from `RDD` of Int, Long and String to `DataFrame` with a single column name `_1`.
