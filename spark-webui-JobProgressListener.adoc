== [[JobProgressListener]] JobProgressListener Spark Listener

`JobProgressListener` is a link:spark-SparkListener.adoc[SparkListener] for link:spark-webui.adoc[web UI].

`JobProgressListener` intercepts the following link:spark-SparkListener.adoc#SparkListenerEvent[Spark events].

.`JobProgressListener` Events
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Handler | Purpose
| <<onJobStart, onJobStart>> |
| <<onJobEnd, onJobEnd>> |
| <<onStageCompleted, onStageCompleted>> |
| <<onTaskStart, onTaskStart>> |
| <<onTaskEnd, onTaskEnd>> |
| <<onExecutorMetricsUpdate, onExecutorMetricsUpdate>> |
| `onBlockManagerAdded` | Records an executor and its block manager in the internal <<executorIdToBlockManagerId, executorIdToBlockManagerId>> registry.
| `onBlockManagerRemoved` | Removes the executor from the internal <<executorIdToBlockManagerId, executorIdToBlockManagerId>> registry.
| `onApplicationStart` | Records a Spark application's start time (in the internal `startTime`).

Used in link:spark-webui-jobs.adoc[Jobs tab] (for a total uptime and the event timeline) and link:spark-webui-jobs.adoc[Job page] (for the event timeline).
| `onApplicationEnd` | Records a Spark application's end time (in the internal `endTime`).

Used in link:spark-webui-jobs.adoc[Jobs tab] (for a total uptime).
| `onTaskGettingResult` | Does nothing.

FIXME: Why is this event intercepted at all?!
|======================

=== [[registries]] Registries

`JobProgressListener` uses registries to collect information about job executions.

.`JobProgressListener` Registries
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Name | Description
| [[stageIdToData]] `stageIdToData` | Holds <<StageUIData, StageUIData>> per stage, i.e. the stage and stage attempt ids.
| [[stageIdToInfo]] `stageIdToInfo` |
| [[stageIdToActiveJobIds]] `stageIdToActiveJobIds` |
| [[jobIdToData]] `jobIdToData` |
| [[activeJobs]] `activeJobs` |
| [[pendingStages]] `pendingStages` |
| [[poolToActiveStages]] `poolToActiveStages` |
| [[executorIdToBlockManagerId]] `executorIdToBlockManagerId` | The lookup table for `BlockManagerId` per executor id.

Used to track block managers so the Stage page can display `Address` in  link:spark-webui-StagePage.adoc#ExecutorTable[Aggregated Metrics by Executor].

FIXME: How does Executors page collect the very same information?
|======================

=== [[onEnvironmentUpdate]] `onEnvironmentUpdate` Method

[source, scala]
----
onEnvironmentUpdate(environmentUpdate: SparkListenerEnvironmentUpdate)
----

=== [[onExecutorMetricsUpdate]] `onExecutorMetricsUpdate` Method

[source, scala]
----
onExecutorMetricsUpdate(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit
----

=== [[onTaskStart]] `onTaskStart` Method

[source, scala]
----
onTaskStart(taskStart: SparkListenerTaskStart): Unit
----

=== [[onTaskEnd]] `onTaskEnd` Method

[source, scala]
----
onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit
----

`onTaskEnd` reads the `TaskInfo` from the input `taskEnd`.

NOTE: `onTaskEnd` does its processing when the `TaskInfo` is available and `stageAttemptId` is not `-1`.

`onTaskEnd` reads the `StageUIData` for the stage and stage attempt ids (using <<stageIdToData, stageIdToData>> registry).

`onTaskEnd` saves `accumulables` in the `StageUIData`.

`onTaskEnd` reads the `ExecutorSummary` for the executor (the task has finished on).

Depending on the task end's reason `onTaskEnd` increments `succeededTasks`, `killedTasks` or `failedTasks` counters.

`onTaskEnd` adds the task's duration to `taskTime`.

`onTaskEnd` decrements the number of active tasks (in the `StageUIData`).

_Again_, depending on the task end's reason `onTaskEnd` computes `errorMessage` and updates `StageUIData`.

CAUTION: FIXME Why is the same information in two different registries -- `stageData` and `execSummary`?!

If `taskMetrics` is available, `updateAggregateMetrics` is executed.

CAUTION: FIXME Why is `updateAggregateMetrics` doing?

The task's `TaskUIData` is looked up in `stageData.taskData` and `updateTaskInfo` and `updateTaskMetrics` are executed. `errorMessage` is updated.

`onTaskEnd` makes sure that the number of tasks in `StageUIData` (`stageData.taskData`) is not above <<spark_ui_retainedTasks, spark.ui.retainedTasks>> and drops the excess.

Ultimately, `onTaskEnd` looks the stage in the internal <<stageIdToActiveJobIds, stageIdToActiveJobIds>> and for each active job reads its `JobUIData` (from <<jobIdToData, jobIdToData>>). It then decrements `numActiveTasks` and depending on the task end's reason, `numCompletedTasks`, `numKilledTasks` or `numFailedTasks`.

=== [[onStageSubmitted]] `onStageSubmitted` Method

[source, scala]
----
onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit
----

=== [[onStageCompleted]] `onStageCompleted` Method

[source, scala]
----
onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit
----

=== [[onJobEnd]] `onJobEnd` Method

[source, scala]
----
onJobEnd(jobEnd: SparkListenerJobEnd): Unit
----

=== [[onJobStart]] `onJobStart` Method

[source, scala]
----
onJobStart(jobStart: SparkListenerJobStart): Unit
----

When called, `onJobStart` reads the optional Spark Job group id (using `SparkListenerJobStart.properties` and `SparkContext.SPARK_JOB_GROUP_ID` key).

It then creates a <<JobUIData, JobUIData>> (as `jobData`) based on the input `jobStart`. `status` attribute is `JobExecutionStatus.RUNNING`.

The internal <<jobGroupToJobIds, jobGroupToJobIds>> is updated with the job group and job ids.

The internal <<pendingStages, pendingStages>> is updated with `StageInfo` for the stage id (for every `StageInfo` in `SparkListenerJobStart.stageInfos` collection).

`numTasks` attribute in the `jobData` (as `JobUIData` instance created above) is set to the sum of tasks in every stage (from `jobStart.stageInfos`) for which `completionTime` attribute is not set.

The internal <<jobIdToData, jobIdToData>> and <<activeJobs, activeJobs>> are updated with `jobData` for the current job.

The internal <<stageIdToActiveJobIds, stageIdToActiveJobIds>> is updated with the stage id and job id (for every stage in the input `jobStart`).

The internal <<stageIdToInfo, stageIdToInfo>> is updated with the stage id and `StageInfo` (for every `StageInfo` in `jobStart.stageInfos`).

A <<StageUIData, StageUIData>> is added to the internal <<stageIdToData, stageIdToData>> for every `StageInfo` (in `jobStart.stageInfos`).

NOTE: `onJobStart` is a part of link:spark-SparkListener.adoc[SparkListener contract] to handle...FIXME

=== [[JobUIData]] JobUIData

CAUTION: FIXME

=== [[blockManagerIds]] blockManagerIds method

[source, scala]
----
blockManagerIds: Seq[BlockManagerId]
----

CAUTION: FIXME

=== [[StageUIData]] StageUIData

CAUTION: FIXME

=== [[schedulingMode]] `schedulingMode` Property

`schedulingMode` property is used to show the link:spark-taskscheduler-schedulingmode.adoc[scheduling mode] for the Spark application in link:spark-webui.adoc[Spark UI].

NOTE: It corresponds to link:spark-taskschedulerimpl.adoc#spark.scheduler.mode[spark.scheduler.mode] setting.

When link:spark-SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] is received, `JobProgressListener` looks up `spark.scheduler.mode` key in `Spark Properties` map to set the internal `schedulingMode` field.

NOTE: It is used in Jobs and Stages tabs.

=== [[settings]] Settings

.Spark Properties
[frame="topbot",options="header",width="100%"]
|======================
| Setting | Default Value | Description
| [[spark_ui_retainedJobs]] `spark.ui.retainedJobs` | `1000` | The number of jobs to hold information about
| [[spark_ui_retainedStages]] `spark.ui.retainedStages` | `1000` | The number of stages to hold information about
| [[spark_ui_retainedTasks]] `spark.ui.retainedTasks` | `100000` | The number of tasks to hold information about
|======================
