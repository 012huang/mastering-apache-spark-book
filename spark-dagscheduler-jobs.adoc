== Jobs

A *job* (aka _action job_ or _active job_) is a top-level work item (computation) submitted to DAGScheduler to <<stages, compute a stage>>.

.A single actions triggers a single jobs
image::diagrams/action-job.png[align="center"]

Computing a job is equivalent to computing the partitions of the RDD the action has been executed upon.

.Computing a job is computing the partitions of an RDD
image::diagrams/rdd-job-partitions.png[align="center"]

NOTE: Note that not all partitions have always to be computed for <<ResultStage, ResultStages>> for actions like `first()` and `lookup()`.

Internally, a job is represented by an instance of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala[private[spark\] class org.apache.spark.scheduler.ActiveJob].

[CAUTION]
====
FIXME

* Where are instances of ActiveJob used?
====

A job can be one of two logical types (that are only distinguished by an internal `finalStage` field of `ActiveJob`):

* *Map-stage job* that computes the map output files for a <<ShuffleMapStage, ShuffleMapStage>> (for `submitMapStage`) before any downstream stages are submitted.
+
It is also used for <<adaptive-query-planning, adaptive query planning>>, to look at map output statistics before submitting later stages.
* *Result job* that computes a <<ResultStage, ResultStage>> to execute an action.

Jobs track how many partitions have already been computed (using `finished` array of `Boolean` elements).
