== Dynamic Allocation in Spark

TIP: See the excellent slides http://www.slideshare.net/databricks/dynamic-allocation-in-spark[Dynamic Allocation in Spark] from Databricks.

* Available since *Spark 1.2.0* with many fixes and extensions up to 1.5.0
* In *static allocation* you reserve resources (CPU, memory) upfront irrespective of how much you really use at a time.
* In *dynamic allocation* you get as much as needed and no more. It allows to scale the number of executors up and down based on workload, i.e. idle executors are removed, and if you need more executors for pending tasks, you request them.
* Scale up / down Policies
** Exponential increase in number of executors due to slow start and we may need slightly more.
** Executor removal after N secs

=== Programmable Dynamic Allocation

* New developer API
** `sc.requestExecutors(5)` to request 5 extra executors
** `sc.killExecutors(Seq("1", "15"))` to kill the executors with IDs 1 and 15.

=== Future

SPARK-4922
4751
7955
