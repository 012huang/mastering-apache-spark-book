== Dynamic Allocation in Spark

TIP: See the excellent slide deck http://www.slideshare.net/databricks/dynamic-allocation-in-spark[Dynamic Allocation in Spark] from Databricks.

* Available since *Spark 1.2.0* with many fixes and extensions up to *1.5.0*.
* Support was first introduced in YARN in 1.2, and then extended to Mesos coarse-grained mode. It is supported in Standalone mode, too.
* In *dynamic allocation* you get as much as needed and no more. It allows to scale the number of executors up and down based on workload, i.e. idle executors are removed, and if you need more executors for pending tasks, you request them.
** In *static allocation* you reserve resources (CPU, memory) upfront irrespective of how much you really use at a time.
* Scale up / down Policies
** Exponential increase in number of executors due to slow start and we may need slightly more.
** Executor removal after N secs

=== Programmable Dynamic Allocation

* New developer API in `SparkContext`:
** `def requestExecutors(numAdditionalExecutors: Int): Boolean` to request 5 extra executors
** `def killExecutors(executorIds: Seq[String]): Boolean` to kill the executors with the IDs.

=== Future

* SPARK-4922
* SPARK-4751
* SPARK-7955
