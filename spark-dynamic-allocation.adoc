== Dynamic Allocation

TIP: See the excellent slide deck http://www.slideshare.net/databricks/dynamic-allocation-in-spark[Dynamic Allocation in Spark] from Databricks.

*Dynamic Allocation* is an opt-in feature that...FIXME.

It is controlled by <<settings, spark.dynamicAllocation.enabled>> property. When `--num-executors` or `spark.executor.instances` are set to non-`0` value, they disable dynamic allocation.

* Available since *Spark 1.2.0* with many fixes and extensions up to *1.5.0*.
* Support was first introduced in YARN in 1.2, and then extended to Mesos coarse-grained mode. It is supported in Standalone mode, too.
* In *dynamic allocation* you get as much as needed and no more. It allows to scale the number of executors up and down based on workload, i.e. idle executors are removed, and if you need more executors for pending tasks, you request them.
** In *static allocation* you reserve resources (CPU, memory) upfront irrespective of how much you really use at a time.
* Scale up / down Policies
** Exponential increase in number of executors due to slow start and we may need slightly more.
** Executor removal after N secs

`ExecutorAllocationManager` is the class responsible for the feature. <<settings, When enabled>>, it is started when the Spark context is initialized.

CAUTION: FIXME Review ExecutorAllocationManager

=== [[metrics]] Metrics

Dynamic Allocation feature uses link:spark-metrics.adoc[Spark Metrics System] (via `ExecutorAllocationManagerSource`) to report metrics about internal status.

The name of the source is *ExecutorAllocationManager*.

It emits the following numbers:

* executors / numberExecutorsToAdd
* executors / numberExecutorsPendingToRemove
* executors / numberAllExecutors
* executors / numberTargetExecutors
* executors / numberMaxNeededExecutors

=== [[settings]] Settings

* `spark.dynamicAllocation.enabled` (default: `false`) - whether dynamic allocation is enabled for the given Spark context. It requires that `spark.executor.instances` (default: `0`) is `0`.

=== Programmable Dynamic Allocation

* New developer API in `SparkContext`:
** `def requestExecutors(numAdditionalExecutors: Int): Boolean` to request 5 extra executors
** `def killExecutors(executorIds: Seq[String]): Boolean` to kill the executors with the IDs.

=== Future

* SPARK-4922
* SPARK-4751
* SPARK-7955
