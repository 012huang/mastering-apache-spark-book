== Dynamic Allocation

TIP: See the excellent slide deck http://www.slideshare.net/databricks/dynamic-allocation-in-spark[Dynamic Allocation in Spark] from Databricks.

*Dynamic Allocation* is an opt-in feature that...FIXME.

It is controlled by <<spark.dynamicAllocation.enabled, spark.dynamicAllocation.enabled>> setting. It assumes that if `--num-executors` command-line option or link:spark-executor.adoc#spark.executor.instances[spark.executor.instances] setting are set their values  are `0`. Otherwise, they disable dynamic allocation.

* Available since *Spark 1.2.0* with many fixes and extensions up to *1.5.0*.
* Support was first introduced in YARN in 1.2, and then extended to Mesos coarse-grained mode. It is supported in Standalone mode, too.
* In *dynamic allocation* you get as much as needed and no more. It allows to scale the number of executors up and down based on workload, i.e. idle executors are removed, and if you need more executors for pending tasks, you request them.
** In *static allocation* you reserve resources (CPU, memory) upfront irrespective of how much you really use at a time.
* Scale up / down Policies
** Exponential increase in number of executors due to slow start and we may need slightly more.
** Executor removal after N secs

`ExecutorAllocationManager` is the class responsible for the feature. <<settings, When enabled>>, it is started when the Spark context is initialized.

CAUTION: FIXME Review ExecutorAllocationManager

=== [[isDynamicAllocationEnabled]] Utils.isDynamicAllocationEnabled method

[source, scala]
----
isDynamicAllocationEnabled(conf: SparkConf): Boolean
----

`isDynamicAllocationEnabled` returns `true` if all the following conditions hold:

1. link:spark-executor.adoc#spark.executor.instances[spark.executor.instances] is `0`
2. <<spark.dynamicAllocation.enabled, spark.dynamicAllocation.enabled>> is enabled
3. link:spark-cluster.adoc[Spark on cluster] is used (by reading link:spark-configuration.adoc#spark.master[spark.master])

Otherwise, it returns `false`.

NOTE: `isDynamicAllocationEnabled` returns `true`, i.e. dynamic allocation is enabled, in link:spark-local.adoc[Spark local (pseudo-cluster)] for testing only (with <<spark.dynamicAllocation.testing, spark.dynamicAllocation.testing>> enabled).

Internally, `isDynamicAllocationEnabled` reads link:spark-executor.adoc#spark.executor.instances[spark.executor.instances] (assumes `0`) and <<spark.dynamicAllocation.enabled, spark.dynamicAllocation.enabled>> setting (assumes `false`).

If the value of `spark.executor.instances` is not `0` and `spark.dynamicAllocation.enabled` is enabled, `isDynamicAllocationEnabled` prints the following WARN message to the logs:

```
WARN Utils: Dynamic Allocation and num executors both set, thus dynamic allocation disabled.
```

NOTE: `isDynamicAllocationEnabled` is used in link:spark-yarn.adoc#getInitialTargetExecutorNumber[Spark on YARN to calculate the initial number of executors].

[TIP]
====
Enable `WARN` logging level for `org.apache.spark.util.Utils` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.Utils=WARN
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[metrics]] Metrics

Dynamic Allocation feature uses link:spark-metrics.adoc[Spark Metrics System] (via `ExecutorAllocationManagerSource`) to report metrics about internal status.

The name of the source is *ExecutorAllocationManager*.

It emits the following numbers:

* executors / numberExecutorsToAdd
* executors / numberExecutorsPendingToRemove
* executors / numberAllExecutors
* executors / numberTargetExecutors
* executors / numberMaxNeededExecutors

=== [[settings]] Settings

==== [[spark.dynamicAllocation.enabled]] spark.dynamicAllocation.enabled

`spark.dynamicAllocation.enabled` (default: `false`) - whether dynamic allocation is enabled for the given Spark context. It requires that link:spark-executor.adoc#spark.executor.instances[spark.executor.instances] is `0` (which is the default value).

==== [[spark.dynamicAllocation.testing]] spark.dynamicAllocation.testing

`spark.dynamicAllocation.testing` is...FIXME

=== Programmable Dynamic Allocation

* New developer API in `SparkContext`:
** `def requestExecutors(numAdditionalExecutors: Int): Boolean` to request 5 extra executors
** `def killExecutors(executorIds: Seq[String]): Boolean` to kill the executors with the IDs.

=== Future

* SPARK-4922
* SPARK-4751
* SPARK-7955
