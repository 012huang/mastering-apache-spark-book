== Dataset

*Dataset* can be seen as a SQL query or the result thereof in Spark SQL. It offers convenience of RDDs with the performance optimizations of DataFrames and the strong static type-safety of Scala. The last feature of bringing the strong type-safety to link:spark-sql-dataframe.adoc[DataFrame] makes Dataset so appealing. All the features together give you a more functional programming interface to work with structured data.

[NOTE]
====
`Dataset` was first introduced in Apache Spark *1.6.0* as an experimental feature, but has since turned itself into a fully supported one.

As a matter of fact, link:spark-sql-dataframe.adoc[DataFrame] - the flagship of previous versions of Spark SQL - is currently a _mere_ type alias for `Dataset[Row]`:

[source, scala]
----
type DataFrame = Dataset[Row]
----

See  https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L45[package object sql].
====

It is only with Datasets to have syntax and analysis checks at compile time (that is not possible using link:spark-sql-dataframe.adoc[DataFrame], regular SQL queries or even RDDs).

Using `Dataset` objects turns `DataFrames` of link:spark-sql-dataframe-row.adoc[Row] instances into a `DataFrames` of case classes with proper names and types (following their equivalents in the case classes). Instead of using indices to access respective fields in a DataFrame and cast it to a type, all this is automatically handled by Datasets and checked by the Scala compiler.

Datasets run on link:spark-sql-tungsten.adoc[Tungsten] to optimize its performance.

[source, scala]
----
scala> val ds = Seq("I am a shiny Dataset!").zipWithIndex.toDS
ds: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]

scala> val df = Seq("I am an old grumpy DataFrame!").zipWithIndex.toDF
df: org.apache.spark.sql.DataFrame = [_1: string, _2: int]

scala> val df = Seq("I am an old grumpy DataFrame!").zipWithIndex.toDF("text", "id")
df: org.apache.spark.sql.DataFrame = [text: string, id: int]
----

A `Dataset` object requires a link:spark-sql-sqlcontext.adoc[SQLContext], a link:spark-sql-query-execution.adoc[QueryExecution], and an <<Encoder, Encoder>>. link:spark-sql-sqlcontext.adoc#creating-datasets[In same cases], a `Dataset` can also be seen as a pair of link:spark-sql-logical-plan.adoc[LogicalPlan] in a given link:spark-sql-sqlcontext.adoc[SQLContext].

NOTE: `SQLContext` and link:spark-sql-query-execution.adoc[QueryExecution] are transient and hence do not participate in Dataset serialization. The only _firmly-tied_ feature of a Dataset is the Encoder.

A `Dataset` is <<Queryable, Queryable>> and `Serializable`, i.e. can be saved to a persistent storage.

A Dataset has a <<schema, schema>>.

You can convert a Dataset to a DataFrame (using `toDF()` method) or an RDD (using `rdd` method).

[source, scala]
----
import sqlContext.implicits._

case class Token(name: String, productId: Int, score: Double)
val data = Seq(
  Token("aaa", 100, 0.12),
  Token("aaa", 200, 0.29),
  Token("bbb", 200, 0.53),
  Token("bbb", 300, 0.42))

// Transform data to a Dataset[Token]
// It doesn't work with type annotation yet
// https://issues.apache.org/jira/browse/SPARK-13456
val ds: Dataset[Token] = data.toDS

// Transform data into a DataFrame with no explicit schema
val df = data.toDF

// Transform DataFrame into a Dataset
val ds = df.as[Token]

scala> ds.show
+----+---------+-----+
|name|productId|score|
+----+---------+-----+
| aaa|      100| 0.12|
| aaa|      200| 0.29|
| bbb|      200| 0.53|
| bbb|      300| 0.42|
+----+---------+-----+

scala> ds.printSchema
root
 |-- name: string (nullable = true)
 |-- productId: integer (nullable = false)
 |-- score: double (nullable = false)

// In DataFrames we work with Row instances
scala> df.map(_.getClass.getName).show(false)
+--------------------------------------------------------------+
|value                                                         |
+--------------------------------------------------------------+
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
+--------------------------------------------------------------+

// In Datasets we work with case class instances
scala> ds.map(_.getClass.getName).show(false)
+---------------------------+
|value                      |
+---------------------------+
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
+---------------------------+

scala> ds.map(_.name).show
+-----+
|value|
+-----+
|  aaa|
|  aaa|
|  bbb|
|  bbb|
+-----+
----

=== [[features]] Features of Dataset API

The features of the Dataset API in Spark SQL:

* **Type-safety** as Datasets are Scala domain objects and operations operate on their attributes. All is checked by the Scala compiler at compile time.

=== [[implicits]] Type Conversions to Dataset[T]

`DatasetHolder` case class offers three methods that do the conversions from `Seq[T]` or `RDD[T]` type to `Dataset[T]`:

* `toDS(): Dataset[T]`
* `toDF(): DataFrame`
* `toDF(colNames: String*): DataFrame`

NOTE: `DataFrame` is a type alias of `Dataset[Row]`.

`DatasetHolder` is used by `SQLImplicits` that is available to use after link:spark-sql-sqlcontext.adoc#implicits[importing implicits object of SQLContext].

[source, scala]
----
scala> Seq("hello").toDS
res0: org.apache.spark.sql.Dataset[String] = [value: string]

scala> Seq("hello").toDF
res1: org.apache.spark.sql.DataFrame = [value: string]

scala> Seq("hello").toDF("text")
res2: org.apache.spark.sql.DataFrame = [text: string]

scala> sc.parallelize(Seq("hello")).toDS
res3: org.apache.spark.sql.Dataset[String] = [value: string]
----

[NOTE]
====
This import is automatically executed in link:spark-shell.adoc[Spark Shell].

```
scala> sc.version
res11: String = 2.0.0-SNAPSHOT

scala> :imports
 1) import sqlContext.implicits._  (52 terms, 31 are implicit)
 2) import sqlContext.sql          (1 terms)
```
====

=== [[rdd]] Converting Datasets into RDDs (using rdd method)

Whenever in need to convert a Dataset into a RDD, executing `rdd` method gives you a RDD of the proper input object type (not link:spark-sql-dataframe.adoc#features[`Row` as in DataFrames]).

[source, scala]
----
scala> val rdd = tokens.rdd
rdd: org.apache.spark.rdd.RDD[Token] = MapPartitionsRDD[11] at rdd at <console>:30
----

=== [[schema]] Schema

A Dataset has a *schema* that is available as `schema`.

You may also use the following methods to learn about the schema:

* `printSchema(): Unit`
* `explain(): Unit`
* `explain(extended: Boolean): Unit`

=== [[plans]] Logical and Physical Plans

CAUTION: FIXME

=== [[types]] Supported Types

CAUTION: FIXME What types are supported by Encoders

=== [[Encoder]] Encoder

CAUTION: FIXME

`Encoder` works with the type of the accompanying Dataset.

An `Encoder` object is used to convert your domain object (a JVM object) into Spark's internal representation. It allows for significantly faster serialization and deserialization (comparing to the default Java serializer).

NOTE: Encoders are part of link:spark-sql-catalyst.adoc[Catalyst Optimizer].

NOTE: link:spark-sql-sqlcontext.adoc#implicits[SQLContext.implicits] object comes with Encoders for many types in Scala.

Encoders map columns (of your dataset) to fields (of your JVM object) by name. It is by Encoders that you can bridge JVM objects to data sources (CSV, JDBC, Parquet, Avro, JSON, Cassandra, Elasticsearch, memsql) and vice versa.

=== [[toJSON]] toJSON

`toJSON` maps the content of `Dataset` to a `Dataset` of JSON strings.

NOTE: A new feature in Spark **2.0.0**.

[source, scala]
----
scala> val ds = Seq("hello", "world", "foo bar").toDS
ds: org.apache.spark.sql.Dataset[String] = [value: string]

scala> ds.toJSON.show
+-------------------+
|              value|
+-------------------+
|  {"value":"hello"}|
|  {"value":"world"}|
|{"value":"foo bar"}|
+-------------------+
----

=== [[explain]] explain

[source, scala]
----
explain(): Unit
explain(extended: Boolean): Unit
----

`explain` prints the logical and physical plans to the console. You can use it for debugging.

TIP: If you are serious about query debugging you could also use the link:spark-sql-query-execution.adoc#debug[Debugging Query Execution facility].

Internally, `explain` uses `SQLContext.executePlan(logicalPlan)`.

[source, scala]
----
val ds = sqlContext.range(10)

scala> ds.explain(extended = true)
== Parsed Logical Plan ==
Range 0, 10, 1, 8, [id#9L]

== Analyzed Logical Plan ==
id: bigint
Range 0, 10, 1, 8, [id#9L]

== Optimized Logical Plan ==
Range 0, 10, 1, 8, [id#9L]

== Physical Plan ==
WholeStageCodegen
:  +- Range 0, 1, 8, 10, [id#9L]
----

=== [[selectExpr]] selectExpr

[source, scala]
----
selectExpr(exprs: String*): DataFrame
----

`selectExpr` is like `select`, but accepts SQL expressions `exprs`.

[source, scala]
----
val ds = sqlContext.range(5)

scala> ds.selectExpr("rand() as random").show
16/04/14 23:16:06 INFO HiveSqlParser: Parsing command: rand() as random
+-------------------+
|             random|
+-------------------+
|  0.887675894185651|
|0.36766085091074086|
| 0.2700020856675186|
| 0.1489033635529543|
| 0.5862990791950973|
+-------------------+
----

Internally, it executes `select` with every expression in `exprs` mapped to link:spark-sql-columns.adoc[Column] (using link:spark-sql-sql-parsers.adoc[SparkSqlParser.parseExpression]).

[source, scala]
----
scala> ds.select(expr("rand() as random")).show
+------------------+
|            random|
+------------------+
|0.5514319279894851|
|0.2876221510433741|
|0.4599999092045741|
|0.5708558868374893|
|0.6223314406247136|
+------------------+
----

NOTE: A new feature in Spark **2.0.0**.

=== [[isStreaming]] isStreaming

`isStreaming` returns `true` when `Dataset` contains link:spark-sql-streamingrelation.adoc[StreamingRelation] or link:spark-sql-streamingrelation.adoc#StreamingExecutionRelation[StreamingExecutionRelation] *streaming sources*.

NOTE: Streaming datasets are created using link:spark-sql-dataframereader.adoc#stream[DataFrameReader.stream] method (for link:spark-sql-streamingrelation.adoc[StreamingRelation]) and contain link:spark-sql-streamingrelation.adoc#StreamingExecutionRelation[StreamingExecutionRelation] after link:spark-sql-dataframewriter.adoc#startStream[DataFrameWriter.startStream].

[source, scala]
----
val reader = sqlContext.read
val helloStream = reader.stream("hello")

scala> helloStream.isStreaming
res9: Boolean = true
----

NOTE: A new feature in Spark **2.0.0**.

=== [[randomSplit]] randomSplit

[source, scala]
----
randomSplit(weights: Array[Double]): Array[Dataset[T]]
randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]
----

`randomSplit` randomly splits the `Dataset` per `weights`.

`weights` doubles should sum up to `1` and will be normalized if they do not.

You can define `seed` and if you don't, a random `seed` will be used.

NOTE: A new feature in Spark **2.0.0**.

NOTE: It is used in link:spark-mllib-estimators.adoc#TrainValidationSplit[TrainValidationSplit] to split dataset into training and validation datasets.

[source, scala]
----
val ds = sqlContext.range(10)
scala> ds.randomSplit(Array[Double](2, 3)).foreach(_.show)
+---+
| id|
+---+
|  0|
|  1|
|  2|
+---+

+---+
| id|
+---+
|  3|
|  4|
|  5|
|  6|
|  7|
|  8|
|  9|
+---+
----

=== [[Queryable]] Queryable

CAUTION: FIXME

=== [[i-want-more]] Further reading or watching

* (video) https://youtu.be/i7l3JQRx7Qw[Structuring Spark: DataFrames, Datasets, and Streaming]
