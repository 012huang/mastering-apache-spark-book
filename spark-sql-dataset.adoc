== Dataset

*Dataset* offers convenience of RDDs with the performance optimizations of DataFrames and the strong static type-safety of Scala. The last feature of bringing the strong type-safety to link:spark-sql-dataframe.adoc[DataFrame] makes Dataset so appealing. All the features together give you a more functional programming interface to work with structured data.

[NOTE]
====
`Dataset` was first introduced in Apache Spark *1.6.0* as an experimental feature, but has since turned itself into a fully supported one. As a matter of fact, link:spark-sql-dataframe.adoc[DataFrame] - the flagship of previous versions of Spark SQL - is currently a _mere_ type alias for `Dataset[Row]`:

[source, scala]
----
type DataFrame = Dataset[Row]
----

See  https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L45[package object sql].
====

It is only with Datasets to have syntax and analysis checks at compile time (that is not possible using link:spark-sql-dataframe.adoc[DataFrame], regular SQL queries or even RDDs).

Using `Dataset` objects turns `DataFrames` of link:spark-sql-dataframe-row.adoc[Row] instances into a `DataFrames` of case classes with proper names and types (following their equivalents in the case classes). Instead of using indices to access respective fields in a DataFrame and cast it to a type, all this is automatically handled by Datasets and checked by the Scala compiler.

Datasets run on link:spark-sql-tungsten.adoc[Tungsten] to optimize its performance.

[source, scala]
----
scala> val ds = Seq("I am a shiny Dataset!").zipWithIndex.toDS
ds: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]

scala> val df = Seq("I am an old grumpy DataFrame!").zipWithIndex.toDF
df: org.apache.spark.sql.DataFrame = [_1: string, _2: int]

scala> val df = Seq("I am an old grumpy DataFrame!").zipWithIndex.toDF("text", "id")
df: org.apache.spark.sql.DataFrame = [text: string, id: int]
----

A `Dataset` object requires a link:spark-sql-sqlcontext.adoc[SQLContext], a <<QueryExecution, QueryExecution>>, and an <<Encoder, Encoder>>. link:spark-sql-sqlcontext.adoc#creating-datasets[In same cases], a `Dataset` can also be seen as a pair of <<LogicalPlan, LogicalPlan>> in a given link:spark-sql-sqlcontext.adoc[SQLContext].

NOTE: `SQLContext` and `QueryExecution` are transient and hence do not participate when a dataset is serialized. The only _firmly-tied_ feature of a Dataset is the Encoder.

A `Dataset` is <<Queryable, Queryable>> and `Serializable`, i.e. can be saved to a persistent storage.

A Dataset has a <<schema, schema>>.

You can convert a Dataset to a DataFrame (using `toDF()` method) or an RDD (using `rdd` method).

[source, scala]
----
import sqlContext.implicits._

case class Token(name: String, productId: Int, score: Double)
val data = Seq(
  Token("aaa", 100, 0.12),
  Token("aaa", 200, 0.29),
  Token("bbb", 200, 0.53),
  Token("bbb", 300, 0.42))

// Transform data to a Dataset[Token]
// It doesn't work with type annotation yet
// https://issues.apache.org/jira/browse/SPARK-13456
val ds: Dataset[Token] = data.toDS

// Transform data into a DataFrame with no explicit schema
val df = data.toDF

// Transform DataFrame into a Dataset
val ds = df.as[Token]

scala> ds.show
+----+---------+-----+
|name|productId|score|
+----+---------+-----+
| aaa|      100| 0.12|
| aaa|      200| 0.29|
| bbb|      200| 0.53|
| bbb|      300| 0.42|
+----+---------+-----+

scala> ds.printSchema
root
 |-- name: string (nullable = true)
 |-- productId: integer (nullable = false)
 |-- score: double (nullable = false)

// In DataFrames we work with Row instances
scala> df.map(_.getClass.getName).show(false)
+--------------------------------------------------------------+
|value                                                         |
+--------------------------------------------------------------+
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
+--------------------------------------------------------------+

// In Datasets we work with case class instances
scala> ds.map(_.getClass.getName).show(false)
+---------------------------+
|value                      |
+---------------------------+
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
+---------------------------+

scala> ds.map(_.name).show
+-----+
|value|
+-----+
|  aaa|
|  aaa|
|  bbb|
|  bbb|
+-----+
----

=== [[features]] Features of Dataset API

The features of the Dataset API in Spark SQL:

* **Type-safety** as Datasets are Scala domain objects and operations operate on their attributes. All is checked by the Scala compiler at compile time.

=== [[implicits]] Type Conversions to Dataset[T]

`DatasetHolder` case class offers three methods that do the conversions from `Seq[T]` or `RDD[T]` type to `Dataset[T]`:

* `toDS(): Dataset[T]`
* `toDF(): DataFrame`
* `toDF(colNames: String*): DataFrame`

NOTE: `DataFrame` is a type alias of `Dataset[Row]`.

`DatasetHolder` is used by `SQLImplicits` that is available to use after link:spark-sql-sqlcontext.adoc#implicits[importing implicits object of SQLContext].

[source, scala]
----
scala> Seq("hello").toDS
res0: org.apache.spark.sql.Dataset[String] = [value: string]

scala> Seq("hello").toDF
res1: org.apache.spark.sql.DataFrame = [value: string]

scala> Seq("hello").toDF("text")
res2: org.apache.spark.sql.DataFrame = [text: string]

scala> sc.parallelize(Seq("hello")).toDS
res3: org.apache.spark.sql.Dataset[String] = [value: string]
----

[NOTE]
====
This import is automatically executed in link:spark-shell.adoc[Spark Shell].

```
scala> sc.version
res11: String = 2.0.0-SNAPSHOT

scala> :imports
 1) import sqlContext.implicits._  (52 terms, 31 are implicit)
 2) import sqlContext.sql          (1 terms)
```
====

=== [[rdd]] Converting Datasets into RDDs (using rdd method)

Whenever in need to convert a Dataset into a RDD, executing `rdd` method gives you a RDD of the proper input object type (not link:spark-sql-dataframe.adoc#features[`Row` as in DataFrames]).

[source, scala]
----
scala> val rdd = tokens.rdd
rdd: org.apache.spark.rdd.RDD[Token] = MapPartitionsRDD[11] at rdd at <console>:30
----

=== [[schema]] Schema

A Dataset has a *schema* that is available as `schema`.

You may also use the following methods to learn about the schema:

* `printSchema(): Unit`
* `explain(): Unit`
* `explain(extended: Boolean): Unit`

=== [[plans]] Logical and Physical Plans

CAUTION: FIXME

=== [[types]] Supported Types

CAUTION: FIXME What types are supported by Encoders

=== [[Encoder]] Encoder

CAUTION: FIXME

`Encoder` works with the type of the accompanying Dataset.

An `Encoder` object is used to convert your domain object (a JVM object) into Spark's internal representation. It is designed for fast serialization and deserialization.

NOTE: Encoders are part of link:spark-sql-catalyst.adoc[Catalyst Optimizer].

NOTE: link:spark-sql-sqlcontext.adoc#implicits[SQLContext.implicits] object comes with Encoders for many types in Scala.

Encoders map columns (of your dataset) to fields (of your JVM object) by name. It is by Encoders that you can bridge JVM objects to data sources (CSV, JDBC, Parquet, Avro, JSON, Cassandra, Elasticsearch, memsql) and vice versa.

=== [[toJSON]] toJSON

`toJSON` maps the content of `Dataset` to a `Dataset` of JSON strings.

NOTE: A new feature in Spark **2.0.0**.

[source, scala]
----
scala> val ds = Seq("hello", "world", "foo bar").toDS
ds: org.apache.spark.sql.Dataset[String] = [value: string]

scala> ds.toJSON.show
+-------------------+
|              value|
+-------------------+
|  {"value":"hello"}|
|  {"value":"world"}|
|{"value":"foo bar"}|
+-------------------+
----

=== [[explain]] explain

[source, scala]
----
explain(): Unit
explain(extended: Boolean): Unit
----

`explain` prints the logical and physical plans to the console. You can use it for debugging.

[source, scala]
----
scala> ds.explain
== Physical Plan ==
LocalTableScan [value#35], [[0,1000000005,6f6c6c6568],[0,1000000005,646c726f77],[0,1000000007,726162206f6f66]]

scala> ds.explain(true)
== Parsed Logical Plan ==
LocalRelation [value#35], [[0,1000000005,6f6c6c6568],[0,1000000005,646c726f77],[0,1000000007,726162206f6f66]]

== Analyzed Logical Plan ==
value: string
LocalRelation [value#35], [[0,1000000005,6f6c6c6568],[0,1000000005,646c726f77],[0,1000000007,726162206f6f66]]

== Optimized Logical Plan ==
LocalRelation [value#35], [[0,1000000005,6f6c6c6568],[0,1000000005,646c726f77],[0,1000000007,726162206f6f66]]

== Physical Plan ==
LocalTableScan [value#35], [[0,1000000005,6f6c6c6568],[0,1000000005,646c726f77],[0,1000000007,726162206f6f66]]
----

=== [[isStreaming]] isStreaming

`isStreaming` returns `true` when `Dataset` contains `StreamingRelation` or `StreamingExecutionRelation` that represent *streaming sources*.

NOTE: A new feature in Spark **2.0.0**.

=== [[QueryExecution]] QueryExecution

CAUTION: FIXME

NOTE: It is a transient feature of a Dataset, i.e. it is not preserved across serializations.

=== [[Queryable]] Queryable

CAUTION: FIXME

=== [[LogicalPlan]] LogicalPlan

CAUTION: FIXME

=== [[i-want-more]] Further reading or watching

* (video) https://youtu.be/i7l3JQRx7Qw[Structuring Spark: DataFrames, Datasets, and Streaming]
