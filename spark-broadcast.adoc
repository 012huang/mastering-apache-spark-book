== Broadcast Variables

From http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables[the official documentation about Broadcast Variables]:

> Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.

And later in the document:

> Explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.

.Broadcasting a value to executors
image::images/sparkcontext-broadcast-executors.png[align="center"]

To use a broadcast value in a transformation you have to create it first using <<sparkcontext-broadcast, SparkContext.broadcast()>> and then use `value` method to access the shared value. Learn it in <<introductory-example, Introductory Example>> section.

The Broadcast feature in Spark uses SparkContext to create broadcast values and link:spark-service-broadcastmanager.adoc[BroadcastManager] and link:spark-service-contextcleaner.adoc[ContextCleaner] to manage their lifecycle.

.SparkContext to broadcast using BroadcastManager and ContextCleaner
image::images/sparkcontext-broadcastmanager-contextcleaner.png[align="center"]

=== [[introductory-example]] Introductory Example

Let's start with an introductory example to check out how to use broadcast variables and build your initial understanding.

You're going to use a static mapping of interesting projects with their websites, i.e. `Map[String, String]` that the tasks, i.e. closures (anonymous functions) in transformations, use.

```
scala> val pws = Map("Apache Spark" -> "http://spark.apache.org/", "Scala" -> "http://www.scala-lang.org/")
pws: scala.collection.immutable.Map[String,String] = Map(Apache Spark -> http://spark.apache.org/, Scala -> http://www.scala-lang.org/)

scala> val websites = sc.parallelize(Seq("Apache Spark", "Scala")).map(pws).collect
...
websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/)
```

It works, but is very ineffective as the `pws` map is sent over the wire to executors while it could have been there already. If there were more tasks that need the `pws` map, you could improve their performance by minimizing the number of bytes that are going to be sent over the network for task execution.

Enter broadcast variables.

```
val pwsB = sc.broadcast(pws)
val websites = sc.parallelize(Seq("Apache Spark", "Scala")).map(pwsB.value).collect
// websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/)
```

Semantically, the two computations - with and without the broadcast value - are exactly the same, but the broadcast-based one wins performance-wise when there are more executors spawned to execute many tasks that use `pws` map.

=== [[introduction]] Introduction

*Broadcast* is part of Spark that is responsible for broadcasting information across nodes in a cluster.

You use broadcast variable to implement *map-side join*, i.e. a join using a `map`. For this, lookup tables are distributed across nodes in a cluster using `broadcast` and then looked up inside `map` (to do the join implicitly).

When you broadcast a value, it is copied to executors only once (while it is copied multiple times for tasks otherwise). It means that broadcast can help to get your Spark application faster if you have a large value to use in tasks or there are more tasks than executors.

It appears that a Spark idiom emerges that uses `broadcast` with `collectAsMap` to create a `Map` for broadcast. When an RDD is `map` over to a smaller dataset (column-wise not record-wise), `collectAsMap`, and `broadcast`, using the very big RDD to map its elements to the broadcast RDDs is computationally faster.

[source, scala]
----
val acMap = sc.broadcast(myRDD.map { case (a,b,c,b) => (a, c) }.collectAsMap)
val otherMap = sc.broadcast(myOtherRDD.collectAsMap)

myBigRDD.map { case (a, b, c, d) =>
  (acMap.value.get(a).get, otherMap.value.get(c).get)
}.collect
----

Use large broadcasted HashMaps over RDDs whenever possible and leave RDDs with a key to lookup necessary data as demonstrated above.

Spark comes with a BitTorrent implementation.

It is not enabled by default.

=== [[sparkcontext-broadcast]] SparkContext.broadcast

Read about `SparkContext.broadcast` method in link:spark-sparkcontext.adoc#creating-broadcast-variables[Creating broadcast variables].

=== Further Reading

* http://dmtolpeko.com/2015/02/20/map-side-join-in-spark/[Map-Side Join in Spark]
