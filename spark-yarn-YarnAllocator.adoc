== YarnAllocator -- Container Allocator

`YarnAllocator` requests containers from the YARN ResourceManager to run Spark link:spark-executor.adoc[executors] on and kills them when link:spark-yarn-applicationmaster.adoc[ApplicationMaster] no longer needs them.

It uses YARN's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/AMRMClient.html[AMRMClient] API.

It is used in link:spark-yarn-applicationmaster.adoc[ApplicationMaster] (via an internal `allocator`) and is created when link:spark-yarn-yarnrmclient.adoc#register[`YarnRMClient` registers an application master with the YARN ResourceManager].

.ApplicationMaster uses YarnAllocator (via allocator attribute)
image::images/spark-yarn-YarnAllocator.png[align="center"]

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.deploy.yarn.YarnAllocator` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.deploy.yarn.YarnAllocator=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[requestTotalExecutorsWithPreferredLocalities]] Requesting Executors with Locality Preferences (requestTotalExecutorsWithPreferredLocalities method)

[source, scala]
----
requestTotalExecutorsWithPreferredLocalities(
  requestedTotal: Int,
  localityAwareTasks: Int,
  hostToLocalTaskCount: Map[String, Int]): Boolean
----

`requestTotalExecutorsWithPreferredLocalities` returns `true` if the current requested total is different than the new value (as the input `requestedTotal`).

`requestTotalExecutorsWithPreferredLocalities` sets the internal `numLocalityAwareTasks` and `hostToLocalTaskCounts` attributes to the input `localityAwareTasks` and `hostToLocalTaskCount` arguments, respectively.

If the input `requestedTotal` is different than the internal `targetNumExecutors` attribute you should see the following INFO message in the logs:

```
INFO Driver requested a total number of [requestedTotal] executor(s).
```

It sets the internal `targetNumExecutors` attribute to the input `requestedTotal` and returns `true`. Otherwise, it returns `false`.

NOTE: `requestTotalExecutorsWithPreferredLocalities` is executed in response to `RequestExecutors` message to link:spark-yarn-AMEndpoint.adoc[ApplicationMaster].

=== [[numLocalityAwareTasks]] numLocalityAwareTasks Internal Counter

[source, scala]
----
numLocalityAwareTasks: Int = 0
----

CAUTION: FIXME

=== [[updateResourceRequests]] Updating YARN Container Allocation Requests (updateResourceRequests method)

[source, scala]
----
updateResourceRequests(): Unit
----

`updateResourceRequests` gets the list of pending YARN's `ContainerRequests` to allocate executors.

`updateResourceRequests` consists of two main branches:

1. <<updateResourceRequests-missing-executors, missing executors>>, i.e. when the number of executors allocated already or pending does not match the needs and so there are missing executors.

2. <<updateResourceRequests-cancelling-executor-allocations, executors to cancel>>, i.e. when the number of pending executor allocations is positive, but the number of all the executors is more than Spark needs.

==== [[updateResourceRequests-missing-executors]] Case 1. Missing Executors

You should see the following INFO message in the logs:

```
INFO YarnAllocator: Will request [count] executor containers, each with [vCores] cores and [memory] MB memory including [memoryOverhead] MB overhead
```

It then splits pending container allocation requests per locality preference of pending tasks (in the internal <<hostToLocalTaskCounts, hostToLocalTaskCounts>> registry).

CAUTION: FIXME Review `splitPendingAllocationsByLocality`

It removes stale container allocation requests (using YARN's link:++https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/AMRMClient.html#removeContainerRequest(T)++[AMRMClient.removeContainerRequest]).

CAUTION: FIXME Stale?

You should see the following INFO message in the logs:

```
INFO YarnAllocator: Canceled [cancelledContainers] container requests (locality no longer needed)
```

It computes locality of requested containers (based on the internal <<numLocalityAwareTasks, numLocalityAwareTasks>>, <<hostToLocalTaskCounts, hostToLocalTaskCounts>> and <<allocatedHostToContainersMap, allocatedHostToContainersMap>> lookup table).

CAUTION: FIXME Review `containerPlacementStrategy.localityOfRequestedContainers` + the code that follows.

For any new container needed `updateResourceRequests` adds a container request (using YARN's link:++https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/AMRMClient.html#addContainerRequest(T)++[AMRMClient.addContainerRequest]).

You should see the following INFO message in the logs:

```
INFO YarnAllocator: Submitted container request (host: [host], capability: [resource])
```

==== [[updateResourceRequests-cancelling-executor-allocations]] Case 2. Cancelling Pending Executor Allocations

When there are executors to cancel (case 2.), you should see the following INFO message in the logs:

```
INFO Canceling requests for [numToCancel] executor container(s) to have a new desired total [targetNumExecutors] executors.
```

It checks whether there are pending allocation requests and removes the excess (using YARN's link:++https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/AMRMClient.html#removeContainerRequest(T)++[AMRMClient.removeContainerRequest]). If there are no pending allocation requests, you should see the WARN message in the logs:

```
WARN Expected to find pending requests, but found none.
```

=== [[handleAllocatedContainers]] handleAllocatedContainers

[source, scala]
----
handleAllocatedContainers(allocatedContainers: Seq[Container]): Unit
----

CAUTION: FIXME

=== [[processCompletedContainers]] processCompletedContainers

[source, scala]
----
processCompletedContainers(completedContainers: Seq[ContainerStatus]): Unit
----

`processCompletedContainers` accepts a collection of YARN's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/ContainerStatus.html[ContainerStatus]'es.

[NOTE]
====
`ContainerStatus` represents the current status of a YARN `Container` and provides details such as:

* Id
* State
* Exit status of a completed container.
* Diagnostic message for a failed container.
====

For each completed container in the collection, `processCompletedContainers` removes it from the internal <<releasedContainers, releasedContainers>> registry.

It looks the host of the container up (in the internal <<allocatedContainerToHostMap, allocatedContainerToHostMap>> lookup table). The host may or may not exist in the lookup table.

CAUTION: FIXME The host may or may not exist in the lookup table?

The `ExecutorExited` exit reason is computed.

When the host of the completed container has been found, the internal <<numExecutorsRunning, numExecutorsRunning>> counter is decremented.

You should see the following INFO message in the logs:

```
INFO Completed container [containerId] [host] (state: [containerState], exit status: [containerExitStatus])
```

For `ContainerExitStatus.SUCCESS` and `ContainerExitStatus.PREEMPTED` exit statuses of the container (which are not considered application failures), you should see one of the two possible INFO messages in the logs:

```
INFO Executor for container [id] exited because of a YARN event (e.g., pre-emption) and not because of an error in the running job.
```

```
INFO Container [id] [host] was preempted.
```

Other exit statuses of the container are considered application failures and reported as a WARN message in the logs:

```
WARN Container killed by YARN for exceeding memory limits. [diagnostics] Consider boosting spark.yarn.executor.memoryOverhead.
```

or

```
WARN Container marked as failed: [id] [host]. Exit status: [containerExitStatus]. Diagnostics: [containerDiagnostics]
```

The host is looked up in the internal <<allocatedHostToContainersMap, allocatedHostToContainersMap>> lookup table. If found, the container is removed from the containers registered for the host or the host itself is removed from the lookup table when this container was the last on the host.

The container is removed from the internal <<allocatedContainerToHostMap, allocatedContainerToHostMap>> lookup table.

The container is removed from the internal <<containerIdToExecutorId, containerIdToExecutorId>> translation table. If an executor is found, it is removed from the internal <<executorIdToContainer, executorIdToContainer>> translation table.

If the executor was recorded in the internal <<pendingLossReasonRequests, pendingLossReasonRequests>> lookup table, the exit reason (as calculated earlier as `ExecutorExited`) is sent back for every pending RPC message recorded.

If no executor was found, the executor and the exit reason are recorded in the internal <<releasedExecutorLossReasons, releasedExecutorLossReasons>> lookup table.

In case the container was not in the internal <<releasedContainers, releasedContainers>> registry, the internal <<numUnexpectedContainerRelease, numUnexpectedContainerRelease>> counter is increased and a `RemoveExecutor` RPC message is sent to the driver (as specified when <<creating-instance, `YarnAllocator` was created>>) to notify about the failure of the executor.

=== [[numUnexpectedContainerRelease]] numUnexpectedContainerRelease Internal Counter

=== [[releasedExecutorLossReasons]] releasedExecutorLossReasons Internal Lookup Table

CAUTION: FIXME

=== [[pendingLossReasonRequests]] pendingLossReasonRequests Internal Lookup Table

CAUTION: FIXME

=== [[executorIdToContainer]] executorIdToContainer Internal Translation Table

CAUTION: FIXME

=== [[containerIdToExecutorId]] containerIdToExecutorId Internal Translation Table

CAUTION: FIXME

=== [[allocatedHostToContainersMap]] allocatedHostToContainersMap Internal Lookup Table

CAUTION: FIXME

=== [[numExecutorsRunning]] numExecutorsRunning Internal Counter

CAUTION: FIXME

=== [[allocatedContainerToHostMap]] allocatedContainerToHostMap Internal Lookup Table

CAUTION: FIXME

=== [[releasedContainers]] releasedContainers Internal Registry

CAUTION: FIXME

=== [[allocateResources]] allocateResources

[source, scala]
----
allocateResources(): Unit
----

`allocateResources` is...???

It starts by executing <<updateResourceRequests, updateResourceRequests>>. It then link:++https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/client/api/AMRMClient.html#allocate(float)++[requests additional containers using YARN's `AMRMClient`] with progress indicator of `0.1f`.

It link:++https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/protocolrecords/AllocateResponse.html#getAllocatedContainers()++[gets the list of newly-allocated containers by the YARN ResourceManager].

If the number of allocated containers is greater than `0`, you should see the following DEBUG message in the logs (in stderr on YARN):

```
DEBUG YarnAllocator: Allocated containers: [allocatedContainersSize]. Current executor count: [numExecutorsRunning]. Cluster resources: [availableResources].
```

It <<handleAllocatedContainers, launches executors on the allocated YARN containers>>.

It link:++https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/protocolrecords/AllocateResponse.html#getCompletedContainersStatuses()++[gets the list of completed containers' statuses from YARN].

If the number of completed containers is greater than `0`, you should see the following DEBUG message in the logs (in stderr on YARN):

```
DEBUG YarnAllocator: Completed [completedContainersSize] containers
```

It <<processCompletedContainers, processes completed containers>>.

You should see the following DEBUG message in the logs (in stderr on YARN):

```
DEBUG YarnAllocator: Finished processing [completedContainersSize] completed containers. Current running executor count: [numExecutorsRunning].
```

It is executed when link:spark-yarn-applicationmaster.adoc#registerAM[`ApplicationMaster` is registered to the YARN ResourceManager].

=== [[creating-instance]] Creating YarnAllocator Instance

When `YarnAllocator` is created, it sets the `org.apache.hadoop.yarn.util.RackResolver` logger to `WARN` (unless set to some log level already).

It creates an empty <<releasedContainers, releasedContainers>> and sets the internal `numExecutorsRunning` variable to `0`.

It sets the internal `executorIdCounter` counter to the link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc#RetrieveLastAllocatedExecutorId[last allocated executor id].

It creates an empty <<failedExecutorsTimeStamps, failedExecutorsTimeStamps>> queue.

It sets the internal `executorFailuresValidityInterval` to link:spark-yarn-settings.adoc#spark.yarn.executor.failuresValidityInterval[spark.yarn.executor.failuresValidityInterval].

It sets the internal `targetNumExecutors` counter to link:spark-yarn-YarnSparkHadoopUtil.adoc#getInitialTargetExecutorNumber[the initial number of executors].

It creates an empty <<pendingLossReasonRequests, pendingLossReasonRequests>> collection of...FIXME

It creates an empty <<releasedExecutorLossReasons, releasedExecutorLossReasons>> collection of...FIXME

It creates an empty <<executorIdToContainer, executorIdToContainer>> collection of...FIXME

It sets the internal `numUnexpectedContainerRelease` counter to `0L`.

It creates an empty <<containerIdToExecutorId, containerIdToExecutorId>> collection of...FIXME

It sets the internal `executorMemory` to link:spark-executor.adoc#spark.executor.memory[spark.executor.memory].

It sets the internal `memoryOverhead` to link:spark-yarn-settings.adoc#spark.yarn.executor.memoryOverhead[spark.yarn.executor.memoryOverhead]. If unavailable, it is set to the maximum of 10% of `executorMemory` and `384`.

It sets the internal `executorCores` to link:spark-executor.adoc#spark.executor.cores[spark.executor.cores].

It creates the internal `resource` to Hadoop YARN's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/Resource.html[Resource] with both `executorMemory + memoryOverhead` memory and `executorCores` CPU cores.

It creates the internal `launcherPool` called *ContainerLauncher* with maximum link:spark-yarn-settings.adoc#spark.yarn.containerLauncherMaxThreads[spark.yarn.containerLauncherMaxThreads] threads.

It sets the internal `launchContainers` to link:spark-yarn-settings.adoc#spark.yarn.launchContainers[spark.yarn.launchContainers].

It sets the internal `labelExpression` to link:spark-yarn-settings.adoc#spark.yarn.executor.nodeLabelExpression[spark.yarn.executor.nodeLabelExpression].

It sets the internal `nodeLabelConstructor` to...FIXME

CAUTION: FIXME nodeLabelConstructor?

It creates an empty <<hostToLocalTaskCounts, hostToLocalTaskCounts>> collection of...FIXME

It sets the internal `numLocalityAwareTasks` counter to `0`.

It sets the internal `containerPlacementStrategy` to...FIXME

CAUTION: FIXME LocalityPreferredContainerPlacementStrategy?

=== [[internal-registries]] Internal Registries

==== [[hostToLocalTaskCounts]] hostToLocalTaskCounts

[source, scala]
----
hostToLocalTaskCounts: Map[String, Int] = Map.empty
----

CAUTION: FIXME

==== [[containerIdToExecutorId]] containerIdToExecutorId

CAUTION: FIXME

==== [[executorIdToContainer]] executorIdToContainer

CAUTION: FIXME

==== [[releasedExecutorLossReasons]] releasedExecutorLossReasons

CAUTION: FIXME

==== [[pendingLossReasonRequests]] pendingLossReasonRequests

CAUTION: FIXME

==== [[failedExecutorsTimeStamps]] failedExecutorsTimeStamps

CAUTION: FIXME

==== [[releasedContainers]] releasedContainers

`releasedContainers` contains containers of no use anymore by their globally unique identifier https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/ContainerId.html[ContainerId] (for a `Container` in the cluster).

NOTE: Hadoop YARN's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/yarn/api/records/Container.html[Container] represents an allocated resource in the cluster. The YARN ResourceManager is the sole authority to allocate any `Container` to applications. The allocated `Container` is always on a single node and has a unique `ContainerId`. It has a specific amount of `Resource` allocated.
