== RDD - Resilient Distributed Datasets

The original paper that gave birth to the concept of RDD - https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing]

* Transformations and actions
* checkpointing (how it works, what is checkpointed, and at what frequency)

* *Actions* launch *a job* to return a value to the user program
** See `sc.runJob`
* an RDD comprises a fixed number of *partitions*
* each partition comprises of a number of records.
* _narrow_ transformations - the result of `map`, `filter` and such is from the data from a single partition. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.
* _wide_ transformations - the result of `groupByKey` and `reduceByKey`. the data required to compute the records in a single partition may reside in many partitions of the parent RDD. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions.

Spark does jobs in parallel, and RDDs and dataframes are split into parts, processed and written in parallel, and saving the parts results in part-files instead of single csv.

Interesting methods:

* `coalesce`

=== Creating RDDs

One way to create a RDD is with `SparkContext.parallelize` method. It accepts a collection of elements as shown below (`sc` is a SparkContext instance):

```
scala> val rdd = sc.parallelize(1 to 1000)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25
```

FIXME: How is this different from `makeRDD`?

```
scala> sc.makeRDD(0 to 1000)
res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25
```

Given the reason to use Spark to process more data than your own laptop could handle, `SparkContext.parallelize` is mainly used to learn Spark in the Spark shell. `SparkContext.parallelize` requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop.

The other option to create RDDs is to use `sc.textFile` (see http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[SparkContext] API) that automatically distributes the data across a Spark cluster.

=== Different sorts of RDDs

* `reduceByKey`
