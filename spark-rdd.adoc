== RDD - Resilient Distributed Dataset

=== Introduction

.The origins of RDD
****
The original paper that gave birth to the concept of RDD is https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al.
****

*Resilient Distributed Dataset (RDD)* is the main data abstraction in Spark. It is simply the bread and butter of Spark, and mastering the concept is of utmost importance to become a Spark pro. _And you wanna be a Spark pro, don't you?_

With RDD the creators of Spark managed to hide data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages.

Learning about RDD by its name:

* *Resilient*, i.e. fault-tolerant and so able to recompute missing or damaged partitions on node failures with the help of <<lineage, RDD Lineage>>.
* *Distributed* across link:spark-cluster.adoc[clusters].
* *Dataset* is a collection of link:spark-rdd-partitions.adoc[partitioned data].

.RDDs
image::diagrams/spark-rdds.png[align="center"]

From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD]:

> A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel.

From the original paper about RDD - https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]:

> Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a
fault-tolerant manner.

Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits:

* *Immutable*, i.e. it does not change once created.
* *Lazy evaluated*, i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution.
* *Cacheable*, i.e. you can hold all the data in a persistent "storage" like memory (default and the most preferred) or disk (the least preferred due to access speed).
* *Parallel*, i.e. process data in parallel.

RDDs are distributed by design and to achieve even *data distribution* as well as leverage link:spark-data-locality.adoc[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are *partitioned* to a fixed number of link:spark-rdd-partitions.adoc[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of *records*.

.RDDs
image::diagrams/spark-rdd-partitioned-distributed.png[align="center"]

link:spark-rdd-partitions.adoc[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using `repartition` or `coalesce` operations. Spark tries to be as close to data as possible without wasting time to send data across network by means of link:spark-rdd-shuffle.adoc[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions.

RDDs support two kinds of operations:

* <<transformations, transformations>> - lazy operations that return another RDD.
* <<actions, actions>> - operations that trigger computation and return values.

The motivation to create RDD were (https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently:

* *iterative algorithms* in machine learning and graph computations.
* *interactive data mining tools* as ad-hoc queries on the same dataset.

The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the
network.

Each RDD is characterized by five main properties:

* An array of link:spark-rdd-partitions.adoc[Partitions]
* A function to compute partitions
* link:spark-rdd-dependencies.adoc[Dependencies on other RDDs]
* A link:spark-rdd-partitions.adoc#partitioner[Partitioner] (for key-value RDDs)
* <<preferred-locations, Preferred locations>>

An RDD is a named (by *name*) and uniquely identified (by *id*) entity inside a SparkContext. An RDD can optionally have a friendly name accessible using `name` that can be changed using `=`:


```
scala> val ns = sc.parallelize(0 to 10)
ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24

scala> ns.id
res0: Int = 2

scala> ns.name
res1: String = null

scala> ns.name = "Friendly name"
ns.name: String = Friendly name

scala> ns.name
res2: String = Friendly name

scala> ns.toDebugString
res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 []
```

RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using link:spark-execution-model.adoc#executors[executors]) can hold some of them.

In general, data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory.

Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially.

Saving partitions results in part-files instead of one single file (unless there is a single partition).

A RDD lives inside a SparkContext and since SparkContexts are logical boundaries, RDDs can't be shared between SparkContexts (see link:spark-sparkcontext.adoc#sparkcontext-and-rdd[SparkContext and RDDs]).

=== Types of RDDs

There are a few more interesting types of RDDs:

* *MapPartitionsRDD* - a result of calling operations like `map`, `flatMap`, `filter`, `mapPartitions`, etc.
* *CoalescedRDD* - a result of calling operations like `repartition` and `coalesce`
* *ShuffledRDD* - a result of shuffling, e.g. after `repartition` and `coalesce`
* *PipedRDD* - an RDD created by piping elements to a forked external process.
* *PairRDD* (implicit conversion as `org.apache.spark.rdd.PairRDDFunctions`) that is an RDD of key-value pairs that is a result of `groupByKey` and `join` operations.
* *DoubleRDD* (implicit conversion as `org.apache.spark.rdd.DoubleRDDFunctions`) that is an RDD of `Double` type.
* *SequenceFileRDD* (implicit conversion as `org.apache.spark.rdd.SequenceFileRDDFunctions`) that is an RDD that can be saved as a `SequenceFile`.

Appropriate operations of a given RDD type are automatically available on a RDD of the right type, e.g. `RDD[(Int, Int)]`, through implicit conversion in Scala.

==== [[HadoopRDD]] HadoopRDD

https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.HadoopRDD[HadoopRDD] is an RDD that provides core functionality for reading data stored in HDFS using the older MapReduce API (https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/package-summary.html[org.apache.hadoop.mapred]).

Partitions are of type `HadoopPartition`.

When an HadoopRDD is computed, i.e. an action is called, you should see the INFO message `Input split:` in the logs.

```
scala> sc.textFile("README.md").count
...
15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784
15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:1784+1784
...
```

The following properties are set upon partition exection:

* *mapred.tip.id* - task id of this task's attempt
* *mapred.task.id* - task attempt's id
* *mapred.task.is.map* as `true`
* *mapred.task.partition* - split id
* *mapred.job.id*

Spark settings for HadoopRDD:

* *spark.hadoop.cloneConf* (default: `false`) - shouldCloneJobConf - should a Hadoop job configuration `JobConf` object be cloned before spawning a Hadoop job. Refer to https://issues.apache.org/jira/browse/SPARK-2546[[SPARK-2546\] Configuration object thread safety issue]. When `true`, you should see a DEBUG message `Cloning Hadoop Configuration`.

You can register callbacks on TaskContext.

HadoopRDDs are not checkpointed. They do nothing when `checkpoint()` is called.

HadoopRDD is created in the following methods in link:spark-sparkcontext.adoc[SparkContext]:

* `hadoopFile`
* `textFile` (the most often used in examples!)
* `sequenceFile`

[CAUTION]
====
FIXME

* What are `InputMetrics`?
* What is `JobConf`?
* What are the InputSplits: `FileSplit` and `CombineFileSplit`? (review `SparkHadoopUtil.get.getFSBytesReadOnThreadCallback()`)
* What are `InputFormat` and `Configurable` subtypes?
* What's InputFormat's RecordReader? It creates a key and a value. What are they?
* What does TaskContext do?
* What's Hadoop Split? input splits for Hadoop reads? See `InputFormat.getSplits`
====

=== [[transformations]] Transformations

A *transformation* is a lazy operation on a RDD that returns another RDD, like `RDD.map`, `RDD.flatMap`, `RDD.filter`, `RDD.reduceByKey`, `RDD.join`, `RDD.cogroup`.

[source,scala]
----
scala> val words = lines.flatMap(_.split("\\s+"))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:26
----

You can chain transformations to create *pipelines* (lazy computations), but their execution is only performed after an action executes.

There are two kinds of transformations:

* <<narrow-transformations, narrow transformations>>
* <<wide-transformations, wide transformations>>

==== [[narrow-transformations]] Narrow transformations

*Narrow transformations* are the result of `map`, `filter` and such that is from the data from a single partition only, i.e. it is self-sustained.

An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.

Spark groups narrow transformations as a stage.

==== [[wide-transformations]] Wide transformations

*Wide transformations* are the result of `groupByKey` and `reduceByKey`. The data required to compute the records in a single partition may reside in many partitions of the parent RDD.

All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute link:spark-rdd-shuffle.adoc[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions.

=== [[actions]] Actions

An *action* triggers execution of <<transformations, RDD transformations>> and returns a value (to a Spark driver - the user program).

You can think of actions as a valve and until no action is fired, the data to be processed is not even in the pipes, i.e. transformations. They are only actions to materialize the entire processing pipeline with real data.

Action operations:

* `count`
* `reduce`
* `collect`
* `take`
* `first`
* <<saving-content-to-files, saveAs* actions>>, e.g. `saveAsTextFile`, `saveAsHadoopFile`.
* `countByValue`

Actions launch link:spark-scheduler.adoc#jobs[jobs] (on all partitions of an RDD) using link:spark-sparkcontext.adoc#running-jobs[SparkContext.runJob].

[source,scala]
----
scala> words.count  // <1>
res0: Long = 502
----
<1> `words` is an RDD of `String`.

TIP: You should `cache` an RDD you work with when you want to execute two or more actions on it for better performance. Refer to link:spark-rdd-caching.adoc[RDD Caching / Persistence].

Before calling an action, Spark does closure/function cleaning (using `SparkContext.clean`) to make it ready to be serialized and send to executors.

NOTE: Spark uses `ClosureCleaner` to clean closures.

Refer to link:spark-execution-model.adoc[Spark Execution Model] to learn the low-level execution details of actions.

==== [[saving-content-to-files]] Saving content to files

An RDD, i.e. the partitions, can be saved to a file using the following actions:

* saveAsTextFile
* saveAsObjectFile
* saveAsSequenceFile

=== Creating RDDs

==== SparkContext.parallelize

One way to create a RDD is with `SparkContext.parallelize` method. It accepts a collection of elements as shown below (`sc` is a SparkContext instance):

```
scala> val rdd = sc.parallelize(1 to 1000)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25
```

You may also want to randomize the sample data:

```
scala> val data = Seq.fill(10)(util.Random.nextInt)
data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586)

scala> val rdd = sc.parallelize(data)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29
```

Given the reason to use Spark to process more data than your own laptop could handle, `SparkContext.parallelize` is mainly used to learn Spark in the Spark shell. `SparkContext.parallelize` requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop.

==== SparkContext.makeRDD

CAUTION: FIXME What's the use case for `makeRDD`?

```
scala> sc.makeRDD(0 to 1000)
res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25
```

==== SparkContext.textFile

One of the easiest ways to create an RDD is to use `SparkContext.textFile` to read files. You can use the local `README.md` file (and then `map` it over to have an RDD of sequences of words):

```
scala> val words = sc.textFile("README.md").flatMap(_.split("\\s+")).cache()
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24
```

NOTE: You `cache()` it so the computation is not performed every time you work with `words`.

Refer to link:spark-files.adoc[Using Files] to learn about creating RDDs out of files.

==== Transformations

RDD transformations by definition transform an RDD into another RDD and hance are the way to create new ones.

Refer to <<transformations, Transformations>> section to learn more.

=== RDDs in Web UI

It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for link:spark-shell.adoc[Spark shell].

Execute the following Spark application (type all the lines in `spark-shell`):

[source,scala]
----
val ints = sc.parallelize(1 to 100) // <1>
ints.setName("Hundred ints")        // <2>
ints.cache                          // <3>
ints.count                          // <4>
----
<1> Creates an RDD with hundreds of numbers (with as many partitions as possible)
<2> Sets the name of the RDD
<3> Caches the RDD (so it shows up in Storage in UI)
<4> Executes action (and materializes the RDD)

With the above executed, you should see the following in the Web UI:

.RDD with custom name
image::images/spark-ui-rdd-name.png[]

Click the name of the RDD (under *RDD Name*) and you will get the details of how the RDD is cached.

.RDD Storage Info
image::images/spark-ui-storage-hundred-ints.png[]

Execute the following Spark job and you will see how the number of partitions decreases.

```
ints.repartition(2).count
```

.Number of tasks after repartition
image::images/spark-ui-repartition-2.png[]

=== Internals of RDDs

* `compute(split: Partition, context: TaskContext): Iterator[T]` computes a given link:spark-rdd-partitions.adoc[RDD partition]. It is implemented by any RDD in Spark.
** Called unless RDD is link:spark-rdd-checkpointing.adoc[checkpointed].

=== [[preferred-locations]] Preferred Locations

A *preferred location* (aka _placement preferences_) is a block location for an HDFS file where to compute each partition on.

`def getPreferredLocations(split: Partition): Seq[String]` specifies placement preferences for a partition in an RDD.

=== [[lineage]] RDD Lineage

==== toDebugString

```
scala> val wordsCount = sc.textFile("README.md").flatMap(_.split("\\s+")).map((_, 1)).reduceByKey(_ + _)
wordsCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[24] at reduceByKey at <console>:24

scala> wordsCount.toDebugString
res2: String =
(2) ShuffledRDD[24] at reduceByKey at <console>:24 []
 +-(2) MapPartitionsRDD[23] at map at <console>:24 []
    |  MapPartitionsRDD[22] at flatMap at <console>:24 []
    |  MapPartitionsRDD[21] at textFile at <console>:24 []
    |  README.md HadoopRDD[20] at textFile at <console>:24 []
```

==== spark.logLineage

Enable `spark.logLineage` (assumed: `false`) to see recursive dependencies of RDDs using `RDD.toDebugString` method every time an action is called.

```
$ ./bin/spark-shell --conf spark.logLineage=true
...
scala> System.getProperty("spark.logLineage")
res0: String = true

scala> sc.textFile("README.md", 4).count
...
15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25
15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies:
(4) MapPartitionsRDD[1] at textFile at <console>:25 []
 |  README.md HadoopRDD[0] at textFile at <console>:25 []
```

=== Other findings

* Some (all?) operations of an RDD clean computations (closures) so they are ready to be serialized and sent to tasks. Cleaning computations throws `SparkException` if computation cannot be cleaned.
