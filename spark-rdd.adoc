== RDD - Resilient Distributed Dataset

TIP: The original paper that gave birth to the concept of RDD - https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing].

RDD = *Resilient Distributed Dataset*

* *Resilient* means fault-tolerant (since RDDs are *distributed* and nodes will fail and often).
* *Distributed* means using many machines (aka nodes).
* *Dataset* is a collection of data (since RDDs are *partitioned* across distributed nodes).

TIP: RDD is the only abstraction in Spark. Learn it in depth and you understand Spark.

Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits:

* *immutable*, i.e. it does not change once created.
* *lazy evaluated*, i.e. the data inside is not going to be transformed until action execution.
* *cacheable*, i.e. you can hold all the data in a persistent "storage" like memory (the most preferred) or disk (the least preferred due to access speed).

RDDs are distributed by definition and to achieve proper *data distribution* as well as leverage *data locality* (in distributed systems like HDFS or Cassandra), it is *partitioned* to a fixed number of *partitions* - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of a number of records.

RDD data is a collection of partitions that are in turn references to physical chunks of data. There's a one-to-one mapping between data in HDFS and partitions (see link:spark-data-locality.adoc[data locality]).

In general, data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory.

Partitions are the units of parallelism.

Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed serially.

Saving partitions results in part-files instead of one single file.

=== Transformations and actions

* *Actions* launch *a job* to return a value to the user program
** See `sc.runJob`
* _narrow_ transformations - the result of `map`, `filter` and such is from the data from a single partition. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.
* _wide_ transformations - the result of `groupByKey` and `reduceByKey`. the data required to compute the records in a single partition may reside in many partitions of the parent RDD. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions.

Explore:

* `coalesce` method

=== Creating RDDs

One way to create a RDD is with `SparkContext.parallelize` method. It accepts a collection of elements as shown below (`sc` is a SparkContext instance):

```
scala> val rdd = sc.parallelize(1 to 1000)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25
```

FIXME: How is this different from `makeRDD`?

```
scala> sc.makeRDD(0 to 1000)
res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25
```

Given the reason to use Spark to process more data than your own laptop could handle, `SparkContext.parallelize` is mainly used to learn Spark in the Spark shell. `SparkContext.parallelize` requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop.

The other option to create RDDs is to use `sc.textFile` (see http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[SparkContext] API) that automatically distributes the data across a Spark cluster.

=== RDDs in Web UI

It's quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for `spark-shell`.

Execute the following Spark application:

[source,scala]
----
val ints = sc.parallelize(1 to 100) // <1>
ints.setName("Hundred ints")        // <2>
ints.cache                          // <3>
ints.count                          // <4>
----
<1> Creates an RDD with hundreds of numbers (with as many partitions as possible)
<2> Sets the name of the RDD
<3> Caches the RDD (so it shows up in Storage in UI)
<4> Executes action (and materializes the RDD)

With the above executed, you should see the following in the Web UI:

.RDD with custom name
image::images/spark-ui-rdd-name.png[]

Click the name of the RDD (under *RDD Name*) and you will get the details of how the RDD is cached.

.RDD Storage Info
image::images/spark-ui-storage-hundred-ints.png[]

Execute the following Spark job and you will see how the number of partitions decreases.

```
ints.repartition(2).count
```

.Number of tasks after repartition
image::images/spark-ui-repartition-2.png[]

=== Different sorts of RDDs

* `reduceByKey`

=== Partitions

Concepts:

* size
* number
* partitioning scheme
* node distribution
* repartitioning

Depending on how you look at Spark (programmer, devop, admin), an RDD is about the content (developer's and data scientist's perspective) or how it gets spread out over a cluster (performance), i.e. how many partitions an RDD represents.

[TIP]
Read https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?]

RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application.

As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]:

> When a stage executes, you can see the number of partitions for a given stage in the Spark UI.

Start `spark-shell` and see it yourself!

```
$ ./bin/spark-shell
Spark context available as sc.

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc.parallelize(1 to 100).count
res0: Long = 100
```

When you execute the Spark job, i.e. `sc.parallelize(1 to 100).count`, you should see the following in http://localhost:4040/jobs[Spark shell application UI].

.The number of partition as Total tasks in UI
image::images/spark-partitions-ui-stages.png[]

The reason for `8` Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of _all_ available cores.

```
$ sysctl -n hw.ncpu
8
```

You can control the number of partitions, using the second input parameter to many transformations.

```
scala> sc.parallelize(1 to 100, 2).count
res1: Long = 100
```

.Total tasks in UI shows 2 partitions
image::images/spark-partitions-ui-stages-2-partitions.png[]

You can always ask for the number of partitions using `partitions` method of a RDD:

```
scala> val ints = sc.parallelize(1 to 100, 4)
ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24

scala> ints.partitions.size
res2: Int = 4
```

In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks,  which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Also, the number of partitions determines how many files get generated by actions that save RDDs to files.

The maximum size of any one partition is ultimately limited by the available memory of any single executor.

In the first RDD transformation, e.g. reading from a file using `sc.textFile(path, partition)`, the `partition` parameter will be applied to all further transformations and actions on this RDD.

Partitions get redistributed among nodes whenever `shuffle` occurs. Repartitioning may cause `shuffle` to occur in some situations,  but it is not guaranteed to occur in all cases. And it usually happens during action stage.
