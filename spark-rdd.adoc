== RDD - Resilient Distributed Dataset

TIP: The original paper that gave birth to the concept of RDD - https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing].

RDD = *Resilient Distributed Dataset*

* *Resilient* means fault-tolerant (since RDDs are *distributed* and nodes will fail and often).
* *Distributed* means using many machines (aka nodes).
* *Dataset* is a collection of data (since RDDs are *partitioned* across distributed nodes).

TIP: RDD is the only abstraction in Spark. Learn it in depth and you understand Spark.

Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits:

* *immutable*, i.e. it does not change once created.
* *lazy evaluated*, i.e. the data inside is not going to be transformed until action execution.
* *cacheable*, i.e. you can hold all the data in a persistent "storage" like memory (the most preferred) or disk (the least preferred due to access speed).

RDDs are distributed by definition and to achieve even *data distribution* as well as leverage *data locality* (in distributed systems like HDFS or Cassandra), it is *partitioned* to a fixed number of *partitions* - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of a number of records.

RDDs are a container of instructions on how to materialize big (arrays of) data, and how to split it into partitions so executors can hold some of them.

RDD data is a collection of partitions that are in turn references to physical chunks of data. There's a one-to-one mapping between data in HDFS and partitions (see link:spark-data-locality.adoc[data locality]).

In general, data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory.

Increasing partitions count will make each partition to have less data.

Partitions are the units of parallelism.

Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially.

Saving partitions results in part-files instead of one single file.

=== Transformations and actions

* *Actions* launch *a job* to return a value to the user program
** See `sc.runJob`
* _narrow_ transformations - the result of `map`, `filter` and such is from the data from a single partition. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.
* _wide_ transformations - the result of `groupByKey` and `reduceByKey`. the data required to compute the records in a single partition may reside in many partitions of the parent RDD. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions.

Explore:

* `coalesce` method

=== Creating RDDs

One way to create a RDD is with `SparkContext.parallelize` method. It accepts a collection of elements as shown below (`sc` is a SparkContext instance):

```
scala> val rdd = sc.parallelize(1 to 1000)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25
```

FIXME: How is this different from `makeRDD`?

```
scala> sc.makeRDD(0 to 1000)
res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25
```

Given the reason to use Spark to process more data than your own laptop could handle, `SparkContext.parallelize` is mainly used to learn Spark in the Spark shell. `SparkContext.parallelize` requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop.

The other option to create RDDs is to use `sc.textFile` (see http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[SparkContext] API) that automatically distributes the data across a Spark cluster.

=== RDDs in Web UI

It's quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for `spark-shell`.

Execute the following Spark application:

[source,scala]
----
val ints = sc.parallelize(1 to 100) // <1>
ints.setName("Hundred ints")        // <2>
ints.cache                          // <3>
ints.count                          // <4>
----
<1> Creates an RDD with hundreds of numbers (with as many partitions as possible)
<2> Sets the name of the RDD
<3> Caches the RDD (so it shows up in Storage in UI)
<4> Executes action (and materializes the RDD)

With the above executed, you should see the following in the Web UI:

.RDD with custom name
image::images/spark-ui-rdd-name.png[]

Click the name of the RDD (under *RDD Name*) and you will get the details of how the RDD is cached.

.RDD Storage Info
image::images/spark-ui-storage-hundred-ints.png[]

Execute the following Spark job and you will see how the number of partitions decreases.

```
ints.repartition(2).count
```

.Number of tasks after repartition
image::images/spark-ui-repartition-2.png[]

=== Different sorts of RDDs

* `reduceByKey`
