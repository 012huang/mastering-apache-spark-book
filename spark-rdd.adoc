== RDD - Resilient Distributed Datasets

The original paper that gave birth to the concept of RDD - https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing]

* Transformations and actions
* checkpointing (how it works, what is checkpointed, and at what frequency)

* *Actions* launch *a job* to return a value to the user program
** See `sc.runJob`
* an RDD comprises a fixed number of *partitions*
* each partition comprises of a number of records.
* _narrow_ transformations - the result of `map`, `filter` and such is from the data from a single partition. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.
* _wide_ transformations - the result of `groupByKey` and `reduceByKey`. the data required to compute the records in a single partition may reside in many partitions of the parent RDD. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions.

Spark does jobs in parallel, and RDDs and dataframes are split into parts, processed and written in parallel, and saving the parts results in part-files instead of single csv.

Interesting methods:

* `coalesce`

=== Creating RDDs

One way to create a RDD is with `SparkContext.parallelize` method. It accepts a collection of elements as shown below (`sc` is a SparkContext instance):

```
scala> val rdd = sc.parallelize(1 to 1000)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25
```

FIXME: How is this different from `makeRDD`?

```
scala> sc.makeRDD(0 to 1000)
res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25
```

Given the reason to use Spark to process more data than your own laptop could handle, `SparkContext.parallelize` is mainly used to learn Spark in the Spark shell. `SparkContext.parallelize` requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop.

The other option to create RDDs is to use `sc.textFile` (see http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[SparkContext] API) that automatically distributes the data across a Spark cluster.

=== Different sorts of RDDs

* `reduceByKey`

=== Partitions

Concepts:

* size
* number
* partitioning scheme
* node distribution
* repartitioning

Depending on how you look at Spark (programmer, devop, admin), an RDD is about the content (developer's and data scientist's perspective) or how it gets spread out over a cluster (performance), i.e. how many partitions an RDD represents.

[TIP]
Read https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#how-many-partitions-does-an-rdd-have[How Many Partitions Does An RDD Have?]

RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application.

As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]:

> When a stage executes, you can see the number of partitions for a given stage in the Spark UI.

Start `spark-shell` and see it yourself!

```
$ ./bin/spark-shell
Spark context available as sc.

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc.parallelize(1 to 100).count
res0: Long = 100
```

When you execute the above very simple Spark application `sc.parallelize(1 to 100).count` you should see the following in http://localhost:4040/jobs[Spark shell application UI].

.The number of partition as Total tasks in UI
image::images/spark-partitions-ui-stages.png[]

The reason for `8` Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of _all_ available cores.

```
$ sysctl -n hw.ncpu
8
```

You can control the number of partitions, using the second input parameter to many transformations.

```
scala> sc.parallelize(1 to 100, 2).count
res1: Long = 100
```

.Total tasks in UI shows 2 partitions
image::images/spark-partitions-ui-stages-2-partitions.png[]

In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks,  which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Also, the number of partitions determines how many files get generated by actions that save RDDs to files.

The maximum size of any one partition is ultimately limited by the available memory of any single executor.

In the first RDD transformation, e.g. reading from a file using `sc.textFile(path, partition)`, the `partition` parameter will be applied to all further transformations and actions on this RDD.

Partitions get redistributed among nodes whenever `shuffle` occurs. Repartitioning may cause `shuffle` to occur in some situations,  but it is not guaranteed to occur in all cases. And it usually happens during action stage.
