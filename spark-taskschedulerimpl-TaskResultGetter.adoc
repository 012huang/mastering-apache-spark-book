== [[TaskResultGetter]] TaskResultGetter

`TaskResultGetter` is a helper class used exclusively by link:spark-taskschedulerimpl.adoc#statusUpdate[TaskSchedulerImpl] for _asynchronous_ <<enqueueSuccessfulTask, fetching the task results of tasks that have finished successfully>> or <<enqueueFailedTask, fetching the reasons of failures for failed tasks>>. It then sends the "results" back to `TaskSchedulerImpl`.

CAUTION: FIXME Image with the dependencies

TIP: Consult link:spark-taskscheduler-tasks.adoc#states[Task States] in Tasks to learn about the different task states.

NOTE: The only instance of `TaskResultGetter` is created while link:spark-taskschedulerimpl.adoc#creating-instance[`TaskSchedulerImpl` is created].

`TaskResultGetter` requires a link:spark-sparkenv.adoc[SparkEnv] and link:spark-taskschedulerimpl.adoc[TaskSchedulerImpl] to be created and is stopped when link:spark-taskschedulerimpl.adoc#stop[`TaskSchedulerImpl` stops].

`TaskResultGetter` uses <<task-result-getter, `task-result-getter` asynchronous task executor>> for operation.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.scheduler.TaskResultGetter` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.TaskResultGetter=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[getTaskResultExecutor]][[task-result-getter]] `task-result-getter` Asynchronous Task Executor

[source, scala]
----
getTaskResultExecutor: ExecutorService
----

`getTaskResultExecutor` creates a daemon thread pool with <<spark_resultGetter_threads, spark.resultGetter.threads>> threads and `task-result-getter` prefix.

TIP: Read up on https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor] that `getTaskResultExecutor` uses under the covers.

=== [[stop]] `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` stops the internal <<task-result-getter, `task-result-getter` asynchronous task executor>>

=== [[serializer]] `serializer` Attribute

[source, scala]
----
serializer: ThreadLocal[SerializerInstance]
----

`serializer` is a thread-local `SerializerInstance` that `TaskResultGetter` uses to deserialize byte buffers (with ``TaskResult``s or a `TaskEndReason`).

When created for a new thread, `serializer` is initialized with a new instance of `Serializer` (using link:spark-sparkenv.adoc#closureSerializer[SparkEnv.closureSerializer]).

NOTE: `TaskResultGetter` uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal] for the thread-local `SerializerInstance` variable.

=== [[enqueueSuccessfulTask]] `enqueueSuccessfulTask` Method

[source, scala]
----
enqueueSuccessfulTask(
  taskSetManager: TaskSetManager,
  tid: Long,
  serializedData: ByteBuffer): Unit
----

`enqueueSuccessfulTask` submits a task to <<getTaskResultExecutor, `task-result-getter` asynchronous task executor>>.

`enqueueSuccessfulTask` first deserializes the input `serializedData` to a `TaskResult` (using the internal thread-local <<serializer, serializer>>).

Given the type of `TaskResult`, i.e. `DirectTaskResult` or `IndirectTaskResult`, `enqueueSuccessfulTask` returns a tuple of the result and the size.

`enqueueSuccessfulTask` updates `resultSize` internal accumulator to the size of the result.

With no exceptions thrown, `enqueueSuccessfulTask` link:spark-taskschedulerimpl.adoc#handleSuccessfulTask[informs the TaskSchedulerImpl that the `tid` task was completed and the task result was received].

If the result is `DirectTaskResult`, the method checks `taskSetManager.canFetchMoreResults(serializedData.limit())` and possibly quits. If not, it deserializes the result (using `SparkEnv.serializer`).

CAUTION: FIXME Review `taskSetManager.canFetchMoreResults(serializedData.limit())`.

If the result is `IndirectTaskResult`, the method checks `taskSetManager.canFetchMoreResults(size)` and possibly removes the block id (using `SparkEnv.blockManager.master.removeBlock(blockId)`) and quits. If not, you should see the following DEBUG message in the logs:

```
DEBUG Fetching indirect task result for TID [tid]
```

`scheduler.handleTaskGettingResult(taskSetManager, tid)` gets called. And `sparkEnv.blockManager.getRemoteBytes(blockId)`.

Failure in getting task result from BlockManager results in calling link:spark-taskschedulerimpl.adoc#handleFailedTask[TaskSchedulerImpl.handleFailedTask] and quitting.

The task result is deserialized to `DirectTaskResult` (using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]) and `sparkEnv.blockManager.master.removeBlock(blockId)` is called afterwards.

`TaskSchedulerImpl.handleSuccessfulTask(taskSetManager, tid, result)` is called.

CAUTION: FIXME What is `TaskSchedulerImpl.handleSuccessfulTask` doing?

Any `ClassNotFoundException` or non fatal exceptions lead to link:spark-tasksetmanager.adoc#aborting-taskset[TaskSetManager.abort].

=== [[enqueueFailedTask]] `enqueueFailedTask` Method

`enqueueFailedTask(taskSetManager: TaskSetManager, tid: Long, taskState: TaskState, serializedData: ByteBuffer)` checks whether `serializedData` contains any data and if it does it deserializes it to a `TaskEndReason` (using the global link:spark-sparkenv.adoc#closureSerializer[closure Serializer]).

Either `UnknownReason` or the deserialized instance is passed on to link:spark-taskschedulerimpl.adoc#handleFailedTask[TaskSchedulerImpl.handleFailedTask] as the reason of the failure.

Any `ClassNotFoundException` leads to printing out the ERROR message to the logs:

```
ERROR Could not deserialize TaskEndReason: ClassNotFound with classloader [loader]
```

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|======================
| Spark Property | Default Value | Description
| [[spark_resultGetter_threads]] `spark.resultGetter.threads` | `4` | The number of threads for `TaskResultGetter`.
|======================
