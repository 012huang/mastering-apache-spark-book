== [[ReorderJoin]] ReorderJoin Logical Plan Optimization

`ReorderJoin` is a logical plan optimization in link:spark-sql-Optimizer.adoc#Operator-Optimizations[Operator Optimizations] batch of rules in link:spark-sql-Optimizer.adoc#ReorderJoin[Optimizer] that <<apply, transforms `Filter` (with CROSS and INNER joins) and `Join` logical plans>> with 3 or more joins and non-empty join conditions.

NOTE: Operator Optimizations is a link:spark-sql-Optimizer.adoc#fixedPoint[fixed-point batch of rules].

[TIP]
====
Import `ReorderJoin` and apply the rule directly on your structured queries to learn how the rule works.

[source, scala]
----
import org.apache.spark.sql.catalyst.optimizer.ReorderJoin
val rj = ReorderJoin(spark.sessionState.conf)

// Build analyzed logical plan with at least 3 joins and zero or more filters
scala> val one = spark.range(4)
one: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> val two = spark.range(4)
two: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> val three = spark.range(4)
three: org.apache.spark.sql.Dataset[Long] = [id: bigint]

val query = one.join(two)
  .where(one("id") === two("id"))
  .join(three)
  .where(three("id") === one("id"))
  .filter(one("id") % 2 === 0)

scala> val plan = query.queryExecution.analyzed
plan: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Filter ((id#63L % cast(2 as bigint)) = cast(0 as bigint))
+- Filter (id#69L = id#63L)
   +- Join Inner
      :- Filter (id#63L = id#66L)
      :  +- Join Inner
      :     :- Range (0, 4, step=1, splits=Some(8))
      :     +- Range (0, 4, step=1, splits=Some(8))
      +- Range (0, 4, step=1, splits=Some(8))

// Apply ReorderJoin rule
// Note...FIXME
scala> rj.apply(plan)
res9: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Filter ((id#63L % cast(2 as bigint)) = cast(0 as bigint))
+- Join Inner, (id#69L = id#63L)
   :- Join Inner, (id#63L = id#66L)
   :  :- Range (0, 4, step=1, splits=Some(8))
   :  +- Range (0, 4, step=1, splits=Some(8))
   +- Range (0, 4, step=1, splits=Some(8))
----
====

=== [[apply]] Transforming Logical Plan -- `apply` Method

`apply` transforms `Filter` (with CROSS and INNER join types) and link:spark-sql-LogicalPlan-Join.adoc[Join] logical plans.

NOTE: `apply` uses `ExtractFiltersAndInnerJoins` Scala extractor object (using <<ExtractFiltersAndInnerJoins-unapply, unapply>> method) to "destructure" a logical plan to its logical operators.

=== [[createOrderedJoin]] `createOrderedJoin` Recursive Method

CAUTION: FIXME

=== [[ExtractFiltersAndInnerJoins-unapply]] Extracting Filter and Join Operators from Logical Plan -- `unapply` Method (of ExtractFiltersAndInnerJoins)

[source, scala]
----
unapply(plan: LogicalPlan): Option[(Seq[(LogicalPlan, InnerLike)], Seq[Expression])]
----

`unapply` takes `Filter` (with CROSS and INNER joins) and any `Join` logical operators out of the input logical `plan` and <<ExtractFiltersAndInnerJoins-flattenJoin, flattens the joins>>.

=== [[ExtractFiltersAndInnerJoins-flattenJoin]] Flattening Join -- `flattenJoin` Method (of ExtractFiltersAndInnerJoins)

[source, scala]
----
flattenJoin(plan: LogicalPlan, parentJoinType: InnerLike = Inner)
  : (Seq[(LogicalPlan, InnerLike)], Seq[Expression])
----

`flattenJoin` takes CROSS and INNER join types...FIXME
