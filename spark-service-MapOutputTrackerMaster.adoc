== [[MapOutputTrackerMaster]] MapOutputTrackerMaster -- MapOutputTracker For Driver

*MapOutputTrackerMaster* is the link:spark-service-mapoutputtracker.adoc[MapOutputTracker] for the driver.

A `MapOutputTrackerMaster` is the source of truth for the collection of link:spark-MapStatus.adoc[MapStatus] objects (map output locations) per shuffle id (as recorded from link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTask]s).

`MapOutputTrackerMaster` uses Java's thread-safe https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] for link:spark-service-mapoutputtracker.adoc#mapStatuses[`mapStatuses` internal cache].

NOTE: There is currently a hardcoded limit of map and reduce tasks above which Spark does not assign preferred locations aka locality preferences based on map output sizes -- `1000` for map and reduce each.

It uses `MetadataCleaner` with `MetadataCleanerType.MAP_OUTPUT_TRACKER` as `cleanerType` and <<cleanup, cleanup>> function to drop entries in `mapStatuses`.

You should see the following INFO message when the MapOutputTrackerMaster is created (FIXME it uses `MapOutputTrackerMasterEndpoint`):

```
INFO SparkEnv: Registering MapOutputTracker
```

[[internal-registries]]
.`MapOutputTrackerMaster` Internal Registries and Counters
[frame="topbot",cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[cachedSerializedStatuses]] `cachedSerializedStatuses`
| Internal cache with...FIXME

Used when...FIXME

| [[shuffleIdLocks]] `shuffleIdLocks`
| Internal registry with...FIXME

Used when...FIXME
|===

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.MapOutputTrackerMaster` logger to see what happens in `MapOutputTrackerMaster`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.MapOutputTrackerMaster=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[getSerializedMapOutputStatuses]] `getSerializedMapOutputStatuses` Method

CAUTION: FIXME

=== [[unregisterMapOutput]] `unregisterMapOutput` Method

CAUTION: FIXME

=== [[cleanup]] cleanup Function for MetadataCleaner

`cleanup(cleanupTime: Long)` method removes old entries in `mapStatuses` and `cachedSerializedStatuses` that have timestamp earlier than `cleanupTime`.

It uses `org.apache.spark.util.TimeStampedHashMap.clearOldValues` method.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.util.TimeStampedHashMap` logger to see what happens in TimeStampedHashMap.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.TimeStampedHashMap=DEBUG
```
====

You should see the following DEBUG message in the logs for entries being removed:

```
DEBUG Removing key [entry.getKey]
```

=== [[getPreferredLocationsForShuffle]] Finding Preferred BlockManagers with Most Map Outputs (for `ShuffleDependency` and `Partition`) -- `getPreferredLocationsForShuffle` Method

[source, scala]
----
getPreferredLocationsForShuffle(dep: ShuffleDependency[_, _, _], partitionId: Int): Seq[String]
----

`getPreferredLocationsForShuffle` finds the locations (i.e. link:spark-blockmanager.adoc[BlockManagers]) with the most map outputs for the input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and link:spark-rdd-Partition.adoc[Partition].

NOTE: `getPreferredLocationsForShuffle` is simply <<getLocationsWithLargestOutputs, getLocationsWithLargestOutputs>> with a guard condition.

NOTE: A *map output* are shuffle blocks across BlockManagers.

Internally, `getPreferredLocationsForShuffle` checks whether <<spark_shuffle_reduceLocality_enabled, `spark.shuffle.reduceLocality.enabled` Spark property>> is enabled (it is by default) with the number of partitions of the link:spark-rdd-ShuffleDependency.adoc#rdd[RDD of the input `ShuffleDependency`] and partitions in the link:spark-rdd-ShuffleDependency.adoc#partitioner[partitioner of the input `ShuffleDependency`] both being less than `1000`.

NOTE: The thresholds for the number of partitions in the RDD and of the partitioner when computing the preferred locations are `1000` and are not configurable.

If the condition holds, `getPreferredLocationsForShuffle` <<getLocationsWithLargestOutputs, finds locations with the largest number of shuffle map outputs>> for the input `ShuffleDependency` and `partitionId` (with the number of partitions in the partitioner of the input `ShuffleDependency` and `0.2`) and returns the hosts of the preferred `BlockManagers`.

NOTE: `0.2` is the fraction of total map output that must be at a location to be considered as a preferred location for a reduce task. It is not configurable.

NOTE: `getPreferredLocationsForShuffle` is used when link:spark-rdd-ShuffledRDD.adoc#getPreferredLocations[ShuffledRDD] and link:spark-sql-ShuffledRowRDD.adoc#getPreferredLocations[ShuffledRowRDD] ask for *preferred locations for a partition*.

=== [[incrementEpoch]] Incrementing Epoch -- `incrementEpoch` Method

[source, scala]
----
incrementEpoch(): Unit
----

`incrementEpoch` increments the internal link:spark-service-mapoutputtracker.adoc#epoch[epoch].

You should see the following DEBUG message in the logs:

```
DEBUG MapOutputTrackerMaster: Increasing epoch to [epoch]
```

NOTE: `incrementEpoch` is used when `MapOutputTrackerMaster` <<registerMapOutputs, registers map outputs>> (with `changeEpoch` flag enabled -- it is disabled by default) and <<unregisterMapOutput, unregisters map outputs>> (for a shuffle, mapper and block manager), and when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[`DAGScheduler` is notified that an executor got lost] (with `filesLost` flag enabled).

=== [[getLocationsWithLargestOutputs]] Finding Locations with Largest Number of Shuffle Map Outputs -- `getLocationsWithLargestOutputs` Method

[source, scala]
----
getLocationsWithLargestOutputs(
  shuffleId: Int,
  reducerId: Int,
  numReducers: Int,
  fractionThreshold: Double): Option[Array[BlockManagerId]]
----

`getLocationsWithLargestOutputs` returns link:spark-blockmanager.adoc#BlockManagerId[BlockManagerId]s with the largest size (of all the shuffle blocks they manage) above the input `fractionThreshold` (given the total size of all the shuffle blocks for the shuffle across all link:spark-blockmanager.adoc[BlockManagers]).

NOTE: `getLocationsWithLargestOutputs` may return no `BlockManagerId` if their shuffle blocks do not total up above the input `fractionThreshold`.

NOTE: The input `numReducers` is not used.

Internally, `getLocationsWithLargestOutputs` queries the <<mapStatuses, mapStatuses>> internal cache for the input `shuffleId`.

[NOTE]
====
One entry in `mapStatuses` internal cache is a link:spark-MapStatus.adoc[MapStatus] array indexed by partition id.

`MapStatus` includes link:spark-MapStatus.adoc#contract[information about the `BlockManager` (as `BlockManagerId`) and estimated size of the reduce blocks].
====

`getLocationsWithLargestOutputs` iterates over the `MapStatus` array and builds an interim mapping between link:spark-blockmanager.adoc#BlockManagerId[BlockManagerId] and the cumulative sum of shuffle blocks across link:spark-blockmanager.adoc[BlockManagers].

NOTE: `getLocationsWithLargestOutputs` is used exclusively when <<getPreferredLocationsForShuffle, `MapOutputTrackerMaster` finds the preferred locations (BlockManagers and hence executors) for a shuffle>>.

=== [[containsShuffle]] Checking If Shuffle Map Output Is Tracked Already -- `containsShuffle` Method

[source, scala]
----
containsShuffle(shuffleId: Int): Boolean
----

`containsShuffle` checks if the input `shuffleId` is registered in the <<cachedSerializedStatuses, cachedSerializedStatuses>> or link:spark-service-mapoutputtracker.adoc#mapStatuses[mapStatuses] internal caches.

NOTE: `containsShuffle` is used exclusively when link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a `ShuffleMapStage`] (for `ShuffleDependency` and `ActiveJob`).

=== [[registerShuffle]] `registerShuffle` Method

[source, scala]
----
registerShuffle(shuffleId: Int, numMaps: Int): Unit
----

`registerShuffle` registers the input `shuffleId` in the link:spark-service-mapoutputtracker.adoc#mapStatuses[mapStatuses] internal cache.

NOTE: The number of link:spark-MapStatus.adoc[MapStatus] entries in the new array in `mapStatuses` internal cache is exactly the input `numMaps`.

`registerShuffle` adds a lock in the <<shuffleIdLocks, `shuffleIdLocks` internal registry>> (without using it).

If the `shuffleId` has already been registered, `registerShuffle` throws a `IllegalArgumentException` with the following message:

```
Shuffle ID [id] registered twice
```

NOTE: `registerShuffle` is used exclusively when link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a `ShuffleMapStage`] (for `ShuffleDependency` and `ActiveJob`).

=== [[registerMapOutputs]] Registering Map Outputs for Shuffle (Possibly with Epoch Change) -- `registerMapOutputs` Method

[source, scala]
----
registerMapOutputs(
  shuffleId: Int,
  statuses: Array[MapStatus],
  changeEpoch: Boolean = false): Unit
----

`registerMapOutputs` registers the input `statuses` (as the shuffle map output) with the input `shuffleId` in the link:spark-service-mapoutputtracker.adoc#mapStatuses[`mapStatuses` internal cache].

`registerMapOutputs` <<incrementEpoch, increments epoch>> if the input `changeEpoch` is enabled (it is not by default).

[NOTE]
====
`registerMapOutputs` is used when `DAGScheduler` handles link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ShuffleMapTask[successful `ShuffleMapTask` completion] and link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[executor lost events].

In both cases, the input `changeEpoch` is enabled.
====

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|======================
| Spark Property
| Default Value
| Description

| [[spark_shuffle_reduceLocality_enabled]] `spark.shuffle.reduceLocality.enabled`
| `true`
| Controls whether to compute locality preferences for reduce tasks.

When enabled (i.e. `true`), `MapOutputTrackerMaster` computes the preferred hosts on which to run a given map output partition in a given shuffle, i.e. the nodes that the most outputs for that partition are on.
|======================
