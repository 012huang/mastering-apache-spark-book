== [[MapOutputTrackerMaster]] MapOutputTrackerMaster

*MapOutputTrackerMaster* is the link:spark-service-mapoutputtracker.adoc[MapOutputTracker] for the driver.

A `MapOutputTrackerMaster` is the source of truth for the collection of link:spark-MapStatus.adoc[MapStatus] objects (map output locations) per shuffle id (as recorded from link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTask]s).

`MapOutputTrackerMaster` uses Spark's `org.apache.spark.util.TimeStampedHashMap` for `mapStatuses`.

NOTE: There is currently a hardcoded limit of map and reduce tasks above which Spark does not assign preferred locations aka locality preferences based on map output sizes -- `1000` for map and reduce each.

It uses `MetadataCleaner` with `MetadataCleanerType.MAP_OUTPUT_TRACKER` as `cleanerType` and <<cleanup, cleanup>> function to drop entries in `mapStatuses`.

You should see the following INFO message when the MapOutputTrackerMaster is created (FIXME it uses `MapOutputTrackerMasterEndpoint`):

```
INFO SparkEnv: Registering MapOutputTracker
```

=== [[getLocationsWithLargestOutputs]] Finding Locations with Largest Number of Shuffle Map Outputs -- `getLocationsWithLargestOutputs` Method

CAUTION: FIXME

=== [[unregisterShuffle]] `unregisterShuffle` Method

CAUTION: FIXME

=== [[getSerializedMapOutputStatuses]] `getSerializedMapOutputStatuses` Method

CAUTION: FIXME

=== [[containsShuffle]] `containsShuffle` Method

CAUTION: FIXME

=== [[registerShuffle]] `registerShuffle` Method

CAUTION: FIXME

=== [[unregisterMapOutput]] `unregisterMapOutput` Method

CAUTION: FIXME

=== [[registerMapOutputs]] `registerMapOutputs` Method

[source, scala]
----
registerMapOutputs(
  shuffleId: Int,
  statuses: Array[MapStatus],
  changeEpoch: Boolean = false): Unit
----

CAUTION: FIXME

=== [[incrementEpoch]] `incrementEpoch` Method

CAUTION: FIXME

=== [[cleanup]] cleanup Function for MetadataCleaner

`cleanup(cleanupTime: Long)` method removes old entries in `mapStatuses` and `cachedSerializedStatuses` that have timestamp earlier than `cleanupTime`.

It uses `org.apache.spark.util.TimeStampedHashMap.clearOldValues` method.


[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.util.TimeStampedHashMap` logger to see what happens in TimeStampedHashMap.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.TimeStampedHashMap=DEBUG
```
====

You should see the following DEBUG message in the logs for entries being removed:

```
DEBUG Removing key [entry.getKey]
```

=== [[getEpoch]] `getEpoch` Method

CAUTION: FIXME

=== [[getPreferredLocationsForShuffle]] Finding BlockManagers with Most Map Outputs (for `ShuffleDependency` and `Partition`) -- `getPreferredLocationsForShuffle` Method

[source, scala]
----
getPreferredLocationsForShuffle(dep: ShuffleDependency[_, _, _], partitionId: Int): Seq[String]
----

`getPreferredLocationsForShuffle` finds the locations (i.e. link:spark-blockmanager.adoc[BlockManagers]) with the most map outputs for the input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and link:spark-rdd-Partition.adoc[Partition].

NOTE: `getPreferredLocationsForShuffle` is simply <<getLocationsWithLargestOutputs, getLocationsWithLargestOutputs>> with a guard condition.

Internally, `getPreferredLocationsForShuffle` checks whether <<spark_shuffle_reduceLocality_enabled, `spark.shuffle.reduceLocality.enabled` Spark property>> is enabled (it is by default) with the number of partitions of the link:spark-rdd-ShuffleDependency.adoc#rdd[RDD of the input `ShuffleDependency`] and partitions in the link:spark-rdd-ShuffleDependency.adoc#partitioner[partitioner of the input `ShuffleDependency`] both being less than `1000`.

NOTE: The thresholds for the number of partitions in the RDD and of the partitioner when computing the preferred locations are `1000` and are not configurable.

If the condition holds, `getPreferredLocationsForShuffle` <<getLocationsWithLargestOutputs, finds locations with the largest number of shuffle map outputs>> for the input `ShuffleDependency` and `partitionId` (with the number of partitions in the partitioner of the input `ShuffleDependency` and `0.2`) and returns the hosts of the preferred `BlockManagers`.

NOTE: `0.2` is the fraction of total map output that must be at a location to be considered as a preferred location for a reduce task. It is not configurable.

NOTE: `getPreferredLocationsForShuffle` is used when link:spark-rdd-ShuffledRDD.adoc#getPreferredLocations[ShuffledRDD] and link:spark-sql-ShuffledRowRDD.adoc#getPreferredLocations[ShuffledRowRDD] find preferred locations for a partition.

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|======================
| Spark Property
| Default Value
| Description

| [[spark_shuffle_reduceLocality_enabled]] `spark.shuffle.reduceLocality.enabled`
| `true`
| Controls whether to compute locality preferences for reduce tasks.

When enabled (i.e. `true`), `MapOutputTrackerMaster` computes the preferred hosts on which to run a given map output partition in a given shuffle, i.e. the nodes that the most outputs for that partition are on.
|======================
